symbolic_computation	by employing the generalized ( g'/ g)- expansion method and symbolic computation, we obtain new exact solutions of the ( 3 + 1) dimensional generalized b-type kadomtsev- petviashvili equation, which include the traveling wave exact solutions and the non- traveling wave exact solutions showed by the hyperbolic function and the trigonometric function. meanwhile, some interesting physics structure are discussed.
symbolic_computation	the korteweg-de vries equation (kdv) and the (2+ 1)-dimensional nizhnik-novikov-veselov system (nnv) are presented. multi-soliton rational solutions of these equations are obtained via the generalized unified method. the analysis emphasizes the power of this method and its capability of handling completely (or partially) integrable equations. compared with hirota 's method and the inverse scattering method, the proposed method gives more general exact multi-wave solutions without much additional effort. the results show that, by virtue of symbolic computation, the generalized unified method may provide us with a straightforward and effective mathematical tool for seeking multi-soliton rational solutions for solving many nonlinear evolution equations arising in different branches of sciences.
symbolic_computation	in this paper, the (g'/g)-expansion method is suggested to establish new exact solutions for fractional differential-difference equations in the sense of modified riemann-liouville derivative. the fractional complex transform is proposed to convert a fractional partial differential difference equation into its differential difference equation of integer order. with the aid of symbolic computation, we choose nonlinear lattice equations to illustrate the validity and advantages of the algorithm. it is shown that the proposed algorithm is effective and can be used for many other nonlinear lattice equations in mathematical physics and applied mathematics.
symbolic_computation	we present an accurate method for the numerical integration of polynomials over arbitrary polyhedra. using the divergence theorem, the method transforms the domain integral into integrals evaluated over the facets of the polyhedra. the necessity of performing symbolic computation during such transformation is eliminated by using one dimensional gauss quadrature rule. the facet integrals are computed with the help of quadratures available for triangles and quadrilaterals. numerical examples, in which the proposed method is used to integrate the weak form of the navier-stokes equations in an embedded interface method (eim), are presented. the results show that our method is as accurate and generalized as the most widely used volume decomposition based methods. moreover, since the method involves neither volume decomposition nor symbolic computations, it is much easier for computer implementation. also, the present method is more efficient than other available integration methods based on the divergence theorem. efficiency of the method is also compared with the volume decomposition based methods and moment fitting methods. to our knowledge, this is the first article that compares both accuracy and computational efficiency of methods relying on volume decomposition and those based on the divergence theorem. (c) 2014 elsevier inc. all rights reserved.
symbolic_computation	the coupled equations for the incoherent optical spatial solitons in a nonlocal nonlinear medium is studied analytically. with the soliton solutions hereby obtained via the symbolic computation, the optical-soliton motion in the nonlocal nonlinearmedium is studied: |psi| is inversely related to kappa, omega, and n(0), while delta n is positively related to omega and n(0), but delta(n) is independent of kappa, with psi as the slowly varying amplitude of the beam, delta n as the refractive index change, kappa as the beam intensity distribution, omega as the frequency of the propagating beam, and n(0) as the unperturbed refractive index. head-on and overtaking interactions are observed, and head-on interaction is transformed into an overtaking one with. increasing. bound-state interaction is displayed, and with n0 increasing, the period of. decreases, while that of delta n increases. considering the external forces in the nonlocal nonlinear medium , we explore the chaotic motions in the nonlinear nonlocal medium, including effects of the external forces on such motions. it is seen that when vertical bar alpha(1)vertical bar >>vertical bar 2 omega(2)n(0)/c vertical bar and vertical bar alpha(2)vertical bar >>vertical bar kappa vertical bar, the two-dimensional attractors with stretching-and-folding structures are exhibited, and the developed chaos occurs, where alpha(1) and alpha(2) are the amplitudes of external forces, c is the speed of light in vacuum. such chaotic motions are weakened with omega, kappa,(omega) over bar (1) and (omega) over bar (2) increasing, or with n(0) decreasing, where (omega) over bar (1) and (omega) over bar (2) represent the frequencies of external forces.
computer_vision	the qualities of color images captured by digital imaging devices are vulnerable to the scene illumination settings of a given environment. the colors of captured objects may not be accurately reproduced when the illumination settings are uncontrollable or not known a priori. this undesirable property can inevitably degrade the qualities of captured images and lead to difficulties in subsequent image-processing stages. considering that the task of controlling scene illumination is nontrivial, color correction has emerged as a plausible post-processing procedure to efficiently restore the scene chromatics of a given image. in this study, a new color correction technique called the saturation avoidance color correction (sacc) algorithm is proposed to remove the undesirable effect of scene illuminants. unlike most well-established color correction algorithms, the proposed sacc comprises a nonlinear pixel adjustment mechanism to avoid the saturation effect during the color manipulation process. a collection of color images including indoor, outdoor, and underwater images are used to verify the capability of sacc. extensive experimental studies reveal that the proposed algorithm is preferable to some existing techniques because the former has a high capability to mitigate the color saturation issue and is able to produce corrected images with more pleasant visualization.
computer_vision	the use of unmanned aerial vehicles (uavs) is growing significantly for many and varied purposes. during the mission, an outdoor uav is guided by following the planned path using gps signals. however, the gps capability may become defective or the environment may be gps-denied, and an additional safety aid is therefore required for the automatic landing phase that is independent of gps data. most uavs are equipped with machine vision systems which, together with onboard analysis, can be used for safe, automatic landing. this contributes greatly to the overall success of autonomous flight. this paper proposes an automatic expert system, based on image segmentation procedures, that assists safe landing through recognition and relative orientation of the uav and platform. the proposed expert system exploits the human experience that has been incorporated into the machine vision system, which is mapped into the proposed image processing modules. the result is an improved reliability capability that could be incorporated into any uav, and is especially robust for rotary wing uavs. this is clearly a desirable fail-safe capability. (c) 2017 elsevier ltd. all rights reserved.
computer_vision	modern competitive shooting is a strenuous test of the human perceptual and motor systems. like driving a race car or piloting a high performance aircraft, speed shooting matches require precise, rapid movements coordinated using a careful mixture of planning and reaction. in some disciplines of competitive shooting, this mixture is further complicated by complex moving targets. this paper develops a set of state space equations for a moving, reactive steel target used in professional 3-gun competition. the equations for target motion are nonlinear, time-varying, and chaotic in certain regions of the state space. once derived, the equations for target motion are validated against motion extracted from video. then, the calibrated equations are implemented in real-time on a portable, laser-activated simulator. applications of the simulation environment to marksmanship training and human motor control studies are also discussed.
computer_vision	hypergraph is a powerful representation for several computer vision, machine learning, and pattern recognition problems. in the last decade, many researchers have been keen to develop different hypergraph models. in contrast, no much attention has been paid to the design of hyperedge weighting schemes. however, many studies on pairwise graphs showed that the choice of edge weight can significantly influence the performances of such graph algorithms. we argue that this also applies to hypergraphs. hi this paper, we empirically study the influence of hyperedge weights on hypergraph learning via proposing three novel hyperedge weighting schemes from the perspectives of geometry, multivariate statistical analysis, and linear regression. extensive experiments on orl, coil20, jaffe, sheffield, scenel 5 and caltech256 datasets verified our hypothesis for both classification and clustering problems. for each of these classes of problems, our empirical study concludes with suggesting a suitable hypergraph weighting scheme. moreover, the experiments also demonstrate that the combinations of such weighting schemes and conventional hyper graph models can achieve competitive classification and clustering performances in comparison with some recent state-of-the-art algorithms. (c) 2016 elsevier b.v. all rights reserved.
computer_vision	image segmentation is a fundamental problem in computer vision, and the color and texture information are usually both employed to obtain more satisfactory segmentation results. however, the traditional color-texture segmentation methods usually assume that the color-based and texture-based segmentation results are globally consistent, which is not always the case. sometimes, the color-texture based segmentation results may be worse than some single feature (color or texture) based ones if the consistency constraints are taken into account inappropriately. to address the problem, a graph cuts based color-texture cosegmentation method is proposed in this paper, where just the similarity constrains are considered rather than the global consistencies, and a penalty term is included to adaptively balance the possible local inconsistencies. additionally, in order to extract the texture features effectively, a comprehensive texture descriptor is designed by integrating the nonlinear compact multi-scale structure tensor (ncmsst) and total variation flow (tv-flow). a large number of segmentation comparison experiments using the synthesis color-texture images and real natural scene images verify the superiorities of our proposed texture descriptor and color-texture cosegmentation method. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	perfect matchings and maximum weight matchings are two fundamental combinatorial structures. we consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching. motivated by the computer graphics application in triangle meshes, where we seek to convert a triangulation into a quadrangulation by merging pairs of adjacent triangles, we focus mainly on bridgeless cubic graphs. first, we characterize graphs that attain the extreme ratios. second, we present a lower bound for all bridgeless cubic graphs. third, we present upper bounds for subclasses of bridgeless cubic graphs, most of which are shown to be tight. additionally, we present tight bounds for the class of regular bipartite graphs. (c) 2014 elsevier b.v. all rights reserved.
computer_graphics	traffic jam is one of the hardest problems of the crowded cities, and it needs to be solved. in this study, the effect of the minimum speed limit signs in addition to the maximum speed signs and their locations in traffic flow has been examined by using cellular automata (ca). urban traffic is modeled by two dimensional ca. the model includes traffic signs, traffic lights and some kinds of vehicles (such as automobiles, vans, buses, metro buses) that are often encountered in traffic.
computer_graphics	immersive multi-user virtual environments give good support for closely-coupled collaboration between co-located users. more complex collaborative tasks may require individual user navigation to achieve loosely-coupled collaboration. we designed a navigation framework based on the human joystick metaphor with some alterations for cohabitation management. this model allows each user to navigate independently using a human joystick based control while avoiding physical obstacles and staying within the usable part of the system. we conducted a series of user studies to investigate the influence of each alteration by testing their combinations on various navigation tasks. the results show that modified transfer functions and adaptive neutral orientations improve users' cohabitation performance, while the impact of adaptive neutral positions need to be further studied.
computer_graphics	accurately and reliably tracking the undulatory motion of deformable fish body is of great significance for not only scientific researches but also practical applications such as robot design and computer graphics. however, it remains a challenging task due to severe body deformation, erratic motion and frequent occlusions. this paper proposes a tracking method which is capable of tracking the midlines of multiple fish based on midline evolution and head motion pattern modeling with long short-term memory (lstm) networks. the midline and head motion state are predicted using two lstm networks respectively and the predicted state is associated with detections to estimate the state of each target at each moment. experiment results show that the system can accurately track midline dynamics of multiple zebrafish even when mutual occlusions occur frequently.
computer_graphics	the paper presents the formalism of the parametric integral equation system (pies) for two-dimensional elastoplastic problems and the algorithm for its numerical solution. the efficiency of the proposed approach lies in the global modeling of a plastic zone, without classic discretization into elements, using surface patches popular in computer graphics. lagrange polynomials with various number and arrangement of interpolation nodes are used to approximate plastic strains. three test examples are solved and the obtained results are compared with analytical and numerical solutions. (c) 2016 elsevier ltd. all rights reserved.
operating_systems	accurate measurement of cardiomyocyte contraction is a critical issue for scientists working on cardiac physiology and physiopathology of diseases implying contraction impairment. cardiomyocytes contraction can be quantified by measuring sarcomere length, but few tools are available for this, and none is freely distributed. we developed a plug-in (sarcoptim) for the imagej/fiji image analysis platform developed by the national institutes of health. sarcoptim computes sarcomere length via fast fourier transform analysis of video frames captured or displayed in imagej and thus is not tied to a dedicated video camera. it can work in real time or offline, the latter overcoming rotating motion or displacement-related artifacts. sarcoptim includes a simulator and video generator of cardiomyocyte contraction. acquisition parameters, such as pixel size and camera frame rate, were tested with both experimental recordings of rat ventricular cardiomyocytes and synthetic videos. it is freely distributed, and its source code is available. it works under windows, mac, or linux operating systems. the camera speed is the limiting factor, since the algorithm can compute online sarcomere shortening at frame rates >10 khz. in conclusion, sarcoptim is a free and validated user-friendly tool for studying cardiomyocyte contraction in all species, including human.
operating_systems	the advent of native mobile technologies particularly advances in the mobile operating systems (os), popularly among android, windows mobile os, ios and their acceptance as an alternative to existing browser based access; has enabled the remote access of equipment and instruments over the world wide web (www). while more and more remote control solutions have been implemented in the industry via local area networks (lans), wide area networks (wans), and the web, online remote accessible laboratory (oral) has surpassed the simulation as a tool for learning scientific methods with the results and observations being real time rather than ideal and facilitate the conduct of experiments on specific instrumentation from any location. the aim of this paper is to introduce an easy accessible remote laboratory measuring system (rlms) to provide the students to perform laboratory experiments on actual measurement instrumentation over any smart mobile device, tablets or desktops and pcs. the proposed architecture on real laboratory setup provides solutions for remote access and opens ways for future developments. the system is realized using labview graphical programming environment which confers its advantages.
operating_systems	this work suggests a method for systematically constructing a software-level environment model for safety checking automotive operating systems by introducing a constraint specification language, osek_csl. osek_csl is designed to specify the usage constraints of automotive operating systems using a pre-defined set of constraint types identified from the international standard osek/vdx. each constraint specified in osek_csl is interpreted as either a regular language or a context-free language that can be checked by a finite automaton or a pushdown automaton. the set of usage constraints is used to systematically classify the universal usage model of osek-/vdx-based operating systems and to generate test sequences with varying degrees of constraint satisfaction using ltl model checking. with pre-defined constraint patterns and the full support of automation, test engineers can choose the degree of constraint satisfaction and generate test cases using combinatorial intersections of selected constraints that cover all corner cases classified by constraints. a series of experiments on an open-source automotive operating system show that our approach finds safety issues more effectively than conventional specification-based testing, scenario-based testing, and conformance testing.
operating_systems	purpose - in recent times, progression of technology and growing demands of customers have substantially influenced the services sector to introduce fast real-time mechanisms for providing up-to-mark services. to meet these requirements, organizations are going to change their end-user operating systems but success rate of change is very low. the purpose of this paper is to address one of the practitioners' complaint ""no one tells us how to do it"" and uncovers the indirect effects of knowledge management (km) strategies: personalization and codification, toward organizational change via organizational learning and change readiness. the current study also highlights how organizational learning and change readiness are helpful to reduce the detrimental effects of organizational change cynicism toward success of a change process. design/methodology/approach - temporal research design is used to get the appropriate responses from the targeted population in two stages such as pre-change (time-1) and post-change (time-2). in cumulative, 206 responses have been obtained from the banking sector of pakistan. findings - the results of the current study are very promising as it has been stated that km strategies have an indirect effect on successful organizational change through organizational learning and change readiness. moreover, change cynicism has a weakening effect on a change process and can be managed through effective learning orientation of employees and developing readiness for change in organizations. research limitations/implications - change agents have to use an optimal mix of personalization and codification strategies to develop learning environment and readiness for change in organizations that are beneficial for implementing a change successfully. moreover, change readiness and organizational learning in the context of change are equally beneficial to reduce organizational change cynicism as well. originality/value - this study is introducing a unique model to initiate a change with the help of km strategies, organizational learning and readiness for change.
operating_systems	this article presents an introductory microcontroller programming course on digital signal processing for undergraduate university level. the course is intended to provide insight into information technology and to prepare students for more complex exercises later on in their studies. solutions to overcome pedagogical obstacles like the fear of new technologies and to minimise technological incompatibilities between different operating systems while setting up a programming tool chain are presented. this leads to an increased scalability of the course, allowing hundreds of students to attend each year. in the case presented here, the average number of participants was 300. the problem-oriented task assignments are defined leading to a final creative improvement task, for which the students' solutions are analysed. the course is evaluated and an outlook on further improvements is given.
machine_learning	one of the backbones of the indian economy is agriculture, which is conditioned by the poor soil fertility. in this study we use chemical soil measurements to classify many relevant soil parameters: village-wise fertility indices of organic carbon (oc), phosphorus pentoxide (p2o5), manganese (mn) and iron (fe); soil ph and type; soil nutrients nitrous oxide (n2o), p2o5 and potassium oxide (k2o), in order to recommend suitable amounts of fertilizers; and preferable crop. to classify these soil parameters allows to save time of specialized technicians developing expensive chemical analysis. these ten classification problems are solved using a collection of twenty very diverse classifiers, selected by their high performances, of families bagging, boosting, decision trees, nearest neighbors, neural networks, random forests (rf), rule based and support vector machines (svm). the rf achieves the best performance for six of ten problems, overcoming 90% of the maximum performance in all the cases, followed by adaboost, svm and gaussian extreme learning machine. although for some problems (ph,n2o,p2o5 and k2o) the performance is moderate, some classifiers (e.g. for fertility indices of p2o5, mn and fe) trained in one region revealed valid for other indian regions. (c) 2017 elsevier b.v. all rights reserved.
machine_learning	we demonstrate a model order reduction technique for a system of nonlinear equations arising from the finite element method (fem) discretization of the three-dimensional quasistatic equilibrium equation equipped with a perzyna viscoplasticity constitutive model. the procedure employs the proper orthogonal decomposition-galerkin (pod-g) in conjunction with the discrete empirical interpolation method (deim). for this purpose, we collect samples from a standard full order fem analysis in the offline phase and cluster them using a novel k-means clustering algorithm. the pod and the deim algorithms are then employed to construct a corresponding reduced order model. in the online phase, a sample from the current state of the system is passed, at each time step, to a nearest neighbor classifier in which the cluster that best describes it is identified. the force vector and its derivative with respect to the displacement vector are approximated using deim, and the system of nonlinear equations is projected onto a lower dimensional subspace using the pod-g. the constructed reduced order model is applied to two typical solid mechanics problems showing strain-localization (a tensile bar and a wall under compression) and a three-dimensional squarefooting problem. (c) 2017 the authors. published by elsevier b.v. this is an open access article under the cc by-nc-nd license.
machine_learning	demand management in residential buildings is a key component toward sustainability and efficiency in urban environments. the recent advancements in sensor based technologies hold the promise of novel energy consumption models that can better characterize the underlying patterns. in this paper, we propose a probabilistic data-driven predictive model for consumption forecasting in residential buildings. the model is based on bayesian network (bn) framework which is able to discover dependency relations between contributing variables. thus, we can relax the assumptions that are often made in traditional forecasting models. moreover, we are able to efficiently capture the uncertainties in input variables and quantify their effect on the system output. we test our proposed approach to the data provided by pacific northwest national lab (pnnl) which has been collected through a pilot smart grid project. we examine the performance of our model in a multiscale setting by considering various temporal (i.e., 15 min, hourly intervals) and spatial (i.e., all households in a region, each household) resolutions for analyzing data. demand forecasting at the individual households' levels is a first step toward designing personalized and targeted policies for each customer. while this is a widely studied topic in digital marketing, few researches have been done in the energy sector. the results indicate that bayesian networks can be efficiently used for probabilistic energy modeling in residential buildings by discovering the dependencies between variables. (c) 2017 elsevier ltd. all rights reserved.
machine_learning	this work presents a novel approach for decisionmaking for multi-objective binary classification problems. the purpose of the decision process is to select within a set of pareto-optimal solutions, one model that minimizes the structural risk (generalization error). this new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of pareto-optimal solutions. results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches.
machine_learning	oscillometric measurement is widely used to estimate systolic blood pressure (sbp) and diastolic blood pressure (dbp). in this paper, we propose a deep belief network (dbn)-deep neural network (dnn) to learn about the complex nonlinear relationship between the artificial feature vectors obtained from the oscillometric wave and the reference nurse blood pressures using the dbn-dnn-based-regression model. our dbn-dnn is a powerful generative network for feature extraction and can address to stick in local minima through a special pretraining phase. therefore, this model provides an alternative way for replacing a popular shallow model. based on this, we apply the dbn-dnn-based regression model to estimate the sbp and dbp. however, there are a small amount of data samples, which is not enough to train the dbn-dnn without the overfitting problem. for this reason, we use the parametric bootstrap-based artificial features, which are used as training samples to efficiently learn the complex nonlinear functions between the feature vectors obtained and the reference nurse blood pressures. as far as we know, this is one of the first studies using the dbn-dnn-based regression model for bp estimation when a small training sample is available. our dbn-dnn-based regression model provides a lower standard deviation of error, mean error, and mean absolute error for the sbp and dbp as compared with the conventional methods.
data_structures	data clustering is an important step in data mining and machine learning. it is especially crucial to analyze the data structures for further procedures. recently a new clustering algorithm known as 'neutrosophic c-means' (ncm) was proposed in order to alleviate the limitations of the popular fuzzy c-means (fcm) clustering algorithm by introducing a new objective function which contains two types of rejection. the ambiguity rejection which concerned patterns lying near the cluster boundaries, and the distance rejection was dealing with patterns that are far away from the clusters. in this paper, we extend the idea of ncm for nonlinear-shaped data clustering by incorporating the kernel function into ncm. the new clustering algorithm is called kernel neutrosophic c-means (kncm), and has been evaluated through extensive experiments. nonlinear-shaped toy datasets, real datasets and images were used in the experiments for demonstrating the efficiency of the proposed method. a comparison between kernel fcm (kfcm) and kncm was also accomplished in order to visualize the performance of both methods. according to the obtained results, the proposed kncm produced better results than kfcm. (c) 2016 elsevier b.v. all rights reserved.
data_structures	maze puzzle usually has only one user. this research has adopted network technology to develop a maze game, called multi-maze, which allows multiple players competing across the network concurrently. it contains three sub-systems. the maze editor is capable of building 3, 4, or 6 degrees multi-component maze and supporting automatic, semi-automatic, or manual construction of the maze. the contour of the maze can be configured freely and the process of building a maze is recorded for replaying. the maze runner supports single-user or multi-user mode. in multi-user mode, teammates communicate with multi-view display and real-time chats. foot prints, marks, flags are used for avoiding repeated movements. the maze can be shown as wall or channel map. the maze manager allows editing, upload, download, and grouping of the teams. before coding the multi-maze, two important utility tools have been developed: multi-view graph library and network communication module. formulas for calculating the coordination for different degrees has been presented. algorithms of spanning tree have been used for building a perfect maze. the adding and deleting of walls allow the maze to be separated into components and forming circuits. the state diagrams, data structures and class hierarchies are presented for related application projects.
data_structures	most published concurrent data structures which avoid locking do not provide any fairness guarantees. that is, they allow processes to access a data structure and complete their operations arbitrarily many times before some other trying process can complete a single operation. such a behavior can be prevented by enforcing fairness. however, fairness requires waiting or helping. helping techniques are often complex and memory consuming. furthermore, it is known that it is not possible to automatically transform every data structure, which has a non-blocking implementation, into the corresponding data structure which in addition satisfies a very weak fairness requirement. does it mean that for enforcing fairness it is best to use locks? the answer is negative. we show that it is possible to automatically transfer any non-blocking or wait-free data structure into a similar data structure which satisfies a strong fairness requirement, without using locks and with limited waiting. the fairness we require is that no process can initiate and complete two operations on a given resource while some other process is kept waiting on the same resource. our approach allows as many processes as possible to access a shared resource at the same time as long as fairness is preserved. to achieve this goal, we introduce and solve a new synchronization problem, called fair synchronization. solving the new problem enables us to add fairness to existing implementations of concurrent data structures, and to transform any solution to the mutual exclusion problem into a fair solution. (c) 2016 elsevier inc. all rights reserved.
data_structures	city governments and energy utilities are increasingly focusing on the development of energy efficiency strategies for buildings as a key component in emission reduction plans and energy supply strategies. to support these diverse needs, a new generation of urban building energy models (ubem) is currently being developed and validated to estimate citywide hourly energy demands at the building level. however, in order for cities to rely on ubems, effective model generation and maintenance workflows are needed based on existing urban data structures. within this context, the authors collaborated with the boston redevelopment authority to develop a citywide ubem based on official gis datasets and a custom building archetype library. energy models for 83,541 buildings were generated and assigned one of 52 use/age archetypes, within the cad modelling environment rhinoceros3d. the buildings were then simulated using the us doe energyplus simulation program, and results for buildings of the same archetype were crosschecked against data from the us national energy consumption surveys. a district-level intervention combining photovoltaics with demand side management is presented to demonstrate the ability of ubem to provide actionable information. lack of widely available archetype templates and metered energy data, were identified as key barriers within existing workflows that may impede cities from effectively applying ubem to guide energy policy. (c) 2016 elsevier ltd. all rights reserved.
data_structures	asynchronous tasks in programming are those tasks executed free of context of the main task. therefore asynchronous tasks are methods implemented in a non-blocking style, permitting the main method to continue running. functional programming is a programming style to express the hierarchy and components of computer code. in this style calculations are treated similarly to treating computations of functions in mathematics. hence memory-states and modifiable data structures are not needed. functional programming can be introduced as a declarative style of coding in the sense that expressions replace programs. on a functional object-oriented model, this paper presents an accurate type system for asynchronous operations. the job of the type system is to stop undefined functions from execution and hence from aborting programs. in other words, the type system ensures soundness of data types and hence avoiding static errors like field-not-defined and method-not-defined from occurring at execution time. the paper introduces as well a programming example for the proposed system.
network_security	with introducing demand response aggregator (dra) in smart paradigm, small customers can actively participate in price and incentive-based demand response programs. this new matter can significantly affect many factors of power system such as transmission network security. accordingly, it is notably useful to define an adjusted framework for transmission expansion planning in smart environment. a long-term market simulation is performed using a tri-level iterative framework to find the best expansion decisions for transmission company (transco) in a pool-based market. the transco 's investment decisions are made by a merchant approach consistent with transmission network security and smart environment. the effects of smart environment on future network configuration, strategic bidding of generators in the market operation, and contingency analysis are considered during planning process. the effectiveness of proposed method is examined on the ieee 24-bus system with one dra, one transco, and two generation companies under the independent system operator 's supervision. copyright (c) 2016 john wiley & sons, ltd.
network_security	session initiation protocol (sip) is an application layer protocol used for signaling purposes to manage voice over ip connections. sip being a text-based protocol is vulnerable to a range of denial of service (dos) attacks. these dos attacks can render the sip servers/sip proxy servers unusable by depleting memory and cpu time. in this paper, we consider two types of dos attacks, namely, flooding attacks and coordinated attacks for detection. flooding attacks affect both stateless and stateful sip servers while coordinated attacks affect stateful sip servers. we model the sip operation as discrete event system (des) and design a new state transition machine, which we name as probabilistic counting deterministic timed automata (pcdta) to describe the behavior of sip operations. we also identify different types of anomalies that can occur in a des model, which appear in the form of illegal transitions, violating timing constraints, and appear in number which is otherwise not seen. subsequently, we map various dos attacks in sip to a type of anomaly in des. pcdta can learn probabilities of various transitions and timings delay from a set of nonmalicious training sequences. a trained pcdta can detect anomalies, and hence various dos attacks in sip. we perform a thorough experiment with computer simulated sip traffic and report the detection performance of pcdta on various attacks generated through custom scripts.
network_security	coordinated intrusion, like ddos, worm outbreak and botnet, is a major threat to network security nowadays and will continue to be a threat in the future. to ensure the internet security, effective detection and mitigation for such attacks are indispensable. in this paper, we propose a novel collaborative intrusion prevention architecture, i.e. cipa, aiming at confronting such coordinated intrusion behavior. cipa is deployed as a virtual network of an artificial neural net over the substrate of networks. taking advantage of the parallel and simple mathematical manipulation of neurons in a neural net, cipa can disperse its lightweight computation power to the programmable switches of the substrate. each programmable switch virtualizes one to several neurons. the whole neural net functions like an integrated ids/ips. this allows cipa to detect distributed attacks on a global view. meanwhile, it does not require high communication and computation overhead. it is scalable and robust. to validate cipa, we have realized a prototype on software-defined networks. we also conducted simulations and experiments. the results demonstrate that cipa is effective. (c) 2016 elsevier ltd. all rights reserved.
network_security	detection of ddos (distributed denial of service) traffic is of great importance for the availability protection of services and other information and communication resources. the research presented in this paper shows the application of artificial neural networks in the development of detection and classification model for three types of ddos attacks and legitimate network traffic. simulation results of developed model showed accuracy of 95.6% in classification of pre-defined classes of traffic.
network_security	with rapidly growing internet traffic, energy efficient operation of ip over wdm networks with sleep enabled routers is of increasing interest. however, for network security and to provide guaranteed communications, it would still be desirable to ensure that a certain fraction of the bandwidth is assured through routers which cannot be put to sleep using software control. this paper presents an energy-minimized ip over wdm network using a mixture of sleep-enabled and non-sleep-enabled router cards where a certain percentage of the network bandwidth is guaranteed to the offered traffic. such a mixed configuration is also motivated by the fact that there will always be some traffic demand between each node pair at any time even though the traffic between node pairs may fluctuate to very low levels. this implies a need for some non-sleeping router cards at any time. another motivation for this mixed configuration is because in the course of migration from today 's networks with non-sleep-enabled cards to future networks with sleep-enabled cards, the non-sleep-enabled network devices will not be quickly abandoned but will be gradually replaced. this also causes a network situation with the mixed router card types. to design an ip over wdm network where both sleep-enabled and non-sleep-enabled router cards are used, we propose a mixed integer linear programming (milp) model which jointly minimizes the energy consumption of all the router cards while guaranteeing a secured fractional bandwidth for all the node pairs. modified milps with subsequent port-channel association are also proposed along with efficient heuristic algorithms which perform almost as well as the joint milp approaches. the performance of these approaches is studied through simulations on a wide variety of networks.
image_processing	in this article we propose a novel framework for the modelling of non-stationary multivariate lattice processes. our approach extends the locally stationary wavelet paradigm into the multivariate two-dimensional setting. as such the framework we develop permits the estimation of a spatially localised spectrum within a channel of interest and, more importantly, a localised cross-covariance which describes the localised coherence between channels. associated estimation theory is also established which demonstrates that this multivariate spatial framework is properly defined and has suitable convergence properties. we also demonstrate how this model-based approach can be successfully used to classify a range of colour textures provided by an industrial collaborator, yielding superior results when compared against current state-of-the-art statistical image processing methods.
image_processing	in animals, self-grooming is an important component of their overall hygiene because it reduces the risk of disease and parasites. the european honey bee (apis mellifera) exhibits hygienic behavior, which refers to the ability of the members of a colony to remove diseased or dead brood from the hive. individual grooming behavior, however, is when a bee grooms itself to remove parasites. while both behaviors are critical for the mitigation of disease, hygienic behavior is overwhelmingly more studied because, unlike grooming behavior, it has a simple bioassay to measure its phenotype. here, we develop a novel bioassay to expedite data collection of grooming behavior by testing different honey bee genotypes (stocks). individual worker bees from different commercial stocks were coated in baking flour, placed in an observation arena, and digitally recorded to automatically measure grooming rates. the videos were analyzed in matlab, and an exponential function was fit to the pixel data to calculate individual grooming rates. while bees from the different commercial stocks were not significantly different in their grooming rates, the automation of grooming measurements may facilitate future research and stock selection for this important mechanism of social immunity. (c) 2017 elsevier b.v. all rights reserved.
image_processing	in wire and arc additive manufacture (waam), the twist of wire during a robot 's movement can result in the sudden changes of the wire-feeding position and thus cause deposition defects and dimensional errors. in the worst case, it may cause wire jamming and damage of the wire-feeding system. therefore, online monitoring and correction of the wire deflection are very important for waam. in this paper, a vision-based measuring method is proposed for detecting the deviations of the wire-feeding position of a plasma welding-based waam process. it uses adaptive threshold and hough transform to extract the wire edges, judges and merges the coincident lines, and applies radon transform to measure the wire deflection. software to automatically detect the wire deviation was developed based on the proposed method. the method and the software were verified with experiments.
image_processing	in the last two decades, we have seen an amazing development of image processing techniques targeted for medical applications. we propose multi-gpu-based parallel real-time algorithms for segmentation and shape-based object detection, aiming at accelerating two medical image processing methods: automated blood detection in wireless capsule endoscopy (wce) images and automated bright lesion detection in retinal fundus images. in the former method we identified segmentation and object detection as being responsible for consuming most of the global processing time. while in the latter, as segmentation was not used, shape-based object detection was the compute-intensive task identified. experimental results show that the accelerated method running on multi-gpu systems for blood detection in wce images is on average 265 times faster than the original cpu version and is able to process 344 frames per second. by applying the multi-gpu framework for bright lesion detection in fundus images we are able to process 62 frames per second with a speedup average 667 times faster than the equivalent cpu version.
image_processing	halftoning and inverse halftoning algorithms are very important image processing tools, widely used in the development of digital printers, scanners, steganography and image authentication systems. because such applications require to obtain high quality gray scale images from its halftone versions, the development of efficient inverse halftoning algorithms, that be able to provide gray scale images with peak signal to noise ratio (psnr) higher than 25, have been research topic during the last several years. although a psnr of about 25db may be enough for several applications, exist several other that require higher image quality. to reduce this problem, this paper proposes inverse halftoning algorithms based on atomic function and multi-layer perceptron neural network which provides gray scale images with psnrs higher than 30db independently of the method used to generate the halftone image.
parallel_computing	new imaging techniques enable visualizing and analyzing a multitude of unknown phenomena in many areas of science at high spatio-temporal resolution. the rapidly growing amount of image data, however, can hardly be analyzed manually and, thus, future research has to focus on automated image analysis methods that allow one to reliably extract the desired information from large-scale multidimensional image data. starting with infrastructural challenges, we present new software tools, validation benchmarks and processing strategies that help coping with large-scale image data. the presented methods are illustrated on typical problems observed in developmental biology that can be answered, e.g., by using time-resolved 3d microscopy images.
parallel_computing	for many tiller crops, the plant architecture (pa), including the plant fresh weight, plant height, number of tillers, tiller angle and stem diameter, significantly affects the grain yield. in this study, we propose a method based on volumetric reconstruction for high-throughput three-dimensional (3d) wheat pa studies. the proposed methodology involves plant volumetric reconstruction from multiple images, plant model processing and phenotypic parameter estimation and analysis. this study was performed on 80 triticum aestivum plants, and the results were analyzed. comparing the automated measurements with manual measurements, the mean absolute percentage error (mape) in the plant height and the plant fresh weight was 2.71% (1.08 cm with an average plant height of 40.07 cm) and 10.06% (1.41 g with an average plant fresh weight of 14.06 g), respectively. the root mean square error (rmse) was 1.37 cm and 1.79 g for the plant height and plant fresh weight, respectively. the correlation coefficients were 0.95 and 0.96 for the plant height and plant fresh weight, respectively. additionally, the proposed methodology, including plant reconstruction, model processing and trait extraction, required only approximately 20 s on average per plant using parallel computing on a graphics processing unit (gpu), demonstrating that the methodology would be valuable for a high-throughput phenotyping platform.
parallel_computing	parallelization is applied to the neutron calculations performed by the heterogeneous method on a graphics processing unit. the parallel algorithm of the modified trec code is described. the efficiency of the parallel algorithm is evaluated.
parallel_computing	agent-based models (abms) are increasingly being used to study population dynamics in complex systems, such as the human immune system. previously, folcik et al. (the basic immune simulator: an agent-based model to study the interactions between innate and adaptive immunity. theor biol med model 2007; 4: 39) developed a basic immune simulator (bis) and implemented it using the recursive porous agent simulation toolkit (repast) abm simulation framework. however, frameworks such as repast are designed to execute serially on central processing units and therefore cannot efficiently handle large model sizes. in this paper, we report on our implementation of the bis using flame gpu, a parallel computing abm simulator designed to execute on graphics processing units. to benchmark our implementation, we simulate the response of the immune system to a viral infection of generic tissue cells. we compared our results with those obtained from the original repast implementation for statistical accuracy. we observe that our implementation has a 13x performance advantage over the original repast implementation.
parallel_computing	we propose a many-core gpu implementation of robotic motion planning formulated as a semi-infinite optimization program. our approach computes the constraints and their gradients in parallel, and feeds the result to a nonlinear optimization solver running on the cpu. to ensure the continuous satisfaction of our constraints, we use polynomial approximations over time intervals. because each constraint and its gradient can be evaluated independently for each time interval, we end up with a highly parallelizable problem that can take advantage of many-core architectures. classic robotic computations (geometry, kinematics, and dynamics) can also benefit from parallel processors, and we carefully study their implementation in our context. this results in having a full constraint evaluator running on the gpu. we present several optimization examples with a humanoid robot. they reveal substantial improvements in terms of computation performance compared to a parallel cpu version.
distributed_computing	as a commercial distributed computing mode, cloud computing needs to meet the quality of service (qos) requirement of users, which is its top priority. however, cloud computing service providers also need to consider how to reduce the overhead of data center, and keep load balancing is one of the key points to maximize the use of the resource in the data center. in this paper, we propose an improved multi-objective niched pareto genetic algorithm (npga) to take load balancing into consideration without affecting performance of time consumption and financial cost of handling the user 's cloud computing tasks by presenting the load balancing shift mutation operator. the simulation results and analysis show that the proposed algorithm performs better than npga in maintaining the diversity and the distribution of the pareto-optimal solutions in the cloud tasks scheduling under the same population size and evolution generation.
distributed_computing	the growing number of scientific computation-intensive applications calls for an efficient utilization of large-scale, potentially interoperable distributed infrastructures. parameter sweep applications represent a large body of workflows. while the principle of workflows is easy to conceive, their execution is very complex and no universally accepted solution exists. in this paper we focus on the resource allocation challenges of parameter study jobs in distributed computing infrastructures. to cope with this np-hard problem and the high uncertainty present in these systems, we propose a series of job allocation models that helps refining and simplifying the problem complexity. in this way we present some special cases that are polynomial and show how more complex scenarios can be reduced to these models. it is known from practice that a small number of job sizes improves the result of job allocation, therefore we state a hypothesis relying on this fact in one of our models. unfortunately, the reduction of the general problem (using k-means clustering) did not help, and thus the hypothesis has proved to be false. in the future, we shall look for clustering techniques which fit this goal better.
distributed_computing	compute-intensive applications have gradually changed focus from massively parallel supercomputers to capacity as a resource obtained on-demand. this is particularly true for the large-scale adoption of cloud computing and mapreduce in industry, while it has been difficult for traditional high-performance computing (hpc) usage in scientific and engineering computing to exploit this type of resources. however, with the strong trend of increasing parallelism rather than faster processors, a growing number of applications target parallelism already on the algorithm level with loosely coupled approaches based on sampling and ensembles. while these cannot trivially be formulated as mapreduce, they are highly amenable to throughput computing. there are many general and powerful frameworks, but in particular for sampling-based algorithms in scientific computing there are some clear advantages from having a platform and scheduler that are highly aware of the underlying physical problem. here, we present how these challenges are addressed with combinations of dataflow programming, peer-to-peer techniques and peer-to-peer networks in the copernicus platform. this allows automation of sampling-focused workflows, task generation, dependency tracking, and not least distributing these to a diverse set of compute resources ranging from supercomputers to clouds and distributed computing (across firewalls and fragile networks). workflows are defined from modules using existing programs, which makes them reusable without programming requirements. the system achieves resiliency by handling node failures transparently with minimal loss of computing time due to checkpointing, and a single server can manage hundreds of thousands of cores e.g. for computational chemistry applications. (c) 2016 the authors. published by elsevier b.v. this is an open access article under the cc by license
distributed_computing	with the increasing demand of machine to machine (m2m) communications and internet of things (iot) services it is necessary to develop a new network architecture and protocols to support cost effective, distributed computing systems. generally, m2m and iot applications serve a large number of intelligent devices, such as sensors and actuators, which are distributed over large geographical areas. to deploy m2m communication and iot sensor nodes in a cost-effective manner over a large geographical area, it is necessary to develop a new network architecture that is cost effective, as well as energy efficient. this paper presents an ieee 802.11 and ieee 802.15.4 standards-based heterogeneous network architecture to support m2m communication services over a wide geographical area. for the proposed heterogeneous network, we developed a new cooperative carrier sense multiple access with collision avoidance (csma/ca) medium access control (mac) protocol to transmit packets using a shared channel in the 2.4 ghz ism band. one of the key problems of the ieee 802.11/802.15.4 heterogeneous network in a dense networking environment is the coexistence problem in which the two protocols interfere with each other causing performance degradation. this paper introduces a cooperative mac protocol that utilizes a new signaling technique known as the blank burst (bb) to avoid the coexistence problem. the proposed mac protocol improves the network qos of m2m area networks. the developed network architecture offers significant energy efficiency, and operational expenditure (opex) and capital expenditure (capex) advantages over 3g/4g cellular standards-based wide area networks.
distributed_computing	linear max-plus systems describe the behavior of a large variety of complex systems. it is known that these systems show a periodic behavior after an initial transient phase. assessment of the length of this transient phase provides important information on complexity measures of such systems, and so is crucial in system design. we identify relevant parameters in a graph representation of these systems and propose a modular strategy to derive new upper bounds on the length of the transient phase. by that we are the first to give asymptotically tight and potentially subquadratic transience bounds. we use our bounds to derive new complexity results, in particular in distributed computing. (c) 2016 elsevier b.v. all rights reserved.
algorithm_design	counting the solution number of combinational optimization problems is an important topic in the study of computational complexity, which is concerned with vertex-cover in this paper. first, we investigate organizations of vertex-cover solution spaces by the underlying connectivity of unfrozen vertices and provide facts on the global and local environment. then, a vertex-cover solution number counting algorithm is proposed and its complexity analysis is provided, the results of which fit very well with the simulations and have a better performance than those by 1-rsb in the neighborhood of c = e for random graphs. based on the algorithm, variation and fluctuation on the solution number the statistics are studied to reveal the evolution mechanism of the solution numbers. furthermore, the marginal probability distributions on the solution space are investigated on both the random graph and scale-free graph to illustrate the different evolution characteristics of their solution spaces. thus, doing solution number counting based on the graph expression of the solution space should be an alternative and meaningful way to study the hardness of np-complete and #p-complete problems and the appropriate algorithm design can help to achieve better approximations of solving combinational optimization problems and the corresponding counting problems.
algorithm_design	mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property-incentive properties are only useful if the strategic agents also believe this fact. verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals. we introduce a relational refinement type system, called hoare(2), for verifying mechanism design and differential privacy. we show that hoare(2) is sound w.r.t. a denotational semantics, and correctly models (epsilon, delta)-differential privacy; moreover, we show that it subsumes dfuzz, an existing linear dependent type system for differential privacy. finally, we develop an smt-based implementation of hoare(2) and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy.
algorithm_design	this paper presents a novel approach for estimating the ego-motion of a vehicle in dynamic and unknown environments using tightly-coupled inertial and visual sensors. to improve the accuracy and robustness, we exploit the combination of point and line features to aid navigation. the mathematical framework is based on trifocal geometry among image triplets, which is simple and unified for point and line features. for the fusion algorithm design, we employ the extended kalman filter (ekf) for error state prediction and covariance propagation, and the sigma point kalman filter (spkf) for robust measurement updating in the presence of high nonlinearities. the outdoor and indoor experiments show that the combination of point and line features improves the estimation accuracy and robustness compared to the algorithm using point features alone.
algorithm_design	target classification algorithms have generally kept pace with developments in the academic and commercial sectors since the 1970s. however, most recently, investment into object classification by internet companies and various human brain projects have far outpaced that of the defense sector. implications are noteworthy. there are some unique characteristics of the military classification problem. target classification is not solely an algorithm design problem, but is part of a larger system design task. the design flows down from a concept of operations (conops) and key performance parameters (kpps). inputs are image and/or signal data and time-synchronized metadata. the operation is real-time. the implementation minimizes size, weight and power (swap). the output must be conveyed to a time-strapped operator who understands the rules of engagement. it is assumed that the adversary is actively trying to defeat recognition. the target list is often mission dependent, not necessarily a closed set, and may change on a daily basis. it is highly desirable to obtain sufficiently comprehensive training and testing data sets, but costs of doing so are very high and data on certain target types are scarce. the training data may not be representative of battlefield conditions suggesting the avoidance of highly tuned designs. a number of traditional and emerging target classification strategies are reviewed in the context of the military target problem.
algorithm_design	it was an important part of the aggregation simulation model building that optimization of the simulation model. in this paper, the concept of the model element correlation was introduced and the reduction algorithm for the model based on the genetic-rough set algorithm was designed. the algorithm design and implementation process were described in this paper, then the correctness of the algorithm was described. at last, applicability of this algorithm was proved by an example.
computer_programming	the usefulness of online and hybrid delivery methods in education has long been realized and with the advancement of computer and communication technologies and the web based authoring tools, their effectiveness have been further extended. we are at a point that online and hybrid course offerings for undergraduates are quickly becoming an integrated and regular part of engineering departments' course offerings. many institutions are offering online courses, as part of their regular schedule. in this paper, based on our experience that started with hybrid delivery and ultimately extended to a full online offering, we will present a set of recommended steps that can be used as a guide by those who are interested in designing an online introductory computer-programming course. we will then present a set of observations that we feel needs to be considered and discussed by the research community. the paper will include examples of students' comments, which were received in teaching evaluations, for hybrid and online offerings.
computer_programming	existing adaptive educational hypermedia systems have been using learning resources sequencing approaches in order to enrich the learning experience. in this context, educational resources, either expository or evaluative, play a central role. however, there is a lack of tools that support sequencing essentially due to the fact that existing specifications are complex. this paper presents seqins as a sequencing tool of digital educational resources. seqins includes a simple and flexible sequencing model that will foster heterogeneous students to learn at different rhythms. the tool communicates through the ims learning tools interoperability specification with a plethora of e-learning systems such as learning management systems, repositories, authoring and automatic evaluation systems. in order to validate seqins we integrate it in an e-learning ensemble framework instance for the computer programming learning domain.
computer_programming	computer programming and related courses account for a large proportion in current information courses. such courses typically include c/c++ language programming, vb language programming, java language programming, c # language programming, data structures and algorithms. there are many common aspects in terms of knowledge points among programming courses. however, currently, the courses are taught separately. furthermore, under the trend of shrinking time allocation and declining number of courses in undergraduate curriculum, it is critical to maximize the use of limited hours of programming courses to enhance teaching effectiveness. based on inquiry-based teaching model in accordance with cdio teaching philosophy, it is proposed in this paper to conduct programming curriculum group construction. therefore, knowledge of program design courses within the group will be shared and courses instruction within the group will be coordinated. knowledge sharing facilitates its application to experiments and projects, which can improve students' comprehensive capabilities, especially their ability to solve practical problems.
computer_programming	combined cycle power plant (ccpp) is one of the most efficient systems of energy conversion with different topping and bottoming cycles. one of the acceptable schemes, the combination of brayton and rankine cycle, is analyzed for various design parameters. in the present analysis thermodynamic modelling of a ccpp with single steam extraction from bottoming rankine cycle is carried out to study the effect of inlet air temperature (tat), cycle ratio (cr), turbine inlet temperature (tit), air compressor and gas turbine efficiency on the first and second law efficiency. for parametric analysis computer programming tool engineering equation solver (ees) is used and thermodynamic properties of many fluids and gases are inbuilt function of the software. from the results it is concluded that combustion chamber is the source of highest exergy destruction followed by heat recovery steam generator, gas turbine, air compressor and steam turbine.,with increase in tit, optimum cr is also found to be increased because both the gas turbine efficiency and the gas turbine exhaust temperature are increased for the optimum cycle ratio.
computer_programming	to implement bilingual teaching is an inevitable choice of higher vocational colleges to be geared to international standards and improve their professional competitive advantages. the practice of bilingual teaching of computer courses has been implemented by the school of information, guangzhou international economics college concerning foreign affairs or foreign nationals. in this paper, the actual problems in the bilingual teaching of computer programming course are emphatically analyzed, and then the related countermeasures are introduced from bilingual course selection, improving students' enthusiasm with mooc and flipped classroom, step-by-step bilingual teaching model, and practice teaching material selection, aiming to promote the development of bilingual teaching of computer courses in higher vocational colleges and the competitive power of students at workplace.
relational_databases	a potential problem for persisting large volume of streaming logs with conventional relational databases is that loading large volume of data logs produced at high rates is not fast enough due to the strong consistency model and high cost of indexing. as a possible alternative, state-of-the-art nosql data stores that sacrifice transactional consistency to achieve higher performance and scalability can be utilized. in this paper, we describe the challenges in large scale persisting and analysis of numerical streaming logs. we propose to develop a benchmark comparing relational databases with state-of-the-art nosql data stores to persist and analyze numerical logs. the benchmark will investigate to what degree a state-of-the-art nosql data store can achieve high performance persisting and large-scale analysis of data logs. the benchmark will serve as basis for investigating query processing and indexing of large-scale numerical logs.
relational_databases	the traditional olap (on-line analytical processing) systems store data in relational databases. unfortunately, it is difficult to manage big data volumes with such systems. as an alternative, nosql systems (not-only sql) provide scalability and flexibility for an olap system. we define a set of rules to map star schemas and its optimization structure, a precomputed aggregate lattice, into two logical nosql models: column-oriented and document-oriented. using these rules we analyse and implement two decision support systems, one for each model (using mongodb and hbase). we compare both systems during the phases of data (generated using the tpc-ds benchmark) loading, lattice generation and querying.
relational_databases	referential integrity is one of the three inherent integrity rules and can be enforced in databases using foreign keys. however, in many real world applications referential integrity is not enforced since foreign keys remain disabled to ease data acquisition. important applications such as anomaly detection, data integration, data modeling, indexing, reverse engineering, schema design, and query optimization all benefit from the discovery of foreign keys. therefore, the profiling of foreign keys from dirty data is an important yet challenging task. we raise the challenge further by diverting from previous research in which null markers have been ignored. we propose algorithms for profiling unary and multi-column foreign keys in the real world, that is, under the different semantics for null markers of the sql standard. while state of the art algorithms perform well in the absence of null markers, it is shown that they perform poorly in their presence. extensive experiments demonstrate that our algorithms perform as well in the real world as state of the art algorithms perform in the idealized special case where null markers are ignored.
relational_databases	in recent years, the application of laser point cloud data has increased dramatically. how to efficiently store and fast process the data becomes an important research direction at present. point cloud data contain a wealth of geographic information, belonging to the category of spatial data. traditional relational databases are relatively weak in massive spatial data storage and processing, while the application of non-relational databases provides a new perspective of study for this fact. sharding technology is a solution for database level extension. in this thesis, sharding cluster for mongodb is established under distributed environment, while distributed storage, spatial query and mapreduce operation test for numerous laser-point cloud data will be implemented through scope sharding and hash-based sharding, which completely reflects huge advantages of mongodb under distribution in storage and processing for spatial data.
relational_databases	the article deals with semantic data model and its application in data integration. it analyses the issue of mapping database schemes (particularly relational models) for common data models expressed in the form of ontology. substantial part is dedicated to methods of acquiring ontology from relational databases, where rules for creating classes, properties, hierarchies, cardinalities and instances are specified.
software_engineering	programming-specific q&a sites (e.g., stack overflow) are being used extensively by software developers for knowledge sharing and acquisition. due to the cross-reference of questions and answers (note that users also reference urls external to the q&a site. in this paper, url sharing refers to internal urls within the q&a site, unless otherwise stated), knowledge is diffused in the q&a site, forming a large knowledge network. in stack overflow, why do developers share urls? how is the community feedback to the knowledge being shared? what are the unique topological and semantic properties of the resulting knowledge network in stack overflow? has this knowledge network become stable? if so, how does it reach to stability? answering these questions can help the software engineering community better understand the knowledge diffusion process in programming-specific q&a sites like stack overflow, thereby enabling more effective knowledge sharing, knowledge use, and knowledge representation and search in the community. previous work has focused on analyzing user activities in q&a sites or mining the textual content of these sites. in this article, we present a methodology to analyze url sharing activities in stack overflow. we use open coding method to analyze why users share urls in stack overflow, and develop a set of quantitative analysis methods to study the structural and dynamic properties of the emergent knowledge network in stack overflow. we also identify system designs, community norms, and social behavior theories that help explain our empirical findings. through this study, we obtain an in-depth understanding of the knowledge diffusion process in stack overflow and expose the implications of url sharing behavior for q&a site design, developers who use crowdsourced knowledge in stack overflow, and future research on knowledge representation and search.
software_engineering	many automated finite state machine (fsm) based test generation algorithms require that a characterising set or a set of harmonised state identifiers is first produced. the only previously published algorithms for partial fsms were brute-force algorithms with exponential worst case time complexity. this paper presents polynomial time algorithms and also massively parallel implementations of both the polynomial time algorithms and the brute-force algorithms. in the experiments the parallel algorithms scaled better than the sequential algorithms and took much less time. interestingly, while the parallel version of the polynomial time algorithm was fastest for most sizes of fsms, the parallel version of the brute-force algorithm scaled better due to lower memory requirements.
software_engineering	modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. however, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). in this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: pluss and msvcm. the first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. after evaluating these approaches through the specifications of several systems, we find out that msvcm reduces feature scattering and improves scenario cohesion. these results suggest that evolving a product line specification using msvcm requires only localized changes. on the other hand, the results of six experiments reveal that msvcm requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.
software_engineering	online genetic improvement embeds the ability to evolve and adapt inside a target software system enabling it to improve at runtime without any external dependencies or human intervention. we recently developed a general purpose tool enabling online genetic improvement in software systems running on the java virtual machine. this tool, dubbed ecselr, is embedded inside extant software systems at runtime, enabling such systems to self-improve and adapt autonomously online. we present this tool, describing its architecture and focusing on its design choices and possible uses.
software_engineering	requirement prioritization are considered crucial towards the development of successful and high quality software. nowadays, most software development project and people in software industries are confronted with the challenge of implementing a large scale, complex and dynamic software applications. hence, the need to use a requirement prioritization technique which able to deliver a right set of requirement seems very crucial. the available techniques still suffer from several weaknesses. this motivate us to identify comprehensively what are the limitations and future directions of the existing requirement prioritization techniques. in this study, we conducted an in-depth software engineering based systematic literature review, as it helps to execute a thorough and fair literature review due to its predefined review protocol. we gathered a list of 55 requirement prioritization techniques and investigate the limitation and weaknesses each one of this technique. findings from this study later will be used to design a new effective and efficient requirement prioritization technique.
bioinformatics	background: micrornas (mirnas) are a class of small non-coding rnas that are strongly involved in various types of carcinogenesis, including hepatocellular carcinoma (hcc). this study aimed to clarify whether mir-4417 promotes hcc growth by targeting trim35 and regulating pkm2 phosphorylation. material/methods: online software, including targetscan and miranda, was used to predict the potential target of mir-4417. real-time pcr (qrt-pcr) and western blot assays were performed to detect the expression levels of mrna and protein, respectively. cell proliferation was measured by mtt assay and apoptosis in a549 cells was examined by flow cytometry. results: bioinformatics reveal that trim35 mrna contains 1 conserved target site of mir-4417. high level of mir-4417 and low levels of trim35 mrna and protein were observed in hcc cells compared with a normal liver cell line. biological function analysis showed that mir-4417 inhibitor inhibits cell proliferation and promotes apoptosis in hcc cells. furthermore, we verified that trim35 is a functional target of mir-4417 by use of luciferase reporter assay, and trim35 overexpressing showed an elevation of proliferation and a reduction of apoptosis in hcc cells. we subsequently investigated whether mir-4417 and trim35 regulate hcc cell proliferation and apoptosis through pkm2 y105 phosphorylation, and the results supported our speculation that mir-4417 targets trim35 and regulates the y105 phosphorylation of pkm2 to promote hepatocarcinogenesis. conclusions: our findings indicate that mir-4417 may function as an oncogene in hcc and is a potential alternative therapeutic target for this deadly disease.
bioinformatics	antimicrobial peptides (amps) from cuticular extracts of worker ants of trichomyrmex criniceps (mayr, hymenoptera: formicidae) were isolated and evaluated for their antimicrobial activity. eight peptides ranging in mass from 804.42 to 1541.04 da were characterized using a combination of analytical and bioinformatics approach. all the eight peptides were novel with no similarity to any of the amps archived in the antimicrobial peptide database. two of the eight novel peptides, the smallest and the largest by mass were named crinicepsin-1 and crinicepsin-2 and were chemically synthesized by solid phase peptide synthesis. the two synthetic peptides had antibacterial and weak hemolytic activity.
bioinformatics	background: bacteria present in cave often survive by modifying their metabolic pathway or other mechanism. understanding these adopted bacteria and their survival strategy inside the cave is an important aspect of microbial ecology. present study focuses on the bacterial community and geochemistry in five caves of mizoram, northeast india. the objective of this study was to explore the taxonomic composition and presumed functional diversity of cave sediment metagenomes using paired end illumina sequencing using v3 region of 16s rrna gene and bioinformatics pipeline. results: actinobacteria, proteobacteria, verrucomicrobia and acidobacteria were the major phyla in all the five cave sediment samples. among the five caves the highest diversity is found in lamsialpuk with a shannon index 12.5 and the lowest in bukpuk (shannon index 8.22). in addition, imputed metagenomic approach was used to predict the functional role of microbial community in biogeochemical cycling in the cave environments. functional module showed high representation of genes involved in amino acid metabolism in (20.9%) and carbohydrate metabolism (20.4%) in the kegg pathways. genes responsible for carbon degradation, carbon fixation, methane metabolism, nitrification, nitrate reduction and ammonia assimilation were also predicted in the present study. conclusion: the cave sediments of the biodiversity hotspot region possessing a oligotrophic environment harbours high phylogenetic diversity dominated by actinobacteria and proteobacteria. among the geochemical factors, ferric oxide was correlated with increased microbial diversity. in-silico analysis detected genes involved in carbon, nitrogen, methane metabolism and complex metabolic pathways responsible for the survival of the bacterial community in nutrient limited cave environments. present study with paired end illumina sequencing along with bioinformatics analysis revealed the essential ecological role of the cave bacterial communities. these results will be useful in documenting the biospeleology of this region and systematic understanding of bacterial communities in natural sediment environments as well.
bioinformatics	background: ascending thoracic aortic aneurysm (ataa) is a major cause of morbidity and mortality worldwide. the pathogenesis of medial degeneration of the aorta remains undefined. high-throughput secretome analysis by mass spectrometry may be useful to elucidate the molecular mechanisms involved in aneurysm formation as well as to identify biomarkers for early diagnosis or targets of therapy. the purpose of the present study was to analyze the secreted/released proteins from ataa specimens of both tricuspid aortic valve (tav) and bicuspid aortic valve (bav) patients. methods: aortic specimens were collected from patients undergoing elective surgery and requiring graft replacement of the ascending aorta. each sample of the ascending aortic aneurysm, 4 bav (3 males; aged 53.5 +/- 11.4 years) and 4 tav (1 male; 78 +/- 7.5 years), was incubated for 24 h in serum-free medium. released proteins were digested with trypsin. peptide mixtures were fractioned by nano-high performance liquid chromatography and analyzed by mass spectrometry. following identification of differentially expressed proteins, quantitative real time polymerase chain reaction (qrt-pcr) analysis was performed. results: the comparison between the proteins released from bav and tav aneurysmatic tissues showed significantly diverging expression fingerprints in the two groups of patients. bioinformatics analysis revealed 38 differentially released proteins; in particular 7 proteins were down-regulated while 31 were up-regulated in bav with respect to tav. most of the proteins that were up-released in bav were related to the activation of transforming growth factor (tgf)-beta signaling. latent tgf-beta binding protein 4 (ltbp4) exhibited one of the highest significant under-expressions (10-fold change) in bav secretomes with respect to tav. qrt-pcr analysis validated this significant difference at ltbp4 gene level (bav: 1.03 +/- 0.9 vs tav: 3.6 +/- 3.2; p < 0.05). conclusion: hypothesis-free secretome profiling clearly showed diverging expression fingerprints in the ataa of tav and bav patients, confirming the crucial role of tgf-beta signaling in modulating ataa development in bicuspid patients. (c) 2016 japanese college of cardiology. published by elsevier ltd. all rights reserved.
bioinformatics	the current study aimed to devise eco-friendly, safe, and cost-effective strategies for enhanced degradation of low- and high-density polyethylene (ldpe and hdpe) using newly formulated thermophilic microbial consortia from cow dung and to assess the biodegradation end products. the plastic-degrading bacteria from cow dung samples gathered from highly plastic-acclimated environments were enriched by standard protocols. the degradation ability was comprehended by zone of clearance method, and the percentage of degradation was monitored by weight reduction process. the best isolates were characterized by standard microbiological and molecular biology protocols. the best isolates were employed to form several combinations of microbial consortia, and the degradation end products were analyzed. the stability of 16s ribosomal dna (rdna) was predicted by bioinformatics approach. this study identified 75 +/- 2, 55 +/- 2, 60 +/- 3, and 43 +/- 3% degradation for ldpe strips, pellets, hdpe strips, and pellets, respectively, for a period of 120 days (p < 0.05) at 55 degrees c by the formulated consortia of is1-is4, and the degradation efficiency was found to be better in comparison with other formulations. the end product analysis by fourier transform infrared, scanning electron microscopy, energy-dispersive spectroscopy, and nuclear magnetic resonance showed major structural changes and formation of bacterial biofilm on plastic surfaces. these novel isolates were designated as bacillus vallismortis bt-dsce01, psuedomonas protegens bt-dsce02, stenotrophomonas sp. bt-dsce03, and paenibacillus sp.bt-dsce04 by 16s rdna sequencing and suggested good gene stability with minimum gibb 's free energy. therefore, this study imparts substantial information regarding the utilization of these thermophilic microbial consortia from cow dung for rapid polyethylene removal.
cryptography	substitution-boxes, or simply s-boxes, are used to increase confidentiality in the substitution stage of most cryptosystem approaches. in this paper, an efficient method for the construction of an s-box based on a sine map is proposed. the proposed s-box is able to generate random integer sequences with highly efficient nonlinearity in the generated values. the cryptographic analyses show that the proposed s-box method is of high performance and can be used with great potential for prominent prevalence in designing symmetric cryptosystems and copy right protection. (c) 2016 published by elsevier gmbh.
cryptography	for a large class of functions to the group of points of an elliptic curve (typically obtained from certain algebraic correspondences between e and ), farashahi et al. (math comput 82(281):491-512, 2013) established that the map is regular, in the sense that for a uniformly random choice of , the elliptic curve point is close to uniformly distributed in . this result has several applications in cryptography, mainly to the construction of elliptic curve-valued hash functions and to the ""elligator squared"" technique by tibouchi (in: christin and safavi-naini (eds) financial cryptography. lncs, vol 8437, pp 139-156. springer, heidelberg, 2014) for representating uniform points on elliptic curves as close to uniform bitstrings. in this paper, we improve upon farashahi et al. 's character sum estimates in two ways: we show that regularity can also be obtained for a function of the form where g has a much smaller domain than , and we prove that the functions f considered by farashahi et al. also satisfy requisite bounds when restricted to large intervals inside . these improved estimates can be used to obtain more efficient hash function constructions, as well as much shorter ""elligator squared"" bitstring representations.
cryptography	safer is a family of block ciphers, which is comprised of safer k, safer sk, safer+ and safer++. safer sk was proposed to strengthen the key schedule of safer k. safer+ was designed as an aes candidate and safer++ was among the cryptographic primitives selected for the second phase of the nessie project. this paper presented the first zero-correlation linear cryptanalytic attack against the safer block cipher family. we investigated the linear properties of pht employed as the linear layer of the safer block ciphers, and identified zero-correlation linear approximations for safer sk, safer+ and safer++. moreover, we displayed several characterizations of the undisturbed bits, and found that there exists an undisturbed bit in the exponential s-box, which can be applied to reduce the computational complexity in the key recovery attacks on 5 rounds of safer sk/128 and 4(5) rounds of safer+/128(256), 5(6) rounds of safer++/128(256). more rounds of the safer block ciphers can be attacked with the linear relations of correlation zero.
cryptography	session initiation protocol (sip) has proved to be the integral part and parcel of any multimedia based application or ip-based telephony service that requires signaling. sip supports http digest based authentication, and is responsible for creating, maintaining and terminating sessions. to guarantee secure sip based communication, a number of authentication schemes are proposed, typically most of these are based on smart card due to its temper resistance property. recently zhang et al. presented an authenticated key agreement scheme for sip based on elliptic curve cryptography. however tu et al. (peer to peer netw. appl 1-8, 2014) finds their scheme to be insecure against user impersonation attack, furthermore they presented an improved scheme and claimed it to be secure against all known attacks. very recently farash (peer to peer netw. appl 1-10, 2014) points out that tu et al. 's scheme is vulnerable to server impersonation attack, farash also proposed an improvement on tu et al. 's scheme. however, our analysis in this paper shows that tu et al. 's scheme is insecure against server impersonation attack. further both tu et al. 's scheme and farash 's improvement do not protect user 's privacy and are vulnerable to replay and denial of services attacks. in order to cope with these limitations, we have proposed a privacy preserving improved authentication scheme based on ecc. the proposed scheme provides mutual authentication as well as resists all known attacks as mentioned by tu et al. and farash.
cryptography	since substitution box (s-box) is the only nonlinear component related to confusion properties for many block encryption algorithms, it is a necessity for the strong block encryption algorithms. s-box is a vital component in cryptography due to having the effect on the security of entire system. therefore, alternative s-box construction techniques have been proposed in many researches. in this study, a new s-box construction method based on fractional-order (fo) chaotic chen system is presented. in order to achieve that goal, numerical results of the fo chaotic chen system for a = 35, b = 3, c = 28 and alpha = 0.9 are obtained by employing the predictor-corrector scheme. besides, a simpler algorithm is suggested for the construction of s-box via time response of the fo chaotic chen system. the performance of suggested s-box design is compared with other s-box designs developed by chaotic systems, and it is observed that this method provides a stronger s-box design.
structured_storage	geodynamics simulations are characterized by theological nonlinearity, localization, three-dimensional effects, and separate but interacting length scales. these features represent a challenge for computational science. we discuss how a leading software framework for advanced scientific computing (the portable extensible toolkit for scientific computation, petsc) can facilitate the development of geodynamics simulations. to illustrate our use of petsc, we describe simulations of (i) steady-state, non-newtonian passive flow and thermal structure beneath a mid-ocean ridge, (ii) magmatic solitary waves in the mantle, and (iii) the formation of localized bands of high porosity in a two-phase medium being deformed under simple shear. we highlight two supplementary features of petsc, structured storage of application parameters and self-documenting output, that are especially useful for geodynamics simulations. (c) 2007 elsevier b.v. all rights reserved.
structured_storage	the atlas experiment at the lhc collects billions of events each data-taking year, and processes them to make them available for physics analysis in several different formats. an even larger amount of events is in addition simulated according to physics and detector models and then reconstructed and analysed to be compared to real events. the eventindex is a catalogue of all events in each production stage; it includes for each event a few identification parameters, some basic non-mutable information coming from the online system, and the references to the files that contain the event in each format (plus the internal pointers to the event within each file for quick retrieval). each eventindex record is logically simple but the system has to hold many tens of billions of records, all equally important. the hadoop technology was selected at the start of the eventindex project development in 2012 and proved to be robust and flexible to accommodate this kind of information; both the insertion and query response times are acceptable for the continuous and automatic operation that started in spring 2015. this paper describes the eventindex data input and organisation in hadoop and explains the operational challenges that were overcome in order to achieve the expected performance.
structured_storage	engineering designs are typically constrained by requirements and specifications. the design analogy performance parameters system (d-apps) project seeks to provide engineers with a means to transform these product requirements and specifications into functional analogies. d-apps employs design by-analogy (dba) via critical functionality and the engineering performance parameters from the specifications to produce functional alternative options to design engineers. repositories, or databases, provide structured storage of information for retrieval by users in an systemic manner. d-apps utilizes a repository to store analogy entries, capable of being polled by the developed algorithm. analogous matches are formulated with the weighting scheme of the algorithm. this research investigates the structures of three alternative repositories, and the current d-apps database with its interface to the algorithm and design engineer. the architecture of the repository is discussed with example entries and rationale for inclusion.
structured_storage	hydrological models are always related to time and spatial domains, so the model results produced by these models are very large. microsoft component structured storage can be employed to save the model results, but it is lack of mechanism to reduce the data size. in order to tackle this situation, compressed structured storage method is introduced which based on combining component structured storage and zlib compression library. in this method, standard component rules are complied and containment as most common mechanism for object reuse in com is applied so as to simplify the usage.
structured_storage	due to the increasing complexity of the surgical working environment, increasingly technical solutions must be found to help relieve the surgeon. this objective is supported by a structured storage concept for all relevant device data. in this work, we present a concept and prototype development of a storage system to address intraoperative medical data. the requirements of such a system are described, and solutions for data transfer, processing, and storage are presented. in a subsequent study, a prototype based on the presented concept is tested for correct and complete data transmission and storage and for the ability to record a complete neurosurgical intervention with low processing latencies. in the final section, several applications for the presented data recorder are shown. the developed system based on the presented concept is able to store the generated data correctly, completely, and quickly enough even if much more data than expected are sent during a surgical intervention. the surgical data recorder supports automatic recognition of the interventional situation by providing a centralized data storage and access interface to the or communication bus. in the future, further data acquisition technologies should be integrated. therefore, additional interfaces must be developed. the data generated by these devices and technologies should also be stored in or referenced by the surgical data recorder to support the analysis of the or situation.
