machine_learning	a high degree of uncertainty associated with the emission inventory for china tends to degrade the performance of chemical transport models in predicting pm2.5 concentrations especially on a daily basis. in this study a novel machine learning algorithm, geographically -weighted gradient boosting machine (gw-gbm), was developed by improving gbm through building spatial smoothing kernels to weigh the loss function. this modification addressed the spatial nonstationarity of the relationships between pm2.5 concentrations and predictor variables such as aerosol optical depth (aod) and meteorological conditions. gw-gbm also overcame the estimation bias of pm2.5 concentrations due to missing aod retrievals, and thus potentially improved subsequent exposure analyses. gw-gbm showed good performance in predicting daily pm2.5 concentrations (r-2 = 0.76, rmse = 23.0 g/m(3)) even with partially missing aod data, which was better than the original gbm model (r-2 = 0.71, rmse = 25.3 g/m(3)). on the basis of the continuous spatiotemporal prediction of pm2.5 concentrations, it was predicted that 95% of the population lived in areas where the estimated annual mean pm2.5 concentration was higher than 35 g/m(3), and 45% of the population was exposed to pm2.5 >75 g/m(3) for over 100 days in 2014. gw-gbm accurately predicted continuous daily pm2.5 concentrations in china for assessing acute human health effects. (c) 2017 elsevier ltd. all rights reserved.
data_structures	this paper presents an unsupervised approach to feature binary coding for efficient semantic image retrieval. although the majority of the existing methods aim to preserve neighborhood structures of the feature space, semantically similar images are not always in such neighbors but are rather distributed in non-linear low-dimensional manifolds. moreover, images are rarely alone on the internet and are often surrounded by text data such as tags, attributes, and captions, which tend to carry rich semantic information about the images. on the basis of these observations, the approach presented in this paper aims at learning binary codes for semantic image retrieval using multimodal information sources while preserving the essential low-dimensional structures of the data distributions in the hamming space. specifically, after finding the low-dimensional structures of the data by using an unsupervised sparse coding technique, our approach learns a set of linear projections for binary coding by solving an optimization problem which is designed to jointly preserve the extracted data structures and multimodal data correlations between images and texts in the hamming space as much as possible. we show that the joint optimization problem can readily be transformed into a generalized eigenproblem that can be efficiently solved. extensive experiments demonstrate that our method yields significant performance gains over several existing methods.
machine_learning	the detection of negative emotions through daily activities such as writing and drawing is useful for promoting wellbeing. the spread of human-machine interfaces such as tablets makes the collection of handwriting and drawing samples easier. in this context, we present a first publicly available database which relates emotional states to handwriting and drawing, that we call emothaw (emotion recognition from handwriting and drawing). this database includes samples of 129 participants whose emotional states, namely anxiety, depression, and stress, are assessed by the depression-anxiety-stress scales (dass) questionnaire. seven tasks are recorded through a digitizing tablet: pentagons and house drawing, words copied in handprint, circles and clock drawing, and one sentence copied in cursive writing. records consist in pen positions, on-paper and in-air, time stamp, pressure, pen azimuth, and altitude. we report our analysis on this database. from collected data, we first compute measurements related to timing and ductus. we compute separate measurements according to the position of the writing device: on paper or inair. we analyze and classify this set of measurements (referred to as features) using a random forest approach. this latter is a machine learning method [1], based on an ensemble of decision trees, which includes a feature ranking process. we use this ranking process to identify the features which best reveal a targeted emotional state. we then build random forest classifiers associated with each emotional state. we provide accuracy, sensitivity, and specificity evaluation measures obtained from cross-validation experiments. our results showthat anxiety and stress recognition perform better than depression recognition.
cryptography	a seamless and secure handover is always one of the important design goals of the cellular networks. the handover scheme of 4g long term evolution (lte) wireless networks is complex due to the presence of two possible different types of base stations. in lte communication systems, a normal base station is referred to as an enodeb (enb). what increases the level of complexity of the system is the fact that the other kind of base stations, namely, home enodeb (henb), cannot directly communicate with enb. in the lte networks, the handover scenarios involving a henb could result in a complicated handover procedure. besides, since key chains have been used in the handover processes, it is found to be lack of backward security. therefore, in order to handle the handover involving a henb efficiently with security provisioning, in this paper, a proxy signature based handover scheme is proposed. the proposed scheme works based on the elliptic curve cryptography (ecc) algorithm, which makes the computational cost of the handover process smaller compared to other handover schemes.
cryptography	we present new connections between quantum information and the field of classical cryptography. in particular, we provide examples where simon 's algorithm can be used to show insecurity of commonly used cryptographic symmetric-key primitives. specifically, these examples consist of a quantum distinguisher for the 3-round feistel network and a forgery attack on cbc-mac which forges a tag for a chosen-prefix message querying only other messages (of the same length). we assume that an adversary has quantum-oracle access to the respective classical primitives. similar results have been achieved recently in independent work by kaplan et al. [kllnp16]. our findings shed new light on the post-quantum security of cryptographic schemes and underline that classical security proofs of cryptographic constructions need to be revisited in light of quantum attackers.
algorithm_design	with the development of the communication technology and the intelligent terminal, the artificial attendance based on the intelligent terminal technology and mobile communication technology is replaced by the attendance and replacement. based on this, on the basis of existing research on the aloha anti-collision strategy, and improved it for mobile positioning attendance. firstly, the algorithm design, and then gives terminal specific operation, and finally carried out experiments and comparative analysis of simulation results show that the improved aloha algorithm outperforms traditional anti-collision algorithm, ensuring the system has a shorter at the same time delay, can effectively improve the throughput performance.
distributed_computing	one of the major challenges in digital forensics today is data encryption. due to the leaked information about unlawful sniffing, many users decided to protect their data by encryption. in case of criminal activities, forensic experts are challenged how to decipher suspects' data that are subject to investigation. a common method how to overcome password-based protection is a brute force password recovery using gpu-accelerated hardware. this approach seems to be expensive. this paper presents an alternative approach using task distribution based on boinc platform. the cost, time, and energy efficiency of this approach is discussed and compared to the gpu-based solution.
symbolic_computation	factorization of polynomials is one of the foundations of symbolic computation. its applications arise in numerous branches of mathematics and other sciences. however, the present advanced programming languages such as c++ and j++, do not support symbolic computation directly. hence, it leads to difficulties in applying factorization in engineering fields. in this paper, the authors present an algorithm which use numerical method to obtain exact factors of a bivariate polynomial with rational coefficients. the proposed method can be directly implemented in efficient programming language such c++ together with the gnu multiple-precision library. in addition, the numerical computation part often only requires double precision and is easily parallelizable.
parallel_computing	for a decision table, it is firstly proved that the effective pair is equivalent to improved discernibility matrix to guarantee that the attribute reduction based on discernibility matrix can be calculated by effective pair. to get the attribute reduction more quickly, a parallel radix sort model is proposed. then a simple decision table and effective pair are obtained. taking the length of distinguishable or indistinguishable elements string among the effective pair as heuristic information, an attribute reduction algorithm based on parallel logical or is proposed. finally, the examples and experiments are shown to verify the effectiveness and feasibility of the proposed algorithm.
operating_systems	modern discrete gpus have been the processors of choice for accelerating compute-intensive applications, but using them in large-scale data processing is extremely challenging. unfortunately, they do not provide important i/o abstractions long established in the cpu context, such as memory mapped files, which shield programmers from the complexity of buffer and i/o device management. however, implementing these abstractions on gpus poses a problem: the limited gpu virtual memory system provides no address space management and page fault handling mechanisms to gpu developers, and does not allow modifications to memory mappings for running gpu programs. we implement activepointers, a software address translation layer and paging system that introduces native support for page faults and virtual address space management to gpu programs, and enables the implementation of fully functional memory mapped files on commodity gpus. files mapped into gpu memory are accessed using active pointers, which behave like regular pointers but access the gpu page cache under the hood, and trigger page faults which are handled on the gpu. we design and evaluate a number of novel mechanisms, including a translation cache in hardware registers and translation aggregation for deadlock-free page fault handling of threads in a single warp. we extensively evaluate activepointers on commodity nvidia gpus using microbenchmarks, and also implement a complex image processing application that constructs a photo collage from a subset of 10 million images stored in a 40gb file. the gpu implementation maps the entire file into gpu memory and accesses it via active pointers. the use of active pointers adds only up to 1% to the application 's runtime, while enabling speedups of up to 3.9x over a combined cpu+gpu implementation and 2.6x over a 12-core cpu-only implementation which uses avx vector instructions.
relational_databases	protein secondary structure describe protein construction in terms of regular spatial shapes, including alpha-helices, beta-strands, and loops, which protein amino acid chain can adopt in some of its regions. this information is supportive for protein classification, functional annotation, and 3d structure prediction. the relevance of this information and the scope of its practical applications cause the requirement for its effective storage and processing. relational databases, widely-used in commercial systems in recent years, are one of the serious alternatives honed by years of experience, enriched with developed technologies, equipped with the declarative sql query language, and accepted by the large community of programmers. unfortunately, relational database management systems are not designed for efficient storage and processing of biological data, such as protein secondary structures. in this paper, we present a new search method implemented in the search engine of the pss-sql language. the pss-sql allows formulation of queries against a relational database in order to find proteins having secondary structures similar to the structural pattern specified by a user. in the paper, we will show how the search process can be accelerated by multiple scanning of the segment index and parallel implementation of the alignment procedure using multiple threads working on multiple-core cpus.
computer_programming	this paper presents a substantially simplified axiomatization of map theory and proves the consistency of this axiomatization (called mt) in zfc under the assumption that there exists an inaccessible ordinal. map theory axiomatizes lambda calculus plus hilbert 's epsilon operator. all theorems of zfc set theory including the axiom of foundation are provable in map theory, and if one omits hilbert 's epsilon operator from map theory then one is left with a computer programming language. map theory fulfills church 's original aim of lambda calculus. map theory is suited for reasoning about classical mathematics as well as computer programs. furthermore, map theory is suited for eliminating the barrier between classical mathematics and computer science rather than just supporting the two fields side by side. map theory axiomatizes a universe of ""maps"", some of which are ""wellfounded"". the class of wellfounded maps in map theory corresponds to the universe of sets in zfc. the first axiomatization mt0 of map theory had axioms which populated the class of wellfounded maps, much like the power set axiom along with others populate the universe of zfc. the new axiomatization mt of map theory is ""synthetic"" in the sense that the class of wellfounded maps is defined inside map theory rather than being introduced through axioms. in the paper we define the notions of canonical and non-canonical kappa- and kappa sigma-expansions and prove that if a is the smallest strongly inaccessible ordinal then canonical kappa sigma-expansions are models of mt (which proves the consistency). furthermore, in appendix a, we prove that canonical w-expansions are fully abstract models of the computational part of map theory. (c) 2015 elsevier b.v. all rights reserved.
computer_vision	low-cost, high-performance vision sensors in conjunction with aerial sensing platforms are providing new possibilities for achieving autonomous visual inspection in civil engineering structures. a large volume of images of a given structure can readily be collected for use in visual inspection, overcoming spatial and temporal limitations associated with human-based inspection. although researchers have explored several algorithms and techniques for visionbased inspection in recent decades, a major challenge in past implementations lies in dealing with a high volume of images while only a small fraction of them are important for actual inspection. because processing irrelevant images can generate a significant number of falsepositives, automated visual inspection techniques should be used in coordination with methods to localize relevant regions on the images. when combined, automated visual inspection will be able to meet the objectives and quality of human visual inspection. to enable this technology, we develop and validate a novel automated image localization technique to extract regions of interest (rois) on each of the images before utilizing vision-based damage detection techniques. rois are the portions of an image that contain the physical region of the structure that is targeted for visual interrogation, denoted as the targeted region of interest (tri). rois are computed based on the geometric relationship between the collected images and the tris. analysis of such highly relevant and localized images would enable efficient and reliable visual inspection. we successfully demonstrate the capability of the technique to extract the rois using a full-scale highway sign structure in the case where weld connections serve as the tris.
algorithm_design	designed with the goal of mimicking key features of real hpc workloads, mini-apps have become an important tool for co-design. an investigation of mini-app behavior can provide system designers with insight into the impact of architectures, programming models, and tools on application performance. mini-apps can also serve as a platform for fast algorithm design space exploration, allowing the application developers to evaluate their design choices before significantly redesigning the application codes. consequently, it is prudent to develop a mini-app alongside the full blown application it is intended to represent. in this paper, we present cmt-bone a mini-app for the compressible multiphase turbulence (cmt) application, cmt-nek, being developed to extend the physics of the cesar nek5000 application code. cmt-bone consists of the most computationally intensive kernels of cmt-nek and the communication operations involved in nearest-neighbor updates and vector reductions. the mini-app represents cmt-nek in its most mature state and going forward it will be developed in parallel with the cmt-nek application to keep pace with key new performance impacting changes. we describe these kernels and discuss the role that cmt-bone has played in enabling interdisciplinary collaboration by allowing application developers to work with computer scientists on performance optimization on current architectures and performance analysis on notional future systems.
image_processing	the image thresholding techniques are considered as a must for objects segmentation, compression and target recognition, and they have been widely studied for the last few decades; for example, the multilevel thresholding methods, and as such ( they) render more great challenges for image segmentation techniques that remain computationally more expensive, when their choices of threshold numbers were increased. therefore, our aim was to propose an algorithm based on bayesian theorem and the so-called honeybee- mating algorithm (hbma), called a bayesian honey bee mating algorithm bhbma. it can notonly reduce the computational time and curse of dimensionality, but also can run more reliably and more stably. this enhanced capability was technically accomplished by embedding a new population initialization strategy based on the characteristics of multi-level thresholding technique in pixel-basedintensity images arranged from lower grey levels to higher ones. extensive experiments have shown that our proposed method outperformed other state-of-the-art algorithms empirically in terms of their effectiveness and efficiency, when applying to complex image processing scenario such as automatic target recognition. (c) 2016 elsevier b.v. all rights reserved.
computer_vision	full human body shape scans provide valuable data for a variety of applications including anthropometric surveying, clothing design, human-factors engineering, health, and entertainment. however, the high price, large volume, and difficulty of operating professional 3-d scanners preclude their use in home entertainment. recently, portable low-cost red green blue-depth cameras such as the kinect have become popular for computer vision tasks. however, the infrared mechanism of this type of camera leads to noisy and incomplete depth images. we construct a stereo full-body scanning environment composed of multiple depth cameras and propose a novel registration algorithm. our algorithm determines a segment constrained correspondence for two neighboring views, integrating them using rigid transformation. furthermore, it aligns all of the views based on uniform error distribution. the generated 3-d mesh model is typically sparse, noisy, and even with holes, which makes it lose surface details. to address this, we introduce a geometric and topological fitting prior in the form of a professionally designed high-resolution template model. we formulate a template deformation optimization problem to fit the high-resolution model to the low-quality scan. its solution overcomes the obstacles posed by different poses, varying body details, and surface noise. the entire process is free of body and template markers, fully automatic, and achieves satisfactory reconstruction results.
computer_graphics	shape correspondence is a fundamental problem in computer graphics and vision, with applications in various problems including animation, texture mapping, robotic vision, medical imaging, archaeology and many more. in settings where the shapes are allowed to undergo non-rigid deformations and only partial views are available, the problem becomes very challenging. to this end, we present a non-rigid multi-part shape matching algorithm. we assume to be given a reference shape and its multiple parts undergoing a non-rigid deformation. each of these query parts can be additionally contaminated by clutter, may overlap with other parts, and there might be missing parts or redundant ones. our method simultaneously solves for the segmentation of the reference model, and for a dense correspondence to (subsets of) the parts. experimental results on synthetic as well as real scans demonstrate the effectiveness of our method in dealing with this challenging matching scenario.
bioinformatics	background: viral hepatitis c is an important global health problem that affects about 2.2% of humans. strategies on the control of this hepatotropic virus focused on chemotherapy and surveillance of emerging hcv drug resistant mutants, respectively. hcv genotype 1 response to therapy is one of major interests. the aim of this research was to study the prevalence of resistant associated variants (ravs) in the naive hcv patient candidate for direct acting antiviral (daa) therapy. methods: a total of 70 hcv confirmed patients which referred to hospitals affiliated to iran university of medial sciences, tehran, iran from may 2014 to march 2015 were enrolled in this cross sectional study. after rna extraction, rflp-rt-nested-pcr was performed for hcv genotyping, then some genotypes 1 and 3 strains were used for further amplification of ns5b gene s282t mutation site and purified products were sequenced. bioinformatics software was used for analysis of sequences. results: from a total of 70 hcv patients, 54 were male (mean age (y)+/- sd 35.1 +/- 8.2) and 16 were female (mean age (y)+/- sd 43.4 +/- 10.1); 26 isolates from 1 a, lb and 3a showed that there were no s282t resistant mutants. moreover, 2 (4.8%) had a synonymous point mutation (c to t). statistical analysis did n't found any significant correlation between age, sex and genotype variables. conclusion: finally, it can be concluded that there were no resistant mutants in our hcv genotypes 1 and 3 infected patients and broader scale of studies are required in this area using larger specimens, genotype groups and stages of treatment. (c) 2017 published by elsevier ltd.
operating_systems	hybridcheck is a software package to visualize the recombination signal in large dna sequence data set, and it can be used to analyse recombination, genetic introgression, hybridization and horizontal gene transfer. it can scan large (multiple kb) contigs and whole-genome sequences of three or more individuals. hybridcheck is written in the r software for os x, linux and windows operating systems, and it has a simple graphical user interface. in addition, the r code can be readily incorporated in scripts and analysis pipelines. hybridcheck implements several abba-baba tests and visualizes the effects of hybridization and the resulting mosaic-like genome structure in high-density graphics. the package also reports the following: (i) the breakpoint positions, (ii) the number of mutations in each introgressed block, (iii) the probability that the identified region is not caused by recombination and (iv) the estimated age of each recombination event. the divergence times between the donor and recombinant sequence are calculated using a jc, k80, f81, hky or gtr correction, and the dating algorithm is exceedingly fast. by estimating the coalescence time of introgressed blocks, it is possible to distinguish between hybridization and incomplete lineage sorting. hybridcheck is libre software and it and its manual are free to download from .
computer_programming	currently, educational games are being developed to teach children the basics of computer programming. research and design of such games is usually based on general learning theories. yet, computer programming poses specific types of difficulties to novice programmers. taking into account these particular characteristics and problems of computer programming as a learning content in the design of programming games could allow for producing games that are more suitable to the needs of novice programmers. this paper first reports on a novice programmer problems analysis, to gain insight into learners' specific difficulties. then, a review of existing programming games is presented to investigate how and to which extent these games deal with specific programming problems. the results of these studies aim to contribute to the requirements and ideation phases of a programming game design process, thereby informing a learning content-driven design perspective.
computer_vision	this paper presents a new linear velocity estimator based on the unscented kalman filter and making use of image information aided with inertial measurements. the proposed technique is independent of the scale factor in case of planar observed scene and does not require a priori knowledge of the scene. image moments of virtual objects, i.e. sets of classical image features such as corners collected online, are employed as the sole correcting information to be fed back to the estimator. experimental results performed with a quadrotor equipped with a fisheye camera highlight the potential of the proposed approach.
image_processing	although the importance of cellular forces to a wide range of embryogenesis and disease processes is widely recognized, measuring these forces is challenging, especially in three dimensions. here, we introduce cellfit-3d, a force inference technique that allows tension maps for three-dimensional cellular systems to be estimated from image stacks. like its predecessors, video force microscopy and cellfit, this cell mechanics technique assumes boundary-specific interfacial tensions to be the primary drivers, and it constructs force-balance equations based on triple junction (tj) dihedral angles. the technique involves image processing, segmenting of cells, grouping of cell outlines, calculation of dihedral planes, averaging along three-dimensional tjs, and matrix equation assembly and solution. the equations tend to be strongly overdetermined, allowing indistinct tjs to be ignored and solution error estimates to be determined. application to clean and noisy synthetic data generated using surface evolver gave tension errors of 1.6-7%, and analyses of eight-cell murine embryos gave estimated errors smaller than the 10% uncertainty of companion aspiration experiments. other possible areas of application include morphogenesis, cancer metastasis and tissue engineering. this article is part of the themed issue 'systems morphodynamics: understanding the development of tissue hardware'.
parallel_computing	this paper describes a fully coupled finite element/finite volume approach for simulating field-scale hydraulically driven fractures in three dimensions, using massively parallel computing platforms. the proposed method is capable of capturing realistic representations of local heterogeneities, layering and natural fracture networks in a reservoir. a detailed description of the numerical implementation is provided, along with numerical studies comparing the model with both analytical solutions and experimental results. the results demonstrate the effectiveness of the proposed method for modeling large-scale problems involving hydraulically driven fractures in three dimensions. (c) 2016 the authors. international journal for numerical and analytical methods in geomechanics published by john wiley & sons ltd.
computer_programming	amid growing calls for greater collaboration between journalism and computer programming, this article examines a salient case study that reveals processes of communication, exchange, and work production at the intersection of these social and occupational worlds. we focus on a key stage of the knight-mozilla news technology partnership - namely, an online 'learning lab' through which 60 individuals sought to coordinate around a shared interest in the innovation of journalism through open-source software. drawing on the science and technology studies concepts of trading zones and boundary objects, we explore how distinct understandings about news and technology converged, diverged, and ultimately blended around three thematic ambitions: making news more process-oriented, participatory, and socially curated. this window onto boundary negotiations in journalism provides a glimpse into the future development of news and its norms and values, as programmers and their ethics assume a greater role in the journalistic field - in the very heart of some of its leading institutions.
algorithm_design	it is well known that laser scanner has better accuracy than stereo vision in detecting the distance and velocity of the obstacles, whereas stereo vision can distinguish the objects better than the laser scanner. these advantages of each sensor can be maximized by sensor fusion approach so that the obstacles in front can be detected accurately. in this paper, high-level sensor fusion for the laser scanner and stereo vision is developed for object matching between the sensors. time synchronization, object age, and reordering algorithms are designed for robust tracking of the objects. a time-delay update algorithm is also developed to determine the process time delay of the laser scanner. the expanded laser scanner data at every 1 ms is predicted by kalman filter and is matched with the stereo vision data at every 66 ms. a cost function is formulated to describe the object matching similarity between the sensors, and the best matching candidate is selected for theminimumcost function. the proposedmatching algorithms are verified experimentally in field tests of various maneuvering cases.
cryptography	security features such as privacy and device authentication are required in wireless sensor networks, electronic ids, radio frequency identification tags, and many other applications. these features are provided using cryptography. symmetric key cryptography, where the key is distributed between the communication parties prior to communication, does not provide adequate solution for large scalable systems such as sensor networks. in these cases, public-key cryptography should be used. however, public-key algorithms are typically more computationally intensive than their symmetric key counterparts, which creates difficulties in meeting the strict area, power, and energy requirements. elliptic curve cryptography, because of relatively small operand sizes, can be used to answer the imposed challenges. in this paper, we present a processor for elliptic curve cryptography over gf(2(163)). this processor can perform elliptic curve point multiplication as well as general modular operations. the processor is flexible enough to support multiple cryptographic protocols. the chip is fabricated using umc .13 m 1p8m process, resulting in a core area of 0.54 mm(2). the energy consumption to perform one elliptic curve point multiplication is 5.1j. the design features lightweight countermeasures against side-channel attacks. a security evaluation shows the effectiveness of such countermeasures. copyright (c) 2016 john wiley & sons, ltd.
relational_databases	the rapid growth of digital data storage of medical or health, social media, education and many more in the world has amplified the demand for big data storage, which requires trillion of files having exabytes of data. this growth in data has put up the key question of how we can effectively manage and find the data in the emergent ocean of information. the upcoming demand for data storage in petabytes and exabytes of data has also resulted in putting pressure in organizing the file structure in such a way that retrieval results of searching a keyword should match with the growing pace of data storage. as a result, there is an increase in demand for keyword indexing and searching of file systems. directly implementing searching methodology in file system has resulted in inefficient and inconsistent results. general purpose indexes may not be suitable for file system searching as it relies on relational databases and may limit the scalability and performance. this proposed bilingual framework for english and hindi addresses these problems through a novel approach for indexing and searching queries in large scale file system.
symbolic_computation	in this paper, hirota 's bilinear method is extended to a new kdv hierarchy with variable coefficients. as a result, one-soliton solution, two-soliton solution and three-soliton solutions are obtained, from which the uniform formula of n-soliton solution is derived. thanks to the arbitrariness of the included functions, these obtained solutions possess rich local structural features like the ridge soliton and the concave column soliton. it is shown that the hirota 's bilinear method can be used for constructing multi-soliton solutions of some other hierarchies of nonlinear partial differential equations with variable coefficients.
bioinformatics	germline mutations in pole and pold1 have been shown to cause predisposition to colorectal multiple polyposis and a wide range of neoplasms, early-onset colorectal cancer being the most prevalent. in order to find additional mutations affecting the proofreading activity of these polymerases, we sequenced its exonuclease domain in 155 patients with multiple polyps or an early-onset colorectal cancer phenotype without alterations in the known hereditary colorectal cancer genes. interestingly, none of the previously reported mutations in pole and pold1 were found. on the other hand, among the genetic variants detected, only two of them stood out as putative pathogenic in the pole gene, c. 1359 + 46del71 and c. 1420g >a (p.val474ile). the first variant, detected in two families, was not proven to alter correct rna splicing. contrarily, c. 1420g >a (p. val474ile) was detected in one early-onset colorectal cancer patient and located right next to the exonuclease domain. the pathogenicity of this change was suggested by its rarity and bioinformatics predictions, and it was further indicated by functional assays in schizosaccharomyces pombe. this is the first study to functionally analyze a pole genetic variant outside the exonuclease domain and widens the spectrum of genetic changes in this dna polymerase that could lead to colorectal cancer predisposition.
symbolic_computation	with the help of symbolic computation, the benjamin-bona-mahony (bbm) equation with variable coefficients is presented, which was proposed for the first time by benjamin as the regularized long-wave equation and originally derived as approximation for surface water waves in a uniform channel. by employing the improved (g'/g)-expansion method, the truncated painleve expansion method, we derive new auto-backlund transformation, hyperbolic solutions, a variety of traveling wave solutions, soliton-type solutions and two solitary wave solutions of the bbm equation. these obtained solutions possess abundant structures. the figures corresponding to these solutions are illustrated to show the particular localized excitations and the interactions between two solitary waves.
cryptography	to evaluate the visual quality in visual secret sharing schemes, most of the existing metrics fail to generate fair and uniform quality scores for tested reconstructed images. we propose a new approach to measure the visual quality of the reconstructed image for visual secret sharing schemes. we developed an object detection method in the context of secret sharing, detecting outstanding local features and global object contour. the quality metric is constructed based on the object detection-weight map. the effectiveness of the proposed quality metric is demonstrated by a series of experiments. the experimental results show that our quality metric based on secret object detection outperforms existing metrics. furthermore, it is straightforward to implement and can be applied to various applications such as performing the security test of the visual secret sharing process.
data_structures	with the increasing complexity of both data structures and computer architectures, the performance of applications needs fine tuning in order to achieve the expected runtime execution time. performance tuning is traditionally based on the analysis of performance data. the analysis results may not be accurate, depending on the quality of the data and the applied analysis approaches. therefore, application developers may ask: can we trust the analysis results? this paper introduces our research work in performance optimization of the memory system, with a focus on the cache locality of a shared memory and the memory locality of a distributed shared memory. the quality of the data analysis is guaranteed by using both real performance data acquired at the runtime while the application is running and well-established data analysis algorithms in the field of bioinformatics and data mining. we verified the quality of the proposed approaches by optimizing a set of benchmark applications. the experimental results show a significant performance gain.
computer_graphics	graphic clipping algorithm is a hotspot all the time in computer graphics. based upon non-intersect polygon boundary, a clipping algorithm on vector graphics is proposed in this paper. the proposed algorithm can be divided into three steps. first, eliminating the boundary which has no intersection with vector graphics and calculating effective intersections; second, dividing graphics into several parts; finally, determining whether each part within boundaries and achieving graphic clipping. besides, it can be demonstrated by experiments that compared with traditional algorithm, the proposed algorithm is clear, simple, effective and can be applied widely. furthermore, the proposed algorithm only consumes about 7 seconds in millions of data and the memory consumption nearly unchanged.
image_processing	the behaviors of the keyhole and the weld pool are dynamically-coupled in controlled-pulse plasma arc welding and can be used to indicate weld quality. the vision system was improved to detect the geometries of both the keyhole and weld pool at the backside simultaneously during a whole controlled-pulse keyholing period by one single ccd camera without auxiliary illumination. with the assistance of an appropriate optical filter system, the unchanged aperture and exposure time of the camera was also adopted and can be used under a different current period. an image processing method was then proposed to extract the keyhole boundary in open keyhole status and weld pool boundary for the whole welding process. the influences of current waveform parameters on the welding process are studied and discussed. it was found that dimensions of the weld pool at the backside are determined by the heat input of the plasma arc and keyhole duration. the keyhole moves forward during the welding process while the weld pool maximum width at the backside is located behind the keyhole center.
machine_learning	k nearest neighbor (knn) is one of the basic processes behind various machine learning methods in knn, the relation of a query to a neighboring sample is basically measured by a similarity metric, such as euclidean distance. this process starts with mapping the training dataset onto a one-dimensional distance space based on the calculated similarities, and then labeling the query in accordance with the most dominant or mean of the labels of the k nearest neighbors, in classification or regression issues, respectively. the number of nearest neighbors (k) is chosen according to the desired limit of success. nonetheless, two distinct samples may have equal distances to query but, with different angles in the feature space. the similarity of the query to these two samples needs to be weighted in accordance with the angle going between the query and each of the samples to differentiate between the two distances in reference to angular information. this opinion can be analyzed in the context of dependency and can be utilized to increase the precision of classifier. with this point of view, instead of knn, the query is labeled according to its nearest dependent neighbors that are determined by a joint function, which is built on the similarity and the dependency. this method, therefore, may be called dependent nn (d-nn). to demonstrate d-nn, it is applied to synthetic datasets, which have different statistical distributions, and 4 benchmark datasets, which are pima indian, hepatitis, approximate sinc and casp datasets. results showed the superiority of d-nn in terms of accuracy and computation cost as compared to other employed popular machine learning methods. (c) 2017 elsevier b.v. all rights reserved.
computer_vision	during the last decades photogrammetric computer vision systems have been well established in scientific and commercial applications. recent developments in image-based 3d reconstruction systems have resulted in an easy way of creating realistic, visually appealing and accurate 3d models. we present a fully automated processing pipeline for metric and geo-accurate 3d reconstructions of complex geometries supported by an online feedback method for user guidance during image acquisition. our approach is suited for seamlessly matching and integrating images with different scales, from different view points (aerial and terrestrial), and with different cameras into one single reconstruction. we evaluate our approach based on different datasets for applications in mining, archaeology and urban environments and thus demonstrate the flexibility and high accuracy of our approach. our evaluation includes accuracy related analyses investigating camera self-calibration, georegistration and camera network configuration. (c) 2016 elsevier inc. all rights reserved.
computer_vision	the design of computer-assisted decision (cad) systems for different biomedical imaging scenarios is a challenging task in computer vision. sometimes, this challenge can be attributed to the image acquisition mechanisms since the lack of control on the cameras can create different visualizations of the same imaging site under different rotation, scaling, and illumination parameters, with a requirement to get a consistent diagnosis by the cad systems. moreover, the images acquired from different sites have specific colors, making the use of standard color spaces highly redundant. in this paper, we propose to tackle these issues by introducing novel region-based texture, and color descriptors. the proposed texture features are based on the usage of analytic gabor filters (for compensation of illumination variations) followed by the calculation of first-and second-order statistics of the filter responses and making them invariant using some trivial mathematical operators. the proposed color features are obtained by compensating for the illumination variations in the images using homomorphic filtering followed by a bag-of-words approach to obtain the most typical colors in the images. the proposed features are used for the identification of cancer in images from two distinct imaging modalities, i.e., gastroenterology and dermoscopy. experiments demonstrate that the proposed descriptors compares favorably to several other state-of-the-art methods, elucidating on the effectiveness of adapted features for image characterization.
computer_programming	with the flood of publicly available data, it allows scientists to explore and discover new findings. gene expression is one type of biological data which captures the activity inside the cell. studying gene expression data may expose the mechanisms of disease development. however, with the limitation of computing resources or knowledge in computer programming, many research groups are unable to effectively utilize the data. for about a decade now, various web-based data analysis tools have been developed to analyze gene expression data. different tools were implemented by different analytical approaches, often resulting in different outcomes. this study conducts a comparative study of three existing web-based gene expression analysis tools, namely gene-set activity toolbox (gat), networkanalyst and geo2r using six publicly available cancer data sets. results of our case study show that networkanalyst has the best performance followed by gat and geo2r, respectively.
algorithm_design	converting geographic features (e.g., place names) in map images into a vector format is the first step for incorporating cartographic information into a geographic information system (gis). with the advancement in computational power and algorithm design, map processing systems have been considerably improved over the last decade. however, the fundamental map processing techniques such as color image segmentation, (map) layer separation, and object recognition are sensitive to minor variations in graphical properties of the input image (e.g., scanning resolution). as a result, most map processing results would not meet user expectations if the user does not ""properly"" scan the map of interest, preprocess the map image (e.g., using compression or not), and train the processing system, accordingly. these issues could slow down the further advancement of map processing techniques as such unsuccessful attempts create a discouraged user community, and less sophisticated tools would be perceived as more viable solutions. thus, it is important to understand what kinds of maps are suitable for automatic map processing and what types of results and process-related errors can be expected. in this paper, we shed light on these questions by using a typical map processing task, text recognition, to discuss a number of map instances that vary in suitability for automatic processing. we also present an extensive experiment on a diverse set of scanned historical maps to provide measures of baseline performance of a standard text recognition tool under varying map conditions (graphical quality) and text representations (that can vary even within the same map sheet). our experimental results help the user understand what to expect when a fully or semi-automatic map processing system is used to process a scanned map with certain (varying) graphical properties and complexities in map content. (c) 2016 elsevier ltd. all rights reserved.
operating_systems	recent years have witnessed a processor development trend that integrates central processing unit (cpu) and graphic processing unit (gpu) into a single chip. the integration helps to save some host-device data copying that a discrete gpu usually requires, but also introduces deep resource sharing and possible interference between cpu and gpu. this work investigates the performance implications of independently co-running cpu and gpu programs on these platforms. first, we perform a comprehensive measurement that covers a wide variety of factors, including processor architectures, operating systems, benchmarks, timing mechanisms, inputs, and power management schemes. these measurements reveal a number of surprising observations.we analyze these observations and produce a list of novel insights, including the important roles of operating system (os) context switching and power management in determining the program performance, and the subtle effect of cpu-gpu data copying. finally, we confirm those insights through case studies, and point out some promising directions to mitigate anomalous performance degradation on integrated heterogeneous processors.
relational_databases	this article focuses on testing a path-oriented querying approach to hierarchical data in relational databases. the authors execute a user study to compare the path-oriented approach and traditional sql from two perspectives: correctness of queries and time spent in querying. they also analyze what kinds of errors are typical in path-oriented sql. path-oriented query languages are popular in the context of object-orientation and xml. however, relational databases are the most common paradigm for storing data and sql is most common for manipulating data. when querying hierarchical data in sql, the user must specify join conditions explicitly between hierarchy levels. path-oriented sql is a new alternative for expressing hierarchical queries in relational databases. in the authors' study, the users spent significantly less time in writing path-oriented sql queries and made fewer errors in query formulation.
software_engineering	context: software defect prediction (sdp) is an important task in software engineering. along with estimating the number of defects remaining in software systems and discovering defect associations, classifying the defect-proneness of software modules plays an important role in software defect prediction. several machine-learning methods have been applied to handle the defect-proneness of software modules as a classification problem. this type of yes or no decision is an important drawback in the decision-making process and if not precise may lead to misclassifications. to the best of our knowledge, existing approaches rely on fully automated module classification and do not provide a way to incorporate extra knowledge during the classification process. this knowledge can be helpful in avoiding misclassifications in cases where system modules cannot be classified in a reliable way. objective:we seek to develop a sdp method that (i) incorporates a reject option in the classifier to improve the reliability in the decision-making process; and (ii) makes it possible postpone the final decision related to rejected modules for an expert analysis or even for another classifier using extra domain knowledge. method: we develop a sdp method called rejoelm and its variant, irejoelm. both methods were built upon the weighted extreme learning machine (elm) with reject option that makes it possible postpone the final decision of non-classified modules, the rejected ones, to another moment. while rejoelm aims to maximize the accuracy for a rejection rate, irejoelm maximizes the f-measure. hence, irejoelm becomes an alternative for classification with reject option for imbalanced datasets. results: rejoem and irejoelm are tested on five datasets of source code metrics extracted from real world open-source software projects. results indicate that rejoelm has an accuracy for several rejection rates that is comparable to some state-of-the-art classifiers with reject option. although irejoelm shows lower accuracies for several rejection rates, it clearly outperforms all other methods when the f-measure is used as a performance metric. conclusion: it is concluded that rejoelm is a valid alternative for classification with reject option problems when classes are nearly equally represented. on the other hand, irejoelm is shown to be the best alternative for classification with reject option on imbalanced datasets. since sdp problems are usually characterized as imbalanced learning problems, the use of irejoelm is recommended. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	planar shape interpolation is a classic problem in computer graphics. we present a novel shape interpolation method that blends c-infinity planar harmonic mappings represented in closed-form. the intermediate mappings in the blending are guaranteed to be locally injective c-infinity harmonic mappings, with conformal and isometric distortion bounded by that of the input mappings. the key to the success of our method is the fact that the blended differentials of our interpolated mapping have a simple closed-form expression, so they can be evaluated with unprecedented efficiency and accuracy. moreover, in contrast to previous approaches, these differentials are integrable, and result in an actual mapping without further modification. our algorithm is embarrassingly parallel and is orders of magnitude faster than state-of-the-art methods due to its simplicity, yet it still produces mappings that are superior to those of existing techniques due to its guaranteed bounds on geometric distortion.
software_engineering	most of the research effort in the area of software analysis is focused on the perspective of the developer (as in ""software developing company"") and the ways how the software development process could be improved. however, that is not the only type of software assessment common in the industry. there are also assessments that are commissioned by other parties, such as the primary recipients of the software solutions or courts dealing with legal cases that are related to software products or services. this work presents one such case-study that was performed for a public administration in italy. the paper describes the assessment itself and also points out the need for more focused research by providing a comparison between developer-oriented and customer-oriented assessment types. (c) 2015 elsevier inc. all rights reserved.
network_security	a network traffic detection model based on swarm intelligent optimization neural network algorithm is proposed in this paper. qapso algorithm is used to optimize the basis function center and base function width of rbf neural network, and the connection weights of the output layer and the hidden layer as well. this paper analyzes the detection model studied in this paper by an example, and use the collected data to train the network traffic identification system and test its performance. the comparison between the proposed method and the conventional pso algorithm based on the hpso algorithm shows that the proposed method has faster recognition speed and better recognition accuracy, and avoids the problem of falling into the local optimal solution. situation.
computer_graphics	within education, concepts such as distance learning, and open universities, are now becoming more widely used for teaching and learning. however, due to the nature of the subject domain, the teaching of science, technology, and engineering are still relatively behind when using new technological approaches (particularly for online distance learning). the reason for this discrepancy lies in the fact that these fields often require laboratory exercises to provide effective skill acquisition and hands-on experience. often it is difficult to make these laboratories accessible for online access. either the real lab needs to be enabled for remote access or it needs to be replicated as a fully software-based virtual lab. we argue for the latter concept since it offers some advantages over remotely controlled real labs, which will be elaborated further in this paper. we are now seeing new emerging technologies that can overcome some of the potential difficulties in this area. these include: computer graphics, augmented reality, computational dynamics, and virtual worlds. this paper summarizes the state of the art in virtual laboratories and virtual worlds in the fields of science, technology, and engineering. the main research activity in these fields is discussed but special emphasis is put on the field of robotics due to the maturity of this area within the virtual-education community. this is not a coincidence; starting from its widely multidisciplinary character, robotics is a perfect example where all the other fields of engineering and physics can contribute. thus, the use of virtual labs for other scientific and non-robotic engineering uses can be seen to share many of the same learning processes. this can include supporting the introduction of new concepts as part of learning about science and technology, and introducing more general engineering knowledge, through to supporting more constructive (and collaborative) education and training activities in a more complex engineering topic such as robotics. the objective of this paper is to outline this problem space in more detail and to create a valuable source of information that can help to define the starting position for future research. (c) 2016 elsevier ltd. all rights reserved.
relational_databases	the concept of homogeneous integrity constraints in database systems is introduced. the metric space for the state of database schemes with homogeneous integrity constraints is established. it is shown that many common integrity constraints that are used in practice satisfy the condition of homogeneity of integrity constraints. such constraints include key, referential, joint, and many-to-many integrity constraints, etc. the proposed constructs allow one to design metric spaces for the state of database schemes and enable the study of integrity constraints of databases based on the methods and concepts of ""continuous mathematics.
distributed_computing	multiscale, multi-physics applications are central to solve the increasing number of important scientific challenges. computationally speaking, the difficulty is to combine high performance computing with the need to couple various codes or solvers, each representing a different scale or a different physical process. in this paper, we present muscle-hpc a new hpc implementation of muscle-2, a previously developed multiscale coupling library and environment. we present its design and implementation and we demonstrate its advantages compared to muscle-2. we conduct a performance comparison through a tightly coupled mpi application use-case. our results indicate that using muscle-hpc to couple submodels within the same hpc cluster can lead to better computing performance comparable to a native mpi execution and can, thus, reduce the coupling overhead. (c) 2016 elsevier b.v. all rights reserved.
data_structures	background: data capture for clinical registries or pilot studies is often performed in spreadsheet-based applications like microsoft excel or ibm spss. usually, data is transferred into statistic software, such as sas, r or ibm spss statistics, for analyses afterwards. spreadsheet-based solutions suffer from several drawbacks: it is generally not possible to ensure a sufficient right and role management; it is not traced who has changed data when and why. therefore, such systems are not able to comply with regulatory requirements for electronic data capture in clinical trials. in contrast, electronic data capture (edc) software enables a reliable, secure and auditable collection of data. in this regard, most edc vendors support the cdisc odm standard to define, communicate and archive clinical trial meta-and patient data. advantages of edc systems are support for multi-user and multicenter clinical trials as well as auditable data. migration from spreadsheet based data collection to edc systems is labor-intensive and time-consuming at present. hence, the objectives of this research work are to develop a mapping model and implement a converter between the ibm spss and cdisc odm standard and to evaluate this approach regarding syntactic and semantic correctness. results: a mapping model between ibm spss and cdisc odm data structures was developed. spss variables and patient values can be mapped and converted into odm. statistical and display attributes from spss are not corresponding to any odm elements; study related odm elements are not available in spss. the s2o converting tool was implemented as command-line-tool using the spss internal java plugin. syntactic and semantic correctness was validated with different odm tools and reverse transformation from odm into spss format. clinical data values were also successfully transformed into the odm structure. conclusion: transformation between the spreadsheet format ibm spss and the odm standard for definition and exchange of trial data is feasible. s2o facilitates migration from excel-or spss-based data collections towards reliable edc systems. thereby, advantages of edc systems like reliable software architecture for secure and traceable data collection and particularly compliance with regulatory requirements are achievable.
computer_vision	rgb-d human action recognition is a very active research topic in computer vision and robotics. in this paper, an action recognition method that combines gradient information and sparse coding is proposed. first of all, we leverage depth gradient information and distance of skeleton joints to extract coarse depth-skeleton (ds) feature. then, the sparse coding and max pooling are combined to refine the coarse ds feature. finally, the random decision forests (rdf) is utilized to perform action recognition. experimental results on three public datasets show the superior performance of our method.
distributed_computing	extreme hydrometeorological events such as flash floods have caused considerable loss of life and damage to infrastructure over recent years. flood events in the mediterranean region between 1990 and 2006 caused over 4,500 fatalities and cost over (sic)29 billion in damage, with italy one of the worst affected countries. the distributed computing infrastructure for hydro-meteorology (drihm) project is a european initiative aiming at providing an open, fully integrated escience environment for predicting, managing, and mitigating the risks related to such extreme weather phenomena. incorporating both modeled and observational data sources, it enables seamless access to a set of computing resources with the objective of providing a collection of services for performing experiments with numerical models in meteorology, hydrology, and hydraulics. the purpose of this article is to demonstrate how this flexible modeling architecture has been constructed using a set of standards including the netcdf and waterml2 file formats, in-memory coupling with openmi, controlled vocabularies such as cf standard names, iso19139 metadata, and a model map (metadata, adaptors, portability) gateway concept for preparing numerical models for standardized use. hydraulic results, including the impact to buildings and hazards to people, are given for the use cases of the severe and fatal flash floods, which occurred in genoa, italy in november 2011 and october 2014.
machine_learning	in the literature, a number of approaches have been proposed for learning grapheme-to-phoneme (g2p) relationship and inferring pronunciations. in this letter, we present a novel multistream framework for g2p conversion, where various machine learning techniques providing different estimates of probability of phonemes given graphemes can be effectively combined during pronunciation inference. more precisely, analogous to multistream automatic speech recognition, the framework involves obtaining different streams of estimates of probability of phonemes given graphemes, combining them based on probability combination rules, and inferring pronunciations by decoding the probabilities resulting after combination. we demonstrate the potential of the proposed approach by combining probabilities estimated by the state-of-the-art conditional random field-based g2p conversion approach and acoustic data-driven g2p conversion approach in the kullback-leibler-divergence-based hidden markov model framework on the phonebook 600-word task.
symbolic_computation	in this paper, the (3+1)-dimensional jimbo-miwa equation is solved by fan sub-equation method with improved algorithms. as a result, many new and more general travelling wave solutions are obtained including kink-shaped soliton solutions, rational solutions, triangular periodic solutions, jacobi and weierstrass doubly periodic wave solutions. at a certain limit condition, the obtained jacobi elliptic periodic wave solutions can degenerate into soliton solutions. it is shown that the improved algorithms of fan sub-equation method can lead to such solutions with external linear functions possessing two remarkable evolutionary properties: (i) the wave propagation is skew; (ii) the amplitude enlarges along with the increasing time.
cryptography	conditional differential cryptanalysis on nfsr-based cryptosystems was first proposed by knellwolf et al. in asiacrypt 2010 and has been successfully used to attack reduced variants of grain v1. in this paper, we greatly improve conditional differential attacks on grain v1 in the following four aspects. first, a new differential engine is derived to correctly track the differential trails of grain v1. second, we propose a new difference-searching strategy which serves to find suitable differences for the conditional differential attack on a given reduced variant of grain v1. third, a highly iv-saving condition-imposing strategy is presented. last, we propose a further bias-increasing strategy. in particular, the improvements on the difference-searching strategy and the condition-imposing strategy are crucial to mount conditional differential attacks on the variants of grain v1 with more than 106 rounds. it is shown that the improved conditional differential attacks could retrieve 31 distinct secret key expressions for 107-round grain v1 and could retrieve 15 distinct secret key expressions for 110-round grain v1. both the attacks succeed with constant probabilities. thus far, our results are the best known for the reduced variants of grain v1 as far as the number of rounds attacked is concerned.
computer_programming	unlike conventional taught learning, video games are very successful at keeping players constantly motivated and engaged on a set of tasks for many hours without apparent loss of focus. additionally, when playing, gamers solve complex problems without experiencing the fatigue or frustration, which would normally accompany a comparable learning task. any methods able to deliver deep learner engagement are naturally of interest to the academic community, thus resulting in an increasing interest in adopting gamification - the integration of gaming elements, mechanics, and frameworks into non-game situations and scenarios - as a means to drive student engagement and improve information retention. however, its application to education has been a challenging task, as attempts have generally been restricted to a one-dimensional approach, such as transposing a trivial reward system onto existing teaching material. the empirical evidence presented in this paper suggests that a gamified, multi-dimensional, problem-based learning approach may yield improved outcomes even when applied to a very complex and traditionally dry task like the teaching of computer programming. this quasi-experimental study employed a real time sequence of scored quizzes, instructor feedback, and live coding to deliver a fully interactive learning experience. by using a combination of the classroom version of the tv game show ""who wants to be a millionaire?"", the ""kahoot!"" classroom response system (crs), and codecademy 's online interactive platform on a python programming course, students were allowed to experience multiple interlocking methods similar to what would be found in a top quality game experience. empirical data on learning outcomes from the gamified group were compared with a control group that followed a traditional learning path, which had been used during previous cohorts. whilst this was a relatively small study, the results were quite interesting in a number of key metrics, including attendance, downloading of course material, and final grades.
algorithm_design	if we apply the developed local polar edge detection method, or lped method, to a binary image (with each pixel being either black or white), we can obtain the boundary points of all objects embedded in the more randomly distributed noise background in sub-milli-second time. then we can apply our newly developed grouping or clustering algorithm to separate the boundary points for all objects into individual-object, boundary-point groups. then we can apply our fast identification-and-tracking technique to automatically identify each object by its unique geometry shape and track its movement simultaneously for n objects like we did before for two objects. this paper will concentrate at the algorithm design of this superfast grouping technique. it is not like the classical combinatorial clustering algorithm in which the computation time increases exponentially with the number of points to be clustered. it is a linear time grouping method in which the grouping time increases only linearly with the number of the total points to be grouped. the total time for automatic grouping of 100-200 boundary points into separated object boundary groups is about 10 to 50 milli-seconds live computer experiments will be demonstrated in the presentation.
network_security	the cloud computing environment has expanded considerably with the rapid advancement of related technologies. although cloud computing is convenient for users, detecting and preventing possible security breaches remains an unsolved problem. security logs are critical data that indicate events in an operating system or other software, and these data are stored through heterogeneous machines such as network security devices, server systems, and database management systems (dbms). however, existing methods can create problems for efficient analysis because of large-scale heterogeneous security logs in the cloud-computing environment. therefore, because cloud computing provides various services to users, an efficient integration method of security logs must be developed. this study proposes a nosql-based method to collect and integrate security logs using mapreduce. our study shows that log data were reduced by more than 87% when integrating duplicate large-scale security logs. this proposed method provides faster data storage than conventional dbms and is more effective.
parallel_computing	in this paper, we present a bio-inspired parallel implementation of a solution of the problem of looking for the representative geometrical objects of the homology groups in a binary 2d image (extended-hgb2i problem), which is an extended version of a well-known problem in homology theory. in particular, given a binary 2d image, all black connected components and the representative curves of the holes of these components are obtained and labelled. to this aim, a new technique for labelling the connected components of a binary image is presented. in order to compute the solution, the formal framework uses techniques from membrane computing and the implementation has been done in a hardware architecture called compute unified device architecture (cuda). the computational complexity of the proposed solution is o(m) with respect to the input (image) size m similar to n(2). finally, some examples and applications are also presented.
data_structures	the way data structures organize data is often a function of the sequence of past operations. the organization of data is referred to as the data structure 's state, and the sequence of past operations constitutes the data structure 's history. a data structure state can, therefore, be used as an oracle to derive information about its history. for history-sensitive applications, such as privacy in e-voting, it is imperative to conceal historical information contained within data structure states. data structure history can be hidden by making data structures history independent. in this paper, we explore how to achieve history independence (hi). we observe that the current hi notions are significantly limited in number and scope. there are two existing notions of hi: 1) weak hi (whi) and 2) strong hi (shi). whi does not protect against insider adversaries, and shi mandates canonical representations, resulting in inefficiency. we postulate the need for a broad, encompassing notion of hi, which can capture whi, shi, and a broad spectrum of new hi notions. to this end, we introduce delta hi, a generic game-based framework that is malleable enough to accommodate the existing and new hi notions. as an essential step toward formalizing delta hi, we explore the concepts of abstract data types, data structures, machine models, memory representations, and hi. finally, to bridge the gap between theory and practice, we outline a general recipe for building end-to-end, history-independent systems and demonstrate the use of the recipe in designing two history-independent file systems.
bioinformatics	transposable elements (tes) constitute the most dynamic and the largest component of large plant genomes: for example, 80% to 90% of the maize genome and the wheat genome may be tes. de novo te annotation is therefore a computational challenge, and we investigated, using current tools in the repet package, new strategies to overcome the difficulties. we tested our methodological developments on the sequence of the chromosome 3b of the hexaploid wheat; this chromosome is similar to 1 gb, one of the ""fattest"" genomes ever sequenced. we successfully established various strategies for annotating tes in such a complex dataset. our analyses show that all of our strategies can overcome the current limitations for de novo te discovery in large plant genomes. relative to annotation based on a library of known tes, our de novo approaches improved genome coverage (from 84% to 90%), and the number of full length annotated copies from 14 830 to 15905. we also developed two new metrics for qualifying te annotation: nte50 involves measuring the number, and lte50 the smallest sizes of annotations that cover 50% of the genome. nte50 decreased the number of annotations from 124868 to 93633 and lte50 increased it from 1839 to 2659. this work shows how to obtain comprehensive and high-quality automatic te annotation for a number of economically and agronomically important species.
image_processing	a mathematica application providing the user with a graphical interface (gui) is presented and published, which can be used to interactively explore image filtering and segmentation methods to analyse variously shaped particles in a microscopic image. the application functionality is designed around mathematica 's in-built image processing capability with custom designed functions specialized at segmenting greyscale microscope images. the main contribution is a specially designed gui which allows the characterization of segmented particles based on their morphological properties, with focus given to differentiation of the shapes of the segmented particles. the application provides a convenient way of navigating through the myriad of ways to analyse particles in micrographs. (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	this paper presents a new hybrid modeling technique for the efficient simulation of guided wave generation, propagation, and interaction with damage in complex composite structures. a local finite element model is deployed to capture the piezoelectric effects and actuation dynamics of the transmitter, while the global domain wave propagation and interaction with structural complexity (structure features and damage) are solved utilizing a local interaction simulation approach (lisa). this hybrid approach allows the accurate modeling of the local dynamics of the transducers and keeping the lisa formulation in an explicit format, which facilitates its readiness for parallel computing. the global lisa framework was extended through the 3d kelvin-voigt viscoelasticity theory to include anisotropic damping effects for composite structures, as an improvement over the existing lisa formulation. the global lisa framework was implemented using the compute unified device architecture running on graphic processing units. a commercial preprocessor is integrated seamlessly with the computational framework for grid generation and material property allocation to handle complex structures. the excitability and damping effects are successfully captured by this hybrid model, with experimental validation using the scanning laser doppler vibrometry. to demonstrate the capability of our hybrid approach for complex structures, guided wave propagation and interaction with a delamination in a composite panel with stiffeners is presented.
software_engineering	the mobile app market continues to grow at a tremendous rate. the market provides a convenient and efficient distribution mechanism for updating apps. app developers continuously leverage such mechanism to update their apps at a rapid pace. the mechanism is ideal for publishing emergency updates (i.e., updates that are published soon after the previous update). in this paper, we study such emergency updates in the google play store. examining more than 44,000 updates of over 10,000 mobile apps in the google play store, we identify 1,000 emergency updates. by studying the characteristics of such emergency updates, we find that the emergency updates often have a long lifetime (i.e., they are rarely followed by another emergency update). updates preceding emergency updates often receive a higher ratio of negative reviews than the emergency updates. however, the release notes of emergency updates rarely indicate the rationale for such updates. hence, we manually investigate the binary changes of several of these emergency updates. we find eight patterns of emergency updates. we categorize these eight patterns along two categories ""updates due to deployment issues"" and ""updates due to source code changes"". we find that these identified patterns of emergency updates are often associated with simple mistakes, such as using a wrong resource folder (e.g., images or sounds) for an app. we manually examine each pattern and document its causes and impact on the user experience. app developers should carefully avoid these patterns in order to improve the user experience.
machine_learning	this work develops a method for calibrating a crystal plasticity model to the results of discrete dislocation (dd) simulations. the crystal model explicitly represents junction formation and annihilation mechanisms and applies these mechanisms to describe hardening in hexagonal close packed metals. the model treats these dislocation mechanisms separately from elastic interactions among populations of dislocations, which the model represents through a conventional strength-interaction matrix. this split between elastic interactions and junction formation mechanisms more accurately reproduces the dd data and results in a multi-scale model that better represents the lower scale physics. the fitting procedure employs concepts of machine learning-feature selection by regularized regression and crossvalidation- to develop a robust, physically accurate crystal model. the work also presents a method for ensuring the final, calibrated crystal model respects the physical symmetries of the crystal system. calibrating the crystal model requires fitting two linear operators: one describing elastic dislocation interactions and another describing junction formation and annihilation dislocation reactions. the structure of these operators in the final, calibrated model reflect the crystal symmetry and slip system geometry of the dd simulations.
relational_databases	provision of an uniform query interface facade to access the health-care data present in multiple data-stores being used autonomously by various departments of an hospital, will empower the health-care community to make better decisions and conclusions. one naive approach for implementation of such system is to use any one popular database to store and process all the data pertaining to all the departments of an hospital. nonetheless, any single data-model cannot efficiently store and process multitude of data generated by healthcare institutions. for example, not all data fit well into row-column format of traditional relational databases. however, modern nosql data-stores allow data to be stored in a form closer to their actual representation and usage. storage and retrieval of data from disparate data-stores has its challenges and issues like dealing with multiple querying languages, understanding different data modeling techniques and, creation and maintenance of knowledge base of data (kbod). we have developed an intelligent information integration solution named polyglot-persistent healthcare information system-polyglothis which makes use of cooperating agents enabling health-care professionals to retrieve data from heterogeneous data-stores. polyglothis uses multiple data-stores for storage and processing of the his data. the rationale is to select the most appropriate data storage technology that meets the specific requirements of each module of the his. architecture of polyglothis consists primarily of multiple cooperative agents. capabilities and contents of data-stores are stored and inferred using datalog, a declarative logic programming language used to store set of facts and rules. design and working principle of polyglothis is illustrated with the help of a running example. performance analysis of the implemented system showcase that very less latency has been induced by the system.
parallel_computing	to accurately construct the topographic information of a six-legged walking robot in real time, this study proposes a stereo matching algorithm that can conduct disparity estimation on each pixel by using the bayesian posterior probability model based on gpu-accelerated parallel processing. in the proposed algorithm, supporting points construct a disparity space to obtain the prior distribution probability density of each pixel and then substitute it into the bayesian posterior probability model to establish the energy function of the disparity. the estimated disparity value of the unknown pixel can be obtained by minimizing the energy function. by performing a consistency check on the left and right sides of an image, the mismatching pixel can be eliminated. according to the disparity value of the supporting point, the disparity filling of the mismatching area can be achieved by applying the adaptive weight method on the basis of cross extending to obtain the accurate density of the disparity map. parallel computing in each stage of the proposed algorithm is performed by using the compute unified device architecture to reduce the running time. experimental results show that the proposed algorithm has good robustness for different illuminations and texture curved surface reconstruction. the algorithm can also adapt to the fast matching of images in different sizes and reconstruct the disparity map of scenes in real time under the resolution ratio of 640 x 480. the stereoscopic vision test board is employed to construct the disparity map of real scenes and verify the practical application effect of the algorithm. good experiment effect is achieved. (c) 2015 elsevier b.v. all rights reserved.
computer_vision	video object tracking represents a very important computer vision domain. in this paper, a perceptual hashing based template-matching method for object tracking is proposed to efficiently track objects in challenging video sequences. in the tracking process, we first apply three existing basic perceptual hashing techniques to visual tracking, namely average hash (ahash), perceptive hash (phash) and difference hash (dhash). compared with previous tracking methods such as mean-shift or compressive tracking (ct), perceptual hashing-based tracking outperforms in terms of efficiency and accuracy. in order to further improve the accuracy of object localization and the robustness of tracking, we propose laplace-based hash (lhash) and laplace-based difference hash (ldhash). by qualitative and quantitative comparison with some representative tracking algorithms, experimental results show that our improved perceptual hashing-based tracking algorithms perform favorably against the state-of-the-art algorithms under various challenging environments in terms of time cost, accuracy and robustness. since our improved perceptual hashing can be a compact and efficient representation of objects, it can be further applied to fusing with depth information for more robust rgb-d video tracking.
image_processing	visible light positioning (vlp) is widely believed to be a cost-effective answer to the growing demand for real-time indoor positioning. however, due to the high computational cost of image processing, most existing vlc-based systems fail to deliver satisfactory performance in terms of positioning speed and accuracy, both of which are crucial for the performance of indoor navigation. this paper proposes a novel vlp solution that provides accurate and high-speed indoor navigation via the designs of an elaborate flicker-free line coding scheme and a lightweight image processing algorithm. in addition, this solution has the advantage of supporting flicker mitigation and dimming, which are important for illumination. an android-based system prototype has been developed for field tests on an off-the-shelf smartphone. experimental results show that it supports indoor positioning for users moving at a speed of up to 18 km/h. in addition, it can achieve a high accuracy of 7.5 cm, and the computational time is reduced to 22.7 ms for single-luminaire and to 35.7 ms for dual-luminaries, respectively.
image_processing	a new vision-based system is designed for the identification of chatter vibration. chatter vibration is a major obstacle in cnc machining as it causes bad surface finishes and increases tooling damage and costs. this research proposes the use of machine vision for the detection of chatter vibration frequencies during high-speed milling operations. the vision system utilizes digital image processing and texture analysis to determine the frequencies of the chatter marks on the surface of a machined workpiece. the developed system provides a solution to machinists at ease without any expensive measuring equipment. the vision system was verified and compared with other chatter detection methods by identifying and creating tests based on the dynamics of a machine tool.
cryptography	the session initiation protocol (sip) is a signaling protocol widely applied in the world of multimedia communication. numerous sip authenticated key agreement schemes have been proposed with the purpose of ensuring security communication. farash recently put forward an enhancement employing smart cards counted on zhang et al. 's scheme. in this study, we observe that the enhanced scheme presented by farash has also some security pitfalls, such as disclosure of user identity, lack of a pre-authentication in the smart card and vulnerability to key-compromise masquerading attack which results in an off-line guessing attack. we then propose an anonymous modified scheme with elliptic curve cryptography to eliminate the security leakages of the scheme proposed by farash. we demonstrate that our scheme is immune to different kinds of attacks including attacks involved in farash 's scheme. we mention burrows-abadi-needham logic for completeness of the proposed scheme. also, we compare the performance of our scheme with its predecessor schemes and the comparative results shows that it perfectly satisfies the needs of sip.
computer_programming	lateral diffusion and compartmentalization of plasma membrane proteins are tightly regulated in cells and thus, studying these processes will reveal new insights to plasma membrane protein function and regulation. recently, k-space image correlation spectroscopy (kics)(1) was developed to enable routine measurements of diffusion coefficients directly from images of fluorescently tagged plasma membrane proteins, that avoided systematic biases introduced by probe photophysics. although the theoretical basis for the analysis is complex, the method can be implemented by nonexperts using a freely available code to measure diffusion coefficients of proteins. kics calculates a time correlation function from a fluorescence microscopy image stack after fourier transformation of each image to reciprocal (k-) space. subsequently, circular averaging, natural logarithm transform and linear fits to the correlation function yields the diffusion coefficient. this paper provides a step-by-step guide to the image analysis and measurement of diffusion coefficients via kics. first, a high frame rate image sequence of a fluorescently labeled plasma membrane protein is acquired using a fluorescence microscope. then, a region of interest (roi) avoiding intracellular organelles, moving vesicles or protruding membrane regions is selected. the roi stack is imported into a freely available code and several defined parameters (see method section) are set for kics analysis. the program then generates a ""slope of slopes"" plot from the k-space time correlation functions, and the diffusion coefficient is calculated from the slope of the plot. below is a step-by-step kics procedure to measure the diffusion coefficient of a membrane protein using the renal water channel aquaporin-3 tagged with egfp as a canonical example.
software_engineering	fuzzy systems have been used widely thanks to their ability to successfully solve a wide range of problems in different application fields. however, their replication and application require a high level of knowledge and experience. furthermore, few researchers publish the software and/or source code associated with their proposals, which is a major obstacle to scientific progress in other disciplines and in industry. in recent years, most fuzzy system software has been developed in order to facilitate the use of fuzzy systems. some software is commercially distributed, but most software is available as free and open-source software, reducing such obstacles and providing many advantages: quicker detection of errors, innovative applications, faster adoption of fuzzy systems, etc. in this paper, we present an overview of freely available and open-source fuzzy systems software in order to provide a well-established framework that helps researchers to find existing proposals easily and to develop well-founded future work. to accomplish this, we propose a two-level taxonomy, and we describe the main contributions related to each field. moreover, we provide a snapshot of the status of the publications in this field according to the isi web of knowledge. finally, some considerations regarding recent trends and potential research directions are presented.
distributed_computing	while cloud computing led the path towards a revolutionary change in the modern day computing aspects, further developments gave way to the internet of things and its own range of highly interactive applications. while such a paradigm is more distributed in reach, it also brings forth its own set of challenges in the form of latency sensitive applications, where a quick response highly contributes to efficient usage and qos (quality of service). fog computing, which is the answer to all such challenges, is rapidly changing the distributed computing landscape by extending the cloud computing paradigm to include widespread resources located at the network edge. while the fog paradigm makes use of edge-ward devices capable of computing, networking and storage, one of the key impending challenges is to determine where to place the data analytic operators for maximum efficiency and least costs for the network and its traffic, the efficient algorithmic solution to which we seek to propose by way of this work underway.
software_engineering	software product lines (spl) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. an essential activity in spl is variability management, i. e., defining and managing commonality and variability among member products. due to the large scale and complexity of today 's software-intensive systems, variability management has become increasingly complex to conduct. accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining spls. while several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. in this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools' characteristics, maturity, and the challenges in the field. we conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. it was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.
software_engineering	this paper provides an overview of the state of the art technologies for software development in cloud environments. the surveyed systems cover the whole spectrum of cloud-based development including integrated programming environments, code repositories, software modeling, composition and documentation tools, and application management and orchestration. in this work we evaluate the existing cloud development ecosystem based on a wide number of characteristics like applicability (e.g. programming and database technologies supported), productivity enhancement (e.g. editor capabilities, debugging tools), support for collaboration (e.g. repository functionality, version control) and post-development application hosting and we compare the surveyed systems. the conducted survey proves that software engineering in the cloud era has made its initial steps showing potential to provide concrete implementation and execution environments for cloud-based applications. however, a number of important challenges need to be addressed for this approach to be viable. these challenges are discussed in the article, while a conclusion is drawn that although several steps have been made, a compact and reliable solution does not yet exist.
computer_graphics	many recent applications of computer graphics and human computer interaction have adopted both colour cameras and depth cameras as input devices. therefore, an effective calibration of both types of hardware taking different colour and depth inputs is required. our approach removes the numerical difficulties of using non-linear optimization in previous methods which explicitly resolve camera intrinsics as well as the transformation between depth and colour cameras. a matrix of hybrid parameters is introduced to linearize our optimization. the hybrid parameters offer a transformation from a depth parametric space (depth camera image) to a colour parametric space (colour camera image) by combining the intrinsic parameters of depth camera and a rotation transformation from depth camera to colour camera. both the rotation transformation and intrinsic parameters can be explicitly calculated from our hybrid parameters with the help of a standard qr factorisation. we test our algorithm with both synthesized data and real-world data where ground-truth depth information is captured by microsoft kinect. the experiments show that our approach can provide comparable accuracy of calibration with the state-of-the-art algorithms while taking much less computation time (1/50 of herrera 's method and 1/10 of raposo 's method) due to the advantage of using hybrid parameters.
image_processing	background and objectivebreast cancer is one of the most common cancers, and recognized as the third leading cause of mortality in women. optical coherence tomography (oct) enables three dimensional visualization of biological tissue with micrometer level resolution at high speed, and can play an important role in early diagnosis and treatment guidance of breast cancer. in particular, ultra-high resolution (uhr) oct provides images with better histological correlation. this paper compared uhr oct performance with standard oct in breast cancer imaging qualitatively and quantitatively. automatic tissue classification algorithms were used to automatically detect invasive ductal carcinoma in ex vivo human breast tissue. study design/materials and methodshuman breast tissues, including non-neoplastic/normal tissues from breast reduction and tumor samples from mastectomy specimens, were excised from patients at columbia university medical center. the tissue specimens were imaged by two spectral domain oct systems at different wavelengths: a home-built ultra-high resolution (uhr) oct system at 800nm (measured as 2.72m axial and 5.52m lateral) and a commercial oct system at 1,300nm with standard resolution (measured as 6.5m axial and 15m lateral), and their imaging performances were analyzed qualitatively. using regional features derived from oct images produced by the two systems, we developed an automated classification algorithm based on relevance vector machine (rvm) to differentiate hollow-structured adipose tissue against solid tissue. we further developed b-scan based features for rvm to classify invasive ductal carcinoma (idc) against normal fibrous stroma tissue among oct datasets produced by the two systems. for adipose classification, 32 uhr oct b-scans from 9 normal specimens, and 28 standard oct b-scans from 6 normal and 4 idc specimens were employed. for idc classification, 152 uhr oct b-scans from 6 normal and 13 idc specimens, and 104 standard oct b-scans from 5 normal and 8 idc specimens were employed. resultswe have demonstrated that uhr oct images can produce images with better feature delineation compared with images produced by 1,300nm oct system. uhr oct images of a variety of tissue types found in human breast tissue were presented. with a limited number of datasets, we showed that both oct systems can achieve a good accuracy in identifying adipose tissue. classification in uhr oct images achieved higher sensitivity (94%) and specificity (93%) of adipose tissue than the sensitivity (91%) and specificity (76%) in 1,300nm oct images. in idc classification, similarly, we achieved better results with uhr oct images, featured an overall accuracy of 84%, sensitivity of 89% and specificity of 71% in this preliminary study. conclusionin this study, we provided uhr oct images of different normal and malignant breast tissue types, and qualitatively and quantitatively studied the texture and optical features from oct images of human breast tissue at different resolutions. we developed an automated approach to differentiate adipose tissue, fibrous stroma, and idc within human breast tissues. our work may open the door toward automatic intraoperative oct evaluation of early-stage breast cancer. (c) 2017 wiley periodicals, inc.
network_security	optimization of power system restoration path is a key issue to the system restoration following a significant disruption, such as the northeast blackout of 2003 in the united states and canada. the restoration path optimization problem (rpop) is to calculate the shortest restoration path between specified nodes, while subject to network security constraints. the rpop is normally modeled as a large-scale mixed integer nonlinear programming, including both routing components and the nonlinear steady-state power flow equations. intelligent algorithm is widely used to solve this complicated problem due to its excellent optimization capability, but existing research concern about the generate method of initial population. in this paper, an orthogonal genetic algorithm is adopted to achieve the optimal solutions. the orthogonal array method is used to generate an initial population of genetic algorithm. this method has been proven to be optimal to select representative samples from all the possible combinations, due to the selected samples scatter uniformly over the feasible solution space. finally, the ieee standard test systems are used to examine the applicability of proposed method. simulation results demonstrate that the proposed method is more efficient than traditional method.
parallel_computing	parallel computing is widely utilized to speed up automatic test pattern generation (atpg); however, most of today 's parallel atpgs are non-deterministic, which often leads to non-reproducible test pattern sets. this paper presents a fault-parallel test pattern generator: cpp-atpg; it generates the same test pattern set regardless of the thread count and timing. besides, it exhibits good speedup scalability as the thread count increases. these are achieved by the circular pipeline processing (cpp) principle, which guides the proposed parallel atpg to preserve the task processing orders that are necessary to ensure atpg determinism but with low inter-thread synchronization overhead. furthermore, a multi-round test generation and compaction strategy is proposed to avoid possible test pattern inflation. experimental results show that cpp-atpg exhibits close-to-linear speedup for at least up to 12 threads.
data_structures	in many applications, top-k query is an important operation to return a set of interesting points in a potentially huge data space. the existing algorithms, either maintaining too many candidates, or requiring assistant structures built on the specific attribute subset, or returning results with probabilistic guarantee, cannot process top-k query on massive data efficiently. this paper proposes a sorted-list-based tkap algorithm, which utilizes some data structures of low space overhead, to efficiently compute top-k results on massive data. in round-robin retrieval on sorted lists, tkap performs adaptive pruning operation and maintains the required candidates until the stop condition is satisfied. the adaptive pruning operation can be adjusted by the information obtained in round-robin retrieval to achieve a better pruning effect. the adaptive pruning rule is developed in this paper, along with its theoretical analysis. the extensive experimental results, conducted on synthetic and real-life data sets, show the significant advantage of tkap over the existing algorithms.
algorithm_design	there exist two key problems about data aggregation that should be thoroughly explored - algorithm design in networking layer, and algorithm design in application layer. those two problems should be subtlety tackled in termers of high efficiency and robustness. therefore, the former one requires the survivability and highly reliable design at networking layer, the latter one usually asks for high efficiency and robustness at application layer. moreover, the optimization of algorithms is also considered for further enhancement. the integrity check is a key requirement for optimization. the context-aware and cross-layer design is applied in the optimization. a dynamic fragment odd-even parity checking code is proposed, and a context-aware aggregative integrity check code is proposed.
image_processing	objectiveto evaluate whether and to which extent skin redness (erythema) affects investigator blinding in transcranial direct current stimulation (tdcs) trials. material and methodstwenty-six volunteers received sham and active tdcs, which was applied with saline-soaked sponges of different thicknesses. high-resolution skin images, taken before and 5, 15, and 30 min after stimulation, were randomized and presented to experienced raters who evaluated erythema intensity and judged on the likelihood of stimulation condition (sham vs. active). in addition, semi-automated image processing generated probability heatmaps and surface area coverage of erythema. adverse events were also collected. resultserythema was present, but less intense in sham compared to active groups. erythema intensity was inversely and directly associated to correct sham and active stimulation group allocation, respectively. our image analyses found that erythema also occurs after sham and its distribution is homogenous below electrodes. tingling frequency was higher using thin compared to thick sponges, whereas erythema was more intense under thick sponges. conclusionsoptimal investigator blinding is achieved when erythema after tdcs is mild. erythema distribution under the electrode is patchy, occurs after sham tdcs and varies according to sponge thickness. we discuss methods to address skin erythema-related tdcs unblinding.
computer_programming	bayesianism is fast becoming the dominant paradigm in archaeological chronology construction. this paradigm shift has been brought about in large part by widespread access to tailored computer software which provides users with powerful tools for complex statistical inference with little need to learn about statistical modelling or computer programming. as a result, we run the risk that such software will be reduced to the status of black boxes. this would be a dangerous position for our community since good, principled use of bayesian methods requires mindfulness when selecting the initial model, defining prior information, checking the reliability and sensitivity of the software runs and interpreting the results obtained. in this article, we provide users with a brief review of the nature of the care required and offer some comments and suggestions to help ensure that our community continues to be respected for its philosophically rigorous scientific approach.
algorithm_design	mapreduce comes from its simplicity to preparing the input data, the programmer needs only to implement the mapper, the reducer, and optionally, the combiner and the partitioner. all other aspects of execution are handled transparently by the execution framework on clusters ranging from a single node to a few thousand nodes, over datasets ranging from gigabytes to petabytes. however, this also means that any conceivable algorithm that a programmer wishes to develop must be expressed in terms of a small number of rigidly defined components that must fit together in very specific ways. it may not appear obvious how a multitude of algorithms can be recast into this programming model. the purpose of this paper is to provide, a guide to mapreduce algorithm design. this paper presents the notion of design pattern of mapreduce, which instantiate arrangements of components and specific techniques designed to handle frequently encountered situations across a variety of domains.
software_engineering	we present a multi-level formation model for complex software systems. the previous works extract the software systems to software networks for further studies, but usually investigate the software networks at the class level. in contrast to these works, our treatment of software systems as multi-level networks is more realistic. in particular, the software networks are organized by three levels of granularity, which represents the modularity and hierarchy in the formation process of real-world software systems. more importantly, simulations based on this model have generated more realistic structural properties of software networks, such as power-law, clustering and modularization. on the basis of this model, how the structure of software systems effects software design principles is then explored, and it could be helpful for understanding software evolution and software engineering practices.
data_structures	it is well known that there are linear-space data structures for 2-d orthogonal range counting with worst-case optimal query time o(log n/ loglog n). we give an o(n log log n)-space adaptive data structure that improves the query time to o(log log n + log k/ log log n), where k is the output count. when k = o(1), our bounds match the state of the art for the 2-d orthogonal range emptiness problem [chan et al., 2011]. we give an o(n log log n)-space data structure for approximate 2-d orthogonal range counting that can compute a (1 + delta)-factor approximation to the count in o(log log n) time for any fixed constant delta >0. again, our bounds match the state of the art for the 2-d orthogonal range emptiness problem. last, we consider the 1-d range selection problem, where a query in an array involves finding the kth least element in a given subarray. this problem is closely related to 2-d 3-sided orthogonal range counting. recently, jorgensen and larsen [2011] presented a linear-space adaptive data structure with query time o(log log n + log k/ log log n). we give a new linear-space structure that improves the query time to o(1 + log k/log log n), exactly matching the lower bound proved by jorgensen and larsen.
computer_programming	by means of miedema formation enthalpy model with toop model, the excess free-energy, enthalpies of formation, excess entropies and activity values of all components of mg-al-y ternary alloy were calculated with computer programming. the experimental results show that enthalpies of formation, excess free-energy and excess entropies of the ternary alloy are negative in the whole content range, the minimum values at 1 123 k are all obtained at x (al)=55%, x (y)=45%, x (mg)=0%, which are -37.969, -30.961 kj/mol and -6.24 j/(mol center dot k) respectively. activity curves show that the activity values of al and y in mg-al-y ternary alloy rapidly decrease with the decrease of molar fraction, the values of which are very small when the molar fraction decreases to 0.4. it means that there is a strong interaction between al and y and stable compounds can be form in the mg-al-y ternary alloy system.
distributed_computing	in recent years, the technological advancements have led to a deluge of data from distinctive domains and the need for development of solutions based on parallel and distributed computing has still long way to go. that is why, the research and development of massive computing frameworks is continuously growing. at this particular stage, highlighting a potential research area along with key insights could be an asset for researchers in the field. therefore, this paper explores one of the emerging distributed computing frameworks, apache hama. it is a top level project under the apache software foundation, based on bulk synchronous parallel model. we present an unbiased and critical interrogation session about apache hama and conclude research directions in order to assist interested researchers.
parallel_computing	this paper considers the problem of many-to-many disjoint paths in the hypercube q(n) with f faulty vertices and obtains the following result. for any integer k with 1 <= k= 2), if f <= 2n - 2k - 2 and each fault-free vertex has at least two fault-free neighbors, then there exist k fully disjoint fault-free paths linking s and t which contain at least 2(n) - 2f vertices. a linear algorithm for finding such disjoint paths is also given. this result improves some known results in a sense. (c) 2016 elsevier b.v. all rights reserved.
cryptography	we consider a new type of attack on a coherent quantum key distribution protocol [coherent one-way (cow) protocol]. the main idea of the attack consists in measuring individually the intercepted states and sending the rest of them unchanged. we have calculated the optimum values of the attack parameters for an arbitrary length of a communication channel and compared this novel attack with a standard beam-splitting attack.
network_security	a smart grid is delay sensitive and requires the techniques that can identify and react on the abnormal changes (i.e., system fault, attacker, shortcut, etc.) in a timely manner. in this paper, we propose a real-time detection scheme against false data injection attack in smart grid networks. unlike the classical detection test, the proposed algorithm is able to tackle the unknown parameters with low complexity and process multiple measurements at once, leading to a shorter decision time and a better detection accuracy. the objective is to detect the adversary as quickly as possible while satisfying certain detection error constraints. a markov-chain-based analytical model is constructed to systematically analyze the proposed scheme. with the analytical model, we are able to configure the system parameters for guaranteed performance in terms of false alarm rate, average detection delay, and missed detection ratio under a detection delay constraint. the simulations are conducted with matpower 4.0 package for different ieee test systems.
distributed_computing	data processing complexity, partitionability, locality and provenance play a crucial role in the effectiveness of distributed data processing. dynamics in data processing necessitates effective modeling which allows the understanding and reasoning of the fluidity of data processing. through virtualization, resources have become scattered, heterogeneous, and dynamic in performance and networking. in this paper, we propose a new distributed data processing model based on automata where data processing is modeled as state transformations. this approach falls within a category of declarative concurrent paradigms which are fundamentally different than imperative approaches in that communication and function order are not explicitly modeled. this allows an abstraction of concurrency and thus suited for distributed systems. automata give us a way to formally describe data processing independent from underlying processes while also providing routing information to route data based on its current state in a p2p fashion around networks of distributed processing nodes. through an implementation, named pumpkin, of the model we capture the automata schema and routing table into a data processing protocol and show how globally distributed resources can be brought together in a collaborative way to form a processing plane where data objects are self-routable on the plane. (c) 2015 elsevier b.v. all rights reserved.
parallel_computing	in this paper, a corrected parallel smoothed particle hydrodynamics (c-sph) method is proposed to simulate the 3d generalized newtonian free surface flows with low reynolds number, especially the 3d viscous jets buckling problems are investigated. the proposed c-sph method is achieved by coupling an improved sph method based on the incompressible condition with the traditional sph (tsph), that is, the improved sph with diffusive term and first-order kernel gradient correction scheme is used in the interior of the fluid domain, and the tsph is used near the free surface. thus the c-sph method possesses the advantages of two methods. meanwhile, an effective and convenient boundary treatment is presented to deal with 3d multiple-boundary problem, and the mpi parallelization technique with a dynamic cells neighbor particle searching method is considered to improve the computational efficiency. the validity and the merits of the c-sph are first verified by solving several benchmarks and compared with other results. then the viscous jet folding/coiling based on the cross model is simulated by the c-sph method and compared with other experimental or numerical results. specially, the influences of macroscopic parameters on the flow are discussed. all the numerical results agree well with available data, and show that the c-sph method has higher accuracy and better stability for solving 3d moving free surface flows over other particle methods. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	multilevel spin toque transfer ram (stt-ram) is a suitable storage device for energy-efficient neural network accelerators (nnas), which relies on large-capacity on-chip memory to support brain-inspired large-scale learning models from conventional artificial neural networks to current popular deep convolutional neural networks. in this paper, we investigate the application of multilevel stt-ram to general-purpose nnas. first, the error-resilience feature of neural networks is leveraged to tolerate the read/write reliability issue in multilevel cell stt-ram using approximate computing. the induced read/write failures at the expense of higher storage density can be effectively masked by a wide spectrum of nn applications with intrinsic forgiveness. second, we present a precision-tunable stt-ram buffer for the popular general-purpose nna. the targeted stt-ram memory design is able to transform between multiple working modes and adaptable to meet the varying quality constraint of approximate applications. lastly, the reconfigurable stt-ram buffer not only enables precision scaling in nna but also provides adaptiveness to the demand for different learning models with distinct working-set sizes. particularly, we demonstrate the concept of capacity/precision-tunable stt-ram memory with the emerging reconfigurable deep nna and elaborate on the data mapping and storage mode switching policy in stt-ram memory to achieve the best energy efficiency of approximate computing.
operating_systems	the complex span paradigm is one of the most influential and widely used instruments for measuring working memory capacity (wmc). we report the results of four experiments designed to explore the feasibility of obtaining valid estimates of wmc online. we explored the relationships between the complex span tasks and fluid intelligence (gf) in the lab and on the web using a new platform called the online working memory lab (the owl). the owl is universally accessible across all computer operating systems and functions in both local and remote contexts, allowing researchers to sample more diverse subjects from practically anywhere. experiments 1 and 2 showed that the complex span failed to predict gf when the to-be-remembered stimuli were letters and the tests were taken online. we increased the predictive validity of the test battery in experiments 3 and 4 by replacing the letters with memory stimuli that were more difficult to write down in an unproctored setting. this work describes our most recent attempts to measure working memory capacity in the wild.
algorithm_design	a spatiotemporal mining framework is a novel tool for the analysis of marine association patterns using multiple remote sensing images. from data pretreatment, to algorithm design, to association rule mining and pattern visualization, this paper outlines a spatiotemporal mining framework for abnormal association patterns in marine environments, including pixel-based and object-based mining models. within this framework, some key issues are also addressed. in the data pretreatment phase, we propose an algorithm for extracting abnormal objects or pixels over marine surfaces, and construct a mining transaction table with object-based and pixel-based strategies. in the mining algorithm phase, a recursion method to construct a direct association pattern tree is addressed with an asymmetric mutual information table, and a recursive mining algorithm to find frequent items. in the knowledge visualization phase, a ""dimension-attributes"" visualization framework is used to display spatiotemporal association patterns. finally, spatiotemporal association patterns for marine environmental parameters in the pacific ocean are identified, and the results prove the effectiveness and the efficiency of the proposed mining framework. (c) 2014 elsevier b.v. all rights reserved.
cryptography	the smart-grid concept takes the communications from the enclosed and protected environment of a substation to the wider city or nationwide area. in this environment, cyber security takes a key role in order to secure the communications. the challenge is to be able to secure the grid without impacting the latency while, at the same time, maintaining compatibility with older devices and non secure services. at the lower level, added security must not interfere with the redundancy and the latency required for the real-time substation automation communications. this paper studies how to integrate ieee mac security standard (macsec) in the substation environment, especially when used in substation system communications that have stringent response time requirements and zero recovery time as defined in iec 62439-3.
operating_systems	robot software combines the challenges of general purpose and real-time software, requiring complex logic and bounded resource use. physical safety, particularly for dynamic systems such as humanoid robots, depends on correct software. general purpose computation has converged on unix-like operating systems standardized as posix, the portable operating system interface for devices from cellular phones to supercomputers. the modular, multi-process design typical of posix applications is effective for building complex and reliable software. absent from posix, however, is an interproccess communication mechanism that prioritizes newer data as typically desired for control of physical systems. we address this need in the ach communication library which provides suitable semantics and performance for real-time robot control. although initially designed for humanoid robots, ach has broader applicability to complex mechatronic devices humanoid and otherwise that require real-time coupling of sensors, control, planning, and actuation. the initial user space implementation of ach was limited in the ability to receive data from multiple sources. we remove this limitation by implementing ach as a linux kernel module, enabling ach 's high performance and latest-message-favored semantics within conventional posix communication pipelines. we discuss how these posix interfaces and design principles apply to robot software, and we present a case study using the ach kernel module for communication on the baxter robot.
relational_databases	insertion propagation problem is a class of view update problem in relational databases [1]. given a source database d, a monotone relational algebraic query q, and the view v generated by the query q(d), insertion propagation problem is to find a set delta d of tuples whose insertion into d will add the given tuples delta v into the view v via q without producing side-effect on view. in this paper, we consider the fd-restricted version insertion propagation problem 'fd-vsef-ip', in which we aim to find the delta d not only view side-effect free but also without introducing inconsistency with respect to the predefined functional dependencies. we study both data and combined complexity of fd-vsef-ip under both single and group insertion. interestingly, the problem ranges from ptime to sigma(p)(2)-complete, for queries in different classes in either complexity aspect. we show that the fd-restricted version will be harder to get the optimal solution, contrary to its counterpart under deletion. our study of this fd-restricted version insertion propagation problem generalize the computational issues involved in data lineage - the process by which databases updated through view insertion under fd.
computer_graphics	as location-aware applications and location-based services continue to increase in popularity, data sources describing a range of dynamic processes occurring in near real-time over multiple spatial and temporal scales are becoming the norm. at the same time, existing frameworks useful for understanding these dynamic spatio-temporal data, such as time geography, are unable to scale to the high volume, velocity, and variety of these emerging data sources. in this paper, we introduce a computational framework that turns time geography into a scalable analysis tool that can handle large and rapidly changing datasets. the hierarchical prism tree (hpt) is a dynamic data structure for fast queries on spatio-temporal objects based on time geographic principles and theories, which takes advantage of recent advances in moving object databases and computer graphics. we demonstrate the utility of our proposed hpt using two common time geography tasks (finding similar trajectories and mapping potential space-time interactions), taking advantage of open data on space-time vehicle emissions from the envirocar platform.
cryptography	multimedia contents are inherently sensitive signals that must be protected whenever they are outsourced to an untrusted environment. this problem becomes a challenge when the untrusted environment must perform some processing on the sensitive signals; a paradigmatic example is cloud-based signal processing services. approaches based on secure signal processing (ssp) address this challenge by proposing novel mechanisms for signal processing in the encrypted domain and interactive secure protocols to achieve the goal of protecting signals without disclosing the sensitive information they convey. this paper presents a novel and comprehensive set of approaches and primitives to efficiently process signals in an encrypted form, by using number theoretic transforms (ntts) in innovative ways. this usage of ntts paired with appropriate signal pre- and post-coding enables a whole range of easily composable signal processing operations comprising, among others, filtering, generalized convolutions, matrix-based processing or error correcting codes. our main focus is on unattended processing, in which no interaction from the client is needed; for implementation purposes, efficient lattice-based somewhat homomorphic cryptosystems are used. we exemplify these approaches and evaluate their performance and accuracy, proving that the proposed framework opens up a wide variety of new applications for secured outsourced-processing of multimedia contents.
relational_databases	analysis of data stored in a graph enables the discovery of certain information that could be hard to see if the data were stored using some other model (e.g. relational). however, the vast majority of data in information systems today is stored in relational databases, which dominate the data management field over the last decades. in spite of the rise of nosql technologies, the development of new information systems is still mostly based on relational databases. given the increasing awareness about the benefits of data analysis as well as current research interest in graph mining techniques, we aim to enable the usage of those techniques on relational data. in that regard, we propose a universal relational-to-graph data conversion algorithm which can be used in preparation of data to perform a graph mining analysis. our approach leverages the property graph model which is mainly used by the graph databases, while maintaining the level of relational data clarity.
image_processing	human target characteristic parameter extraction is an important approach of behavior monitoring. the extraction of the characteristic can be applied in various backgrounds, such as sanatorium and hospital. therefore, this technology is widely studied. towards extracting physiological characteristic parameters and motion characteristic features of human target, a novel human parameter extraction algorithm is proposed in this paper which has high detection accuracy. the high accuracy detection is achieved by combining the time-frequency analysis and image processing algorithm. besides that, the utilization of short wavelength and evident micro-motion features inherent with terahertz radar also contributes the improvement of detection accuracy. simulations test the effectiveness of proposed the algorithm, and illustrate its performance of high extraction precision and insensitivity to noise. for comparison, the simulations are also performed in x-band radar. via the thorough simulations, we can clearly find the advantage of our proposed algorithm in human target characteristic parameter extraction. (c) 2016 elsevier b.v. all rights reserved.
data_structures	the concept of uncertain pattern mining was recently proposed to fulfill the demand for processing databases with uncertain data, and various relevant methods have been devised. however, previous approaches have the following limitations. state-of-the-art methods based on tree structure can cause fatal problems in terms of runtime and memory usage according to the characteristics of uncertain databases and threshold settings because their own tree data structures can become excessively large and complicated in their mining processes. various approximation approaches have been suggested in order to overcome such problems; however, they are methods that increase their own mining performance at the cost of accuracy of the mining results. in order to solve the problems, we propose an exact, efficient algorithm for mining uncertain frequent patterns based on novel data structures and mining techniques, which can also guarantee the correctness of the mining results without any false positives. the newly proposed list-based data structures and pruning techniques allow a complete set of uncertain frequent patterns to be mined more efficiently without pattern losses. we also demonstrate that the proposed algorithm outperforms previous state-of-the art approaches in both theoretical and empirical aspects. especially, we provide analytical results of performance evaluation for various types of datasets to show efficiency of runtime, memory usage, and scalability in our method. (c) 2016 elsevier b.v. all rights reserved.
network_security	network big data is an important driving force for the upgrading of information technology, and network data has brought great opportunities to the economic survey. the openness and freedom of the network also produced the possibility of private information and data to be destroyed or violated, and the security of the internet is becoming more and more important. in this paper, the author analyzes the impact of fiscal expenditure on the new urbanization and income gap by using data mining technology. the result shows that the public expenditure increase 1% will lead urbanization rate increase 0.0462%; also, the effect of public finance expenditure on urbanization has a certain time lag, and the effect will be more obvious in the long term. at the same time, public expenditure will help to narrow the income gap in the long term, public expenditure at lag 2 increase 1% will lead income gap decrease 0.758%. so that, increasing public expenditure is an important way to promote the equalization of basic public services between urban and rural areas.
distributed_computing	we discuss the future of massively parallel computing from a fundamental architecture standpoint. our central thesis is that various versions of moore 's laws will all unavoidably break down over the next two to three decades, due to fundamental limitations imposed by the laws of physics (especially quantum mechanics). therefore, the end to scaling-up von neumann-based architectures by adding more cores and other incremental technology advancements is within sight. how are these challenges to be overcome, in order to continue making computation faster, cheaper, more energy efficient and more reliable? one possible answer lies in connectionist massively parallel models, based on (artificial) neural networks and other paradigms from biology, neuroscience and statistical physics. we discuss what would such massively parallel connectionist computers be like if they were to be based on ""implementing neural networks in the hardware"", and undertake several thought experiments comparing and contrasting the hypothetical connectionist parallel machines with the ""classical"" multicore supercomputers and large-scale distributed computing systems.
computer_programming	grinding employs an abrasive product, usually a rotating wheel, which is brought into controlled contact with a workpiece surface. the grinding patterning process can be simulated using computer programming with specific user-defined parameters. to predict the surface pattern that will result from grinding with the grooved wheel, calculating the cam parameters is very important. the intention of this research is to formulate cam parameters for the generation of a surface pattern on a flat nominal surface by grinding with a wheel that has been prepared in a special way. a diamond dresser having a rounded tip is applied to prepare helical grooves on the conventional wheel 's surface. with the intention of extracting cam parameters, a mathematical model is developed for a grinding patterning process, starting from the unit pattern geometry of the workpiece plate. the models are characterized by the process parameters of grinding patterning such as unit pattern geometry, dressing parameters, wheel geometry and grinding conditions, as well as work surface geometry. a computer program with a user interface is developed using matlab according to the proposed mathematical model to predict surface pattern. in addition, several examples of simulation results of the 3d wheel geometry model and corresponding patterned surfaces have been displayed using the proposed cam parameters as input. formulated cam parameters can be used to control the actual grinding process or predict the ground surface pattern.
parallel_computing	this paper presents the use of a computer cluster with heterogeneous computing components to provide concurrency and multi-level parallelism at coarse grain and massive fine-grain for multiview video coding (mvc) applications. mvc involves coding of multiple video sequences that are taken from the same scene but different perspective. in addition to motion estimation (me) used in conventional video coding for single view video for exploiting inter-frame temporal similarities, mvc adopts disparity estimation (de) to further increase compression. to overcome the huge computational cost associated with me and by extension with de, attention has been mainly focused on developing fast me/de algorithms. although fast me/de algorithms bring substantial speedup, to achieve realtime mvc encoding, it requires further acceleration of the coding process at higher levels. towards this end, this paper proposes a multiple-view-parallel, multiple-interleaved group of pictures (multiple-igop) scheduling scheme for mvc. when evaluated over eight views, with no loss in rate distortion (rd) performance, the proposed scheme outperforms view-sequential coding by a factor of up to 12.4 and 12.3, respectively, for two popular prediction structures, ibp and ipp.
computer_vision	compared with vertical photogrammtry, oblique photogrammetry is radically different for images acquired from sensor with big yaw, pitch, and roll angles. image matching is a vital step and core problem of oblique low-altitude photogrammetric process. among the most popular oblique images matching methods are currently sift/asift and many affine invariant feature-based approaches, which are mainly used in computer vision, while these methods are unsuitable for requiring evenly distributed corresponding points and high efficiency simultaneously in oblique photogrammetry. in this paper, we present an oblique low-altitude images matching approach using robust perspective invariant features. firstly, the homography matrix is estimated by a few corresponding points obtained from top pyramid images matching in several projective simulation. then images matching are implemented by sub-pixel harris corners and descriptors after shape perspective transforming on the basis of homography matrix. finally, the error or gross error matched points are excluded by epipolar geometry, ransac algorithm and back projection constraint. experimental results show that the proposed approach can achieve more excellent performances in oblique low-altitude images matching than the common methods, including sift and surf. and the proposed approach can significantly improve the computational efficiency compared with asift and affine-surf.
software_engineering	reuse of software components, either closed or open source, is considered to be one of the most important best practices in software engineering, since it reduces development cost and improves software quality. however, since reused components are (by definition) generic, they need to be customized and integrated into a specific system before they can be useful. since this integration is system-specific, the integration effort is non-negligible and increases maintenance costs, especially if more than one component needs to be integrated. this paper performs an empirical study of multi-component integration in the context of three successful open source distributions (debian, ubuntu and freebsd). such distributions integrate thousands of open source components with an operating system kernel to deliver a coherent software product to millions of users worldwide. we empirically identified seven major integration activities performed by the maintainers of these distributions, documented how these activities are being performed by the maintainers, then evaluated and refined the identified activities with input from six maintainers of the three studied distributions. the documented activities provide a common vocabulary for component integration in open source distributions and outline a roadmap for future research on software integration.
algorithm_design	we demonstrate circuits that generate set and integer partitions on a set s of n objects at a rate of one per clock. partitions are ways to group elements of a set together and have been extensively studied by researchers in algorithm design and theory. we offer two versions of a hardware set partition generator. in the first, partitions are produced in lexicographical order in response to successive clock pulses. in the second, an index input determines the set partition produced. such circuits are useful in the hardware implementation of the optimum distribution of tasks to processors. we show circuits for integer partitions as well. our circuits are combinational. for large n, they can have a large delay. however, one can easily pipeline them to produce one partition per clock period. we show (1) analytical and (2) experimental time/complexity results that quantify the efficiency of our designs. for example, our results show that a hardware set partition generator running on a 100mhz fpga produces partitions at a rate that is approximately 10 times the rate of a software implementation on a processor running at 2.26ghz.
image_processing	the circle hough transform (cht) is one of the popular circle detection algorithm in image processing and machine vision application, favored for its tolerance to noise. nevertheless, it involves huge computation and excessive memory requirements. because of its drawbacks, various modifications have been suggested to increase its performances. in this paper, we present a new modification of the cht method developed for an automatic biometric iris recognition system. the novelty of this method resides on the use of the incremental property in order to reduce the resources requirement and the parallel property for decreasing the computation time. the incremental property is obtained using the approximation of the used trigonometric functions, while the parallel property is achieved by calculating, at the same time, several point coordinates of the circle. the software implementation and the validation have been done in c++ and matlab on real images. the errors analysis and the performances of the proposed method against the basic cht method are presented in this paper. (c) 2016 elsevier gmbh. all rights reserved.
algorithm_design	the problem of data representation is fundamental to the efficiency of search algorithms for the traveling salesman problem (tsp). the computational effort required to perform such tour operations as traversal and subpath reversal considerably influence algorithm design and performance. we propose a new data structure-the k-level satellite tree-for representing a tsp tour with a discussion of properties in the framework of general tour operations. the k-level satellite tree representation is shown to be significantly more efficient than its predecessors for large-scale instances.
data_structures	the ability to timely process significant amounts of continuously updated spatial data is mandatory for an increasing number of applications. parallelism enables such applications to face this data-intensive challenge and allows the devised systems to feature low latency and high scalability. in this paper, we focus on a specific data-intensive problem concerning the repeated processing of huge amounts of range queries over massive sets of moving objects, where the spatial extent of queries and objects is continuously modified over time. to tackle this problem and significantly accelerate query processing, we devise a hybrid cpu/gpu pipeline that compresses data output and saves query processing work. the devised system relies on an ad-hoc spatial index leading to a problem decomposition that results in a set of independent data-parallel tasks. the index is based on a point-region quadtree space decomposition and allows to tackle effectively a broad range of spatial object distributions, even those very skewed. also, to deal with the architectural peculiarities and limitations of the gpus, we adopt non-trivial gpu data structures that avoid the need of locked memory accesses while favouring coalesced memory accesses, thus enhancing the overall memory throughput. to the best of our knowledge, this is the first work that exploits gpus to efficiently solve repeated range queries over massive sets of continuously moving objects, possibly characterized by highly skewed spatial distributions. in comparison with state-of-the-art cpu-based implementations, our method highlights significant speedups in the order of 10 - 20x, depending on the dataset. copyright (c) 2016 john wiley & sons, ltd.
computer_graphics	the army 's combat training is very important now, and the simulation of the real battlefield environment is of great significance. two-dimensional information has been unable to meet the demand at present. with the development of virtual reality technology, three-dimensional (3d) simulation of the battlefield environment is possible. in the simulation of 3d battlefield environment, in addition to the terrain, combat personnel and the combat tool, the simulation of explosions, fire, smoke and other effects is also very important, since these effects can enhance senses of realism and immersion of the 3d scene. however, these special effects are irregular objects, which make it difficult to simulate with the general geometry. therefore, the simulation of irregular objects is always a hot and difficult research topic in computer graphics. here, the particle system algorithm is used for simulating irregular objects. we design the simulation of the explosion, fire, smoke based on the particle system and applied it to the battlefield 3d scene. besides, the battlefield 3d scene simulation with the glasses-free 3d display is carried out with an algorithm based on gpu 4k super-multiview 3d video real-time transformation method. at the same time, with the human-computer interaction function, we ultimately realized glasses-free 3d display of the simulated more realistic and immersed 3d battlefield environment.
distributed_computing	phylogenetic analysis has achieved extraordinary results in domains like species delimitation and evolutionary biology. an essential element behind this success has been the introduction of high performance computing techniques in the step of estimating the phylogenetic likelihoods. this paper describes the design and implementation of a distributed and cpu-gpu based heterogeneous computing system on parallelizing the analysis. the parallelization has been implemented in the state-of-the-art version of mrbayes, a widespread phylogeny reconstruction program. we benchmarked the method and another two gpu-based methods by using 8 distributed computing nodes on tianhe-1a. the experimental results indicate that the proposed method outstrips beagle and the nmc(3) method by speedup factors of up to 1.98x and 1.68x, respectively. in comparison to the serially implemented mrbayes, a peak speedup of 188x is finally achieved by using 8 tesla m 2050 gpus. the proposed method is publicly available to facilitate further research on phylogenetic analysis.
computer_programming	timing is critical when trying to engage students in various engineering career paths. while many ""national engineers week"" programs exist for primary and middle school students, there is a lack of hands on activities for students in the 9th-12th grades. it is often difficult to devise experiments for this age group that are interesting and not juvenile. yet, it is during these crucial years that most students are lost to science, math and engineering. engaging students and presenting opportunities for invention and excitement is important in the teen years, when peer pressure and the distractions of friends, social events, and activities are particularly high.(1). there are a number of open source programming and affordable hardware platforms that can be used to implement low cost and interactive programs to promote innovation with various age groups. in this paper we share our work, as well as our learnings on how to make the workshops more effective. we have created various arduino projects that can be customized to grade levels ranging from grades 7-12, and even college undergraduate students. the various projects we describe in this paper have been used to interact with students of different grade levels to engage in basic elements of engineering and computer programming. the classes should be set up to work in groups to promote shared innovation, teamwork and collaboration with peers. the open source and hardware experimenting exposed the students to various career paths ranging from software engineer, to electronic engineers and basic elements of various other engineering paths. this paper is designed to demonstrate the promotion of the engineering profession in schools through the use of arduino uno, raspberry pi gemma kits, and flora kits. the programs are also designed to accommodate classroom setting, workshops, or as an in-class field trip.
image_processing	luminescence imaging is a versatile characterisation technique used for a broad range of research and industrial applications, particularly for the field of photovoltaics where photoluminescence and electroluminescence imaging is routinely carried out for materials analysis and quality control. luminescence imaging can reveal a wealth of material information, as detailed in extensive literature, yet these techniques are often only used qualitatively instead of being utilised to their full potential. part of the reason for this is the time and effort required for image processing and analysis in order to convert image data to more meaningful results. in this work, a custom built, matlab based software suite is presented which aims to dramatically simplify luminescence image processing and analysis. the suite includes four individual programs which can be used in isolation or in conjunction to achieve a broad array of functionality, including but not limited to, point spread function determination and deconvolution, automated sample extraction, image alignment and comparison, minority carrier lifetime calibration and iron impurity concentration mapping. program summary program title: lumitools program files doi: http://dx.dolorg/10.17632/7nd34fbwfg.1 licensing provisions: creative commons by 4.0 (cc by 4.0) programming language: matlab nature of problem: data acquired using the technique of luminescence imaging require unique corrections and processing in order to convert the qualitative image into more meaningful, quantitative results. such processing is often non-trivial and can present a barrier to research. solution method: the lumitools package provides a broad array of common functionality required for the processing and analysis of luminescence images. several tools are available to allow processing for various applications and each tool has been developed with a simple to use graphical user interface. (c) 2017 elsevier b.v. all rights reserved.
computer_vision	this paper mainly studies the image contour detection algorithm which can distinguish edges of different strengths. based on the study of probability-of-boundary operator, we find that defects such as response suppression and offset exist in the algorithm during the detection of corners and curved edges, thus an improved algorithm is proposed. this algorithm retains the advantage in probability-of-boundary algorithm which can effectively distinguish the edge strength, while improves the above-mentioned defects. and an improved algorithm is proposed to characterize the strength of boundary reasonably, making the test results in line with human subjective recognition results.
cryptography	due to an increase in the number of internet users, electronic commerce has grown significantly during the last decade. electronic auction (e-auction) is one of the famous e-commerce applications. even so, security and robustness of e-auction schemes still remain a challenge. requirements like anonymity and privacy of the bid value are under threat from the attackers. any auction protocol must not leak the anonymity and the privacy of the bid value of an honest bidder. keeping these requirements in mind, we have firstly proposed a controlled traceable blind signature scheme (ctbss) because e-auction schemes should be able to trace the bidders. using ctbss, a blind sealed-bid electronic auction scheme is proposed (bsea). we have incorporated the notion of blind signature to e-auction schemes. moreover, both the schemes are based upon elliptic curve cryptography (ecc), which provides a similar level of security with a comparatively smaller key size than the discrete logarithm problem (dlp) based e-auction protocols. the analysis shows that bsea fulfills all the requirements of e-auction protocol, and the total computation overhead is lower than the existing schemes.
operating_systems	this paper presents a real-time operating system ( rtos) implementation. this rtos is based on the osek-os 2.2.3 standard, which will be extended to support an asymmetric multiprocessor system ( amp). the term asymmetric means that the microprocessors included in the system have heterogeneous architectures ( a subgroup of identical processors may exist but not the whole amount of them are identical) and each processor executes an instance of the rtos specifically designed and optimised. also, a description of the development process that took place to communicate each rtos instance, taking advantage of hardware features in order to exchange information between processors, is provided. finally, a functional test application is presented: processors monitor each other using rtos resources as keep-alive events while sharing information about gpio activity and reporting changes and system status using serial communication. in addition, an energy consumption analysis is performed for this test case. the hardware platform used in this development is the argentine open industrial computer ( computadora industrial abierta argentina, ciaa) based on the lpc4337 dual-core amp microcontroller, which includes an arm cortex-m4 ( armv7e-m architecture) acting as the master microprocessor and an arm cortex-m0 ( armv6-m architecture), as the slave coprocessor.
data_structures	given a matrix of size n, two dimensional range minimum queries (2d-rmqs) ask for the position of the minimum element in a rectangular range within the matrix. we study trade-offs between the query time and the additional space used by indexing data structures that support 2d-rmq5. using a novel technique-the discrepancy properties of fibonacci lattices we give an indexing data structure for 2d-rmqs that uses o (n/c) bits additional space with o (c logc(log log c)(2)) query time, for any parameter c, 4 <= c <= n. also, when the entries of the input matrix are from 10,11, we show that the query time can be improved to o (c logc) with the same space usage. (c) 2016 published by elsevier b.v.
algorithm_design	in this paper, needs of trusted iot application are identified and a methodology is proposed to create a framework ""iotee"" for rapid prototyping of secure, trusted and commercial iot applications in absence of hardware. this has been done by analysis of requirements of a sample trusted iot application heartbeat sensor (hbs), classification of bhs services and their decomposition into trusted and non-trusted components. furthermore, the methodology includes algorithm design for services like registration, authentication, authorization and secure communication in the proposed framework. detailed operation sequence of the algorithms is also depicted to understand the overall switching scenario between trusted and non-trusted components of application. finally, a dynamic deployment structure is created to enable and disable the trusted components in the framework.
cryptography	in this paper, we propose a three-party and a multi-party quantum key agreement protocols with single photons in both polarization and spatial-mode degrees of freedom. based on the defined collective unitary operations, the participants can agree on a secure shared key through encoding their sub-secret keys on the particles. moreover, the security of our protocols is discussed comprehensively. it is showed that the presented protocols can defend both the outside attacks and participant attacks. the efficiency analysis also shows that our two protocols can achieve high qubit efficiency. besides, our protocols are feasible since the preparation and themeasurement of singlephoton state in both polarization and spatial-mode degrees of freedom are available with current quantum techniques.
network_security	it is essential to design a protocol to allow sensor nodes to attest to their trustworthiness for mission-critical applications based on wireless sensor networks (wsns). however, it is a challenge to evaluate the trustworthiness without appropriate hardware support. hence, we present a hardware-based remote attestation protocol to tackle the problem within wsns. in our design, each sensor node is equipped with a trusted platform module (tpm) which plays the role of a trusted anchor. we start with the formulation of remote attestation and its security. the complete protocol for both single-hop and multi-hop attestations is then demonstrated. results show the new protocol is effective, efficient, and secure.
network_security	aims: protocol security which is important concern in the network security, which ensures the security and integrity of data transmission over the internet. secure network data from any illegimate attempt to extract content of the data and cloud computing is the advance computing technology for the internet users. materials and methods: we can enhance our system without modify or changing our resources by the internet over cloud technology. so, all the resources we can get through cloud system. our data should be transmit in secure channel. results: this paper focus on protocol security in private cloud system deployment and security against poodle (discovered by google team at oct 2014).
computer_vision	the main objective of this study was to configure the acquisition and analysis of low-field magnetic resonance imaging (mri) to predict physico-chemical characteristics of iberian loin, evaluating the use of different mri sequences (spin echo, se; gradient echo, ge; turbo 3d, t3d), computational texture feature methods (glcm, ngldm, glrlm, glcm + ngldm + glrlm), and data mining techniques (multiple linear regression, mlr; isotonic regression, ir). moderate to very good correlation coefficients and low mean absolute error were found when applying mlr or ir on any method of computational texture features from mri acquired with se or ge. for t3d sequence, accurate results are only obtained by applying ir on glcm or glcm + ngldm + glrlm methods. considering not only the accuracy of the methodology but also consumed time and required resources, the use of se sequences for mri acquisition, glcm method for mri texture analysis, and mlr could be indicated for prediction physico-chemical characteristics of loin.
data_structures	understanding beliefs, values, and preferences of patients is a tenet of contemporary health sciences. this application was motivated by the analysis of multiple partially ordered set (poset) responses from an inventory on layman beliefs about diabetes. the partially ordered set arises because of two features in the data-first, the response options contain a do n't know (dk) option, and second, there were two consecutive occasions of measurement. as predicted by the common sense model of illness, beliefs about diabetes were not necessarily stable across the two measurement occasions. instead of analyzing the two occasions separately, we studied the joint responses across the occasions as a poset response. few analytic methods exist for data structures other than ordered or nominal categories. poset responses are routinely collapsed and then analyzed as either rank ordered or nominal data, leading to the loss of nuanced information that might be present within poset categories. in this paper we developed a general class of item response models for analyzing the poset data collected from the common sense model of diabetes inventory. the inferential object of interest is the latent trait that indicates congruence of belief with the biomedical model. to apply an item response model to the poset diabetes inventory, we proved that a simple coding algorithm circumvents the requirement of writing new codes such that standard irt software could be directly used for the purpose of item estimation and individual scoring. simulation experiments were used to examine parameter recovery for the proposed poset model.
computer_programming	providing students opportunities to appreciate interdisciplinary systems remains a challenge for educators at all levels. sensing and data-logging in the environment and in engineered systems offer a unique opportunity for students to explore the connections between engineering processes, analog signals, and digital outputs. here, we present the design of a device that uses an infrared sensor and microcontroller to measure and record low liquid flow rates. we designed this device using the open-source arduino t microcontroller platform, which can stand alone or interface with a pc to display data in real time. in order to demonstrate the usefulness of this device in the undergraduate learning laboratory, we have incorporated it into a membrane distillation system. building and experimenting with such devices helps students practice and learn creativity, troubleshooting, and programming fundamentals, allowing them to understand the importance of electrical engineering and computer programming in the context of chemical and environmental engineering.
computer_vision	deep convolutional neural networks (cnns) have been widely used to obtain high-level representation in various computer vision tasks. however, in the field of remote sensing, there are not sufficient images to train a useful deep cnn. instead, we tend to transfer successful pre-trained deep cnns to remote sensing tasks. in the transferring process, generalization power of features in pre-trained deep cnns plays the key role. in this paper, we propose two promising architectures to extract general features from pre-trained deep cnns for remote scene classification. these two architectures suggest two directions for improvement. first, before the pre-trained deep cnns, we design a linear pca network (lpcanet) to synthesize spatial information of remote sensing images in each spectral channel. this design shortens the spatial distance of target and source datasets for pre-trained deep cnns. second, we introduce quaternion algebra to lpcanet, which further shortens the spectral distance between remote sensing images and images used to pre-train deep cnns. with five well-known pre-trained deep cnns, experimental results on three independent remote sensing datasets demonstrate that our proposed framework obtains state-of-the-art results without fine-tuning and feature fusing. this paper also provides baseline for transferring fresh pre-trained deep cnns to other remote sensing tasks.
algorithm_design	this paper investigates the development of a mild hybrid powertrain system through the integration of a conventional manual transmission equipped powertrain and a secondary power source in the form of an electric motor driving the transmission output shaft. the primary goal of this paper is to study the performance of partial power-on gear shifts through the implementation of torque hole filling by the electric motor during gear changes. to achieve this goal, mathematical models of both conventional and mild hybrid powertrain are developed and used to compare the system dynamic performance of the two systems. this mathematical modelling is used to run different simulations for gearshift control algorithm design during system development, allowing us to evaluate the achievable performance and its dependency on system properties. the impact of motor power on the degree of torque hole compensation is also investigated, keeping in mind the practical limits to motor specification. this investigation uses both the output torque, vehicle speed as well as vibration dose value to evaluate the quality of gearshifts at different motor sizes. results demonstrate that the torque hole may be eliminated using a motor power of 50 kw. however, the minimum vibration dose value during gear change is achieved using a peak power of 16-20 kw. (c) 2017 elsevier ltd. all rights reserved.
symbolic_computation	a modeling language for hybrid systems hydla and its implementation hylagi are described. hydla is a constraint-based language that can handle uncertainties of models smoothly. hylagi calculates trajectories by symbolic formula manipulation to exclude errors resulting from floating-point arithmetic. hylagi features a nondeterministic simulation algorithm so it can calculate all possible qualitative different trajectories of models with uncertainties.
bioinformatics	objectives. the purpose of this study was to determine the level of heterogeneity in high grade serous ovarian cancer (hgsoc) by analyzing rna expression in single epithelial and cancer associated stromal cells. in addition, we explored the possibility of identifying subgroups based on pathway activation and pre-defined signatures from cancer stem cells and chemo-resistant cells. methods. a fresh, hgsoc tumor specimen derived from ovary was enzymatically digested and depleted of immune infiltrating cells. rna sequencing was performed on 92 single cells and 66 of these single cell datasets passed quality control checks. sequences were analyzed using multiple bioinformatics tools, including clustering, principle components analysis, and geneset enrichment analysis to identify subgroups and activated pathways. immunohistochemistry for ovarian cancer, stem cell and stromal markers was performed on adjacent tumor sections. results. analysis of the gene expression patterns identified two major subsets of cells characterized by epithelial and stromal gene expression patterns. the epithelial group was characterized by proliferative genes including genes associated with oxidative phosphorylation and myc activity, while the stromal group was characterized by increased expression of extracellular matrix (ecm) genes and genes associated with epithelial-to-mesenchymal transition (emt). neither group expressed a signature correlating with published chemo-resistant gene signatures, but many cells, predominantly in the stromal subgroup, expressed markers associated with cancer stem cells. conclusions. single cell sequencing provides a means of identifying subpopulations of cancer cells within a single patient. single cell sequence analysis may prove to be critical for understanding the etiology, progression and drug resistance in ovarian cancer. (c) 2017 elsevier inc. all rights reserved.
image_processing	at nano/micro-scales, the resistance of a particle to rolling is a critical factor in many applications and biological phenomena as it affects particle adhesion, motion and flow as well as particle manipulation (e.g. picking and placing of microparticles). in present work, a non-contact non-invasive experimental method is detailed and applied to determine the pre-rolling critical leaning angle (cla) of single microparticles. transient rayleigh surface acoustic waves (saw) are utilized as the excitation mechanism and interferometry and image processing as detection/monitoring and analysis techniques for capturing the micro/nano-scale dynamics of the microparticles. the cla values for a set of 30 psl (polystyrene latex) microparticles with a diameter of 14.9 +/- 0.6 pm on a soda-lime glass substrate are reported. the cia of the studied particles are determined to be between 0.9 and 7.8 degrees. it is also observed that during a saw field burst cycle, microparticles could change their drifting (rolling and/or sliding) directions, speeds and accelerations. the trajectory of a particle is often found to be non-linear. this nonlinear behavior is attributed to the inhomogeneity of the surface properties of the particles and the substrate as well as possible electric charge density, chemistry variations and potential contamination on surfaces. moreover, the effect of electric charge (developed due to the triboelectric effect) on the particle drifting motion is investigated. it is found that the percentage of drifting particles is not only a function of the amplitude of the excitation field but also a function of possible electrostatic charges developed on the particles due to the drifting motion on the substrate. in addition to their potential uses in particle manipulation, removal and adhesion characterization, the reported results could be utilized in numerical simulations of microparticle motion and deposition. (c) 2017 elsevier b.v. all rights reserved.
computer_vision	a stereovision based methodology to estimate the position, speed and heading of a moving marine vehicle from a pursuing unmanned surface vehicle (usv) is considered, in support of enabling a usv to follow a target vehicle in motion. the methodology involves stereovision ranging, object detection and tracking, and minimization of tracking error due to image quantization limitations and pixel miscorrespondences in the stereo pixel-matching process. the method consists of combining a simple stereovision-matching algorithm, together with a predictive-corrective approach based on an extended kalman filter (ekf), and use of suitable choices of probabilistic models representing the motion of the target vehicle and the stereovision measurements. simple matching algorithms perform faster at the expense of potential errors in depth measurement. the approach considered aims to minimize the tracking errors related to such errors in stereovision measurements, thereby improving the accuracy of the state estimation of the vehicle. results from simulations and a real-time implementation reveal the effectiveness of the system to compute accurate estimates of the state of the target vehicle over non-compliant trajectories subjected to a variety of motion conditions.
software_engineering	the selection of which requirements should be implemented in the next software release is an important and complex task in the software development process, considering the presence of budget constraints and other conflicting aspects. in this context, search based software engineering, has the main objective of applying automatic search methods to solve complex software engineering problems. however, most of these methods do not consider human expertise during the search, especially due to the difficulty in mathematically modeling the user 's preferences. consequently, the user can present some resistance or place little confidence in the final results, given that his/her knowledge and domain expertise was not properly considered in the solution construction. this paper aims at proposing an interactive model for the next release problem using ant colony optimization, where the user can define which requirements he/she would like to include or not in the next release. employing humans and a simulator, an empirical study was performed that considers real-world and artificial instances. the achieved results demonstrate that the loss of score was, on average, 12% when it was compared with a solution with no human intervention. on the other hand, the algorithm generates solutions that have more than 80% of the met preferences, as defined by the users. furthermore, the results showed that aco can be an interesting choice as an interactive search engine, given the low quantity of interactions that are required to reach good solutions. (c) 2016 elsevier b.v. all rights reserved.
software_engineering	double absorption heat transformer (daht) is a promising device in reducing the use of fossil fuels since it can utilize renewable sources or waste heat to provide high temperature energy. the absorber evaporator is an important component in the daht system, and there exists an optimum absorber evaporator temperature (oaet) at which the maximum coefficient of performance (cop) and exergy efficiency (ecop) can be obtained simultaneously. in this paper, an optimization study is carried out by means of a parametric analysis, using a mathematical model developed in the software engineering equation solver. the effects of the operating parameters such as the absorber, condenser, evaporator and generator temperatures and design parameters including the first and second economizer efficiencies on the oaet and corresponding maximum cop and ecop have been analyzed in detail. besides, some suggestions derived from the results are also given. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	computer programming subject is a core ingredient for most of the engineering disciplines. however for the first year engineering, teaching and learning fundamentals of programming language like c has been considered as a great challenge to both teachers and novice learners. the challenges in learning the programming languages are: the learners are not able to analyze the flow of program, cannot fix errors or not able to debug the program, usually dependent on others for problem solving, bug fixing. programming skills and documentation are the most important aspects of software development. the structure of c program with proper indentation; comments & proper named variables can reduce most of the general compiler errors. the study proposes a novice way of teaching c language programming with effective program writing skills for the beginners. the experiment is conducted for the first year civil engineering students and the course is computer programming based on c language. a training session on effective program writing skills is conducted, to improve the ability of program writing skills with minimized compiler errors for novice learners. the effect of the experiment is verified with pretest & posttest before and after the training session. it is observed that the program written with proper indentation, comments and proper named variable can reduce the number of compiler errors and more than 60% of the student have written & executed the given c programs successfully in a stipulated time.
network_security	scheduling for generation units in modern power markets is one of the most important tasks of independent system operators (iso). to ensure secure and economical operation of power systems, isos solve the security constrained unit commitment (scuc) problem. scuc is a very complex and large-scale optimization problem. in addition, uncertainties imposed by wind generators and variable loads add to this complexity. stochastic scuc, one of the most important problems for scheduling generation units, considers different power system uncertainties. a considerable amount of previous works demonstrates the efficacy of decomposition techniques, such as benders' decomposition for stochastic scuc. solving this problem, even with decomposition techniques, for large-scale systems with multiple uncertainties is very time-consuming. a major part of stochastic scuc constraints belongs to network security. this paper presents a method in which cumulants are used to detect and eliminate inactive network constraints during the solving process. as a result, the method reduces the number of network security constraints and decreases the simulation time considerably. implementing the proposed method on a six -bus test system and a modified ieee 118-bus system demonstrates its efficiency and accuracy.
algorithm_design	network control strategies for energy-efficient operation of hetnets need to match the dynamics of spatial and temporal traffic loads and to stabilize the network. in this paper, we develop a stochastic optimization framework, which formulates spatially inhomogeneous traffic distributions and time-varyingly random traffic arrivals and guarantees network stability, to investigate the energy conservation problem in hetnets. in particular, we jointly optimize base station (bs) operation, user association, subcarrier assignment, and power allocation to minimize the average energy consumption. we devise an algorithm without requiring any prior-knowledge of traffic distributions, referred to as the steerable energy expenditure algorithm (seed), to solve the problem. to deal with a highly coupled and mixed combinational subproblem in the seed, we separate optimization variables for suboptimal but cost-efficient and easy-to-implement algorithm design. by this, we develop closed-form solutions for both user association and subcarrier assignment, a fast and tuning-free algorithm that provably achieves at least local optimality for power allocation, and a greedy-style heuristic algorithm for bs operation with polynomial complexity. simulation results exhibit that the seed usually converges fast, can flexibly tune the power-delay tradeoff, and can significantly reduce energy consumption against other existing schemes.
distributed_computing	traditionally, the allocation and dynamic adaptation of federated cyberinfrastructure resources residing across multiple domains for data-intensive application workflows have been performance or quality of service-centric (i.e., qspecs); often compromising the end-to-end security requirements of scientific workflows. lack of standardized formalization methods of the workflows' end-to-end security requirements, and diverse/heterogenous domain resource and security policies make inter-conflict characterization between application 's security and performance requirements non-trivial, and leads to sub-optimal resource allocation. in this paper, we present a joint security and performance-driven federated resource allocation and adaptation scheme to define and characterize a data-intensive scientific application 's security specifications (i.e., sspecs). in order to aid security-driven resource brokering among domains with diverse security postures, we describe an alignment technique inspired by portunes algebra to combine domain-specific resource policies (i.e., rspecs) along the application workflow life cycle. we use standardized guidelines that help in compute/storage resource domain/location selection as well as network path selection based on both application qspecs and sspecs. we implement our security formalization and alignment methods as a framework, viz., ""ontimeurb"" and apply it on an exemplar distributed computing workflow to show the benefits of joint qspecs-sspecs-driven, rspecs-compliant federated workflow management.
operating_systems	today, wireless sensor networks (wsns) with open source operating systems still need many efforts to guarantee that the protocol stack succeeds in delivering its expected performance. this is due to subtle implementation problems and unexpected interactions between protocol layers. the subtleties are often related to the judicious choice of parameters, in particular those related to timing issues. as these issues are often not visible in simulation studies, this paper proposes a low-cost versatile measurement testbed and demonstrates its usefulness in measuring the performance of rdc protocols. we demonstrate how the testbed helped to identify bugs in the implementation of an rdc protocol.
symbolic_computation	evolution equation is among one of the important nonlinear partial differential equations and its exact solutions are of great interest for the researchers around the globe. in this article, a new analytical technique is used to search for exact solutions of nonlinear evolution equation with the aid of symbolic computation. to check the validity of the method developed, we choose the srlw equation and many new and more general exact solutions have been obtained for the equation, which are of great importance. the efficiency of the developed scheme is confirmed since it provides more exact solutions than other techniques used now a day.
operating_systems	internet of things (iot) is an environment in which everywhere and every device became smart in a smart world. internet of things is growing vastly, it is implemented using smart devices which involve with embedded systems (ess). real time operating systems (rtos) are used in ess development due to rtos added important features as rtos simplifies development and makes systems more reliable. many researches directed to the internet of things, rtos became a part of iot development. in this paper, a generic vision and architecture of rtos for iot and the way it could be implemented in different applications are presented. implementation and testing methods of the rtos for iot are discussed. finally, research directions in using rtos for iot are discussed.
distributed_computing	in this paper, a multi-objective framework is proposed for the daily operation of a smart grid (sg) with high penetration of sensitive loads. the virtual power player (vpp) manages the day-ahead energy resource scheduling in the smart grid, considering the intensive use of distributed generation (dg) and vehicle-to-grid (v2g), while maintaining a highly reliable power for the sensitive loads. this work considers high penetration of sensitive loads, i.e. loads such as some industrial processes that require high power quality, high reliability and few interruptions. the weighted-sum approach is used with the distributed and parallel computing techniques to efficiently solve the multi-objective problem. a two-stage optimization method is proposed using a particle swarm optimization (pso) and a deterministic technique based on mixed-integer linear programming (milp). a realistic mathematical formulation considering the electric network constraints for the day-ahead scheduling model is described. the execution time of the large-scale problem can be reduced by using a parallel and distributed computing platform. a pareto front algorithm is applied to determine the set of non-dominated solutions. the maximization of the minimum available reserve is incorporated in the mathematical formulation in addition to the cost minimization, to take into account the reliability requirements of sensitive and vulnerable loads. a case study with a 180-bus distribution network and a fleet of 1000 gridable electric vehicles (evs) is used to illustrate the performance of the proposed method. the execution time to solve the optimization problem is reduced by using distributed computing. (c) 2015 elsevier ltd. all rights reserved.
cryptography	utilizing complex dynamics of chaotic maps and systems in encryption was studied comprehensively in the pas two and a half decades. in 1989, fridrich 's chaotic image encryption scheme was designed by iterating chaotic position permutation and value substitution some rounds, which received intensive attention in the field of chaos-based cryptography. in 2010, solak et al. proposed a chosen-ciphertext attack on the fridrich 's scheme utilizing influence network between cipher-pixels and the corresponding plain-pixels. based on their creative work, this paper scrutinized some properties of fridrich 's scheme with concise mathematical language. then, some minor defects of the real performance of solak 's attack method were given. the work provides some base. for further optimizing attack on the fridrich 's, scheme and its variants.
software_engineering	this paper presents a documentation and development method to facilitate the certification of scientific computing software used in the safety analysis of nuclear facilities. to study the problems faced during quality assurance and certification activities, a case study was performed on legacy software used for thermal analysis of a fuelpin in a nuclear reactor. although no errors were uncovered in the code, 27 issues of incompleteness and inconsistency were found with the documentation. this work proposes that software documentation follow a rational process, which includes a software requirements specification following a template that is reusable, maintainable, and understandable. to develop the design and implementation, this paper suggests literate programming as an alternative to traditional structured programming. literate programming allows for documenting of numerical algorithms and code together in what is termed the literate programmer 's manual. this manual is developed with explicit traceability to the software requirements specification. the traceability between the theory, numerical algorithms, and implementation facilitates achieving completeness and consistency, as well as simplifies the process of verification and the associated certification. copyright (c) 2015, published by elsevier korea llc on behalf of korean nuclear society.
cryptography	this paper proposes a new cryptosystem system that combines dna cryptography and algebraic curves defined over different galois fields. the security of the proposed cryptosystem is based on the combination of dna encoding, a compression process using a hyperelliptic curve over a galois field , and coding via an algebraic geometric code built using a hermitian curve on a galois field , where . the proposed cryptosystem resists the newest attacks found in the literature because there is no linear relationship between the original data and the information encoded with the hermitian code. further, the work factor for such attacks increases proportionally to the number of possible choices for the generator matrix of the hermitian code. simulations in terms of ber and signal-to-noise ratio (snr) are included, which evaluate the gain of the transmitted data in an awgn channel. the performance of the dna/ag cryptosystem scheme is compared with un-coded qpsk, and the mceliece code in terms of ber. further, the proposed dna/ag system outperforms the security level of the mceliece algorithm.
machine_learning	background: phenotyping is a critical component of plant research. accurate and precise trait collection, when integrated with genetic tools, can greatly accelerate the rate of genetic gain in crop improvement. however, efficient and automatic phenotyping of traits across large populations is a challenge; which is further exacerbated by the necessity of sampling multiple environments and growing replicated trials. a promising approach is to leverage current advances in imaging technology, data analytics and machine learning to enable automated and fast phenotyping and subsequent decision support. in this context, the workflow for phenotyping (image capture. data storage and curation ->trait extraction ->machine learning/classification ->models/apps for decision support) has to be carefully designed and efficiently executed to minimize resource usage and maximize utility. we illustrate such an end- to- end phenotyping workflow for the case of plant stress severity phenotyping in soybean, with a specific focus on the rapid and automatic assessment of iron deficiency chlorosis (idc) severity on thousands of field plots. we showcase this analytics framework by extracting idc features from a set of similar to 4500 unique canopies representing a diverse germplasm base that have different levels of idc, and subsequently training a variety of classification models to predict plant stress severity. the best classifier is then deployed as a smartphone app for rapid and real time severity rating in the field. results: we investigated 10 different classification approaches, with the best classifier being a hierarchical classifier with a mean per-class accuracy of similar to 96%. we construct a phenotypically meaningful ' population canopy graph', connecting the automatically extracted canopy trait features with plant stress severity rating. we incorporated this image capture ->image processing ->classification workflow into a smartphone app that enables automated real-time evaluation of idc scores using digital images of the canopy. conclusion: we expect this high-throughput framework to help increase the rate of genetic gain by providing a robust extendable framework for other abiotic and biotic stresses. we further envision this workflow embedded onto a high throughput phenotyping ground vehicle and unmanned aerial system that will allow real-time, automated stress trait detection and quantification for plant research, breeding and stress scouting applications.
network_security	network and computer systems administrators are facing a serious problem of the big network traffic data analysis. it became difficult work of administrators to extract and analysis the abnormal and normal patterns from large amounts of the network traffic data. currently, traditional relational database management systems (rdbms) are unsuitable to store a large amount of data because they are designed for storing and processing the structured data. hive is a data warehouse tool built on top of hadoop for storing, processing, querying, and analysis the large amount of data. hive stores the data in a table similar the relational database management system. in this paper, we propose a hadoop-based traffic querying and analyzing system that handles the tcp, icmp, and udp analysis of the big network traffic data. the system consists of six modules: data collection module, transferring and storing information module, convertor module, data mining process module, dm2sc module, and report module. we also performed complex search queries and compared the query response times of mysql against hive in hadoop environment. as the result, in some scenario, mysql outperform a cluster of four hive nodes on querying the icmp protocol information, nevertheless, mysql database that stored more than the network traffic data about 45 million records cannot be query the tcp protocol information. moreover, we observed that the average query response times of hive in hadoop cluster that reduce continuously be scale up node into the cluster.
algorithm_design	this paper presents the design and experimental validation of a new model-free data-driven iterative reference input tuning (irit) algorithm that solves a reference trajectory tracking problem as an optimization problem with control signal saturation constraints and control signal rate constraints. the irit algorithm design employs an experiment-based stochastic search algorithm to use the advantages of iterative learning control. the experimental results validate the irit algorithm applied to a non-linear aerodynamic position control system. the results prove that the irit algorithm offers the significant control system performance improvement by few iterations and experiments conducted on the real-world process and model-free parameter tuning.
network_security	cloud computing is a paradigm that provides scalable it resources as a service over the internet. vulnerabilities in the cloud infrastructure have been readily exploited by the adversary class. therefore, providing the desired level of assurance to all stakeholders through safeguarding data (sensitive or otherwise) which is stored in the cloud, is of utmost importance. in addition, protecting the cloud from adversarial attacks of diverse types and intents, cannot be understated. economic denial of sustainability (edos) attack is considered as one of the concerns that has stalled many organizations from migrating their operations and/or data to the cloud. this is because an edos attack targets the financial component of the service provider. in this work, we propose a novel and reactive approach based on a rate limit technique, with low overhead, to detect and mitigate edos attacks against cloud-based services. through this reactive scheme, a limited access permission for cloud services is granted to each user. experiments were conducted in a laboratory cloud setup, to evaluate the performance of the proposed mitigation technique. results obtained show that the proposed approach is able to detect and prevent such an attack with low cost and overhead. (c) 2016 elsevier b.v. all rights reserved.
operating_systems	the popularity of smartphones usage especially android mobile platform has increased to 80% of share in global smartphone operating systems market, as a result of which it is on the top in the attacker 's target list. the fact of having more private data and low security assurance letting the attacker to write several malware programs in different ways for smartphone, and the possibility of obfuscating the malware detection applications through different coding techniques is giving more energy to attacker. several approaches have been proposed to detect malwares through code analysis which are now severely facing the problem of code obfuscation and high computation requirement. we propose a machine learning based method to detect android malware by analyzing the visual representation of binary formatted apk file into grayscale, rgb, cmyk and hsl. gist feature from malware and benign image dataset were extracted and used to train machine learning algorithms. initial experimental results are encouraging and computationally effective. among machine learning algorithms random forest have achieved highest accuracy of 91% for grayscale image, which can be further improved by tuning the various parameters.
computer_programming	the computer technology has advanced profoundly that the application seems to have no limit. equipped with programming, our research team has created programs that could be used practically in the field of chemistry. the purpose of this paper is not on the making useful tools for science, but on using analytic tools based on computer programming to find additional evidence that supports a theory. the suport vector machine (also known as its acronym, svm) is used frequently in genetic analysis to find certain patterns in dna sequence. this paper deals with pattern similarity between rrna of mitochondria and that of alphaproteobacteria, which is believed to be the ancestor of the mitochondria. this theory, also known as ""endosymbiotic theory"" has a variety of evidences and has accepted as authentic. the pattern similarity between the two organisms' dna sequence, which is the result of the paper would consolidate the evolutionary endosymbiosis.
parallel_computing	natural source electromagnetic methods have the potential to recover rock property distributions from the surface to great depths. unfortunately, results in complex 3d geo-electrical settings can be disappointing, especially where significant near-surface conductivity variations exist. in such settings, unconstrained inversion of magnetotelluric data is inexorably non-unique. we believe that: (1) correctly introduced information from seismic reflection can substantially improve mt inversion, (2) a cooperative inversion approach can be automated, and (3) massively parallel computing can make such a process viable. nine inversion strategies including baseline unconstrained inversion and new automated/semiautomated cooperative inversion approaches are applied to industry-scale co-located 3d seismic and magnetotelluric data sets. these data sets were acquired in one of the carlin gold deposit districts in north-central nevada, usa. in our approach, seismic information feeds directly into the creation of sets of prior conductivity model and covariance coefficient distributions. we demonstrate how statistical analysis of the distribution of selected seismic attributes can be used to automatically extract subvolumes that form the framework for prior model 3d conductivity distribution. our cooperative inversion strategies result in detailed subsurface conductivity distributions that are consistent with seismic, electrical logs and geochemical analysis of cores. such 3d conductivity distributions would be expected to provide clues to 3d velocity structures that could feed back into full seismic inversion for an iterative practical and truly cooperative inversion process. we anticipate that, with the aid of parallel computing, cooperative inversion of seismic and magnetotelluric data can be fully automated, and we hold confidence that significant and practical advances in this direction have been accomplished.
computer_graphics	due to formal protection and commercial development, cultural ecology and cultural forms of vernacular architecture landscape that have local characteristics have been threatened seriously. however, at present the records of vernacular architecture landscape protection lack support of new and high technology. three-dimensional panoramic hybrid technology can blend the virtual objects accurately into real environment through computer graphics and computer vision technology. this technique provides an effective way to express a stereoscopic three-dimensional real scene. in this research, three-dimensional panoramic hybrid technology is reasonably applied to implementation strategy and process of vernacular architecture landscape protection, which can make up the inadequacy of application field of vernacular architecture heritage protection, and can extend application of information technology based on 3-dimensional digitalization in architecture heritage protection at the same time.
symbolic_computation	under investigation in this paper is a nonisospectral and variable-coefficient fifth order korteweg-de vries equation in fluids. by virtue of the bell polynomials and symbolic computation, the bilinear form, backlund transformation and lax pair are obtained. based on its bilinear form, n-soliton solutions are constructed. furthermore, periodic wave and breather wave solutions are obtained by virtue of the riemann theta function and homoclinic test approach, respectively. in addition, the characteristic-line method is applied to discuss the features of the solitons for the nonisospectral problem, such as the amplitude, velocity and width of the solitary wave. (c) 2016 elsevier ltd. all rights reserved.
bioinformatics	amifostine, 2-(3-aminopropyl) aminoethyl phosphorothioate, is a broad-spectrum cytoprotective agent used to treat nuclear radiation and chemical weapon injuries. recently, amifostine has been shown to have a profound biological influence on tumor cells. to examine the effects and mechanisms underlying the effects of amifostine on human acute megakaryocytic leukemia, we evaluated the efficacy of amifostine against dami cells and observed a cell cycle arrest in g(2)/m phase. amifostine treatment also induced cell apoptosis of dami cells which corresponds to formal studies. through whole-genome microarray and bioinformatics analyses, we found that amifostine affected the gene expression of ccnd1,bcl2, and casp3 which revealed the mechanism amifostine acted on dami cells. thus, ccnd1-bcl2 gene network is predicted to be a direct target of amifostine treating human acute megakaryocytic leukemia, which may provide a novel potential target for the therapy of several subtypes of human aml.
network_security	network intrusion attempts have been on the rise recently. researchers have shown an increased interest in assessing the security situation for entire network instead of single asset. a considerable amount of assessment models have been designed. however, there is a lack of solid and standard guidelines to define the importance of network asset. in addition, based on our knowledge, no research has been found that adequately covered the cost factor in the assessment model. thus, the purpose of this paper is to propose a cost-sensitive entropy-based network security situation assessment model. with the aid of analytic hierarchy process (ahp), the model can quantitatively determine the importance of assets in the network by considering the tangible and intangible criteria. to verify the performance of proposed model, a simulation of national advanced ipv6 centre (nav6) 's network environment has been setup. the simulation results regarding security situation in particular time-interval are promising. hence, the proposed model is able to provide network administrator a more reliable reference before any further decision making for the organization 's network.
computer_programming	dzhafarov and kujala (2015) have introduced a contextual probability theory called contextuality-by-default (c-b-d) which is based on three principles. the first of these principles states that each random variable should be automatically labelled by all conditions under which it is recorded. the aim of this article is to relate this principle to block structured computer programming languages where variables are declared local to a construct called a ""scope"". scopes are syntactic constructs which correspond to the notion of condition used by c-b-d. in this way a variable declared in two scopes can be safely overloaded meaning that they can have the same label but preserve two distinct identities without the need to label each variable in each condition as advocated by c-b-d. by means of examples, the notion of a probabilistic-program, or p-program, is introduced which is based on scopes. the semantics of p-programs will be illustrated using the well known relational database language sql which provides an efficient and understandable operational semantics. a core issue addressed is how to construct a single probabilistic model from the various interim probability distributions returned by each syntactic scope. for this purpose, a probabilistic variant of the natural join operator of relational algebra is used to ""glue"" together interim distributions into a single distribution. more generally, this article attempts to connect contextuality with probabilistic programming by means of relational database theory. (c) 2016 published by elsevier inc.
computer_graphics	changes in the human pigmentary system can lead to imbalances in the distribution of melanin in the skin resulting in artefacts known as pigmented lesions. our work takes as departing point biological data regarding human skin, the pigmentary system and the melanocytes life cycle and presents a reaction-diffusion model for the simulation of the shape features of human-pigmented lesions. the simulation of such disorders has many applications in dermatology, for instance, to assist dermatologists in diagnosis and training related to pigmentation disorders. our study focuses, however, on applications related to computer graphics. thus, we also present a method to seamless blend the results of our simulation model in images of healthy human skin. in this context, our model contributes to the generation of more realistic skin textures and therefore more realistic human models. in order to assess the quality of our results, we measured and compared the characteristics of the shape of real and synthesized pigmented lesions. we show that synthesized and real lesions have no statistically significant differences in their shape features. visually, our results also compare favourably with images of real lesions, being virtually indistinguishable from real images.
machine_learning	background and objective: according to the world health organization (who) epilepsy affects approximately 45-50 million people. electroencephalogram (eeg) records the neurological activity in the brain and it is used to identify epilepsy. visual inspection of eeg signals is a time-consuming process and it may lead to human error. feature extraction and classification are two main steps that are required to build an automated epilepsy detection framework. feature extraction reduces the dimensions of the input signal by retaining informative features and the classifier assigns a proper class label to the extracted feature vector. our aim is to present effective feature extraction techniques for automated epileptic eeg signal classification. methods: in this study, two effective feature extraction techniques (local. neighbor descriptive pattern [lndp] and one-dimensional local gradient pattern [1d-lgp]) have been introduced to classify epileptic eeg signals. the classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers. the benchmark epilepsy eeg dataset provided by the university of bonn is used in this research. the classification performance is evaluated using 10 -fold cross validation. the classifiers used are the nearest neighbor (nn), support vector machine (svm), decision tree (dt) and artificial neural network (ann). the experiments have been repeated for 50 times. results: lndp and 1d-lgp feature extraction techniques with ann classifier achieved the average classification accuracy of 99.82% and 99.80%, respectively, for the classification between normal and epileptic eeg signals. eight different experimental cases were tested. the classification results were better than those of some existing methods. conclusions: this study suggests that lndp and 1d-lgp could be effective feature extraction techniques for the classification of epileptic eeg signals. (c) 2017 elsevier ltd. all rights reserved.
algorithm_design	greedy algorithms is an important class of algorithms. teaching greedy algorithms is a complex task. ensuring that students can design greedy algorithms for new problems is also complex. we have built a guided discovery based greedy algorithm tutor (gatutor), to teach the process of designing greedy algorithms. gatutor guides the student to discover the greedy algorithm for a few well-known problems, by asking two important questions - i) what is the satisfying condition at each step? and ii) what is the selection criterion for the next item? as a result, the students not only learn the algorithms for the given problems, but also the process of designing greedy algorithms for new problems. we conducted a study to compare the greedy algorithm design abilities of the students who were trained with gatutor versus those who worked with traditional algorithm visualizations. the results indicate that students who worked with gatutor performed better in designing a greedy algorithm for a new problem. the students also said that their confidence in greedy algorithm design increased because of gatutor.
cryptography	super-simple designs can be used to provide samples with maximum intersection as small as possible in statistical planning of experiments and can be also applied to cryptography and codes. in this paper, super-simple pairwise balanced designs with block sizes 3 and 4 are investigated and it is proved that the necessary conditions for the existence of a super-simple (nu,{3, 4}, lambda)-pbd for 2 <= lambda <= 6 are sufficient with three possible exceptions. (c) 2016 elsevier b.v. all rights reserved.
cryptography	in the classical secret-key generation model, common randomness is generated by two terminals based on the observation of correlated components of a common source, while keeping it secret from a non-legitimate observer. it is assumed that the statistics of the source are known to all participants. in this paper, the secret-key generation based on a compound source is studied where the realization of the source statistic is unknown. the protocol should guarantee the security and reliability of the generated secret-key, simultaneously for all possible realizations of the compound source. a single-letter lower-bound of the secret-key capacity for a finite compound source is derived as a function of the public communication rate constraint. a multi-letter capacity formula is further computed for a finite compound source for the case in which the public communication is unconstrained. finally, a single-letter capacity formula is derived for a degraded compound source with an arbitrary (possibly infinite) set of source states and a finite set of marginal states.
software_engineering	adaptive educational hypermedia systems (aehs) have provided new perspectives for access to information and enhance learning. however aehs have advantages and positive impact on learning, there are still defies to their design and production. in this paper, we propose an agile learning design method to support the design and production of aehs. it is based on agile practices from software engineering and on practices of learning design. we illustrate our approach with an experiment that validates the proposed method through its application in the design of an aehs called (ald-aehs) and creation of one of the most important component of aehs: the user model.
network_security	honeypots, i.e. networked computer systems specially designed and crafted to mimic the normal operations of other systems while capturing and storing information about the interactions with the world outside, are a crucial technology into the study of cyber threats and attacks that propagate and occur through networks. among them, high-interaction honeypots are considered the most efficient because the attacker (whether automated or not) perceives realistic interactions with the target machine. in the case of automated attacks, propagated by malware, currently available honeypots alone are not specialized enough to allow the analysis of their behaviors and effects on the target system. the research presented in this paper shows how high-interaction honeypots can be enhanced by powering them with specific features that improve the reverse engineering activities needed to effectively analyze captured malicious entities.
computer_vision	when designing and developing scale selection mechanisms for generating hypotheses about characteristic scales in signals, it is essential that the selected scale levels reflect the extent of the underlying structures in the signal. this paper presents a theory and in-depth theoretical analysis about the scale selection properties of methods for automatically selecting local temporal scales in time-dependent signals based on local extrema over temporal scales of scale-normalized temporal derivative responses. specifically, this paper develops a novel theoretical framework for performing such temporal scale selection over a time-causal and time-recursive temporal domain as is necessary when processing continuous video or audio streams in real time or when modelling biological perception. for a recently developed time-causal and time-recursive scale-space concept defined by convolution with a scale-invariant limit kernel, we show that it is possible to transfer a large number of the desirable scale selection properties that hold for the gaussian scale-space concept over a non-causal temporal domain to this temporal scale-space concept over a truly time-causal domain. specifically, we show that for this temporal scale-space concept, it is possible to achieve true temporal scale invariance although the temporal scale levels have to be discrete, which is a novel theoretical construction. the analysis starts from a detailed comparison of different temporal scale-space concepts and their relative advantages and disadvantages, leading the focus to a class of recently extended time-causal and time-recursive temporal scale-space concepts based on first-order integrators or equivalently truncated exponential kernels coupled in cascade. specifically, by the discrete nature of the temporal scale levels in this class of time-causal scale-space concepts, we study two special cases of distributing the intermediate temporal scale levels, by using either a uniform distribution in terms of the variance of the composed temporal scale-space kernel or a logarithmic distribution. in the case of a uniform distribution of the temporal scale levels, we show that scale selection based on local extrema of scale-normalized derivatives over temporal scales makes it possible to estimate the temporal duration of sparse local features defined in terms of temporal extrema of first- or second-order temporal derivative responses. for dense features modelled as a sine wave, the lack of temporal scale invariance does, however, constitute a major limitation for handling dense temporal structures of different temporal duration in a uniform manner. in the case of a logarithmic distribution of the temporal scale levels, specifically taken to the limit of a time-causal limit kernel with an infinitely dense distribution of the temporal scale levels towards zero temporal scale, we show that it is possible to achieve true temporal scale invariance to handle dense features modelled as a sine wave in a uniform manner over different temporal durations of the temporal structures as well to achieve more general temporal scale invariance for any signal over any temporal scaling transformation with a scaling factor that is an integer power of the distribution parameter of the time-causal limit kernel. it is shown how these temporal scale selection properties developed for a pure temporal domain carry over to feature detectors defined over time-causal spatio-temporal and spectro-temporal domains.
symbolic_computation	under investigation in this paper is a (2+1)-dimensional gross-pitaevskii equation with time-varying trapping potential, which describes the dynamics of the (2+1)-dimensional bose-einstein condensate. employing the hirota method and symbolic computation, we obtain the dark one-soliton, two-soliton, three-soliton, breather-wave and rouge-wave solutions, respectively. we graphically study the dark solitons with the time-varying harmonic potential and scaled scattering length. parallel and period solitons are observed. we obtain that when the external trapping potential increases with time, amplitudes of the dark solitons increase and widths of those solitons become narrower; when the external trapping potential is a periodic function, amplitudes and widths of the dark solitons periodically change. decrease in the scaled scattering length leads to the narrower solitons' widths, but does not affect the solitons' amplitudes. breather waves and rouge waves are also displayed: rouge waves emerge when the period of the breather waves go to the infinity.
bioinformatics	campylobacter concisus is a bacterium that is associated with inflammatory bowel disease (ibd). immunosuppressive drugs including azathioprine (aza) and mercaptopurine (mp), and anti-inflammatory drug such as 5-aminosalicylic acid (5-asa) are commonly used to treat patients with ibd. this study aimed to examine the effects of aza, mp, and 5-asa on the growth of ibd-associated bacterial species and to identify bacterial enzymes involved in immunosuppressive drug metabolism. a total of 15 bacterial strains of five species including 11 c. concisus strains, bacteroides fragilis, bacteroides vulgatus, enterococcus faecalis, and escherichia coli were examined. the impact of aza, mp, and 5-asa on the growth of these bacterial species was examined quantitatively using a plate counting method. the presence of enzymes involved in aza and mp metabolism in these bacterial species was identified using bioinformatics tools. aza and mp significantly inhibited the growth of all 11 c. concisus strains. c. concisus strains were more sensitive to aza than mp. 5-asa showed inhibitory effects to some c. concisus strains, while it promoted the growth of other c. concisus strains. aza and mp also significantly inhibited the growth of b. fragilis and b. vulgatus. the growth of e. coli was significantly inhibited by 200 mu g/ml of aza as well as 100 and 200 mu g/ml of 5-asa. bacterial enzymes related to aza and mp metabolism were found, which varied in different bacterial species. in conclusion, aza and mp have inhibitory effects to ibd-associated c. concisus and other enteric microbes, suggesting an additional therapeutic mechanism of these drugs in the treatment of ibd. the strain dependent differential impact of 5-asa on the growth of c. concisus may also have clinical implication given that in some cases 5-asa medications were found to cause exacerbations of colitis.
computer_graphics	the development of technologies nowadays is moving in such speed that the computer graphics and simulations are being convincingly displaced by the augmented reality. the ar sandbox, created by oliver kreylos, is just one of the many examples in which the reality is supplemented with computer generated input. in this case the reality is a box full of sand and the input is hypsometric coloring and elevation contours. a kinect sensor detects the micro relief forms in the sandbox and after unnoticeable computer estimation, a relief map is projected over them. if the sand forms are changing, the coloring and the lines are changing with them to project the new accurate relief. actually ar sandbox represents important conceptions of geology, hydrology, ecology, topographic mapping, etc. in a very entertaining and spectacular way for children and students and this is a main reason this system to be part of our equipment in the laboratory of cartography in uaceg. included in lessons and games the ar sandbox is an irreplaceable tool for improving their knowledge of disastrous events such as floods, drought and fire especially when it gives the opportunity of making virtual rain and isolating flooded areas depending on the relief and the watersheds. in this paper after the detailed presentation of the ar sandbox as a working system, proposals for educational activities for large age range of children and university students are made in order to use the augmented reality as a special instrument for displaying disastrous events. some new ideas are suggested in consideration of future improvement with which ar sandbox will meet more needs of the educational training for disaster response of children.
relational_databases	forensic tools assist analysts with recovery of both the data and system events, even from corrupted storage. these tools typically rely on ""file carving"" techniques to restore files after metadata loss by analyzing the remaining raw file content. a significant amount of sensitive data is stored and processed in relational databases thus creating the need for database forensic tools that will extend file carving solutions to the database realm. raw database storage is partitioned into individual ""pages"" that cannot be read or presented to the analyst without the help of the database itself. furthermore, by directly accessing raw database storage, we can reveal things that are normally hidden from database users. there exists a number of database-specific tools developed for emergency database recovery, though not usually for forensic analysis of a database. in this paper, we present a universal tool that seamlessly supports many different databases, rebuilding table and other data content from any remaining storage fragments on disk or in memory. we define an approach for automatically (with minimal user intervention) reverse engineering storage in new databases, for detecting volatile data changes and discovering user action artifacts. finally, we empirically verify our tool 's ability to recover both deleted and partially corrupted data directly from the internal storage of different databases. (c) 2015 the authors. published by elsevier ltd on behalf of dfrws. this is an open access article under the cc by-nc-nd license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
software_engineering	the proliferation of information technologies and the diversity of problem domains that heavily rely on software tool applications promote computer-supported cooperative work as a challenging discipline that drives the development process of contemporary and future engineering methods, standards, and tools. consequently, a particular domain of expertise in engineering and scientific fields has emerged, demanding more advanced skills and deeper domain knowledge. the essential role of architectural design (ad) and urban planning (up) is to enable a forward-looking approach to building/facility creation. construction engineering (ce) expresses its routine primarily through a transition phase that transforms ideas to sustainable urban artifacts. the ce role appears as a combination of backward and downward looking to the same process/product all three domains are highly cooperative in the context of environmental engineering (ee). currently available software tools that support the ad, up and ce domains are far from simple. several recent software engineering studies suggest that, instead of developing a complex ""all in one solution"", the federation or orchestration of several related simple methods and tools seems promising. in this article, we discuss the basic ad, up and ce domain-cooperation aspects and suggest an extensible orchestration framework (exof) model that may support them. to verify the exof simulation and orchestration potential, we used its architectural model to orchestrate different software tools when performing the urban blocks daylight illumination simulation for different urban block morphology models. for the exact simulation, we used the normalized sun-exposition data for the city of novi sad, serbia. also presented is an illustration of the methodology, modeling and orchestration potential of exof for the selected case study, together with the results, obtained for typical 3d models of selected urban block morphology patterns. (c) 2016 elsevier b.v. all rights reserved.
cryptography	recombined fingerprints have been suggested as a convenient approach to improve the efficiency of anonymous fingerprinting for the legal distribution of copyrighted multimedia contents in p2p systems. the recombination idea is inspired by the principles of mating, recombination and heredity of the dna sequences of living beings, but applied to binary sequences, like in genetic algorithms. however, the existing recombination-based fingerprinting systems do not provide a convenient solution for collusion resistance, since they require ""double-layer"" fingerprinting codes, making the practical implementation of such systems a challenging task. in fact, collusion resistance is regarded as the most relevant requirement of a fingerprinting scheme, and the lack of any acceptable solution to this problem would possibly deter content merchants from deploying, any practical implementation of the recombination approach. in this paper, this drawback is overcome by introducing two non-trivial improvements, paving the way for a future real-life application of recombination-based systems. first, nuida et al. 's collusion-resistant codes are used in segment-wise fashion for the first time. second, a novel version of the traitor-tracing algorithm is proposed in the encrypted domain, also for the first time, making it possible to provide the buyers with security against framing. in addition, the proposed method avoids the use of public-key cryptography for the multimedia content and expensive cryptographic protocols, leading to excellent performance in terms of both computational and communication burdens. in fact, after bootstrapping, the merchant is no longer required to participate in any file transfer, reducing the investment in equipment required for content distribution to the bare minimum. the paper also analyzes the security and privacy properties of the proposed system both formally and informally, whereas the collusion resistance and the performance of the method are shown by means of experiments and simulations. (c) 2016 elsevier ltd. all rights reserved.
machine_learning	backgroundthe liver is the major site for alcohol metabolism in the body and therefore the primary target organ for ethanol (etoh)-induced toxicity. in this study, we investigated the invitro response of human liver cells to different etoh concentrations in a perfused bioartificial liver device that mimics the complex architecture of the natural organ. methodsprimary human liver cells were cultured in the bioartificial liver device and treated for 24hours with medium containing 150mm (low), 300mm (medium), or 600mm (high) etoh, while a control culture was kept untreated. gene expression patterns for each etoh concentration were monitored using affymetrix human gene 1.0 st gene chips. scaled expression profiles of differentially expressed genes (degs) were clustered using fuzzy c-means algorithm. in addition, functional classification methods, kegg pathway mapping and also a machine learning approach (random forest) were utilized. resultsa number of 966 (150mm etoh), 1,334 (300mm etoh), or 4,132 (600mm etoh) genes were found to be differentially expressed. dose-response relationships of the identified clusters of co-expressed genes showed a monotonic, threshold, or nonmonotonic (hormetic) behavior. functional classification of degs revealed that low or medium etoh concentrations operate adaptation processes, while alterations observed for the high etoh concentration reflect the response to cellular damage. the genes displaying a hormetic response were functionally characterized by overrepresented cellular ketone metabolism and carboxylic acid metabolism. altered expression of the genes bahd1 and h3f3b was identified as sufficient to classify the samples according to the applied etoh doses. conclusionsdifferent pathways of metabolic and epigenetic regulation are affected by etoh exposition and partly undergo hormetic regulation in the bioartificial liver device. gene expression changes observed at high etoh concentrations reflect in some aspects the situation of alcoholic hepatitis in humans.
computer_programming	computers are the leading technology of the 21st century. programming, the development of software is thus a fundamental activity in which many people are engaged worldwide. therefore programming courses are included as a part of the curriculum. in these courses, students are primarily introduced to language features. traditionally, the students practice by applying the acquired knowledge to solve some logically straightforward problems giving less scope for the programming skills. this paper focuses on application of coding standards, coding techniques, debugging, code review, refactoring, code optimization, test driven programming and pair programming based learning to master not only programming language features, but also an integrated approach to gain problem solving and programming skills. the subject is introduced as a first year course where the students are without any or with smaller amount of background or experience in computer programming. taking this into consideration, activities like programming were designed. these activities enhanced the learning ability, problem solving skills programming skills and debugging skills
algorithm_design	this paper addresses the makespan minimization problem in scheduling flexible job shops whenever there exist separable sequence-dependent setup times. an extension to the neighborhood search functions of mastrolilli and gambradella, developed for the flexible job shop scheduling problem (fjsp), is provided. it is shown that under certain conditions such an extension is viable. accordingly, a randomized neighborhood search function is introduced, and its best search parameters are determined experimentally using modified fjsp benchmark instances. a tabu search approach utilizing the proposed neighborhood search function is then developed, and experimentations are conducted using the modified instances to benchmark it against a lower bound. experimental results show that on average, the tabu search approach is capable of achieving optimality gaps of below 10% for instances with low average setup time to processing time ratios. (c) 2015 elsevier inc. all rights reserved.
computer_graphics	the use of virtual prototypes and digital models containing thousands of individual objects is commonplace in complex industrial applications like the cooperative design of huge ships. designers are interested in selecting and editing specific sets of objects during the interactive inspection sessions. this is however not supported by standard visualization systems for huge models. in this paper we discuss in detail the concept of rendering front in multiresolution trees, their properties and the algorithms that construct the hierarchy and efficiently render it, applied to very complex cad models, so that the model structure and the identities of objects are preserved. we also propose an algorithm for the interactive inspection of huge models which uses a rendering budget and supports selection of individual objects and sets of objects, displacement of the selected objects and real-time collision detection during these displacements. our solution based on the analysis of several existing view-dependent visualization schemes uses a hybrid multiresolution tree that mixes layers of exact geometry, simplified models and impostors, together with a time-critical, view-dependent algorithm and a constrained front. the algorithm has been successfully tested in real industrial environments; the models involved are presented and discussed in the paper. (c) 2016 elsevier ltd. all rights reserved.
image_processing	this study describes image processing systems based on an artificial neural network to estimate tool wear. the single category-based classifier neural network was used to process tool image data. we present a method to determine the rate of tool wear based on image analysis, and discuss the evaluation of errors. using the proposed algorithm, we made in visual basic the special neural wear software for analysis of the worn part of the cutting edge. for example, the image of worn edge was created determining the optimum setting of neural wear software to automatically indicate the wear area. the result of the analysis was the number of pixels that belonged to the worn area. using these settings, we made an image analysis of edge wear for different working times. we used the calculated parameters of correlation between the number of pixels and v-b index. our results promise a good correlation between the new methods and the commonly used optically measured v-b index, with an absolute mean relative error of 6.7% for the tools' entire life range. automatic detection of wear of the cutting edge can be useful in many applications; for example, in predicting tool life based on the current value of edge wear.
bioinformatics	proteomics is the large-scale analysis of proteins. proteomic techniques, such as liquid chromatography tandem mass spectroscopy (lcms/ms), can characterize thousands of proteins at a time. these powerful techniques allow us to have a systemic understanding of cellular changes, especially when cells are subjected to various stimuli, such as infections, stresses, and specific test conditions. even with recent developments, analyzing the exosomal proteome is time-consuming and often involves complex methodologies. in addition, the resultant large dataset often needs robust and streamlined analysis in order for researchers to perform further downstream studies. here, we describe a silac-based protocol for characterizing the exosomal proteome when cells are infected with hiv-1. the method is based on simple isotope labeling, isolation of exosomes from differentially labeled cells, and mass spectrometry analysis. this is followed by detailed data mining and bioinformatics analysis of the proteomic hits. the resultant datasets and candidates are easy to understand and often offer a wealth of information that is useful for downstream analysis. this protocol is applicable to other subcellular compartments and a wide range of test conditions.
machine_learning	many agitation and mixing processes utilize various sensors for real-time monitoring and control, which can involve complex and costly equipment. for many mixing and agitation processes, such as in dough making, as mixing energy is placed, the resistance to extension increases and then after some point it decreases again. high-quality bread is obtained by stopping mixing at or close to the maximum resistance. the change in resistance causes a change in motor torque. the torque change affects the motor 's current draw for agitation and mixing machines driven by electrical motors. the rheological characteristics of the mixed material are related to motor torque of the mixing machine. therefore, it is related to the motor electric current where the load variation can be estimated by a low-cost current sensor. this paper outlines a novel design for an intelligent agitator/mixer process controller. the design is based on current sensing and on-line learning through reinforcement learning using operator input. the system provides a low-cost approach to automate various kinds of production equipment currently operated manually, which are common in the developing world. additionally, the approach requires minimal modification to the equipment: it requires only a current sensor, an on/off control relay, a set of buttons for operation, and an embedded system. (c) 2017 published by elsevier ltd.
machine_learning	the security of cryptographic systems is a major concern for cryptosystem designers, even though cryptography algorithms have been improved. side-channel attacks, by taking advantage of physical vulnerabilities of cryptosystems, aim to gain secret information. several approaches have been proposed to analyze side-channel information, among which machine learning is known as a promising method. machine learning in terms of neural networks learns the signature (power consumption and electromagnetic emission) of an instruction, and then recognizes it automatically. in this paper, a novel experimental investigation was conducted on field-programmable gate array (fpga) implementation of elliptic curve cryptography (ecc), to explore the efficiency of side-channel information characterization based on a learning vector quantization (lvq) neural network. the main characteristics of lvq as a multi-class classifier are that it has the ability to learn complex non-linear input-output relationships, use sequential training procedures, and adapt to the data. experimental results show the performance of multi-class classification based on lvq as a powerful and promising approach of side-channel data characterization.
machine_learning	there are many types of dependencies between software requirements, such as the contributions dependencies (make, some+, help, break, some-, hurt) and business dependencies modeled in the i* framework. however, current approaches for prioritizing requirements seldom take these dependencies into consideration, because it is difficult for stakeholders to prioritize requirements considering their preferences as well as the dependencies between requirements. to make requirement prioritization more practical, a method called drank is proposed. drank has the following advantages: 1) a prioritization evaluation attributes tree is constructed to make the ranking criteria selection easier and more operable; 2) rankboost is employed to calculate the subjective requirements prioritization according to stakeholder preferences, which reduces the difficulty of evaluating the prioritization; 3) an algorithm based on the weighted pagerank is proposed to analyze the dependencies between requirements, allowing the objective dependencies to be automatically transformed into partial order relations; and 4) an integrated requirements prioritization method is developed to amend the stakeholders' subjective preferences with the objective requirements dependencies and make the process of prioritization more reasonable and applicable. a controlled experiment performed to validate the effectiveness of drank based on comparisons with case based ranking, analytical hierarchy process, and evolve. the results demonstrate that drank is less time-consuming and more effective than alternative approaches. a simulation experiment demonstrates that taking requirement dependencies into consideration can improve the accuracy of the final prioritization sequence. (c) 2016 elsevier inc. all rights reserved.
machine_learning	this work proposes a clusterization algorithm called k-morphological sets (k-ms), based on morphological reconstruction and heuristics. k-ms is faster than the cpu-parallel k-means in worst case scenarios and produces enhanced visualizations of the dataset as well as very distinct clusterizations. it is also faster than similar clusterization methods that are sensitive to density and shapes such as mitosis and triclust. in addition, k-ms is deterministic and has an intrinsic sense of maximal clusters that can be created for a given input sample and input parameters, differing from k-means and other clusterization algorithms. in other words, given a constant k, a structuring element and a dataset, k-ms produces k or less clusters without using random/pseudo-random functions. finally, the proposed algorithm also provides a straightforward means for removing noise from images or datasets in general.
operating_systems	modern operating systems, such as linux, are capable of executing multiple parallel applications concurrently on many-core platforms. different applications may have different characteristics with regard to how they exercise the computation and memory resources in these platforms. this paper aims to investigate the impact of such differences on the overall energy consumption and performance trade offs. to analyze these tradeoffs, three parsec benchmark applications are chosen with different characteristics-memory-intensive, cpu-intensive and a mixture of both. these applications are then concurrently executed in various combinations in experiments, which also help establish optimized run-time controls in terms of dynamic voltage/frequency scaling (dvfs) and thread-to-core allocations at run-time. such controls are based on state-space models derived through linear regression using the feedback from hardware performance counters. using the benchmark applications, we demonstrate the effectiveness of our proposed method, which shows up to 23% improvement in power normalized performance expressed as the ratio between instructions per second (ips) and power consumption (watt).
parallel_computing	the runjags package provides a set of interface functions to facilitate running markov chain monte carlo models in jags from within r. automated calculation of appropriate convergence and sample length diagnostics, user-friendly access to commonly used graphical outputs and summary statistics, and parallelized methods of running jags are provided. template model specifications can be generated using a standard lme4-style formula interface to assist users less familiar with the bugs syntax. automated simulation study functions are implemented to facilitate model performance assessment, as well as drop-k type cross-validation studies, using high performance computing clusters such as those provided by parallel. a module extension for jags is also included within runjags, providing the pareto family of distributions and a series of minimally-informative priors including the dumouchel and half-cauchy priors. this paper outlines the primary functions of this package, and gives an illustration of a simulation study to assess the sensitivity of two equivalent model formulations to different prior distributions.
computer_graphics	the motion of a thin viscous film of fluid on a curved surface exhibits many intricate visual phenomena, which are challenging to simulate using existing techniques. a possible alternative is to use a reduced model, involving only the temporal evolution of the mass density of the film on the surface. however, in this model, the motion is governed by a fourth-order nonlinear pde, which involves geometric quantities such as the curvature of the underlying surface, and is therefore difficult to discretize. inspired by a recent variational formulation for this problem on smooth surfaces, we present a corresponding model for triangle meshes. we provide a discretization for the curvature and advection operators which leads to an efficient and stable numerical scheme, requires a single sparse linear solve per time step, and exactly preserves the total volume of the fluid. we validate our method by qualitatively comparing to known results from the literature, and demonstrate various intricate effects achievable by our method, such as droplet formation, evaporation, droplets interaction and viscous fingering. finally, we extend our method to incorporate non-linear van der waals forcing terms which stabilize the motion of the film and allow additional effects such as pearling.
relational_databases	information over the web is increasingly retrieved from relational databases in which the query language is based on exact matching, data fulfil completely the query or not. the results returned to the user contain only tuples that satisfy the conditions of the query. thereby, the user can be confronted to the problem of empty answers in the case of too selective query. to overcome this problem, several approaches have been proposed in the literature in particularly those based on query conditions relaxation. others works suggest the use of fuzzy sets theory to introduce a flexible queries. another line of research proposes the adaptation of information retrieval (ir) approaches to get an approximate matching in databases. we discuss in this paper, an adaptation of language model of ir to deal with empty answers. the main idea behind our approach is that instead of returning an empty response to the user, a ranked list of tuples that have the most similar values to those specified in user 's query is returned.
parallel_computing	the flux distribution generated by the heliostat field of solar central receiver system (scrs) over the receiver needs to be carefully controlled. it is necessary to avoid dangerous radiation peaks and temperature distributions to maximize the efficiency and keep the system in a safe state. these tasks imply both selecting the subset of heliostats to be activated and assigning each one to a certain aiming point at the receiver. the heliostat field is usually under human control and supervision, what is a potential limiting factor. thus, there is an active research line to define automatic aiming procedures. in fact, a general and autonomous methodology is being developed by the authors of this work. however, the mathematical modeling leads to face a complex large-scale optimization problem. in this work, applying teaching-learning-based optimization (tlbo), a population-based large-scale optimizer, is considered. it is intended to serve to perform large explorations of the search-space to finally deploy further local optimizers over the most promising results. considering the computational cost of the objective function, a parallel version of tlbo has been developed. it significantly accelerates the procedure, and the possibility of being included in a more complex process remains viable. additionally, the parallel version of tlbo is also linked as a generic open-source library.
software_engineering	archivedb is the data archive for all scientific and technical data collected at the wendelstein 7-x project. it is a distributed system allowing continuous data archival. archivedb has demanding requirements regarding performance efficiency (storage performance of 30 gb/s during experiments, expected storage amount of 1.4 pb/year), reliability (availability of 364 days/year), maintainability (testability) and portability (including change of hardware and software). data acquisition with continuous operation and high time resolutions (up to nanoseconds scale) for physics data is supported as well as long-term recording up to 24 h/7 days for operational data (similar to 1 hz rate). moreover, all results of data analysis are stored in the archive. another challenge, uniform retrieval of measured and analyzed data, allowing time and structure information as selection criteria, is mastered as well. the key concepts of data storage and retrieval are: (1) partitioning of incoming data in groups and stream, (2) chunking of data in boxes of manageable size covering a finite time period, and (3) indexing of data using absolute time as ordering and indexing criteria. continuous operation of the archivedb software and hardware for various systems and components relevant to wendelstein 7-x has been done successfully for several years, thus, showing that the key requirements are satisfied. the overall data amount so far has reached 7 terabyte over 9 years of data taking. round-the-clock operation of the archive is in place since 5 years. initial plasma operation op1.1 of wendelstein 7-x has been supported with no downtime during the whole experimental campaign. the paper describes the software engineering concepts that have been used, consolidated and refined over the years of continuing productive archivedb use and development. changes in the underlying techniques, e.g. a change of the data store, have been encapsulated via an application programming interface (api). this api unifies different implementations and is also suitable for data migration. (c) 2016 elsevier b.v. all rights reserved.
symbolic_computation	the paper introduces real logic: a framework that seamlessly integrates logical deductive reasoning with efficient, data-driven relational learning. real logic is based on full first order language. terms are interpreted in n-dimensional feature vectors, while predicates are interpreted in fuzzy sets. in real logic it is possible to formally define the following two tasks: (i) learning from data in presence of logical constraints, and (ii) reasoning on formulas exploiting concrete data. we implement real logic in an deep learning architecture, called logic tensor networks, based on google 's tensorflow (tm) primitives. the paper concludes with experiments on a simple but representative example of knowledge completion.
computer_vision	diabetes disrupts the operation of the eye and leads to vision loss, affecting particularly the nerve layer and capillary vessels in this layer by changes in the blood vessels of the retina. suddenly loss and blurred vision problems occur in the image, depending on the phase of the disease, called diabetic retinopathy. hard exudates are one of the primary signs of diabetic retinopathy. automatic recognition of hard exudates in retinal images can contribute to detection of the disease. we present an automatic screening system for the detection of hard exudates. this system consists of two main steps. firstly, the features were extracted from patch images consisting of hard exudate and normal regions using the daisy algorithm based on the histogram of oriented gradients. after, we utilized the recursive feature elimination (rfe) method, using logistic regression (lr) and support vector classifier (svc) estimators on the raw dataset. therefore, we obtained two datasets containing the most important features. the number of important features in each dataset created with lr and svc was 126 and 259, respectively. afterward, we observed different classifier algorithms' performances by using 5-fold cross validation on these important features' dataset and it was observed that the random forest (rf) classifier is the best classifier. secondly, we obtained important features from the feature vector that corresponds with the region of interest in accordance with the keypoint information in a new retinal fundus image. then we performed detection of hard exudate regions on the retinal fundus image by using the rf classifier.
computer_graphics	discretization by rasterization is introduced into the method of images (mi) in the context of 3d deterministic radio propagation modeling as a way to exploit spatial coherence of electromagnetic propagation for fine-grained parallelism. traditional algebraic treatment of bounding regions and surfaces is replaced by computer graphics rendering of 3d reflections and double refractions while building the image tree. the visibility of reception points and surfaces is also resolved by shader programs. the proposed rasterization is shown to be of comparable run time to that of the fundamentally parallel shooting and bouncing rays. the rasterization does not affect the signal evaluation backtracking step, thus preserving its advantage over the brute force ray-tracing methods in terms of accuracy. moreover, the rendering resolution may be scaled back for a given level of scenario detail with only marginal impact on the image tree size. this allows selection of scene optimized execution parameters for faster execution, giving the method a competitive edge. the proposed variant of mi can be run on any gpu that supports real-time 3d graphics.
software_engineering	the paper presents a series of considerations regarding the role of the conceptual invariants concerning the prevention of the artefacts' obsolescence, with an emphasis on the software engineering. the concept of artefact has the meaning that is defined in (bocu & bocu, 2016). the emphasizing of the invariants' role has the goal to allow the understanding, with the respective approximation, of this invariants' role considering the continuous qualitative progress of the human artefacts, generally speaking, but also in connection to the software systems engineering.
parallel_computing	methods for the polynomial eigenvalue problem sometimes need to be followed by an iterative refinement process to improve the accuracy of the computed solutions. this can be accomplished by means of a newton iteration tailored to matrix polynomials. the computational cost of this step is usually higher than the cost of computing the initial approximations, due to the need of solving multiple linear systems of equations with a bordered coefficient matrix. an effective parallelization is thus important, and we propose different approaches for the message-passing scenario. some schemes use a subcommunicator strategy in order to improve the scalability whenever direct linear solvers are used. we show performance results for the various alternatives implemented in the context of slepc, the scalable library for eigenvalue problem computations. copyright (c) 2016 john wiley & sons, ltd.
cryptography	vote by ballot is the feature in a democratic society and the process of decision-making, tending to achieve the philosophy of democratic politics by having the public who are eligible to vote for competent candidates or leaders. with the rapid development of technologies and network applications, electronization has been actively promoted globally during the social transformation period that the concept of electronic voting is further derived. the major advantages of electronic voting, comparing with traditional voting, lie in the mobility strength of electronic voting, reducing a large amount of election costs and enhancing the convenience for the public. electronic voting allows voters completing voting on the internet that not only are climate and location restrictions overcome, but the voter turnout is also increased and the voting time is reduced for the public. with the development in the past three decades, electronic voting presents outstanding performance theoretically and practically. nevertheless, it is regrettable that electronic voting schemes still cannot be completely open because of lures by money and threats. people to lure by money and threats would confirm the voters following their instructions through various methods that more factors would appear on election results, affecting the quality and fairness of the election. in this study, this project aims to design an electronic voting scheme which could actually defend voters' free will so that lure of money and threats would fail. furthermore, an electronic voting system based on elliptic curve cryptography is proposed to ensure the efficiency and security, and ring signature and signcryption are applied to reducing the computing costs. moreover, this project also focuses on applying voting system to mobile devices. as the system efficiency and security are emphasized, voters do not need to participate in the election, but simply complete voting with smart phones, ipads, and computers. the votes would be automatically calculated and verified the results that the ballots are not necessarily printed, the printing of election mails is reduced, and manual handling is canceled. such a method would effectively reduce voting costs and enhance the economic efficiency.
machine_learning	one of the crucial issues regarding a storm sewer system is the ability to avoid sediment depositions on the pipe invert. in this study, the mean flow velocity under the limit of sediment deposition conditions in partially filled circular storm sewers is evaluated through the use of a support vector machine (svm) model coupled with the firefly algorithm (ffa). the aforemetioned velocity, defined as the velocity at the limit of deposition, and the parameters upon which it depends have been nondimensionalized using the buckingham. theorem. therefore, once the dimensionless parameters are identified, six different functional relationships in terms of dimensionless groups can be obtained. the effects of each of these functional relationships on the dimensionless velocity at limit of deposition, defined as the densimetric particle froude number at the limit of deposition, have been analyzed by using, respectively, the svm-ffa model, svm model, genetic programming (gp) model, and artificial neural network (ann) model. five statistical indices have been used for evaluating the performance of each model (both in training and test phases) and, later, for comparing the performance of the different models between them. finally, the predicted densimetric particle froude number values obtained through the proposed svm-ffa model have been compared with those obtained by three different dimensionless equations for velocity at the limit of deposition. the results indicate that svm-ffa predicts the densimetric particle froude number at limit of deposition fairly accurately. (c) 2016 american society of civil engineers.
operating_systems	one of the fundamental requirements of real time operating systems is the determinism of executing critical tasks and treating multiple periodic or aperiodic events. the present paper presents the hardware support of the nmpra processor (multi pipeline register architecture) dedicated to treating time events, interrupt events and events associated with synchronization and inter-task communication mechanisms. because in real time systems the treatment of events is a very important aspect, this paper describes both the mechanism implemented in hardware for prioritizing and treating multiple events, and the experimental results obtained using virtex-7 fpga circuit. the article 's element of originality is the very short response time required in treating and prioritizing events.
computer_graphics	we proposed the method that generates ground surface texture by evaluating pressure applied at each point. the method standardizes diameter of clod to original size and its 1/2, 1/4, and 1/8 size, then defines condition of land by heft percentage of the clod of the four kinds of size. at each point of the ground, pressure caused by objects that pass through on the point is accumulated. whenever the pressure exceeds the threshold, fixed ratio of weight of clod is shifted to its half size. as a result, at the place where many objects pass, heft percentage of smaller clod increases. ground surface is defined as a multilayered texture. it is created by combining four basic texture images. each of the images is synthesized to correspond to the size of clod by the procedural method. from each of the texture images, round areas are selected and allocated to one of the layers at random. diameter of the area is equal to size of the clod. and the numbers of the area is proportionate to the heft percentage of the clod. the multilayer texture is combined to one layer, after adding shade and shadow. and finally, it is mapped on the ground. (c) 2017 wiley periodicals, inc. electron comm jpn, 100( 2): 25-35, 2017; published online in wiley online library (wileyonlinelibrary.com).
software_engineering	every complex problem now days require multicriteria decision making to get to the desired solution. numerous multi-criteria decision making (mcdm) approaches have evolved over recent time to accommodate various application areas and have been recently explored as alternative to solve complex software engineering problems. most widely used approach is analytic hierarchy process that combines mathematics and expert judgment. analytic hierarchy process suffers from the problem of imprecision and subjectivity. this paper proposes to use fuzzy ahp (fahp) instead of traditional ahp method. the usage of fahp helps decision makers to make better choices both in relation to tangible criteria and intangible criteria. the paper provides a clear guide on how fahp can be applied, particularly in the software engineering area in specific situations. the conclusion of this study would help and motivate practitioners and researchers to use multi-criteria decision making approaches in the area of software engineering.
relational_databases	in the last decade, keyword search over relational databases has been extensively studied because it promises to allow users lacking knowledge of structured query languages or unaware of the database schema to query the database in an intuitive way. the existing works about keyword search on databases proposed many approaches and have gain remarkable results. however, most of these approaches are designed for the centralized setting where keyword search is processed by only a single server. in reality, the scale of databases increases sharply and centralized methods hardly can handle keyword queries over these large databases. moreover, processing keyword search over relational databases is a very time-consuming task, and the efficiency of the existing centralized approaches will degrade notably because the single server cannot provide enough computation power for the keyword search over very large databases. to address these challenges, we propose a distributed keyword search (dks) approach with mapreduce and this approach can be well deployed on a cluster of servers to deal with keyword search over large databases in a parallel way.
computer_graphics	in the twenty-first century now, people are accepting the rapid development of the information age, while emphasizing the fast-paced, high efficiency. and we are also the times of higher education, popularization and application of multimedia teaching. so that educators and students are in conflict with traditional teaching and learning methods, and as a special sector of our art and design education, the use of computers is indispensable, and is a very important one to learn and use tools, but the attendant computer renderings can be said to have replaced the traditional hand-painted renderings status in the past, then the design of the manuscript as well as its position ? the answer is yes, fifteen years ago, i purchased a computer began to study adobe photoshop cs. and teaching about the software (corduroy, autocad) so far. and i have always believed in the program design phase, design manuscripts as the behavior of the human brain design thinking process is computer graphics can not be replaced.
relational_databases	the paper proposes a new model, method and algorithms for processing a subject domain (sd). it considers an sd as a tuple (e, v, t), where e is a set of all objects of this domain, v is a set of all connections between these objects, t is a set of mass problems that can be solved in this domain. each object is represented by its essential attributes. this allows us to describe sd, to solve most of the mass problems from t, and to control a state of the domain. the goal of the paper is to develop a method and algorithms that allow us to find essential attributes of domains, described as relational databases or production systems. for a production system, we build a dependency graph with attributes as vertexes, and calculate for each of the vertexes a value that indicates how many other attributes can be found using a particular vertex. high-ranked vertexes are considered as essential. the method is based on considering as essential of the most frequently accessed attributes in a relational database. having essential attributes of objects of a domain, it allows us to define essential attributes of the domain itself.
image_processing	this paper introduces a new method for clustering of documents, which have been written in a language evolving during different historical periods, with an example of the italian language. in the first phase, the text is transformed into a string of four numerical codes, which have been derived from the energy profile of each letter, defining the height of the letters and their location in the text line. each code represents a gray level and the text is codified as a 1-d image. in the second phase, texture features are extracted from the obtained image in order to create document feature vectors. subsequently, a new clustering algorithm is employed on the feature vectors to discriminate documents from different historical periods of the language. experiments are performed on a database of italian documents given in italian vulgar and modern italian. results demonstrate that this proposed method perfectly identifies the historical periods of the language of the documents, outperforming other well-known clustering algorithms generally adopted for document categorization and other state-of-the-art text-based language models.
relational_databases	we present a logic for reasoning about attribute dependencies in data involving degrees such as a degree to which an object is red or a degree to which two objects are similar. the dependencies are of the form a a double dagger' b and can be interpreted in two ways: first, in data tables with entries representing degrees to which objects (rows) have attributes (columns); second, in database tables where each domain is equipped with a similarity relation. we assume that the degrees form a scale equipped with operations representing many-valued logical connectives. if 0 and 1 are the only degrees, the algebra of degrees becomes the two-element boolean algebra and the two interpretations become well-known dependencies in boolean data and functional dependencies of relational databases. in a setting with general scales, we obtain a new kind of dependencies with naturally arising degrees of validity, degrees of entailment, and related logical concepts. the deduction rules of the proposed logic are inspired by armstrong rules and make it possible to infer dependencies to degrees-the degrees of provability. we provide a soundness and completeness theorem for such a setting asserting that degrees of entailment coincide with degrees of provability, prove the independence of deduction rules, and present further observations.
bioinformatics	the dnaj proteins which function as molecular chaperone played critical roles in plant growth and development and response to heat stress (hs) and also called heat shock protein 40 based on molecular weight. however, little was reported on this gene family in pepper. recently, the release of the whole pepper genome provided an opportunity for identifying putative dnaj homologous. in this study, a total of 76 putative pepper dnaj genes (cadnaj01 to cadnaj76) were identified using bioinformatics methods and classified into five groups by the presence of the complete three domains (j-domain, zinc finger domain, and c-terminal domain). chromosome mapping suggested that segmental duplication and tandem duplication were occurred in evolution. the multiple stress-related cis-elements were found in the promoter region of these cadnaj genes, which indicated that the cadnajs might be involved in the process of responding to complex stress conditions. in addition, expression profiles based on rna-seq showed that the 47 cadnajs were expressed in at least one tissue tested. the result implied that they could be involved in the process of pepper growth and development. qrt-pcr analysis found that 80.60% (54/67) cadnajs were induced by hs, indicated that they could participated in pepper response to high temperature treatments. in conclusion, all these results would provide a comprehensive basis for further analyzing the function of cadnaj members and be also significant for elucidating the evolutionary relationship in pepper.
parallel_computing	we made use of a special algorithm for compute unified device architecture for nvidia graphics cards, a nonlinear conjugate-gradient method to minimize energy functional, and monte-carlo technique to directly observe the forming of the ground state configuration for the 2d hard-core bosons by lowering the temperature and its evolution with deviation away from half-filling. the novel technique allowed us to examine earlier implications and uncover novel features of the phase transitions, in particular, look upon the nucleation of the odd domain structure, emergence of filamentary superfluidity nucleated at the antiphase domain walls of the charge-ordered phase, and nucleation and evolution of different topological structures.
network_security	with the network scales rapidly and new network applications emerge frequently, bandwidth supply for today 's internet could not catch up with the rapid increasing requirements. unfortunately, irrational using of network sources makes things worse. actual network deploys single-next-hop optimization paths for data transmission, but such ""best effort"" model leads to the imbalance use of network resources and usually leads to local congestion. on the other hand multi-path routing can use the aggregation bandwidth of multi paths efficiently and improve the robustness of network, security, load balancing and quality of service. as a result, multi-path has attracted much attention in the routing and switching research fields and many important ideas and solutions have been proposed. this paper focuses on implementing the parallel transmission of multi next-hop data, balancing the network traffic and reducing the congestion. it aimed at exploring the key technologies of the multi-path communication network, which could provide a feasible academic support for subsequent applications of multi-path communication networking. it proposed a novel multi-path algorithm based on node potential in the network. and the algorithm can fully use of the network link resource and effectively balance network link resource utilization.
algorithm_design	large-scale network and graph analysis has received considerable attention recently. graph mining techniques often involve an iterative algorithm, which can be implemented in a variety of ways. using pagerank as a model problem, we look at three algorithm design axes: work activation, data access pattern, and scheduling. we investigate the impact of different algorithm design choices. using these design axes, we design and test a variety of pagerank implementations finding that data-driven, push-based algorithms are able to achieve more than 28x the performance of standard pagerank implementations (e.g., those in graphlab). the design choices affect both single-threaded performance as well as parallel scalability. the implementation lessons not only guide efficient implementations of many graph mining algorithms, but also provide a framework for designing new scalable algorithms.
computer_programming	geographers of technology illustrate software code 's contexts, effects, and agencies as they shape urban space and everyday life, but the consequences of code for nature remain understudied. political ecologists have critiqued remote sensing and geographic information systems (gis) based conservation projects, but have not engaged more broadly with the role of software in the contested production, circulation, and application of ecological knowledge. yet, around the world, data analytics firms and conservation nonprofits argue for optimizing environmental management through faster and bigger data collection and new techniques of data manipulation and visualization. i present a case study from the us state of oregon, illustrating how conservationists and environmental regulators employ computer programming to plan markets in which entrepreneurs restore stream and wetland ecosystem services to earn offset credits. in these markets, code-executed algorithms constituting spreadsheets, web maps, and gis utilities generate, relate, and make sense of the data that define credit commodities. i argue that code tends toward three effects: producing a landscape defined by wetlands' modeled value, performing social relations associated with nature 's neoliberalization and financialization, and legitimating these moves. although emphasis on the performativity of code and other technological objects is warranted, the contexts in which these are authored, deployed, and evaluated should remain central to understanding environmental governance. this is to caution against seeing technology as reducing nature and society to state or capitalist rationalities and to hesitate to differentiate prima facie code 's work on space and on nature. i call for bridging political ecology and geographies of technology in ways that can explain how code is generative of environmental knowledge, change, and conflict.
operating_systems	a significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. the current industrial practice is to develop and maintain these variants separately. any potential change has to be applied across variants manually, which is neither efficient nor scalable. we consider the problem of supporting multiple platforms as a 'software product-line engineering' problem. the paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. we develop a tool moppet that automates the proposed approach. finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time. (c) 2016 elsevier inc. all rights reserved.
image_processing	one of the main complications caused by diabetes mellitus is the development of diabetic foot, which in turn, can lead to ulcerations. because ulceration risks are linked to an increase in plantar temperatures, recent approaches analyze thermal changes. these approaches try to identify spatial patterns of temperature that could be characteristic of a diabetic group. however, this is a difficult task since thermal patterns have wide variations resulting on complex classification. moreover, the measurement of contralateral plantar temperatures is important to determine whether there is an abnormal difference but, this only provides information when thermal changes are asymmetric and in absence of ulceration or amputation. therefore, in this work is proposed a quantitative index for measuring the thermal change in the plantar region of participants diagnosed diabetes mellitus regards to a reliable reference (control) or regards to the contralateral foot (as usual). also, a classification of the thermal changes based on a quantitative index is proposed. such classification demonstrate the wide diversity of spatial distributions in the diabetic foot but also demonstrate that'it is possible to identify common characteristics. an automatic process, based on the analysis of plantar angiosomes and image processing, is presented to quantify these thermal changes and to provide valuable information to the medical expert. (c) 2017 elsevier b.v. all rights reserved.
parallel_computing	in multivariate or spatial extremes, inference for max-stable processes observed at a large collection of points is a very challenging problem and current approaches typically rely on less expensive composite likelihoods constructed from small subsets of data. in this work, we explore the limits of modern state-of-the-art computational facilities to perform full likelihood inference and to efficiently evaluate high-order composite likelihoods. with extensive simulations, we assess the loss of information of composite likelihood estimators with respect to a full likelihood approach for some widely used multivariate or spatial extreme models, we discuss how to choose composite likelihood truncation to improve the efficiency, and we also provide recommendations for practitioners. this article has supplementary material online.
structured_storage	this article leads the concept of data visualization into the huge amounts of news data, design and implements a system of news data visual system (ndvs). it is a news gathering, analysis, structured storage and visual analysis system. ndvs collects news data in the schedule time from seeds website by a web spider that written by java every day. the structured documents that processed and analyzed by detached parameters from url and classify news will be saved in distributed database of nosql - mongodb. web will visually and interactively animate the documents in the means of d3.js. design and realization of ndvs will be emphasized in the article, and we find that ndvs has a good capability when lots of clients concurrent access. we believe ndvs will become a useful tool between news and media works, because system follows the usability policy that helps people to find the potential tendency of news' development and guide the public opinion in the stage of visualization design.
parallel_computing	algorithms for extracting hydrologic features and properties from digital elevation models (dems) are challenged by large datasets, which often cannot fit within a computer 's ram. depression filling is an important preconditioning step to many of these algorithms. here, i present a new, linearly scaling algorithm which parallelizes the priority-flood depression-filling algorithm by subdividing a dem into tiles. using a single-producer, multi-consumer design, the new algorithm works equally well on one core, multiple cores, or multiple machines and can take advantage of large memories or cope with small ones. unlike previous algorithms, the new algorithm guarantees a fixed number of memory access and communication events per subdivision of the dem. in comparison testing, this results in the new algorithm running generally faster while using fewer resources than previous algorithms. for moderately sized tiles, the algorithm exhibits similar to 60% strong and weak scaling efficiencies up to 48 cores, and linear time scaling across datasets ranging over three orders of magnitude. the largest dataset on which i run the algorithm has 2 trillion (2 x 10(12)) cells. with 48 cores, processing required 4.8 h wall-time (93 compute-days). this test is three orders of magnitude larger than any previously performed in the literature. complete, well commented source code and correctness tests are available for download from a repository. (c) 2016 elsevier ltd. all rights reserved.
distributed_computing	the programming of heterogeneous clusters is inherently complex, as these architectures require programmers to manage both distributed memory and computational units with a very different nature. fortunately, there has been extensive research on the development of frameworks that raise the level of abstraction of cluster-based applications, thus enabling the use of programming models that are much more convenient that the traditional one based on message-passing. one of such proposals is the hierarchically tiled array (hta), a data type that represents globally distributed arrays on which it is possible to perform a wide range of data-parallel operations. in this paper we explore for the first time the development of heterogeneous applications for clusters using htas. in order to use a high level api also for the heterogenous parts of the application, we developed them using the heterogeneous programming library (hpl), which operates on top of opencl but providing much better programmability. our experiments show that this approach is a very attractive alternative, as it obtains large programmability benefits with respect to a traditional implementation based on mpi and opencl, while presenting average performance overheads just around 2%.
operating_systems	smartphones have become an integral part of our daily life. businesses now offer services through smartphones. users also store sensitive personal information on their smartphones and perform financial transactions. consequently, security attacks on smartphone platforms have also increased significantly. traditional desktop anti-virus software are not very effective in smartphones due to the restrictive security model and they are heavily dependent on their definition updates. in this paper, we propose a secure anti-malware framework (sam) for smartphone operating systems to prevent malicious activities. the core idea of the framework resembles a smart city. the framework acts as the government of the city and treats the applications as citizens. it has components to enforce laws (prevent) and perform policing (monitor and control). it also provides apis to aid anti-virus software and third-party applications to leverage the functionalities of the framework. our goal is to design an operating system framework that hinders malicious activities and thus protects user resources.
relational_databases	in recent years, the database technology has been widely used in a few fields. the keyword search technology based on relational databases does not require users to master any knowledge of sql grammar and database schema. users only need to input keywords, and conveniently search information via keyword like internet search engines. as results, users are pleased to use the technology. in this paper, we employee a query expansion plan based on the query sentence, and propose a query expansion method based on thesaurus. the method matches key words with the thesaurus. if the key word matches a word in the thesaurus, and the words in the same row are used as a synonym for the key word. test results show that the recall and precision ratio of the system are satisfied with needs of applications.
software_engineering	context: software engineering (se) has a multidisciplinary and dynamic nature that makes it challenging to design its educational material. guide to the software engineering body of knowledge (swebok) which has evolved to become iso/iec 19759 standard has identified various knowledge areas to be part of any se curricula. although there is a number of studies that address the gap between se curricula and software industry, the literature lacks defining a process that can be leveraged for continuously improving se curricula to fulfill the software development market demands. objective: in this paper, we propose a software engineering curricula development and evaluation process (secdep) that takes advantage of the swebok guidelines to improve the quality of se programs based on objective and subjective evidences. method: our process consists of multi-steps in which the local software market needs and the target se program objectives and constraints are all taken into consideration. as a case study, we follow our process to investigate the core se courses delivered as part of the se curricula in a set of universities in our region. results: the conducted case study identifies the factors that might contribute to mitigating the skills shortages in the target software market. we demonstrate the effectiveness of our process by identifying the weaknesses of the studied se curricula and presenting recommendations to align the studied curricula with the demands of the target software market, which assists se educators in the design and evaluation of their se curricula. conclusion: based on the obtained results, the studied se curricula can be enhanced by incorporating latest se technologies, covering most of the swebok knowledge areas, adopting se curricula standards, and increasing the level of industrial involvement in se curricula. we believe that achieving these enhancements by se educators will have a positive impact on the se curricula in question. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	this work presents a parallel approach of the kinetic monte carlo (kmc) algorithm using a distributed memory architecture. the resulting computer software was tested by conducting crystal growth simulations on barite (001) face. execution times, simulated times and crystallization velocities are compared with a shared memory parallel kmc software (mmonca). finally, a approximate to 1 mu m(2) crystal growth simulation is performed and compared with atomic force microscopy crystal growth experiments. the capability of this approach is demonstrated: a) a significant reduction of parallel overhead is achieved when comparing to the shared memory parallel version of the software, b) a distributed memory approach achieves an increase in memory resources enough to perform simulations with lattice sizes about 1 mu m(2), allowing the study of larger structures than those in shared memory or sequential implementations, c) this approach should be used only with large scale simulations to take advantage of the distributed memory architecture, d) further improvements are needed for parallel kmc to be faster than serial kmc in small scale simulations, e) the kmc algorithm used is able to adequately simulate two-dimensional nucleation on large areas of barite (001) faces.
computer_vision	with the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. over the past years, many different successful saliency models have been proposed especially for image saliency prediction. however, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. to date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. in addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. in this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. we employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. in order to explore the strength of feature integration strategies, we investigate four learning-based (svm, gradient boosting, nnls, random forest) and two transformation based (mean, max) fusion methods, resulting in six new dynamic saliency models. our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. in addition, our learning-based models outperform the state-of-the-art dynamic saliency models.
distributed_computing	as distributed generators in distribution networks have brought much influence to the fault current, the traditional fault-section locating algorithm of distribution networks may not work. in this paper, a new switching function is built which can be used in the distribution network with single power supply and multi power supplies. a regional processing method is used in this paper, which divides the distribution network into several independent regions, then the fault locating algorithm is used in these independent regions. the regions without dg connection are eliminated if fault current is not detected. so the complexity of computation of fault-section location is reduced. ant colony algorithm is a good optimization algorithm of swarm intelligence. because of its positive feedback, fault tolerance and distributed computing features, good application effect of locating faulty section in distribution network can be attained. in order to overcome the disadvantages such as large amounts of calculation, long searching time, local optimal. ant colony algorithm is improved by adaptive dynamic modifying pheromone. ant system with elitist strategy and mmas are also used in this paper. this improved algorithm is used in distribution network with multi distributed generators. the results of example analysis show the effective and good fault tolerance of the algorithm.
computer_graphics	the present paper establishes a homotopy based on marcus-wyse (for brevity m-) topology, which can contribute to the classification of 2d digital images in an m-topological approach. since m-topology is considered in the euclidean 2d space with integer coordinates, it can be used in studying 2d digital images from the viewpoints of both digital topology and digital geometry such as image processing, image analysis, computer graphics, mathematical morphology and so forth. to develop the homotopy, the present paper uses two maps such as an m-continuous map and marcus-wyse adjacency (for brevity ma-) map because they have their own features and merits. besides, using this homotopy, the present paper proposes an ma-homotopy equivalence and further, ma-contractibility which can be also used in classifying digital images. finally, the paper establishes an ma-homotopic thinning derived from the above homotopy, which can contribute to the compression of 2d digital images. (c) 2015 elsevier b.v. all rights reserved.
data_structures	the massive amounts of data processed in modern computational systems is becoming a problem of increasing importance. this data is commonly stored directly or indirectly through the use of data exchange languages, such as javascript object notation (json), for human-readable platform agnostic access. this paper focuses on describing and analyzing sjson, a library that explores succinct representations of json documents as a means to achieve reduced memory usage of tiles in main memory, and to permit the compression of json files stored in disk. in sjson we represent the document structure with succinct trees, as opposed to the usual pointer-based implementation. furthermore, the remaining raw data are organized in arrays of attributes and values. attributes are stripped of redundancies and stored in a simple contiguous array, while values are represented through a hit string indexed array. the scheme here proposed is then evaluated with respect to a number of metrics comparing its performance with popular libraries, and possible improvements to the representation are then presented.
relational_databases	lifecycle structural health monitoring (shm) systems provide an abundance of information that is greatly beneficial for securing structural safety over the whole service life. in application to large-scale structures, the management of accumulated massive data from a sophisticated long-term shm system poses a challenge. a robust data management system (dms), which not only facilitates spatiotemporal data management but also enables display in an attractive way, is highly desirable. this article presents the development of an effective visualized dms specific for managing immense and heterogeneous shm data by integrating nested relational database, three-dimensional (3d) model, and virtual reality (vr) technology and demonstrates its application to an instrumented supertall structure. a custom nested data model is designed to store redundant inherent temporal data and hierarchical inherent spatial data. strategies for speeding up querying massive data are set up in the database. making use of openscenegraph (osg) 3d engine, a 3d model is reconstructed from the 3d spatial data, which serves as a platform for data visualization. a four-dimensional (4d) animation protocol is presented by tying temporal data and construction schedule to the 3d model. the efficiency of the proposed dms is exemplified through its application to a supertall structure instrumented with a sophisticated long-term shm system. (c) 2016 american society of civil engineers.
distributed_computing	to solve the adverse effects brought by resource node transfering the using right to local task and the difficult problem of resource load balancing, a two-phase pricing strategy based on qos constraints is proposed in this paper. on the premise of guaranteeing the benefits of the resource provider in the cost price, this strategy balances the load of the resource provider by the profit price. the theoretical analysis proves the effectiveness of the pricing strategy, and the algorithm of the pricing strategy is designed in this paper. resources node information in the real distributed systems is used as the performance parameters of experimental node in the simulation experiments, and the performance of the pricing strategy is tested in a large-scale grid mission. experimental results show that, compared with the traditional pricing strategies, the two-phase pricing strategy based on qos constraints has vastly superior performance on the benefits of the resource provider and the balance of resource utilization.
parallel_computing	in this work we aim to detect faces in violence scenes, in order to help the security control. we used the violent flow (vif) descriptor with horn-schunck proposed in [ 50] for violence scenes detection at first stage. then we applied the non-adaptive interpolation super resolution algorithm to improve the video quality and finally we fire a kanade-lucas-tomasi (klt) face detector. in order to get a very low time processing, we paralleled the super resolution and face detector algorithms with cuda. for the experiments we used the boss dataset and also we built a violence dataset, taking scenes from surveillance cameras. we have promising results detecting faces in this environment, because of the benefits of our proposal.
machine_learning	background: the fourth round of the critical assessment of small molecule identification (casmi) contest (www. casmi-contest. org) was held in 2016, with two new categories for automated methods. this article covers the 208 challenges in categories 2 and 3, without and with metadata, from organization, participation, results and postcontest evaluation of casmi 2016 through to perspectives for future contests and small molecule annotation/identification. results: the input output kernel regression (csi: iokr) machine learning approach performed best in ""category 2: best automatic structural identification-in silico fragmentation only"", won by team brouard with 41% challenge wins. the winner of ""category 3: best automatic structural identification-full information"" was team kind (msfinder), with 76% challenge wins. the best methods were able to achieve over 30% top 1 ranks in category 2, with all methods ranking the correct candidate in the top 10 in around 50% of challenges. this success rate rose to 70% top 1 ranks in category 3, with candidates in the top 10 in over 80% of the challenges. the machine learning and chemistry-based approaches are shown to perform in complementary ways. conclusions: the improvement in (semi-) automated fragmentation methods for small molecule identification has been substantial. the achieved high rates of correct candidates in the top 1 and top 10, despite large candidate numbers, open up great possibilities for high-throughput annotation of untargeted analysis for ""known unknowns"". as more high quality training data becomes available, the improvements in machine learning methods will likely continue, but the alternative approaches still provide valuable complementary information. improved integration of experimental context will also improve identification success further for ""real life"" annotations. the true ""unknown unknowns"" remain to be evaluated in future casmi contests.
symbolic_computation	korteweg-de vries (kdv)-type equation can be used to characterise the dynamic behaviours of the shallow water waves and interfacial waves in the two-layer fluid with gradually varying depth. in this article, by virtue of the bilinear forms, rational solutions and three kind shapes (soliton-like, kink and bell, anti-bell, and bell shapes) for the nth-order soliton-like solutions of a coupled kdv system are derived. propagation and interaction of the solitons are analyzed: (1) potential u shows three kind of shapes (soliton-like, kink, and anti-bell shapes); potential v exhibits two type of shapes (soliton-like and bell shapes); (2) interaction of the potentials u and v both display the fusion phenomena.
computer_programming	the main contribution of this paper is the introduction of a continuous improvement cycle for devising teaching scenarios and conducting learning experiences in engineering. the proposed cycle consists of seven steps on which gamification theory and abet criteria are combined. it arose from the adaptation of a gamification design framework, commonly used in industry, into the specific context of high quality education in engineering. it is formulated at high level. consequently, it should be useful for practitioners having different requirements and expectations. a developed practice, following the proposed cycle, is presented, discussed and evaluated. in particular, the proposal is applied and exemplified, in a scenario for teaching introductory concepts of computer programming in a first-year course. a digital game was used within a gamified learning experience, as a teaching tool. however, the learning process does not rely solely on the use of the game by itself. moreover, the devised scenario has a purpose beyond edutainment: contributing to achievement of student outcomes, under a continuous improvement approach, according to abet. a quantitative and qualitative evaluation of the developed practice was performed. a positive impact on students' emotional engagement and behavior was observed as a result of the evaluation process.
symbolic_computation	we present two new matrix spectral problems, and construct the corresponding soliton hierarchies of levi type with the aid of symbolic computation by maple. bi-hamiltonian structures of the resulting soliton hierarchies are obtained by means of the trace identity. thus it is shown that these soliton hierarchies are liouville integrable. (c) 2014 elsevier b.v. all rights reserved.
cryptography	the authors describe a method for producing boolean functions of degree d3 in n=2dk-1 (k=1, 2, ...) variables, such that the functions are plateaued and balanced, have high nonlinearity and have no linear structures. the nonlinearity is 2(n-1)-2((n-1)/2), which is the same as the largest possible nonlinearity for a quadratic function in n (odd) variables (the so-called quadratic bound'). their theorem uses some new ideas to generalise a theorem, which gave the case d=3, in a 2009 paper by fengrong zhang et al. they discuss the cryptographic properties and applications for the functions.
network_security	network structures and human behaviors are considered as two important factors in virus defense currently. however, due to ignorance of network security, normal users usually take simple activities, such as reinstalling computer system, or using the computer recovery system to clear virus. how system recovery influences virus spreading is not taken into consideration currently. in this paper, a new virus propagation model considering the system recovery is proposed first, and then in its steady-state analysis, the virus propagation steady time and steady states are deduced. experiment results show that models considering system recovery can effectively restrain virus propagation. furthermore, algorithm with system recovery in ba scale-free network is proposed. simulation result turns out that target immunization strategy with system recovery works better than traditional ones in ba network.
software_engineering	context: over the past 50 years numerous studies have investigated the possible effect that software engineers' personalities may have upon their individual tasks and teamwork. these have led to an improved understanding of that relationship; however, the analysis of personality traits and their impact on the software development process is still an area under investigation and debate. further, other than personality traits, ""team climate"" is also another factor that has also been investigated given its relationship with software teams' performance. objective: the aim of this paper is to investigate how software professionals' personality is associated with team climate and team performance. method: in this paper we detail a systematic literature review (slr) of the effect of software engineers' personality traits and team climate on software team performance. results: our main findings include 35 primary studies that have addressed the relationship between personality and team performance without considering team climate. the findings showed that team climate comprises a wide range of factors that fall within the fields of management and behavioral sciences. most of the studies used undergraduate students as subjects and as surrogates of software professionals. conclusions: the findings from this slr would be beneficial for understanding the personality assessment of software development team members by revealing the traits of personality taxonomy, along with the measurement of the software development team working environment. these measurements would be useful in examining the success and failure possibilities of software projects in development processes. general terms: human factors, performance. (c) 2016 elsevier b.v. all rights reserved.
network_security	in recent years, distributed and zero-day attacks have emerged as one of the most serious security threats. the incomplete knowledge and information of a stand-alone intrusion detection system (ids) is one of the main reasons for the success of these attacks. collaborative ids (cids) is one solution to address this problem. idss in this framework share their knowledge and consult with each other. having access to a larger number of detection libraries for ids configuration, along with the possibility of more cooperation with other participants in this collaborative system can lead to improved overall performance. however, a larger number of libraries and more collaborative activities increase resource consumption and communication overhead, which may in turn reduce system performance. there are a large number of papers in the literature that have utilized game theory to describe the optimal configuration of standalone or networked idss. in this paper, those works have been extended and the interactions between the attackers and idss in a cids framework have been modeled with a nonzero-sum stochastic game. in this regard, the solution concept of stationary nash equilibrium has been applied to this game to describe the optimal configuration of each ids in a cids and the expected behavior of attackers.
bioinformatics	viral myocarditis is a common cardiovascular disease, which seriously endangers the health of people and even leads to sudden unexpected death. micrornas play very important roles in various physical and pathological processes including cardiogenesis and heart diseases. in recent years, mir-20b has been implicated in various diseases such as breast cancer, gastric cancer, hepatocellular carcinoma, cardiovascular diseases. however, the function of mir-20b in the pathological progress of viral myocarditis has not been reported. in this study, we found that mir-20b was up-regulated in mouse heart tissues post coxsackievirus b3 (cvb3) infection. bioinformatics analysis identified zfp-148, a transcription factor that plays essential roles in the regulation of virus replication, is one of the predicted targets of mir-20b. mir-20b expression was found to be up-regulated and zfp-148 protein level was markedly repressed during viral myocarditis. further studies demonstrated that mir-20b directly binds to the 3'-utr of zfp-148 and suppresses its translation. moreover, aberrant expression of mir-20b promoted the expression of anti-apoptosis proteins bcl-2 and bcl-xl, suggesting that altered gene expression might promote cardiomyocytes survival in viral myocarditis. our findings indicated that mir-20b might be a potential therapeutic target for cvb3-induced viral myocarditis and a useful marker for the diagnosis of viral myocarditis.
image_processing	understanding of images via features like edges plays a vital role in many image processing applications. however, obtaining an optimum edge detector that performs well in every possible imaging condition is still an open challenge to researchers. in this paper, a quantitative analysis of some significant state of art edge detection techniques such as canny 's, prewitt, sobel, laplacian of gaussian, fuzzy based edge detection, wavelet based edge detector with hybrid edge detection technique is proposed based on the correspondence between their outcomes. the hybrid edge detection method utilizes fuzzy logic partitioning along with wavelet transformation to maintain a proper balance in the false detections i.e., false positives and false negatives rates and provides better tracking of edge information. various subjective as well as objective quality measures are provided for quantitative analysis of edge detectors. the experimental results confirm that compared to other techniques the hybrid edge detection technique outperform in terms of edge detection accuracy exclusively when the images are corrupted by noises.
relational_databases	in the today 's high-tech world the amount of data and especially spatial data is growing from day to day. databases are one of the best ways to store data. several database concepts exist for storing data, while the mostly used is the relational database concept. relational databases are very well in use for the storage of geo and non-spatial data. but in social media, like facebook or twitter, relational databases often reach their limit of performance. for huge amounts of data and frequent data changes nosql-databases can be used. these nosql-databases can be used as basis for the provision of a geo web service, like wms, wfs. the paper gives an overview of selected sql and nosql-databases according to their performance as backend for wms.
software_engineering	context: methods and processes, along with the tools to support them, are at the heart of software engineering as a discipline. however, as we all know, that often the use of the same method neither impacts software projects in a comparable manner nor the software they result in. what is lacking is an understanding of how methods affect software development. objective: the article develops a set of concepts based on the practice-concept in philosophy of sociology as a base to describe software development as social practice, and develop an understanding of methods and their application that explains the heterogeneity in the outcome. practice here is not understood as opposed to theory, but as a commonly agreed upon way of acting that is acknowledged by the team. method: the article applies concepts from philosophy of sociology and social theory to describe software development and develops the concepts of method and method usage. the results and steps in the philosophical argumentation are exemplified using published empirical research. results: the article develops a conceptual base for understanding software development as social and epistemic practices, and defines methods as practice patterns that need to be related to, and integrated in, an existing development practice. the application of a method is conceptualized as a development of practice. this practice is in certain aspects aligned with the description of the method, but a method always under-defines practice. the implication for research, industrial software development and teaching are indicated. conclusion: the theoretical/philosophical concepts allow the explaining of heterogeneity in application of software engineering methods in line with empirical research results. (c) 2015 elsevier b.v. all rights reserved.
distributed_computing	the internet shopping optimization problem arises when a customer aims to purchase a list of goods from a set of web-stores with a minimum total cost. this problem is np-hard in the strong sense. we are interested in solving the internet shopping optimization problem with additional delivery costs associated to the web-stores where the goods are bought. it is of interest to extend the model including price discounts of goods. the aim of this paper is to present a set of optimization algorithms to solve the problem. our purpose is to find a compromise solution between computational time and results close to the optimum value. the performance of the set of algorithms is evaluated through simulations using real world data collected from 32 web-stores. the quality of the results provided by the set of algorithms is compared to the optimal solutions for small-size instances of the problem. the optimization algorithms are also evaluated regarding scalability when the size of the instances increases. the set of results revealed that the algorithms are able to compute good quality solutions close to the optimum in a reasonable time with very good scalability demonstrating their practicability.
distributed_computing	cloud computing is the long envisaged vision of computing as a utility. innovative advances in hardware, networking, middleware, and virtual machine technologies have led to an emergence of new, globally distributed computing platforms, namely cloud computing. cloud computing provides computation facilities and storage as services accessible from anywhere via the internet without investing in new infrastructure, training, or software licensing. in other words, cloud computing is a way to increase the capacity or add capabilities dynamically. the main advantage of cloud computing is that users only utilize what they require and only pay for what they really use. with an exponential growth of the mobile applications and evolution of cloud computing concept, mobile cloud computing (mcc) has been presented as a potential technology for mobile services. mcc incorporates the cloud computing into the mobile environment. mobile cloud computing refers to an infrastructure where data processing and storage can happen away from mobile device. mobile cloud computing (mcc) has transformed the way in which mobile users across the globe leverage services on the go. the obstacles related to performance (e. g. battery life, storage, and bandwidth), environment (e. g. heterogeneity, scalability, availability) and security (e. g. reliability and privacy) are overcome by integrating cloud computing into the mobile environment using mcc. mobile cloud is a service model, where a mobile device can use the cloud for information storage, searching, data mining and multimedia processing. cloud computing technology also brings forth many new challenges for data security and access control when users store sensitive data on cloud servers. as the users no longer have physical possession of the outsourced data, makes the data integrity, privacy and authenticity protection in cloud computing a very challenging and potentially difficult task. though the cloud computing benefits are clear, surrendering physical possession of user data, inevitably poses new security risks. in this paper, we discuss mobile cloud computing security frameworks found in the literature related to cloud computing and its environment.
relational_databases	in this paper, we propose a persistent watermarking technique of information systems supported by relational databases at the back-end. the persistency is achieved by identifying an invariant part of the database which remains unchanged w.r.t. the operations in the associated applications. to achieve this, we apply static data-flow analysis technique to the applications. the watermark is then embedded into the invariant part of the database, leading to a persistent watermark. we also watermark the associated applications in the information system by using opaque predicates which are obtained from the variant part of the database.
image_processing	a new experimental, image-based methodology suitable to track the changes in orientation of non-spherical particles and their influence on the drag coefficient as they settle in fluids is presented. given the fact that non spherical solids naturally develop variations in their angular orientation during the fall, none-intrusiveness of the technique of analysis is of paramount importance in order to preserve the particle/fluid interaction undisturbed. three-dimensional quantitative data about the motion parameters is obtained through single-camera stereo vision whilst qualitative visualizations of the adjacent fluid patterns are achieved with schlieren photography. the methodology was validated by comparing the magnitudes of the drag coefficient of a set of spherical particles at terminal velocity conditions against those estimated from drag correlations published in the literature. a noteworthy similarity was attained. during the fall of non-spherical solids, once the particle reynolds number approximated 163 for disks, and 240 for cylinders, or exceeded those values, secondary motions composed by regular oscillations and tumbling were present they altered the angular orientation of the particles with respect to the main motion direction and caused complete turbulent patterns in the surrounding flow, therefore affecting the instantaneous projected area, drag force, and coefficient of resistance. the impact of the changes in angular orientation onto the drag coefficient was shown graphically as a means for reinforcing existing numerical approaches, however, an explicit relation between both variables could not be observed. (c) 2017 elsevier b.v. all rights reserved.
computer_programming	in this chapter, we report two studies in which 3rd- and 4th-grade students used a distributed computing infrastructure (vimap-tangible) in order to collaboratively invent ""mathematical machines"" for generating geometric shapes. vimap-tangible combines the vimap visual programming language with a distributed computing infrastructure, in which students collaboratively control the behavior of a virtual agent using both mechanical devices and virtual algorithms. the curricular activities integrate engineering practices such as user-centered design; agent-based computer programming; mathematical reasoning about multiplication, rates, and geometry; and physical science concepts central to learning newtonian mechanics. in study 1, we investigate the key affordances of such a distributed computing environment for learning integrated stem, and identify the relationships between the various elements of students' physical constructions and computational models, and their stem learning outcomes. study 2 is a deeper investigation of the effect of iterative user testing on the refinement of children 's designs and their stem learning.
cryptography	in this paper, we propose an innovative quantum private comparison(qpc) protocol based on partial bell-state measurement from the view of linear optics, which enabling two parties to compare the equality of their private information with the help of a semi-honest third party. partial bell-state measurement has been realized by using only linear optical elements in experimental measurement-device-independent quantum key distribution(mdi-qkd) schemes, which makes us believe that our protocol can be realized in the near future. the security analysis shows that the participants will not leak their private information.
distributed_computing	in this paper, we present an intelligent, reliable and storage-efficient video surveillance system using apache storm and opencv. as a storm topology, we have added multiple information extraction modules that only write important content to the disk. our topology is extensible, capable of adding novel algorithms as per the use case without affecting the existing ones, since all the processing is independent of each other. this framework is also highly scalable and fault tolerant, which makes it a best option for organisations that need to monitor a large network of surveillance cameras.
network_security	in this paper of computer network information security, network strategy were reviewed in this article, on the basis of all kinds of network safety measures, and the technology to carry on the analysis to banding mechanical electronic technical computer network as an example, introduces the computer network networking strategy and the computer network information safety protection strategy. the actual operation of the computer network to the hardware, software, internet use management rules and regulations a series of protective measures, set up for the actual situation of the network security environment, guarantee the stable operation of the network, for the education teaching provided support for the computer network construction and computer information safety protection provide the beneficial reference.
network_security	getting higher occurrence of cybercrimes by means of hacking, identity theft, and network security violations necessitates a robust system for resolving these issues. for the new era of it, using conventional user authentication methods like providing login ids, passwords /pin and other two-factor authentication methods are fading to offer the required level of security required. since biometrics come forward as an efficient alternative technique to provide security. keystroke and typing dynamics uses behavioral characteristics like typing rhythms of a person for authentication. this protection method effortlessly integrates with the existing environment and it could be scaled across the web also. this technology is will be getting promoted in the upcoming years because of its non-invasiveness, unobtrusiveness and low deployment cost. thereby security of the physical and logical access can be improved. this proposed technique acts as a supplementary security layer besides the traditional user ids & passwords/pin, most organizations are making supplementary investments in keystroke and typing dynamics to ensure a more robust user authentication system.
computer_vision	the most common way to deal with the uncertainty present in noisy sensorial perception and action is to model the problem with a probabilistic framework. maximum likelihood estimation is a well-known estimation method used in many robotic and computer vision applications. under gaussian assumption, the maximum likelihood estimation converts to a nonlinear least squares problem. efficient solutions to nonlinear least squares exist and they are based on iteratively solving sparse linear systems until convergence. in general, the existing solutions provide only an estimation of the mean state vector, the resulting covariance being computationally too expensive to recover. nevertheless, in many simultaneous localization and mapping (slam) applications, knowing only the mean vector is not enough. data association, obtaining reduced state representations, active decisions and next best view are only a few of the applications that require fast state covariance recovery. furthermore, computer vision and robotic applications are in general performed online. in this case, the state is updated and recomputed every step and its size is continuously growing, therefore, the estimation process may become highly computationally demanding. this paper introduces a general framework for incremental maximum likelihood estimation called slam++, which fully benefits from the incremental nature of the online applications, and provides efficient estimation of both the mean and the covariance of the estimate. based on that, we propose a strategy for maintaining a sparse and scalable state representation for large scale mapping, which uses information theory measures to integrate only informative and non-redundant contributions to the state representation. slam++ differs from existing implementations by performing all the matrix operations by blocks. this led to extremely fast matrix manipulation and arithmetic operations used in nonlinear least squares. even though this paper tests slam++ efficiency on slam problems, its applicability remains general.
computer_graphics	multi-view image-based rendering consists in generating a novel view of a scene from a set of source views. in general, this works by first doing a coarse 3d reconstruction of the scene, and then using this reconstruction to establish correspondences between source and target views, followed by blending the warped views to get the final image. unfortunately, discontinuities in the blending weights, due to scene geometry or camera placement, result in artifacts in the target view. in this paper, we show how to avoid these artifacts by imposing additional constraints on the image gradients of the novel view. we propose a variational framework in which an energy functional is derived and optimized by iteratively solving a linear system. we demonstrate this method on several structured and unstructured multi-view datasets, and show that it numerically outperforms state-of-the-art methods, and eliminates artifacts that result from visibility discontinuities.
operating_systems	introduction worldwide the transport sector faces several issues related to the rising of traffic demand such as congestion, energy consumption, noise, pollution, safety, etc. trying to stem the problem, the european commission is encouraging a modal shift towards railway, considered as one of the key factors for the development of a more sustainable european transport system. the coveted increase in railway share of transport demand for the next decades and the attempt to open up the rail market (for freight, international and recently also local services) strengthen the attention to capacity usage of the system. this contribution proposes a synthetic methodology for the capacity and utilisation analysis of complex interconnected rail networks; the procedure has a dual scope since it allows both a theoretically robust examination of suburban rail systems and a solid approach to be applied, with few additional and consistent assumptions, for feasibility or strategic analysis of wide networks (by efficiently exploiting the use of big data and/or available open databases). method in particular the approach proposes a schematization of typical elements of a rail network (stations and line segments) to be applied in case of lack of more detailed data; in the authors' opinion the strength points of the presented procedure stem from the flexibility of the applied synthetic methods and from the joint analysis of nodes and lines. the article, after building a quasiautomatic model to carry out several analyses by changing the border conditions or assumptions, even presents some general abacuses showing the variability of capacity/utilization of the network 's elements in function of basic parameters. results this has helped in both the presented case studies: one focuses on a detailed analysis of the naples' suburban node, while the other tries to broaden the horizon by examining the whole european rail network with a more specific zoom on the belgium area. the first application shows how the procedure can be applied in case of availability of fine-grained data and for metropolitan/regional analysis, allowing a precise detection of possible bottlenecks in the system and the individuation of possible interventions to relieve the high usage rate of these elements. the second application represents an on-going attempt to provide a broad analysis of capacity and related parameters for the entire european railway system. it explores the potentiality of the approach and the possible exploitation of different 'open and big data' sources, but the outcomes underline the necessity to rely on proper and adequate information; the accuracy of the results significantly depend on the design and precision of the input database. conclusion in conclusion, the proposed methodology aims to evaluate capacity and utilisation rates of rail systems at different geographical scales and according to data availability; the outcomes might provide valuable information to allow efficient exploitation and deployment of railway infrastructure, better supporting policy (e.g. investment prioritization, rail infrastructure access charges) and helping to minimize costs for users. the presented case studies show that the method allows indicative evaluations on the use of the system and comparative analysis between different elementary components, providing a first identification of 'weak' links or nodes for which, then, specific and detailed analyses should be carried out, taking into account more in depth their actual configuration, the technical characteristics and the real composition of the traffic (i.e. other elements influencing the rail capacity, such as: the adopted operating systems, the station traffic/route control & safety system, the elastic release of routes, the overlap of block sections, etc.).
relational_databases	in relational databases and their applications, there are opportunities for evaluating a stream of knn queries submitted one by one at different times. for this issue, we propose a new method with learning-based techniques, region clustering methods and caching mechanisms. this method uses a knowledge base to store related information of some past knn queries, groups the search regions of the past queries into larger regions, and retrieves the tuples from the larger regions. to answer a newly submitted query, our strategy tries to obtain a majority or all of the results from the previously retrieved tuples cached in main memory. thus, this method seeks to minimize the response time by reducing the search region or avoiding the accesses to the underlying database. meanwhile, our method remains effective for high-dimensional data. extensive experiments are carried out to measure the performance of this new strategy and the results indicate that it is significantly better than the state-of-the-art naive methods of evaluating a stream of knn queries for both low-dimensional (2, 3 and 4) and high-dimensional (25, 50 and 104) data.
cryptography	modular exponentiation and modular multiplications are two fundamental operations in various cryptographic applications, and hence the performance of public-key cryptographic algorithms is strongly influenced by the efficient implementation of these operations. reducing the frequency of modular multiplications and the time requirements for modular multiplication will help in developing efficient modular exponential algorithms. this work proposes an energy efficient modular exponential algorithm based on bit forwarding techniques. in particular, two algorithms, bit forwarding 1-bit (bfw1) and bit forwarding 2-bits (bfw2), which are modifications of the existing binary exponential algorithm, have been developed. hardware realizations of the proposed algorithms have been evaluated in terms of throughput, power and energy. results show increased throughput of the order of 11.02% and 15.13%, reduction in power to 1.93% and 6.35% and energy saving of the order of 1.9% and 6.35% for bfw1 and bfw2 algorithms respectively. xilinx ise-14.2 on virtex-5 evaluation board and icarus verilog simulation and synthesis tool are used for hardware realization for fpga and synthesized using cadence for asic. (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	we consider the problem of solving large sparse linear systems where the coefficient matrix is possibly singular but the equations are consistent. block two-stage methods in which the inner iterations are performed using alternating methods are studied. these methods are ideal for parallel processing and provide a very general setting to study parallel block methods including overlapping. convergence properties of these methods are established when the matrix in question is either m-matrix or symmetric matrix. different parallel versions of these methods and implementation strategies, with and without overlapping blocks, are explored. the reported experiments show the behavior and effectiveness of the designed parallel algorithms by exploiting the benefits of shared memory inside the nodes of current smp supercomputers. (c) 2015 civil-comp ltd. and elsevier ltd. all rights reserved.
image_processing	the effects of target emissivity on apparent thermal contrast as well as on detection range capabilities of thermal imagers in long wave infrared and middle wave infrared bands were evaluated. the apparent thermal contrast (to be seen by the thermal imager at standoff distance), considering only the emission from target and background, was first computed in both the ir bands in terms of target emissivity and secondly the apparent thermal contrast, considering the background radiation reflected off the target, was also computed. a graphical user interface simulation in matlab was prepared for the estimation of total apparent thermal contrast taking into account both the emission and reflection. this total apparent thermal contrast was finally used in night vision thermal and image processing model for predicting the detection range performance of thermal imagers. results of the analysis show that the effect of target emissivity on thermal contrast estimates is more pronounced in lwir. the lower thermodynamic temperature difference between target and background at lower values of target emissivity leads to negative thermal contrast which in-turn leads to higher detection ranges.
operating_systems	this paper describes the memory architecture to improve the data transfer and storage in a small satellite. the main objective during the design stage of the architecture is to find a good balance between power consumption, cost, reliability and data processing capability. these variables directly impact each other, and it is important to achieve a suitable balance. for this, a low power flash memory is selected in conjunction with a faster static random access memory to improve the performance of the on-board computer on the satellite. in-built buffers of flash are suitably used to improve system performance. an extensive study of timing requirements to store data in memory is done. a comparison of performance at different voltage levels above the required minimum is done to get a balance between the required speed of programming the memory and power consumption. a highly modular and optimized algorithm is proposed for data transfer and storage which can be easily incorporated into a real time operating system. a method to further save the power is proposed by switching the flash memory to the power saving mode when its usage is not required.
symbolic_computation	the paper describes the mathematica-based software for studying nonlinear control systems. the software relies on an algebraic method, called functions' algebra. the advantage of this approach, over well-known linear algebraic and differential geometric methods is that it is applicable to certain non-smooth systems. the drawback is that the computations are more complicated since the approach manipulates directly with the functions related to the system equations and not with the differential one-forms/vector fields that simplify (linearize) the computations. we have implemented the basic operations of functions' algebra, i.e., partial order, equivalence, summation, and multiplication, but also finding the simplest representative of an equivalence class. the next group of functions is related to the control system and involves binary relation, operators m, m, and computation of certain sequences of invariant vector functions on the basis of system equations. finally, we have developed mathematica functions, allowing us to solve the following control problems in case of discrete-time systems: checking accessibility, static state feedback linearization, and disturbance decoupling.
cryptography	extreme learning machine (elm) is a well-known algorithm for the training of neural networks for two modes of functionality: regression and classification. this paper presents a novel model using elm in ciphering. the study begins with an investigation of the real-time recurrent neural network (rrnn) derived from the gradient-based learning for symmetric cipher. the weakness of this cipher is that the error converges to zero, and that the rrnn will not change regardless of how the plaintext is changed. given the nature of the elm, a technique with an elm-based cipher is proposed to provide the capability of performing the training independently from the input and the error gradient. different simulation scenarios were used to evaluate and validate the effectiveness of the proposed cipher. results revealed that the elm-based cipher performed better than rrnns, especially in terms of security. moreover, the elm-based cipher demonstrated significantly competing performance for a wide range of evaluation measures. using an elm-based cipher instead of an aes or other type of ciphers has the added advantage of providing an addition level of securityby allowing the user to change the algorithmic core of the cipher by simply changing the weights of the neural network. this allows hardware programmed ciphers to be more secure while costing less compared to other ciphers. copyright (c) 2016 john wiley & sons, ltd.
parallel_computing	a parallel version of the nemo complex ocean circulation model has been implemented for the black sea basin; the results of circulation numerical modeling with a high spatial resolution are presented. analysis of the spatial variability is performed for the reconstructed hydrophysical fields in 2005-2008. the resulting simulated spatial variability characteristics of the sea surface temperature are compared with available satellite observational data.
distributed_computing	cloud computing is a specialized form of distributed computing in which the resources such as storage, processors, memory etc. are completely abstracted from the consumer. the number of cloud service providers (csps) who offer computing as a service has increased in recent times and often the customers need to interact with unknown service providers to carry out transactions. in such an open and anonymous environments, trust helps to build consumer confidence and provides a reliable environment for them. a trust based ranking system could also help them to choose between the services as per their requirement. in this paper, multi criteria decision making methods have been used to rank the service providers based on their infrastructure parameters. a combination of analytic and fuzzy method gives a better trust estimate as compared to an analytic method alone.
symbolic_computation	in this paper, we investigate the coupled cubic-quintic nonlinear schrodinger equations with variable coefficients, which describe the effects of quintic nonlinearity for the ultra-short optical pulse propagation in a non-kerr medium, or in the twin-core nonlinear optical fiber or waveguide. under certain constraints on the variable coefficients in such equations, mixed-type (bright-dark) vector one- and two-soliton solutions are derived via the hirota method and symbolic computation, and such vector-soliton solutions are only related to the delayed nonlinear response effect and nonlinearity. through the graphic analysis, we find that the delayed nonlinear response effect and nonlinearity can both affect the vector-soliton amplitude, while the vector-soliton velocity merely depends on the delayed nonlinear response effect. with the choice on the variable coefficients representing the delayed nonlinear response effect and nonlinearity, interactions between the amplitude- and velocity-unchanging, amplitude-changing, velocity-changing and amplitude- and velocity-changing vector two solitons are obtained. we see that the interaction between the vector two solitons is elastic. we also find that the interaction period of the bound vector solitons decreases as the increase of the delayed nonlinear response effect or increases as the decrease of the delayed nonlinear response effect, but is independent of the nonlinearity.
network_security	in recent years, image encryption algorithms have been developed rapidly in order to ensure the security of image transmission. with the assistance of our previous work, this paper proposes a novel chaotic image encryption algorithm based on self-adaptive model and feedback mechanism to enhance the security and improve the efficiency. different from other existing methods where the permutation is performed by the self-adaptive model, the initial values of iteration are generated in a novel way to make the distribution of initial values more uniform. unlike the other schemes which is on the strength of the feedback mechanism in the stage of diffusion, the piecewise linear chaotic map is first introduced to produce the intermediate values for the sake of resisting the differential attack. the security and efficiency analysis has been performed. we measure our scheme through comprehensive simulations, considering key sensitivity, key space, encryption speed, and resistance to common attacks, especially differential attack.
cryptography	aiming at the problem that the fixed radio frequency identification (rfid) system with lightweight cryptography may be easily illegally controlled, a communication authentication protocol based on quantum key distribution using decoy-state method is proposed and developed in this study. a new rfid-system model using quantum key distribution is introduced, which indicates that the quantum keys are distributed to the rfid tags and reader and epc information server via weakly coherent photons transmitted through optical fiber. this work mainly presents the protocol description with detailed theoretical analyses, including rfid system 's initialization, the transmission, reception, and acquisition of the random quantum key, and the authentication process between the epc information server and the rfid tag and reader. the security analysis of the protocol is finally carried out, which proves that the proposed protocol can prevent various eavesdropper 's attacks with solid security.
parallel_computing	this work presents the uncertainty quantification, which includes parametric inference along with uncertainty propagation, for co2 adsorption in a hollow fiber sorbent, a complex dynamic chemical process. parametric inference via bayesian approach is performed using sequential monte carlo, a completely parallel algorithm, and the predictions are obtained by propagating the posterior distribution through the model. the presence of residual variability in the observed data and model inadequacy often present a significant challenge in performing the parametric inference. in this work, residual variability in the observed data is handled by three different approaches: (a) by performing inference with isolated data sets, (b) by increasing the uncertainty in model parameters, and finally, (c) by using a model discrepancy term to account for the uncertainty. the pros and cons of each of the three approaches are illustrated along with the predicted distributions of co2 breakthrough capacity for a scaled-up process. (c) 2016 american institute of chemical engineers
bioinformatics	multiprotein bridging factor 1 (mbf1) is a transcriptional co-activator that mediates transcriptional activation by bridging sequence-specific activator like proteins and the tata-box binding protein (tbp). mbf1 has been well-studied in arabidopsis thaliana, saccharomyces cerevisiae, drosophila melanogaster, and homo sapiens, but it is not well understood in filamentous fungi. in this study, we report the identification and characterization of a mbf1 ortholog (mombf1) in the rice blast fungus magnaporthe oryzae), which causes the devastating rice blast disease and is an ideal model for studying the growth, development and pathogenic mechanisms of filamentous fungi. mombf1 encodes a 161 amino acid protein with a typical mbf1 domain and hth domain. bioinformatics were used to analyze the structural domains in mombf1 and its phylogenetic relationship to other homologs from different organisms. we have generated mombf1 deletion mutants (delta mombf1) and functional complementation transformants, and found that the deletion mutants showed significant defects in vegetative growth and tolerance to exogenous stresses, such as 1 m sorbitol, 0.5 m nacl, and 5 mm h2o2. moreover, delta mombf1 showed reduced pathogenicity with smaller infection lesions than wild type and the complementation strain, and decreased response to the accumulation of ros (reactive oxygen species) in planta at the initial infection stage. taken together, our data indicate that mombf1 is required for vegetative growth, pathogenicity and stress response in m. oryzae.
computer_programming	this paper addresses the road toll pricing and capacity investment problem in a congested road network in a multicriteria decision-making framework. a goal programming approach is used in which the following four major goals are considered: (1)cost recovery, (2)service level, (3)environmental, and (4)equity. the multiobjective road toll pricing and capacity investment problem is formulated as a bilevel goal programming model. the upper level of the model aims to minimize the deviations from stated goals for a given priority ranking of the goals, while the lower level is the road users' route choice equilibrium problem. a simulated annealing-based solution algorithm is developed to solve the proposed model. numerical results show that the priority structure of the goals can significantly affect the road toll pricing and capacity investment decisions. the simulated annealing-based solution algorithm outperforms the sensitivity analysis-based solution algorithm in terms of solution quality. the proposed methodology provides an avenue for understanding the trade-offs among conflicting objectives and for designing a financially and environmentally sustainable transportation system.
algorithm_design	the vertical distribution of plant physiological composition plays an important role in vegetation growth and carbon stock. the hyperspectral lidar was thought to be the most promising solution to assess the vertical distribution of vegetation physiological parameters, such as lai (leaf area index) and chlorophyll content. however, the instrument was not fully feasible, so the simulation and experiment of its response to these parameters was necessary. in this paper, the hyperspectral lidar waveform was simulated with the consideration of single scatter of plant radiative transfer model. the variation of lai and chlorophyll along plant height was investigated and tree scenarios were assumed in the simulation. the hyperspectral lidar data experiment was also carried out to validate the simulation and method. the result showed that the hyperspectral lidar waveform could reflect the variation of plant physiological composition and facilitate the inversion algorithm design and it could play a significant role in parameter estimation and precision agriculture application in future.
image_processing	image segmentation is a crucial step in image processing, especially for medical images. however, the existence of partial volume effect, noise and other artifacts makes this problem much more complex. fuzzy c-means (fcm), as an effective tool to deal with partial volume effect, cannot deal with noise and other artifacts. in this paper, one modified fcm algorithm is proposed to solve the above problems, which includes three main steps: (1) peak detection is used to initialize cluster centers, which can make the initial centers close to the final ones and in turn decrease the number of iterations; (2) fuzzy clustering incorporating spatial information is implemented, which can make the algorithm robust to image artifacts; (3) the segmentation results are refined further by detecting and reallocating the misclassified pixels. experiments are performed on both synthetic and medical images, and the results show that our proposed algorithm is more effective and reliable than other fcm-based algorithms.
data_structures	in 1953, shannon proposed the question of quantification of structural information to analyze communication systems. the question has become one of the longest great challenges in information science and computer science. here, we propose the first metric for structural information. given a graph g, we define the k-dimensional structural information of g (or structure entropy of g), denoted by h-k (g), to be the minimum overall number of bits required to determine the k-dimensional code of the node that is accessible from random walk in g. the k-dimensional structural information provides the principle for completely detecting the natural or true structure, which consists of the rules, regulations, and orders of the graphs, for fully distinguishing the order from disorder in structured noisy data, and for analyzing communication systems, solving the shannon 's problem and opening up new directions. the k-dimensional structural information is also the first metric of dynamical complexity of networks, measuring the complexity of interactions, communications, operations, and even evolution of networks. the metric satisfies a number of fundamental properties, including additivity, locality, robustness, local and incremental computability, and so on. we establish the fundamental theorems of the one-and two-dimensional structural information of networks, including both lower and upper bounds of the metrics of classic data structures, general graphs, the networks of models, and the networks of natural evolution. we propose algorithms to approximate the k-dimensional structural information of graphs by finding the k-dimensional structure of the graphs that minimizes the k-dimensional structure entropy. we find that the k-dimensional structure entropy minimization is the principle for detecting the natural or true structures in real-world networks. consequently, our structural information provides the foundation for knowledge discovering from noisy data. we establish a black hole principle by using the two-dimensional structure information of graphs. we propose the natural rank of locally listing algorithms by the structure entropy minimization principle, providing the basis for a next-generation search engine.
software_engineering	a crucial problem in modern software engineering is maturity assessment of organizations developing software, however, research has shown that efficiency is a clearer indicator of agile transformation readiness. we propose utilizing a process model of rup development methodology, as a pattern for comparing it with the examined process. two factors were proposed to assess maturity and factor determining the efficiency of the rup process. the above mentioned rup model concept is based on a multi-agent based simulation (mabs). it presents goals and behaviours of agents as well as components of the agent system environment. to confirm the usefulness of the method for assessment of organization 's maturity, a two-fold experiment was undertaken. the results confirm the usefulness of the model in efficiency and maturity assessment. first part consisted of tuning the simulation internal parameters to the development process. in the second part we propose three factors that can be used to assess efficiency and maturity of a rup it organization. the proposed coefficients are an extension of previous research by the authors, devoted to the assessment of organizational readiness for agile processes transformation.
operating_systems	switching cost is an important factor for policy makers to consider because it sets a higher price for locked-in consumers by making the market less competitive. though there has been some empirical research analyzing switching costs in the mobile telecommunications market, studies considering the characteristics of smartphones, which have their own operating systems and applications, are still rare. in this study, we conduct a hypothetical conjoint survey to analyze switching cost in the smartphone handset market and derive the cost by using the hierarchical bayesian multinomial logit model to consider respondents' heterogeneity. switching costs of handsets and os are empirically estimated, and the magnitudes depend on the levels of searching cost, learning cost, and uncertainty when purchasing new smartphones. (c) 2016 elsevier ltd. all rights reserved.
computer_vision	recent literature has explored automated pornographic detection a bold move to replace humans in the tedious task of moderating online content. unfortunately, on scenes with high skin exposure, such as people sunbathing and wrestling, the state of the art can have many false alarms. this paper is based on the premise that incorporating motion information in the models can alleviate the problem of mapping skin exposure to pornographic content, and advances the bar on automated pornography detection with the use of motion information and deep learning architectures. deep learning, especially in the form of convolutional neural networks, have striking results on computer vision, but their potential for pornography detection is yet to be fully explored through the use of motion information. we propose novel ways for combining static (picture) and dynamic (motion) information using optical flow and mpeg motion vectors. we show that both methods provide equivalent accuracies, but that mpeg motion vectors allow a more efficient implementation. the best proposed method yields a classification accuracy of 97.9% an error reduction of 64.4% when compared to the state of the art on a dataset of 800 challenging test cases. finally, we present and discuss results on a larger, and more challenging, dataset.
software_engineering	context: the global software industry and the software engineering (se) academia are two large communities. however, unfortunately, the level of joint industry-academia collaborations in se is still relatively very low, compared to the amount of activity in each of the two communities. it seems that the two 'camps' show only limited interest/motivation to collaborate with one other. many researchers and practitioners have written about the challenges, success patterns (what to do, i.e., how to collaborate) and anti-patterns (what not do do) for industry-academia collaborations. objective: to identify (a) the challenges to avoid risks to the collaboration by being aware of the challenges, (b) the best practices to provide an inventory of practices (patterns) allowing for an informed choice of practices to use when planning and conducting collaborative projects. method: a systematic review has been conducted. synthesis has been done using grounded-theory based coding procedures. results: through thematic analysis we identified 10 challenge themes and 17 best practice themes. a key outcome was the inventory of best practices, the most common ones recommended in different contexts were to hold regular workshops and seminars with industry, assure continuous learning from industry and academic sides, ensure management engagement, the need for a champion, basing research on real world problems, showing explicit benefits to the industry partner, be agile during the collaboration, and the co-location of the researcher on the industry side. conclusion: given the importance of industry-academia collaboration to conduct research of high practical relevance we provide a synthesis of challenges and best practices, which can be used by researchers and practitioners to make informed decisions on how to structure their collaborations. (c) 2016 elsevier b.v. all rights reserved.
cryptography	digital watermarking protocols are the one, which have combined fingerprinting technique with watermarking, for embedding digital signal or watermark into an original multimedia object. buyer-seller watermarking protocol is fundamentally applied to continue the digital rights of both buyers and seller. we proposed an identity-based buyer-seller watermarking protocol that encounters various weaknesses of zhang et al. 's watermarking protocol. we ensured that by pointing out these weaknesses, inaccuracy can be minimised for further implementing the buyer-seller watermarking protocol. the suggested protocol uses id-based public key cryptography and digital watermarking scheme to place the ownership of digital content. hence, copyright protection is attained. we claim that our suggested protocol is efficient and has adequate security as compared to traditionally proposed protocols, and therefore suitable for any practical buyer-seller watermarking scheme.
algorithm_design	the current paper describes a hybrid control algorithm for fuel consumption minimization of a compound hybrid excavator. the power train of the excavator integrates an engine assist motor, a super capacitor, and a dc/dc converter for hybridization of the original power train. this power train also incorporates an electrically propelled swing motor, which replaces the conventional hydraulic swing motor, to remove hydraulic loss and recuperate the kinetic energy of the swing motion. since the super capacitor provides the required energy for the electric swing motor, sustaining the charge presents an important consideration in the power management algorithm design. first, the optimal control problem has been applied to the fuel consumption minimization problem, and an algorithm based on the equivalent fuel minimization strategy (ecms) has been applied by analyzing the behavior of the co-state of the optimal control problem. the ecms algorithm is integrated with an engine set speed regulator to increase the overall efficiency of the diesel engine. the engine set speed regulator was designed to change the engine set speed, depending on the engine load. simulations show that the ecms is near optimum compared to the dynamic programming results, maintaining an approximately 3% fuel improvement compared to a thermostat controller, which determines power distribution based on the state of charge. excellent charge-sustaining performance was also achieved. the performance of the control algorithm was verified through real-world vehicle tests, resulting in an approximately 30% improvement in fuel economy, compared to the conventional excavator. (c) 2016 published by elsevier b.v.
computer_vision	segmentation and classification of objects in images is one of the most important and yet one of the most complex problems in computer vision. in this work we propose a new model for natural image object classification using contextual information at the level of image segments. context modeling is largely independent of appearance-based classification and proposed model enables simple upgrade of existing systems with information from global and/or local context. context modeling is based on non-parametric use of appearance-based classification results which is a novel approach compared to previous systems that model context on a limited number of rules expressed with a fixed set of parameters. model implementation resulted in a system that, in our simulations, showed stable improvement of the appearance-based object classification.
bioinformatics	introduction: the effectiveness of lantibiotics against mdr pathogens and the progression of agents mu1140, nai-107, nvb302 and duramycin into pre-clinical and clinical trials have highlighted their potential in the fight against bacterial resistance. the number of known lantibiotics and knowledge of their biosynthetic pathways has increased in recent years due to higher quality genomic data being delivered by next generation sequencing technologies combined with the development of specific genome mining tools, enabling the prediction of lantibiotic clusters. areas covered: in this review, the author describes how the increase of high quality genomic data has increased the discovery of novel lantibiotics. expert opinion: novel apparatus such as the ichip enabling the isolation of uncultable bacteria will undoubtedly increase the identification rate of novel antimicrobial peptides including lantibiotics. the ability to then assess the lantibiotic clusters via recombinant production or synthesis using a high throughput method is one of the next challenges for developing these agents into the clinical environment.
parallel_computing	in the present paper, an ultrafast and scalable parallel program for the radial pair function (rpf) is presented via pure message passing interface (mpi) paradigm. the parallel code computes the radial distribution function for the single-component as well as multi-component systems. the single-component and multi-component systems have been extracted for benchmarking purposes by means of user-written codes in c++ for the graphite structure and mpi c++ for hydrogen adsorption via grand canonical monte carlo (gcmc) in the single-walled carbon nanotube (swnt), respectively. the speedup and efficiency curves substantiate an excellent performance in terms of computing time and computation size as well. additionally, the mentioned mpi paradigms are nearly five times (single-component systems) and two times (multi-component systems) faster than the relevant parallel codes using a machine with 48 cpus and nvidia quadro k5200/pcie/sse2. some conclusions and outlooks pertaining to the numerical implementations, algorithm optimization involving the space decomposition idea have been discussed and provided.
software_engineering	sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. in requirements reuse (rr), the extraction of software features from software requirement specifications (srs) is viable only to practitioners who have access to these software artefacts. due to organisational privacy, srs are always kept confidential and not easily available to the public. as alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the rr process. the aim of this paper is to propose a semi-automated approach, known as feature extraction for reuse of natural language requirements (fenl), to extract phrases that can represent software features from software reviews in the absence of srs as a way to initiate the rr process. fenl is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. in the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (ir) area. as a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. as for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. the performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and f-measure (c) 2016 elsevier b.v. all rights reserved.
cryptography	the continuous auxiliary inputs leakage is more strong side-channel attacks. in this article, we first propose a continuous auxiliary inputs leakage model for the hierarchical attribute-based encryption scheme. under the security model, an adversary has ability to gain partial updated master keys and updated secret keys continually by certain leakage attacks. moreover, a resilient-leakage hierarchical attribute-based encryption scheme is constructed. the security proof for this scheme is provided under the standard model. furthermore, we give the performance comparison between our scheme and relevant scheme. (c) 2016 john wiley & sons, ltd.
operating_systems	partitionfinder 2 is a program for automatically selecting best-fit partitioning schemes and models of evolution for phylogenetic analyses. partitionfinder 2 is substantially faster and more efficient than version 1, and incorporates many new methods and features. these include the ability to analyze morphological datasets, new methods to analyze genome-scale datasets, new output formats to facilitate interoperability with downstream software, and many new models of molecular evolution. partitionfinder 2 is freely available under an open source license and works on windows, osx, and linux operating systems. it can be downloaded from www.robertlanfear.com/partitionfinder. the source code is available at https://github.com/brettc/partitionfinder.
parallel_computing	managing servers integration to realize distributed data computing framework is an important concern. regardless of the underlying architecture and the actual distributed system 's complexity, such framework gives programmers an abstract view of systems to achieve variously data-intensive applications. however, some state-of-the-art frameworks need too much library dependencies and parameters configuration, or lack extensibility in application programming. moreover, general framework 's precise design is a nontrivial work, which is fraught with challenges of task scheduling, message communication and computing efficiency, etc. to address these problems, we present a general, scalable and programmable parallel computing framework called sunwaymr, which only needs gcc/g++ environment. we argue it from the following aspects: (1) distributed data partitioning, message communication and task organization are given to support transparent application execution on parallel hardware. by searching threads table of each node, the task gets an idle thread (with preferred node ip address) for executing data partition. a novel communication component, sunwaymrhelper, is employed to merge periodical results synchronously. through identifying whether current node is master or slave, sunwaymr deals with the periodical task 's results differently. (2) as for optimizations, a simple fault tolerance is given to resume data-parallel applications, and thread-level stringstream is utilized to boost computing. to ensure ease-of-use, open application programming interface (api) excerpts can be invoked by various of applications with fewer handwritten code than openmpl/mpi. we conduct extensively experimental studies to evaluate the performance of sunwaymr over real-world datasets. results indicate that sunwaymr (runs on 16 computational nodes) outperforms spark in various applications, and has good scaling with data sizes, nodes and threads. (c) 2017 elsevier b.v. all rights reserved.
symbolic_computation	under investigation in this paper is a discrete (2+1)-dimensional ablowitz-ladik equation, which has certain applications in nonlinear optics and bose-einstein condensation. employing the hirota method and symbolic computation. we obtain the bright/dark one-, two-, three-and n-soliton solutions. asymptotic analysis indicates that the interactions between the bright/dark two solitons are elastic. amplitudes and velocities of the bright and dark solitons increase with the value of the coupling strength increasing. head-on and overtaking interactions between the bright two solitons as well as the bound state two solitons are depicted. overtaking interaction between the dark two solitons are also plotted. the increasing value of the coupling strength can lead the increasing amplitudes and velocities of the bright/dark two solitons. (c) 2017 published by elsevier b.v.
parallel_computing	the interaction simulations between particles and structures are often performed in the context of the combined discrete-finite element (dem-fem) method, where an efficient, robust and accurate contact algorithm for challenging contact problems is essential. a three-dimensional (3d) discrete and finite element contact algorithm, named zgl here, has been proposed in our research group (zang, m.y. et al., 2011. a contact algorithm for 3d discrete and finite element contact problems based on penalty function method. comput. mech. 48, 541-550.). despite being quite well-established, zgl algorithm may lack efficiency, robustness and accuracy in certain situations, e.g., when special mesh pattern is applied. however, since such cases are readily to appear in practice engineering application, an algorithm named dzceii is developed to resolve these issues and thus significantly applicable to challenging contact problems. the proposed dzcell algorithm includes an improved global phase to directly find the potential segments rather than the nearest node for discrete elements as contact counterparts. furthermore, both the memory cost and time consumption of this algorithm are linear, and the algorithm is readily to extend to parallel computing. several numerical examples demonstrate the achievable improvements in terms of efficiency, robustness and accuracy for 3d contact analysis and validate the capability of the dzceii algorithm in the granular science and mechanical engineering. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	the current development in the area of computer graphics focuses itself on 3d visualization. this direction, set by contemporary trends, further utilizes 3d visualization, increases its quality and improves visualization devices for 3d graphics. this change must be reflected by teaching methods as well, through which future specialists of advanced graphics, modellers, but also programmers and implementation coders. the article presents its own software solution and innovative approach to teaching computer graphics. the solution and all principles are based on the montessori method of child education.
symbolic_computation	in this paper, based on a variable-coefficient nonlinear schrodinger (vcnls) equation, amplification of the fundamental and second-order unchirped solitons in the dispersion-decreasing fiber without any external amplification device, which is different from those in the existing literatures, is studied. via symbolic computation, soliton solutions of the vcnls equation are obtained. for a fundamental-soliton pulse, the amplitude is amplified by the gain during the propagation, whereas the width keeps unchanged. because of the equilibrium between the gain, nonlinearity and varying dispersion, soliton structure is not destroyed, and the amplified fundamental soliton is free from the pedestal and chirp. with the increase of the absolute value of the gain coefficient , magnification of the fundamental-soliton amplitude is enhanced in the same propagation distance. for the second-order soliton, the width is compressed and the amplitude is amplified, because the amplification process is accompanied by the compression of the soliton. period of the second-order soliton decreases exponentially during the propagation, and decreases with the increase of the absolute value of in the same propagation distance.
computer_graphics	realistic animation of an expressive human face has been a great challenge in computer graphics due to the human perception of human faces. it is a complex, costly and time-consuming process requiring a great detail in many aspects. in this paper, a three-stage method is presented that simplifies the creation of realistic facial animations by transferring the expressions from 2d videos onto 3d models with a joint-based system on a real-time basis. the first stage covers the preparation of the model with a joint-based rig for the transfer. the second includes the real time tracking of a human face with a single camera to obtain 2d positions of facial landmarks. also it covers the transfer of 3d relative movement data to animate the prepared model by moving the respective joints. the last stage covers the recording of animation using a partially automated key-framing technique. the presented method provides a fast, easy to use and affordable system that produces visually satisfying facial animations.
distributed_computing	in grid computing environment, several classes of multi-component applications exist. these types of applications may often require additional resources of different types that go beyond what is available in any of the sites making up the grid resource composition. the heterogeneity nature of both the user application and the computing environment makes this a challenging problem. however, the current off-the-shelf scheduling software can hardly cope with these diversities in distributed computing application frameworks. therefore, there is the need for an adequate scheduling system that would grant simultaneous or coordinated access to application of multi-component nature that requires resources of possibly multiple types, in multiple locations, managed by different resource providers. the main focus of this paper is to develop a mobile agent scheduling model that addresses the aforementioned challenge. a scheduling policy that pertains to job scheduling and resource allocation is proposed. the scheduling policy treats different multi-component applications requiring diverse heterogeneous resources fairly. the policy is used by mobile agents to schedule user applications and to also find available and suitable distributed resource that are capable of executing user application at a very minimal time. copyright (c) 2015 john wiley & sons, ltd.
computer_programming	virtual blended learning as the use of virtual 3d environments in education has already delivered utility in a number of cases. this article presents a concept and a prototype of a 3d environment used in the current semester to employ the new medium in programming education at universities. the concept brings gamification aspects as well as interaction mechanisms from social media to the virtual 3d environment. it thereby aims to reduce the large number of student dropouts due to failing the programming modules by motivating more and less capable students alike to an increased participation in the practical exercises. it is concluded that the concept is applicable in a much broader spectrum of exercise and tutorial settings.
machine_learning	the ability to derive new insights from data using advanced machine learning or analytics techniques can enhance the decision-making process in companies. nevertheless, researchers have found that the actual application of analytics in companies is still in its initial stages. therefore, this paper studies by means of a descriptive survey the application of analytics with regards to five different aspects as defined by the delta model: data, enterprise or organization, leadership, targets or techniques and applications, and the analysts who apply the techniques themselves. we found that the analytics organization in companies matures with regards to these aspects. as such, if companies started earlier with analytics, they apply nowadays more complex techniques such as neural networks, and more advanced applications such as hr analytics and predictive analytics. moreover, analytics is differently propagated throughout companies as they mature with a larger focus on department-wide or organization-wide analytics and a more advanced data governance policy. next, we research by means of clustering how these characteristics can indicate the analytics maturity stage of companies. as such, we discover four clusters with a clear growth path: no analytics, analytics bootstrappers, sustainable analytics adopters and disruptive analytics innovators. (c) 2016 elsevier ltd. all rights reserved.
computer_graphics	the presented simulation model of surface tension and wettability based on physical properties of liquids is designed for use in computer graphics. due to the relatively small surface tension forces the model is useful for simulating liquid of small volume such as droplets. this model can be used in conjunction with various fluid simulation methods, one of the most popular - marker and cell - has been selected for this paper. the paper describes also a simple and rapid method of determining the liquid surface as a mesh of triangles. the presented method improves the final visual effect and is well suited for determining the surface of the droplets. the simulation method was applied to create realistic animations of flowing liquid droplets of different types.
computer_graphics	image splicing is a common and widespread type of manipulation, which is defined as pasting a portion of an image onto a second image. several forensic methods have been developed to detect splicing, using various image properties. some of these methods exploit the noise statistics of the image to try and find discrepancies. in this paper, we propose a new counter-forensic approach to eliminate the noise differences that can appear in a spliced image. this approach can also be used when creating computer graphics images, in order to endow them with a realistic noise. this is performed by changing the noise statistics of the spliced elements so that they are closer to those of the original image. the proposed method makes use of a novel way to transfer density functions. we apply this to image noise in order to impose identical noise density functions from a source to a destination image. this method can be used with arbitrary noise distributions. the method is tested against several noise-based splicing detection methods, in order to prove its efficacy.
distributed_computing	mobile agent technology is becoming more popular and has been implemented in many distributed computing domains. several research have been conducted to address its challenges including two most important ones which are agent spawning and agent mobility. this paper discusses the issues of mobile agent technology, the background of mobile agent cloning and spawning, agent mobility as well as the problems faced by many researchers in their research on mobile agents. the paper finally proposes new agent spawning and mobility models to resolve some of the researchers' problems.
network_security	key predistribution schemes for resource-constrained networks are methods for allocating symmetric keys to devices in such a way as to provide an efficient trade-off between key storage, connectivity and resilience. while there have been many suggested constructions for key predistribution schemes, a general understanding of the design principles on which to base such constructions is somewhat lacking. indeed even the tools from which to develop such an understanding are currently limited, which results in many relatively ad hoc proposals in the research literature. it has been suggested that a large edge-expansion coefficient in the key graph is desirable for efficient key predistribution schemes. however, attempts to create key predistribution schemes from known expander graph constructions have only provided an extreme in the trade-off between connectivity and resilience: namely, they provide perfect resilience at the expense of substantially lower connectivity than can be achieved with the same key storage. our contribution is twofold. first, we prove that many existing key predistribution schemes produce key graphs with good expansion. this provides further support and justification for their use, and confirms the validity of expansion as a sound design principle. second, we propose the use of incidence graphs and concurrence graphs as tools to represent, design and analyse key predistribution schemes. we show that these tools can lead to helpful insights and new constructions.
data_structures	formal language theory plays, in computer science, a fundamental role that allows, among other things, the development of one of the cornerstones of information technology: programming languages. they define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines. despite its significance, formal language theory is often taken for granted, even by software developers, who regularly follow the rules of their programming domain. unless the developer is creating its own programming language, data structure, or describing the formal background of an existing one, he will not need to dive deep into formal languages, grammar or automaton theory. those who do need to develop their own rules will first have to understand the theory of formal languages, and the limitations they impose. this paper will do an introduction to the fundamentals of language theory, their classification, restrictions and representation. once this ground rules are set, we will use a worldwide known data structure format such as the javascript object notation (json), to formally define its grammar rules and automaton. all of this formal background will allow us to transform theory into bits, by developing an algorithm that will analyze streams of texts, accepting or rejecting them as they comply or not with the predefined rules. finally, we will analyze the outcomes of this implementation, its benefits, limitations, and alternatives that could have been followed.
operating_systems	traditional network architecture is inflexible and complex. this observation has led to a paradigm shift toward software-defined networks (sdns), in which the network control level is separated from the data link layer. this change became possible because of the control plane transfer from switching equipment to software modules that run on a dedicated server called a controller (or a network operating system) or to network applications that work with this controller. methods of presentation, storage, and communication interfaces with network topology elements available to sdn controller users are the most important aspects of network operating systems because the operation of some key controller modules depends heavily on the internal representation of the network topology. firewall and routing modules can be cited as examples of these modules. this paper considers the methods used to represent and store the network topology, as well as communication interfaces with corresponding modules of the floodlight controller. an alternative algorithm for exchanging messages on the changes in the network topology between the controller and network applications has been proposed and developed. the algorithm makes it possible to issue notifications based on a subscription to relevant events. an api for the module of interacting with applied programs of the sdn controller has been developed. the topology tracker module has been designed based on this algorithm and api. in active mode, this module can inform network applications about the changes in the network topology and store its compact representation for the interaction acceleration.
distributed_computing	cloud computing is a set of information technology services that are provided to a customer over a network on a leased basis and with the ability to scale up or down their service requirements. cloud computing is a new general purpose internet-based technology through which information is stored in servers and provided as a service and on-demand to the clients. it builds on decades of research in virtualization, distributed computing, utility computing, and more recently networking, web and software services. it implies a service oriented architecture, reduced information technology overhead for the end-user, great flexibility, reduced total cost of ownership, on-demand services and many other things. this paper discusses the concept of cloud computing, some of the issues it tries to address, and available cloud computing implementation.
distributed_computing	in this paper, we focus on detecting a special type of anomaly in wireless sensor network (wsn), which appears simultaneously in a collection of neighboring nodes and lasts for a significant period of time. existing point-based techniques, in this context, are not very effective and efficient. with the proposed distributed segment-based recursive kernel density estimation, a global probability density function can be tracked and its difference between every two periods of time is continuously measured for decision making. kullback-leibler (kl) divergence is employed as the measure and, in order to implement distributed in-network estimation at a lower communication cost, several types of approximated kl divergence are proposed. in the meantime, an entropic graph-based algorithm that operates in the manner of centralized computing is realized, in comparison with the proposed kl divergence-based algorithms. finally, the algorithms are evaluated using a real-world data set, which demonstrates that they are able to achieve a comparable performance at a much lower communication cost.
operating_systems	the internet of things (iot) is projected to soon interconnect tens of billions of new devices, in large part also connected to the internet. iot devices include both high-end devices which can use traditional go-to operating systems (oss) such as linux, and low-end devices which cannot, due to stringent resource constraints, e.g., very limited memory, computational power, and power supply. however, large-scale iot software development, deployment, and maintenance requires an appropriate os to build upon. in this paper, we thus analyze in detail the specific requirements that an os should satisfy to run on low-end iot devices, and we survey applicable oss, focusing on candidates that could become an equivalent of linux for such devices, i.e., a one-size-fits-most, open source os for low-end iot devices.
image_processing	a novel depth from defocus (dfd) measurement system is presented, where the extension of the measurement range is performed using an emergent technology based on liquid lenses. a suitable set of different focal lengths, obtained by properly changing the liquid lens supply voltage, provides multiple camera settings without duplicating the system elements or using moving parts. a simple and compact setup, with a single camera/illuminator coaxial assembly, is obtained. the measurement is based on an active dfd technique using modulation measurement profilometry for the estimation of the contrast at each image point as a function of the depth range. two different measurement methods are proposed, both based on a combination of multiple contrast curves, each derived at a specific focal length. in the first method (intensity contrast method), the depth information is recovered directly from the contrast curves, whereas in the second (differential contrast method), the depth is measured using contrast curve pairs. we obtained a measurement sigma(0) of 0.55 mm over a depth range of 60 mm with the intensity contrast method (0.92% of the total range) and an sigma(0) of 0.76 mm over a depth range of 135 mm with the differential contrast method (0.56% of the total range). thus, the intensity contrast method is within the state-of-the-art dfd systems, whereas the differential contrast method allows, sigma(0) being almost equal, a remarkable extension of the depth range.
image_processing	aim of this paper was to assess the clinical effectiveness of a novel ultrasound (us) approach for the estimation of bone fragility. a total of 85 female patients (40-80 years) were recruited and underwent conventional dxa investigations of both lumbar spine and proximal femur, an abdominal us scan of the lumbar spine and the frax (r) questionnaire for the calculation of osteoporotic fracture probabilities. acquired us data were analyzed through an automatic algorithm that calculated the fragility score (f. s.), a parameter that estimates skeletal fragility from dedicated spectral and statistical analyses. f.s. showed a good correlation with the most reliable fracture risk predictions obtained by frax (r) (r=0.71, p<0.001). since this correlation level with frax (r) outcomes was much better than lumbar bmd one ( vertical bar r vertical bar = 0.43) and very similar to that obtained for femoral neck bmd (vertical bar r vertical bar = 0.72), f.s. has the potential to become a simple and non-ionizing method for bone fragility assessment. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	novice programmers have a misconception of what programming is in the early stages of learning programming. a flowchart-based programming environment (fpe) is developed in this research with the aim of introducing the early stages of learning programming to clarify matters. an attempt is made to introduce the basic programming algorithms prior to surface structure using an automatic text-to-flowchart conversion approach in order to improve students' problem-solving skills. thus, this system allows students to focus less on language and syntax and more on solution designing in the form of flowchart development. the main objective of this study is to support the problem-solving ability through designing activities. how exactly fpe employs text-to-flowchart conversion as a visualization-based approach to provide the students with their final flowchart for subsequent stages of programming is discussed in this paper. the proposed system is evaluated using 50 first-year undergraduate students taking their first introductory courses in programming called ""programming 1"" at university of malaya, who gave very positive feedback. a very awarding finding was that an automatic text-to-flowchart conversion approach applied in fpe successfully motivated almost all participants in problem-solving activities. consequently, the results suggest further development of a text-to-flowchart conversion approach in the form of a multi-agent system (mas) in future in order to make the early stages of learning programming more encouraging for students.
machine_learning	identifying network traffics at their early stages accurately is very important for network management and security. recent years, more and more studies have devoted to find effective machine learning models to identify traffics with few packets at the early stage. in this paper, we try to build an effective early stage traffic identification model by applying flexible neural trees (fnt). three network traffic data sets including two open data sets are used for the study. we first extract both packet-level features and statistical features from the first six continuous packets and six noncontinuous packets of each flow. packet sizes are applied as packet-level features. and for statistical features, average, standard deviation, maximum and minimum are selected. eight classical classifiers are employed as the comparing methods in the identification experiments. accuracy, true positive rate (tpr) and false positive rate (fpr) are applied to evaluate the performances of the compared methods. fnt outperforms the other methods for most cases in the identification experiments, and it behaves very well for both tpr and fpr. furthermore, it can show the selected features in the optimal tree result. experiment result shows that fnt is effective for early stage traffic identification.
data_structures	this study contains an examination of the missing data structures, occurring in many fields, especially in livestock. it also examines the processes to obtain the solution for the missing data. for this purpose, linolenic acid measurements obtained from four different anatomic regions of two animal species were taken as dependent variables. for the dependent variable, the observations were deleted at the ratio of 10% and 20%, creating the missing structures of missing completely at random (mcar) and missing at random (mar). subsequently, these data sets were completed using multiple imputation (mi) method. generalized estimating equation (gee) and mixed model methods were used in the missing data structures and for the purpose of evaluating the data completed with mi. the study were obtained almost same results obtained from gee and mixed model in the missing data structures. at the same time, there was not found difference between the methods in completed data using mi method. as a result, it is stated that valid results obtained in missing data structures by used gee and mixed model analysis. when these results are also compared, it can be concluded that multiple imputation (with these ratio of missing) is not necessary before gee and mixed model.
computer_programming	we teach computer programming to students aged 17 through 18 years. the course is structured into two units, with the first and second term each consisting of two 90-minute sessions per week conducted over 30 weeks. the course combines lectures with practical exercises. every year we conduct a survey following the first term of two 90-minute sessions per week held over 30 weeks. the survey 's results show that not a few students consider themselves to have insufficient understanding of programming or think that they are not good at programming. in response, we adopted and implemented a part of the computer science unplugged (cs unplugged) method, which is considered an effective way of teaching information science. however, although cs unplugged has generated considerable results in motivating students to learn and in initial learning, we feel that it is not sufficiently connected to full-fledged programming languages such as c and java. accordingly, we propose a new method of advancing from cs unplugged to full-fledged programming. the proposed method begins with conducting a cs unplugged activity, and then continues on to the writing of a program on the same theme and further to its abstraction in java. in this paper, we describe the thinking and concepts behind this proposed method.
network_security	the rapid growth of residential broadband connections and internet-enabled home devices have driven the success of many useful applications such as video streaming and remote healthcare. however, poorly managed routers and connected devices in the home are vulnerable under persistent threats and exploitations from cyber attackers across the internet who continuously identify, compromise, and control devices as part of botnets for launching click fraud, denial of service attacks, spam campaigns. these growing threats and broad damages have made it imperative to understand, characterize, filter, and reduce exploit traffic towards millions of home routers and billions of connected devices in the home. this paper presents a bloom-filter based analytics framework to capture persistent threats towards the same home routers and to identify correlated attacks towards distributed home networks. our experimental results based on network traffic collected from real homes over 18 months have revealed a number of interesting findings on persistent and correlated threats towards home networks, which calls for improved security and management of home networks. to the best of our knowledge, this paper is the first effort to characterize cyber threats towards home networks and to propose a simple and yet effective approach to identify persistent and aggressive attacks towards home networks. copyright (c) 2016 john wiley & sons, ltd.
software_engineering	the problem of automatically constructing a software component such that when executed in a given environment satisfies a goal, is recurrent in software engineering. controller synthesis is a field which fits into this vision. in this paper we study controller synthesis for partially observable lts models. we exploit the link between partially observable control and non-determinism and show that, unlike fully observable lts or kripke structure control problems, in this setting the existence of a solution depends on the interaction model between the controller-to-be and its environment. we identify two interaction models, namely interface automata and weak interface automata, define appropriate control problems and describe synthesis algorithms for each of them.
computer_graphics	the technology of vr (virtual reality) is the result of the progress of science and technology since the 20th century, it embodied the computer technology, computer graphics, multimedia technology, sensor technology, display technology, human body engineering, human-computer interaction theory, the latest achievements of artificial intelligence, and other fields, has become the latest achievements after relay information field of multimedia technology and network technology is widely attention and research, development and application of hot spots, is currently the fastest growing a multi-disciplinary comprehensive technology. rapid generating equipment and interaction changed in the past, between people and computer dull, stiff, passive way of communication, make the man-machine interaction between become more humanized, blazed a new research field of human-computer interaction interface, which provides a new interface for the application of intelligent engineering tools, for all kinds of engineering provides a new description method of large-scale data visualization, but also changed the way people work and lifestyle and ideology.
network_security	in a node replication attack, an adversary creates replicas of captured sensor nodes in an attempt to control information that is reaching the base station or, more generally, compromise the functionality of the network. in this work, we develop fully distributed and completely decentralized schemes to detect and evict multiple imposters in mobile wireless sensor networks (mwsns). the proposed schemes not only quarantines these malicious nodes but also withstand collusion against collaborating imposters trying to blacklist legitimate nodes of the network. hence the completeness and soundness of the protocols is guaranteed. our protocols are coupled with extensive mathematical and experimental results, proving the viability of our proposals, thus making them fit for realistic mobile sensor network deployments. (c) 2016 elsevier b.v. all rights reserved.
algorithm_design	recently, degree-of-freedom (dof)-based models have been widely used to study mimo network performance. existing dof-based models differ in their interference cancellation (ic) behavior and many of them suffer from either loss of solution space or possible infeasible solutions. to overcome these limitations, a new dof-based model, which employs an ic scheme based on node-ordering was proposed. in this paper, we apply this new dof ic model to study a throughput maximization problem in a multihop mimo network. the problem formulation involves joint consideration of flow routing and dof allocation and falls in the form of a mixed-integer linear program (milp). our main contribution is an efficient polynomial time algorithm that offers a competitive solution to the milp through a series of linear programs (lps). the algorithm employs a sequential fixing framework to obtain an initial feasible solution and then improves the solution by exploiting: 1) the impact of node ordering on dof consumption for ic at a node and 2) route diversity in the network. simulation results show that the solutions obtained by our proposed algorithm are competitive and feasible.
data_structures	multicore architectures are becoming the most promising computing platforms thanks to their high performance. the soft error rate in multicore systems increases by the trend in the transistor sizes and the reduction of the voltage of the transistors. evaluating the impact of soft errors on parallel applications is critical to understand the fault characteristics and to decide the fault tolerance strategies for the reliable execution. in this paper, we examine the soft error vulnerabilities of shared data in parallel java applications. to analyze fault behavior of shared data in parallel programs, we design and implement a bytecode instrumentation based analysis and fault injection framework. we evaluate the fault behavior of shared data fields on a set of parallel applications from nas benchmark suite. our experimental evaluation demonstrates data type and access characteristics of the shared fields, and shows that shared data structures of parallel applications are more vulnerable to soft errors. while error rates for unshared local data stay around 20% in our target applications, the rate for shared data exceeds above 30% for some applications. we further discuss potential directions of our results and how shared data analysis can be employed to apply partial fault tolerance techniques. (c) 2016 elsevier b.v. all rights reserved.
image_processing	to help the clinicians to segment the borders of the left ventricle (lv) efficiently during measurement of the heart, the authors come up with a semi-automatic approach in this study that is capable of identifying the endocardial borders robustly from cine magnetic resonance images. firstly, the deformation flow is computed between the inputted boundary in the previous frame and the extracted edge of the lv in the current frame based on boundary minimum distance principle (bmdp). then, the deformation flow is constrained by optical flow calculated by a partial differential equation model. a smooth deformation boundary is then formed by minimising the energy between the previously inputted boundary and the rough boundary obtained by bmdp and optical flow constraint. to extract edge of the lv as accurate as possible, a threshold selection method is used and improved based on the previous study. the proposed approach is tested on the open access dataset. the computed average perpendicular distance is 1.36 +/- 0.24mm and the computed dice measure is 90.7%+/- 0.15%. experimental results show that the proposed approach is significantly more accurate than the referenced state of art methods.
computer_graphics	the aspect ratio of a plot can strongly influence the perception of trends in the data. arc length based aspect ratio selection (al) has demonstrated many empirical advantages over previous methods. however, it is still not clear why and when this method works. in this paper, we attempt to unravel its mystery by exploring its mathematical foundation. first, we explain the rationale why this method is parameterization invariant and follow the same rationale to extend previous methods which are not parameterization invariant. as such, we propose maximizing weighted local curvature (mlc), a parameterization invariant form of local orientation resolution (lor) and reveal the theoretical connection between average slope (as) and resultant vector (rv). furthermore, we establish a mathematical connection between al and banking to 45 degrees and derive the upper and lower bounds of its average absolute slopes. finally, we conduct a quantitative comparison that revises the understanding of aspect ratio selection methods in three aspects: (1) showing that al, awo and rv always perform very similarly while ms is not; (2) demonstrating the advantages in the robustness of rv over al; (3) providing a counterexample where all previous methods produce poor results while mlc works well.
computer_programming	among the computer science courses intended for high school non-majors, the visual basic (vb) program design is a very important foundation curriculum. however, in the traditional vb teaching, acquiring programming theory knowledge is especially difficult for the non-computer major students. it is urgent to find an effective way to cultivate a solid theoretical foundation and strong practical ability of students in university computer teaching. problem based learning is a novel teaching method based on the guidance of the construct theory, which has good teaching effects in many cases. the paper presented a brief introduction to the pbl teaching method, and proposed a novel instruction design method for computer programming course teaching. the extra high teaching effects obtained in the practical application of the vb program design course has demonstrated the effectiveness of our proposed instruction design method with problem based learning.
computer_graphics	to assist the rehearsal and planning of robot-assisted partial nephrectomy, a real-time simulation platform is presented that allows surgeons to visualise and interact with rapidly constructed patient-specific biomechanical models of the anatomical regions of interest. coupled to a framework for volumetric deformation, the platform furthermore simulates intracorporeal 2d ultrasound image acquisition, using preoperative imaging as the data source. this not only facilitates the planning of optimal transducer trajectories and viewpoints, but can also act as a validation context for manually operated freehand 3d acquisitions and reconstructions. the simulation platform was implemented within the gpu-accelerated nvidia flex position-based dynamics framework. in order to validate the model and determine material properties and other simulation parameter values, a porcine kidney with embedded fiducial beads was ct-scanned and segmented. acquisitions for the rest position and three different levels of probe-induced deformation were collected. optimal values of the cluster stiffness coefficients were determined for a range of different particle radii, where the objective function comprised the mean distance error between real and simulated fiducial positions over the sequence of deformations. the mean fiducial error at each deformation stage was found to be compatible with the level of ultrasound probe calibration error typically observed in clinical practice. furthermore, the simulation exhibited unconditional stability on account of its use of clustered shape-matching constraints. a novel position-based dynamics implementation of soft tissue deformation has been shown to facilitate several desirable simulation characteristics: real-time performance, unconditional stability, rapid model construction enabling patient-specific behaviour and accuracy with respect to reference ct images.
relational_databases	relational learning algorithms mine complex databases for interesting patterns. usually, the search space of patterns grows very quickly with the increase in data size, making it impractical to solve important problems. in this work we present the design of a relational learning system, that takes advantage of graphics processing units (gpus) to perform the most time consuming function of the learner, rule coverage. to evaluate performance, we use four applications: a widely used relational learning benchmark for predicting carcinogenesis in rodents, an application in chemo-informatics, an application in opinion mining, and an application in mining health record data. we compare results using a single and multiple cpus in a multicore host and using the gpu version. results show that the gpu version of the learner is up to eight times faster than the best cpu version.
software_engineering	nowadays, remote collaborative learning tools for computer science education mostly emphasize providing learning resources and realizing virtual collaborative learning environment for students. many people in this field tend to have their mind fixed on the process improvement of such a collaboration as a whole, while few notice that individuals may have different roles and impacts on this type of teamwork. there usually is a ""supervisor'' on the team, who offers support to all members in the collaborative learning environment. however, such support may not always be as accessible as students demand it to be. therefore, this paper describes a cloud-based tool to support software engineering practice courses in collaboration with remote tutors. this system utilizes a cloud storage platform to provide sharing of multimedia study materials and a better management of project developing cycles. a remote collaborative component called the virtual debug laboratory is designed to improve and share students' debugging experience in the same team. the most innovative feature of this system is that it amplifies the role of tutoring in remote collaborative learning environments so that tutors can, in real time, assist students in debugging during actual project developing and demonstrate step by step to the students the process of debugging. the results of the analyzed data regarding the use of this system indicate that the system can potentially enhance students' abilities in project developing and debugging in software engineering practice courses. it is our hope that these preliminary data can provide a future reference for the software education community.
image_processing	wave theories of heating of the chromosphere, corona and solar wind due to photospheric fluctuations are strengthened by the existence of the wave coherency observed up to the transition region. the coherency of intensity oscillations of solar spicules was explored using the solar optical telescope (sot) on the hinode spacecraft with increasing height above the solar limb in the active region. we used time sequences near the south-east region from the hinode/sot for the ca ii h line obtained on 2015 april 3 and applied the de-convolution procedure to the spicule to illustrate how effectively our restoration method works on fine structures such as spicules. moreover, the intensity oscillations at different heights above the solar limb were analysed through wavelet transforms. afterwards, the phase difference was measured between oscillations at two heights in search of evidence for coherent oscillations. the results of the wavelet transformations revealed dominant period peaks for 2, 4, 5.5 and 6.5 min at four separate heights. the dominant frequencies for a coherency level higher than 75 per cent were found to be around 5.5 and 8.5 mhz. mean phase speeds of 155-360 km s (1) were measured. we found that the mean phase speeds increased with height. the results suggest that the energy flux carried by coherent waves into the corona and heliosphere may be several times larger than previous estimates that were based solely on constant velocities. we provide compelling evidence for the existence of upwardly propagating coherent waves.
computer_graphics	nowadays, in romania, centralized district heating is still widely used in many romanian cities. all the heating facilities were made during 1950-1970. the whole system is largely improper due to large amounts of heat leakage, lack of heat and energy efficiency measures and financial shortages. all these limitations are made worst due to the continuous limitation of resources and also due to the high price of the energy. by taking this aspects in consideration, the measurement and the management of the heating agent is a real priority for the power plants and also for the heating distribution companies from romania. the construction of small and efficient cogenerative plants is another major issue for the municipalities. there is a huge trend in the romanian district heating industry to introduce as many as possible it and tc solutions, or excellent measuring capacities. this paper presents human-machine interface, conceived by the authors and applied in the case of the small power plant located in freidorf-timisoara. starting from computer science, computer graphics and thermal engineering, to electronics and electrical engineering, this paper is a true example of a multidisciplinary research.
bioinformatics	premise of the study: dna metabarcoding has broad-ranging applications in ecology, aerobiology, biosecurity, and forensics. a bioinformatics pipeline has recently been published for identification using a comprehensive database of its2, one of the common plant dna barcoding markers. there is, however, no corresponding database for rbcl, the other primary marker used in plants. methods: using publicly available data, we compiled a reference library of rbcl sequences and trained databases for use with utax and rdp classifier algorithms. we used this reference library, along with the existing bioinformatics pipeline and its2 reference library, to identify species in an artificial mixture of nine species of pollen. we have made this database publicly available in multiple formats, to allow use with multiple bioinformatics pipelines, now and in the future. results: using the rbcl database, in addition to the its2 database, we succeeded in making species-level identifications for eight species and a family-level identification of the ninth species. this is an improvement on its2 sequence alone. discussion: the reference library described here will assist with identification of plant species using rbcl. by making another gene region available for standard barcoding, this will increase the resolution and accuracy of identifications.
cryptography	in this paper, homomorphic visual cryptographic scheme (hvcs) is proposed. the proposed hvcs inherits the good features of traditional vcs, such as, loss-tolerant (e.g., (k, n) threshold) and simply reconstructed method, where simply reconstructed method means that the decryption of the secret image is based on human visual system (hvs) without any cryptographic computation. in addition, the proposed hvcs can support signal processing in the encrypted domain (sped), e.g., homomorphic operations and authentication, which can protect the user 's privacy as well as improve the security in some applications, such as, cloud computing and so on. both the theoretical analysis and simulation results demonstrate the effectiveness and security of the proposed hvcs.
data_structures	let be a collection of d string documents of n characters in total, that are drawn from an alphabet set . the top-k document retrieval problem is to preprocess into a data structure that, given a query , can return the k documents of most relevant to the pattern p. the relevance is captured using a predefined ranking function, which depends on the set of occurrences of p in . for example, it can be the term frequency (i.e., the number of occurrences of p in ), or it can be the term proximity (i.e., the distance between the closest pair of occurrences of p in ), or a pattern-independent importance score of such as pagerank. linear space and optimal query time solutions already exist for the general top-k document retrieval problem. compressed and compact space solutions are also known, but only for a few ranking functions such as term frequency and importance. however, space efficient data structures for term proximity based retrieval have been evasive. in this paper we present the first sub-linear space data structure for this relevance function, which uses only o(n) bits on top of any compressed suffix array of and solves queries in time. we also show that scores that consist of a weighted combination of term proximity, term frequency, and document importance, can be handled using twice the space required to represent the text collection.
software_engineering	context a software system 's structure often degrades due to repetitive maintenance. to make a sustainable evolution of such systems, it becomes mandatory to improve their modular structure after a certain time. many remodularization approaches were proposed to improve the modular structure of software systems. most of the existing approaches rely on structural or lexical dependencies. however, there is a lack of research that distinguishes different types of structural (e.g., inheritance, method calls, references, etc.) or lexical (name of classes, methods, variables, etc.) dependencies, but assumes that they are equivalent, which is illogical from a software developer 's point of view. objective: in this paper, we propose an approach that considers various types of structural as well as lexical dependencies along with their relative importance to remodularize the object-oriented (00) systems. the main goal of the paper is to generate remodularization solutions that can reflect the developers' perspective (as visible in the well-modularized software system) of remodularization, which is highly desirable in software evolution. method: the paper computes coupling strength among classes using different weights (computed on basis of well-modularized software system) in terms of various mechanisms of structural and lexical dependencies. software remodularization problem is formulated as a single and multi-objective optimization problem and solved using genetic algorithnis (ga). based on the different types of structural and lexical dependencies and as per their un-weighted/weighted variants, we have designed following 24 coupling schemes: structural-based (i.e., sbuw, sbw, sauw, saw, stfuw, stfw, stfidfuw, and stfidfw), lexical based (i.e., lbuw, lbw, lauw, law, ltfuw, ltfw, ltfidfuw, and ltfidfw), and combined structural lexical based (i.e., slbuw, slbw, slauw, slaw, sltfuw, sltfw, sltfidfuw, and sltfidfw). values obtained through these coupling schemes are used in coupling and cohesion objective function of the ga. along with this objective, some supportive objective functions such as mci and msi have been used to drive the optimization process towards a good quality modularization solution. results: we assess the effectiveness of our proposed remodularization approach over eight real-world object-oriented software systems in terms of original design of the experimented software systems and modularization decisions provided by the developers. results indicate that tfidf based weighted variants (i.e. stfidfw, ltfidfw, and sltfidfw) of each broad three categories outperformed rest of variants within each category. however, tfidf weighted variant in the third broad category (i.e., sltfidfw) outperformed all others. conclusion: our combined lexical-structural approach (sltfidfw) considering various types of dependencies along with their relative weights performs well and results into better remodularization compared to rest of considered alternates. it also shows significant improvement over techniques based on only lexical or structural information. thus this approach can be very useful to improve the quality of the software whose remodularization quality deteriorates beyond accepted level. (c) 2016 elsevier b.v. all rights reserved.
symbolic_computation	in this paper, a (2 + 1)-dimensional nonlinear evolution equation generated via the jaulent-miodek hierarchy is investigated. based on the bell polynomials and hirota method, bilinear forms and backlund transformations are derived. one- and two-soliton solutions are constructed via symbolic computation. soliton solutions are obtained through the backlund transformations. we can get three types by choosing different parameters: the kink, bell-shape, and anti-bell-shape solitons. propagation of the one soliton and elastic interactions between the two solitons are discussed graphically. after the interaction of the two bell-shape or anti-bell-shape solitons, solitonic shapes and amplitudes keep invariant except for some phase shifts, while after the interaction of the kink soliton and anti-bell-shape soliton, the anti-bell-shape soliton turns into a bell-shape one, and the kink soliton keeps its shape, with their amplitudes unchanged.
computer_programming	providing adaptive support to users engaged in learning tasks is the central focus of intelligent tutoring systems. there is evidence that female and male users may benefit differently from adaptive support, yet it is not understood how to most effectively adapt task support to gender. this paper reports on a study with four versions of an intelligent tutoring system for introductory computer programming offering different levels of cognitive (conceptual and problem-solving) and affective (motivational and engagement) support. the results show that female users reported significantly more engagement and less frustration with the affective support system than with other versions. in a human tutorial dialogue condition used for comparison, a consistent difference was observed between females and males. these results suggest the presence of the mars and venus effect, a systematic difference in how female and male users benefit from cognitive and affective adaptive support. the findings point toward design principles to guide the development of gender-adaptive intelligent tutoring systems.
computer_programming	in this paper two types of tensor product finite macro-elements are contrasted, the former being the well known lagrange type and the latter the bezier (bernstein) type elements. although they have a different mathematical origin and seemingly are irrelevant, they both are based on complete polynomials thus sharing the same functional space, i.e. the classes {x(n)} and {y(n)}. therefore, from the theoretical point of view it is anticipated that they should lead to numerically identical results in both static and dynamic analysis. for both types of elements details are provided concerning the main computer programming steps, while selective parts of a typical matlab((r)) code are presented. numerical application includes static (laplace, poisson), eigenvalue (acoustics) and transient (heat conduction) problems of rectangular, circular and elliptic shapes, which were treated as a single macroelement. in agreement to the theory, in all six examples the results obtained using bezier and lagrange polynomials were found to be identical and of exceptional accuracy. (c) 2014 elsevier ltd. all rights reserved.
network_security	various security devices which produce a large volume of logs and alerts have been used widely. it is such a troublesome and time-consuming task for network managers to analyze and deal with the information. this paper presented an improved alerts aggregation method based on grey correlation and attribute similarity method. we used grey correlation to ascertain the importance of alert attributes in network security, and considered it as the weight of attributes. then we combined with the attribute similarity method and calculated the overall feature similarity in order to complete alert aggregation. experiments results showed that this method had a strict mathematical theory basis and a higher practical value, which can effectively reduce raw alerts and reduce redundancy for alert data fusion.
network_security	the determination of network equipment weaknesses and the discovery of intrusion intention is one of the difficulties that troubled network security management personnel. based on previous studies, further proposed a double attack graph based on domain equipment. by the underlying network topology data collected and analyzed, using bayesian theory to complete the quantify for the double attack graph and generation strategy in minimal power key set, with the cost of calculation of key equipment in the automatic recognition network topology, we provide an important basis for network maintenance. experimental results show that the measure of using quantitative domain equipment double attack graph to recognize the intrusion intention is not only effective and feasible, but also has the feature of easy promotion.
symbolic_computation	in this paper, we investigate a (2+ 1)-dimensional bogoyavlenskii-kadontsev-petviashili equation in a fluid, plasma or ferromagnetic thin film. through the bell polynomials, hirota method and symbolic computation, the one-and two-kink-soliton solutions are derived. backlund transformation, lax pair and conservation laws are presented. elastic collisions including the oblique, parallel, unidirectional and bidirectional collisions between the two-kink solitons are discussed. in addition, the relation between the velocities and wave numbers of the two-kink solitons are analysed. when wave numbers b(j) >0, upsilon(jx), the velocities in the x axis, increase with wave numbers a(j) increasing. with b(j) increasing, upsilon(jx) increase when a(j)(4) - 3b(2) j >0, while decrease when a(j)(4) - 3b(j)(2) < 0. upsilon(jy), the velocities in the y axis, increase with a(j) increasing and b(j) decreasing.
bioinformatics	the organization of the mammalian genome into gene subsets corresponding to specific functional classes has provided key tools for systems biology research. here, we have created a web-accessible resource called the mammalian metabolic enzyme database ( https:// hpcwebapps. cit. nih. gov/ esbl/database/metabolicenzymes/metabolicenzymedatabase. html) keyed to the biochemical reactions represented on iconic metabolic pathway wall charts created in the previous century. overall, we have mapped 1,647 genes to these pathways, representing similar to 7 percent of the protein-coding genome. to illustrate the use of the database, we apply it to the area of kidney physiology. in so doing, we have created an additional database (database of metabolic enzymes in kidney tubule segments: https://hpcwebapps. cit. nih. gov/esbl/database/metabolicenzymes/), mapping mrna abundance measurements (mined from rna-seq studies) for all metabolic enzymes to each of 14 renal tubule segments. we carry out bioinformatics analysis of the enzyme expression pattern among renal tubule segments and mine various data sources to identify vasopressin-regulated metabolic enzymes in the renal collecting duct.
computer_graphics	in computer graphics and related fields, bidirectional texture function (btf) is used for realistic and predictive rendering. btf allows for the capture of fine appearance effects such as self-shadowing, inter-reflection and subsurface scattering needed for true realism when used in rendering algorithms. the goal of current research is to get a surface representation indistinguishable from the real world. we developed, produced and tested a portable instrument for btf acquisition based on kaleidoscopic imaging. here we discuss the colour issues we experienced after the initial tests. we show that the same colour balance cannot be applied to the whole picture as the spectral response of the instrument varies with the position in the image. all optical elements were inspected for their contributions to the spectral behaviour of the instrument. the off-the-shelf parts were either measured or the manufacturer 's data were considered. the custom made mirrors' spectral reflectivity was simulated. the mathematical model of the instrument was made. we found a way how to implement all these contributions to the image processing pipeline. in this way, a correct white balance for each individual pixel in the image is found and applied, allowing for a more faithful colour representation. also proposed is an optimized dielectric protective layer for the kaleidoscope 's mirrors.
relational_databases	a multi-craft asteroid survey has significant data synchronization needs. limited communication speeds drive exacting performance requirements. tables have been used in relational databases, which are structure; however, domba (distributed objects management based articulation) deals with data in terms of collections. with this, no read/write roadblocks to the data exist. a master/slave architecture is created by utilizing the gossip protocol. this facilitates expanding a mission that makes an important discovery via the launch of another spacecraft. the open space box framework facilitates the foregoing while also providing a virtual caching layer to make sure that continuously accessed data is available in memory and that, upon closing the data file, recharging is applied to the data.
symbolic_computation	this paper provides a selective eraser of curvature extrema for b-spline curves. it is introduced as an extension to the standard least squares method for approximating a series of points using a b-spline. the extension consists in adding constraints to produce segments of curve with monotone increase or decrease of curvature. the primal-dual interior point method is used to solve the constrained optimization problem. the method requires gradients that are computed using b-spline symbolic operators. therefore, the algorithm relies on the arithmetic and differential properties of b-splines. the variation-diminishing property of b-splines is also exploited to apply the constraints. the application examples consist in producing fair curves from measured points over airfoils. the data come from the publicly available uiuc database. the data contains a fair amount of noise for some airfoils. especially in these circumstances, a fixed number of segments with monotonic variation of curvature hold great promise to produce curves that are, at once, very general and uncompromising over oscillations. (c) 2015 elsevier ltd. all rights reserved.
computer_vision	automated computer vision-based fire detection has gained popularity in recent years, as every fire detection needs to be fast and accurate. in this paper, a new fire detection method using image processing techniques is proposed. we explore how to create a fire flame-based colour space via a linear multiplication of a conversion matrix and colour features of a sample image. we show how the matrix multiplication can result in a differentiating colour space, in which the fire part is highlighted and the non-fire part is dimmed. particle swarm optimization (pso) and sample pixels from an image are used to obtain the weights of the colour-differentiating conversion matrix, and k-medoids provides a fitness metric for the pso procedure. the obtained conversion matrix can be used for fire detection on different fire images without performing the pso procedure. this allows a fast and easy implementable fire detection system. the empirical results indicate that the proposed method provides both qualitatively and quantitatively better results when compared to some of the conventional and state-of-the-art algorithms. (c) 2016 elsevier ltd. all rights reserved.
image_processing	reconstruction of the point-spread function (psf) is a critical process in weak lensing measurement. we develop a real-data based and galaxy-oriented pipeline to compare the performances of various psf reconstruction schemes. making use of a large amount of the cfhtlens data, the performances of three classes of interpolating schemes-polynomial, kriging, and shepard-are evaluated. we find that polynomial interpolations with optimal orders and domains perform the best. we quantify the effect of the residual psf reconstruction error on shear recovery in terms of the multiplicative and additive biases, and their spatial correlations using the shear measurement method of zhang et al. we find that the impact of psf reconstruction uncertainty on the shear-shear correlation can be significantly reduced by cross correlating the shear estimators from different exposures. it takes only 0.2 stars (s/n greater than or similar to >100) per square arcmin on each exposure to reach the best performance of psf interpolation, a requirement that is satisfied in most of the cfhtlens data.
operating_systems	the growing computational power of modern cpus allows increasingly complex signal processing applications to be successfully implemented and executed on general-purpose processors and operating systems. in this regard, the application 's architecture, its design, and operating system integration directly affect the maximal achievable processing bandwidth. in this paper, we present alternative driver architectures for signal processing applications that differ in the distribution of processing stages between kernel space and user space. using the processing of ads-b air traffic radio signals for civil aviation as case study, we evaluate the performance of the design alternatives on a linux system and quantify their strengths and weaknesses with respect to data overhead, usage of vector units, applicable compiler optimizations, and cache behavior. based on our results, we determine the best design choice and derive guidelines for the development of efficient signal processing applications.
computer_graphics	recent research on interactive electronic systems, like computers, can improve the quality of life of many researchers, students, professors, etc. in the case of disabled people, technology helps them to engage more fully into the world. our study aims to evaluate interfaces for curves drawing with movements of the face. this article discusses about motivations to build such software, how the software works, iterative development of the software, and user testing by people with and without disabilities.
data_structures	the article presents an innovative concept of applying graph databases in transport information systems. the model of a graph database has been presented together with implementation of data structures and search operations in a graph. the transformation concept of relational model to a graph data model has been developed. the schema of graph database has been proposed for public transport information system purposes. the realization methods have been illustrated by the use of search function based on the cypher query language.
software_engineering	the relationship between customers and suppliers remains a challenge in agile software development. two trends seek to improve this relationship, the increased focus on value and the move towards continuous deployment. in this special section on continuous value delivery, we describe these emerging research themes and show the increasing interest in these topics over time. further, we discuss implications for future research. (c) 2016 the authors. published by elsevier b.v.
computer_vision	unmanned aerial vehicles have become more widely used for entertainment, security, building inspection and for other similar tasks. inertial navigations systems (ins) is one of the main area of research for uavs to control their flights through buildings or near constructions where flight paths must be controlled or recorded. in this paper is collected some approaches, which can be used for uav onboard trajectory determination where gps cannot be used are determined. approach includes onboard inertial measurement unit system and image sensors. fusing uavs controlling methods and computer vision gives possibility to increase inertial navigation system accuracy. to determine distance to obstacles dual vision cameras must be used. (c) 2017 the authors. published by elsevier b.v.
software_engineering	this work describes a system for the automatic generation of full-fledged api layers from rdf schemas, providing the whole set of object-oriented functionalities to retrieve, store, edit and delete the corresponding data in a semantic triplestore. the layers the system is capable of producing range from an underlying domain model, resulting from the classes, data properties and object properties of the input schema, to the related lower-level data source and access components, up to higher-level facades and web service interfaces, all of which are immediately operational and can be used out-of-the-box for development purposes either as stand-alone components or integrated into external applications. a user-friendly graphical interface allows for an easy configuration and customization of the generation process to suit specific development needs. once configured, the execution of the generation process takes place almost instantaneously, bringing about a full set of api components in a matter of seconds and thus dramatically saving design and development time and effort. experimentation of the system has been carried out within the context of a eu-funded research project featuring a large semantic schema, a significant portion of which represented a learning model specifically engineered to be used for a plethora of e-learning solutions; nevertheless, the system is generic enough to be employed for a variety of applications relying upon semantic schemas and data.
machine_learning	rice (oryza sativa) is one of the most important staple foods for more than half of the global population. many rice traits are quantitative, complex and controlled by multiple interacting genes. thus, a full understanding of genetic relationships will be critical to systematically identify genes controlling agronomic traits. we developed a genome-wide rice protein-protein interaction network (riceppinet, ) using machine learning with structural relationship and functional information. riceppinet contained 708819 predicted interactions for 16895 non-transposable element related proteins. the power of the network for discovering novel protein interactions was demonstrated through comparison with other publicly available protein-protein interaction (ppi) prediction methods, and by experimentally determined ppi data sets. furthermore, global analysis of domain-mediated interactions revealed riceppinet accurately reflects ppis at the domain level. our studies showed the efficiency of the riceppinet-based method in prioritizing candidate genes involved in complex agronomic traits, such as disease resistance and drought tolerance, was approximately 2-11 times better than random prediction. riceppinet provides an expanded landscape of computational interactome for the genetic dissection of agronomically important traits in rice. significance statement a genome-wide rice protein-protein interaction network is developed by using machine-learning with structural evidence and functional information. it provides an expanded landscape of protein interactome to help plant biologists better understand complex agronomic traits in rice.
structured_storage	to achieve energy-conservation and prompt responses simultaneously, in this paper we propose a novel energy-saving data placement strategy, called striping-based energy-aware (sea), which can be applied to raid-structured storage systems to noticeably save energy while providing quick responses. further, to illustrate the effectiveness of sea, we implement two sea-powered raid-based data placement algorithms, sea0 and sea5, by incorporating the sea strategy into raid-0 and raid-5, respectively. extensive experimental results demonstrate that compared with three well-known data placement algorithms greedy, sp, and rp, sea0 and sea5 reduce mean response time on average at least 52.15% and 48.04% while saving energy on average no less than 10.12% and 9.35%, respectively.
machine_learning	multi-label learning draws great interests in many real world applications. it is a highly costly task to assign many labels by the oracle for one instance. meanwhile, it is also hard to build a good model without diagnosing discriminative labels. can we reduce the label costs and improve the ability to train a good model for multi-label learning simultaneously? active learning addresses the less training samples problem by querying the most valuable samples to achieve a better performance with little costs. in multi-label active learning, some researches have been done for querying the relevant labels with less training samples or querying all labels without diagnosing the discriminative information. they all cannot effectively handle the outlier labels for the measurement of uncertainty. since maximum correntropy criterion (mcc) provides a robust analysis for outliers in many machine learning and data mining algorithms, in this paper, we derive a robust multi-label active learning algorithm based on an mcc by merging uncertainty and representativeness, and propose an efficient alternating optimization method to solve it. with mcc, our method can eliminate the influence of outlier labels that are not discriminative to measure the uncertainty. to make further improvement on the ability of information measurement, we merge uncertainty and representativeness with the prediction labels of unknown data. it cannot only enhance the uncertainty but also improve the similarity measurement of multi-label data with labels information. experiments on benchmark multi-label data sets have shown a superior performance than the state-of-the- art methods.
machine_learning	background: brain networks in fmri are typically identified using spatial independent component analysis (ica), yet other mathematical constraints provide alternate biologically-plausible frameworks for generating brain networks. non-negative matrix factorization (nmf) would suppress negative bold signal by enforcing positivity. spatial sparse coding algorithms (l1 regularized learning and k-svd) would impose local specialization and a discouragement of multitasking, where the total observed activity in a single voxel originates from a restricted number of possible brain networks. new method: the assumptions of independence, positivity, and sparsity to encode task-related brain networks are compared; the resulting brain networks within scan for different constraints are used as basis functions to encode observed functional activity. these encodings are then decoded using machine learning, by using the time series weights to predict within scan whether a subject is viewing a video, listening to an audio cue, or at rest, in 304 fmri scans from 51 subjects. results and comparison with existing method: the sparse coding algorithm of l1 regularized learning outperformed 4 variations of ica (p < 0.001) for predicting the task being performed within each scan using artifact-cleaned components. the nmf algorithms, which suppressed negative bold signal, had the poorest accuracy compared to the ica and sparse coding algorithms. holding constant the effect of the extraction algorithm, encodings using sparser spatial networks (containing more zero-valued voxels) had higher classification accuracy (p < 0.001). lower classification accuracy occurred when the extracted spatial maps contained more csf regions (p < 0.001). conclusion: the success of sparse coding algorithms suggests that algorithms which enforce sparsity, discourage multitasking, and promote local specialization may capture better the underlying source processes than those which allow inexhaustible local processes such as ica. negative bold signal may capture task-related activations. (c) 2017 elsevier b.v. all rights reserved.
parallel_computing	the audio-to-score framework consists of two separate stages: preprocessing and alignment. the alignment is commonly solved through offline dynamic time warping (dtw), which is a method to find the path over the distortion matrix with the minimum cost to determine the relation between the performance and the musical score times. in this work we propose a parallel online dtw solution based on a client-server architecture. the current version of the application has been implemented for multi-core architectures (86, 64 and arm), thus covering either powerful systems or mobile devices. an extensive experimentation has been conducted to validate the software. the experiments also show that our framework allows to achieve a good score alignment within the real-time window using parallel computing techniques.
machine_learning	in this paper, we propose a novel fuzzy inference system on picture fuzzy set called picture inference system (pis) to enhance inference performance of the traditional fuzzy inference system. in pis, the positive, neutral and negative degrees of the picture fuzzy set are computed using the membership graph that is the combination of three gaussian functions with a common center and different widths expressing a visual view of degrees. then, the positive and negative defuzzification values, synthesized from three degrees of the picture fuzzy set, are used to generate crisp outputs. learning in pis including training centers, widths, scales and defuzzification parameters is also discussed. the system is adapted for all architectures such as the mamdani, the sugeno and the tsukamoto fuzzy inferences. experimental results on benchmark uci machine learning repository datasets and an example in control theory - the lorenz system are examined to verify the advantages of pis.
data_structures	the internet, as a global system of interconnected networks, carries an extensive array of information resources and services. key requirements include good quality-of-service and protection of the infrastructure from nefarious activity [e.g., distributed denial of service (ddos) attacks]. network monitoring is essential to network engineering, capacity planning, and prevention/mitigation of threats. we develop an open-source architecture, all-packet monitor (amon), for online monitoring and analysis of multi-gigabit network streams. it leverages the high-performance packet monitor pf_ring and is readily deployable on commodity hardware. amon examines all packets, partitions traffic into sub-streams by using rapid hashing and computes certain real-time data products. the resulting data structures provide views of the intensity and connectivity structure of network traffic at the time-scale of routing. the proposed integrated framework includes modules for the identification of heavy-hitters as well as for visualization and statistical detection at the time-of-onset of high-impact events such as ddos. this allows operators to quickly visualize and diagnose attacks, and limit offline and time-consuming post-mortem analysis. we demonstrate our system in the context of real-world attack incidents, and validate it against state-of-the-art alternatives. amon has been deployed and is currently processing multi-gigabit live internet traffic at merit network. it is extensible and allows the addition of further statistical and filtering modules for real-time forensics.
image_processing	proposed is a smart single viewing axis optical laser line illumination-based 3-d shape sensor that uses an electronically controlled variable focus lens (ecvfl) for 3-d optical beamforming. specifically, the novel sensor design deploys laser line illumination scanning and camera-based spatial image processing of the target illuminated laser line spot that leads to a faster 3-d shape reconstruction versus the previously demonstrated point scanned ecvfl-based 3-d shape sensor. the proposed sensor is experimentally demonstrated using a liquid ecvfl and transverse direction motion mechanics to successfully 3-d map 40-mm depth steep surface profile objects with a mean measurement error of <5%. the line illumination sensor demonstrates a 53 times shorter 3-d mapping time per scanned point when compared with the point-scan-based sensor. the proposed 3-d shape sensor is suitable for scenarios where high transverse resolution inspections is required of objects having holes, crevices, or other complex external structures.
operating_systems	modern operating systems use hardware support to protect against control-flow hijacking attacks such as code-injection attacks. typically, write access to executable pages is prevented and kernel mode execution is restricted to kernel code pages only. however, current cpus provide no protection against code-reuse attacks like rop. aslr is used to prevent these attacks by making all addresses unpredictable for an attacker. hence, the kernel security relies fundamentally on preventing access to address information. we introduce prefetch side-channel attacks, a new class of generic attacks exploiting major weaknesses in prefetch instructions. this allows unprivileged attackers to obtain address information and thus compromise the entire system by defeating smap, smep, and kernel aslr. prefetch can fetch inaccessible privileged memory into various caches on intel x86. it also leaks the translation-level for virtual addresses on both intel x86 and armv8-a. we build three attacks exploiting these properties. our first attack retrieves an exact image of the full paging hierarchy of a process, defeating both user space and kernel space aslr. our second attack resolves virtual to physical addresses to bypass smap on 64-bit linux systems, enabling ret2dir attacks. we demonstrate this from unprivileged user programs on linux and inside amazon ec2 virtual machines. finally, we demonstrate how to defeat kernel aslr on windows 10, enabling rop attacks on kernel and driver binary code. we propose a new form of strong kernel isolation to protect commodity systems incuring an overhead of only 0. 06-5.09%.
operating_systems	ip identification (ipid) is an ip header field which is designed to identify a packet in a communication session. the main purpose of ipid is to recover from ip fragmentation. to the best of our knowledge, most existing ipid based information hiding methods assume that the ipid number is a pseudo random number, which is found to be false. in this paper, we propose a steganographic method by exploiting the ipid field while considering the information from the user data field. first, we analyze the ipid distribution of various operating systems. subsequently, we put forward a simple data embedding method, which is then refined to mimic the ordinary ipid traffic. experiments are carried out and the results empirically prove that the proposed method is of high undetectability as compared to the existing ipid based steganographic methods. (c) 2016 elsevier ltd. all rights reserved.
cryptography	lightweight cipher designs try to minimize the implementation complexity of the cipher while maintaining some specified security level. using only a small number of and gates lowers the implementation costs, and enables easier protections against side-channel attacks. in our paper we study the connection between the number of and gates (multiplicative complexity) and the complexity of algebraic attacks. we model the encryption with multiple right-hand sides (mrhs) equations. the resulting equation system is transformed into a syndrome decoding problem. the complexity of the decoding problem depends on the number of and gates, and on the relative number of known output bits with respect to the number of unknown key bits. this allows us to apply results from coding theory, and to explicitly connect the complexity of the algebraic cryptanalysis to the multiplicative complexity of the cipher. this means that we can provide asymptotic upper bounds on the complexity of algebraic attacks on selected families of ciphers based on the hardness of the decoding problem.
relational_databases	the semantic web uses ontological descriptions, in particularly web ontology language owl, as a universal medium to formally describe and exchange knowledge of various domains. currently, many owl ontologies for different domains come into being successively. therefore, how to store owl ontologies becomes one of ordinary needs of the semantic web. based on the efficient storage mechanism of object-oriented databases, they may be used to store owl ontologies for realizing the management of large amounts of knowledge in the semantic web. to this end, the main objective of this paper is to investigate how to store owl ontologies in object-oriented databases, and we propose a formal approach and develop a prototype tool for storing owl ontologies in object-oriented databases. firstly, after giving a complete formal definition of owl ontologies, we propose an overall architecture of storing owl ontologies in object-oriented databases. based on the architecture, we further give storage rules and explain how to store owl ontologies in object-oriented databases with a running example in detail. the correctness and quality of the storage approach are proved and analyzed. finally, we implement a prototype tool which can store owl ontologies in a widely used open source object database db4o. also, a query interface is developed in the prototype tool for querying the stored owl ontologies. the storage and query examples are provided to show that the approach is feasible and the tool is efficient. (c) 2014 elsevier b.v. all rights reserved.
symbolic_computation	two classes of rational solutions to a shallow water wave-like non-linear differential equation are constructed. the basic object is a generalized bilinear differential equation based on a prime number, p = 3. through this new transformation and with the help of symbolic computation with maple, both the new equation and its rational solutions are obtained.
algorithm_design	two players wishing to communicate are placed each in a room with n telephones connecting the two rooms. the players do not know how the telephones are interconnected. in each round, each player picks up a phone and says ""hello"" until when they hear each other. the problem is to devise an algorithm minimising the delay to establish communication. the above problem, called the telephone coordination game, also termed as the telephone problem, is of fundamental importance in distributed algorithm design. in this paper, we investigate a generalised version where among n telephones, only a subset can establish communication between the two players. we are interested in devising the deterministic strategy achieving bounded rendezvous delay and minimising the worst-case rendezvous delay. specifically, we first establish the lower-bound of worst-case rendezvous delay. we then characterise the structure of the phone pick sequences that can guarantee rendezvous without any prior coordination. assuming each player has a globally unique id, we further devise a deterministic strategy that (1) guarantees rendezvous between the players regardless of their telephone labeling functions and their relative time difference and (2) approaches the performance bound within a constant factor proportional to the id length.
distributed_computing	on one hand, compared with traditional relational and xml models, graphs have more expressive power and are widely used today. on the other hand, various applications of social computing trigger the pressing need of a new search paradigm. in this article, we argue that big graph search is the one filling this gap. we first introduce the application of graph search in various scenarios. we then formalize the graph search problem, and give an analysis of graph search from an evolutionary point of view, followed by the evidences from both the industry and academia. after that, we analyze the difficulties and challenges of big graph search. finally, we present three classes of techniques towards big graph search: query techniques, data techniques and distributed computing techniques.
computer_programming	the purpose of this study is to examine the effectiveness of review question and content object as advanced organizer used for prior knowledge activation in an introductory computer programming. the students' engagement when using the strategies was examined to reach the primary findings. content object (co) as the advanced organizer to activate prior knowledge used before a new programming concept was learnt. review questions (rq) on programming concepts and solutions were designed to encourage the paper-pen method. findings have shown similar performance in post-test. the outcome of this study showed co useful to foster better learning programming. (c) 2015 published by elsevier ltd.
machine_learning	skilled human full-body movements are often planned in a highly predictive manner. for example, during walking while reaching towards a goal object, steps and body postures are adapted to the goal position already multiple steps before the goal contact. the realization of such highly predictive behaviors for humanoid robots is a challenge because standard approaches, such as optimal control, result in computation times that are prohibitive for the predictive control of complex coordinated full body movements over multiple steps. we devised a new architecture that combines the online-planning of complex coordinated full-body movements, based on the flexible combination of learned dynamic movement primitives, with a walking pattern generator (wpg), based on model predictive control (mpc), which generates dynamically feasible locomotion of the humanoid robot hrp-2. a dynamic filter corrects the zero moment point (zmp) trajectories in order to guarantee the dynamic feasibility of the executed behavior taking into account the upper-body movements, at the same time ensuring an accurate approximation of the planned motion trajectories. we demonstrate the high flexibility of the chosen movement planning approach, and the accuracy and feasibility of the generated motion. in addition, we show that a na ve approach, which generates adaptive motion by using machine learning methods by the interpolation between feasible training motion examples fails to guarantee the stability and dynamic feasibility of the generated behaviors. (c) 2017 elsevier b.v. all rights reserved.
distributed_computing	wireless technologies combined with advanced computing are changing industrial communications. industrial wireless networks can improve the monitoring and the control of the entire system by jointly exploiting massively interacting communication and distributed computing paradigms. in this paper, we develop a wireless cloud platform for supporting critical data publishing and distributed sensing of the surrounding environment. the cloud system is designed as a self-contained network that interacts with devices exploiting the time synchronized channel hopping protocol (tsch), supported by wirelesshart (iec 62591). the cloud platform augments industry-standard networking functions as it handles the delivery (or publishing) of latency and throughput-critical data by implementing a cooperative-multihop forwarding scheme. in addition, it supports distributed sensing functions through consensus-based algorithms. experimental activities are presented to show the feasibility of the approach in two real industrial plant sites representative of typical indoor and outdoor environments. validation of cooperative forwarding schemes shows substantial improvements compared with standard industrial solutions. distributed sensing functions are developed to enable the autonomous identification of recurring cochannel interference patterns.
algorithm_design	this article examines the effectiveness of different forms of performance-based adaptive automation (pbaa). using data from three experiments (n = 10, n = 38, n = 40), different models of algorithm design were compared for their effectiveness in driving pbaa. the following components were varied: type of task (i.e. primary or secondary tasks), baseline of performance data (e.g. moving average), and triggering criterion (i.e. level of deviation from standard performance). the data were generated by operators working with a computer-based simulation of a process control environment. the results showed that none of the models enjoyed a convincing level of effectiveness. the automation algorithms generally achieved higher levels of miss prevention than false alarm prevention. surprisingly, primary task performance was generally better at driving pbaa than secondary task performance. the results suggest that it may be difficult to design an effective algorithm of pbaa if the work environment is highly complex.
machine_learning	background and objective: various digital pathology tools have been developed to aid in analyzing tissues and improving cancer pathology. the multi-resolution nature of cancer pathology, however, has not been fully analyzed and utilized. here, we develop an automated, cooperative, and multi-resolution method for improving prostate cancer diagnosis. methods: digitized tissue specimen images are obtained from 5 tissue microarrays (tmas). the tmas include 70 benign and 135 cancer samples (tma1), 74 benign and 89 cancer samples (tma2), 70 benign and 115 cancer samples (tma3), 79 benign and 82 cancer samples (tma4), and 72 benign and 86 cancer samples (tma5). the tissue specimen images are segmented using intensity- and texture-based features. using the segmentation results, a number of morphological features from lumens and epithelial nuclei are computed to characterize tissues at different resolutions. applying a multiview boosting algorithm, tissue characteristics, obtained from differing resolutions, are cooperatively combined to achieve accurate cancer detection. results: in segmenting prostate tissues, the multiview boosting method achieved >= 0.97 auc using tma1. for detecting cancers, the multiview boosting method achieved an auc of 0.98 (95% ci: 0.97-0.99) as trained on tma2 and tested on tma3, tma4, and tma5. the proposed method was superior to single view approaches, utilizing features from a single resolution or merging features from all the resolutions. moreover, the performance of the proposed method was insensitive to the choice of the training dataset. trained on tma3, tma4, and tma5, the proposed method obtained an auc of 0.97 (95% ci: 0.96-0.98), 0.98 (95% ci: 0.96-0.99), and 0.97 (95% ci: 0.96-0.98), respectively. conclusions: the multiview boosting method is capable of integrating information from multiple resolutions in an effective and efficient fashion and identifying cancers with high accuracy. the multiview boosting method holds a great potential for improving digital pathology tools and research. (c) 2017 elsevier b.v. all rights reserved.
data_structures	purpose-additive manufacturing (am) processes are the integration of many different science and engineering-related disciplines, such as material metrology, design, process planning, in-situ and off-line measurements and controls. major integration challenges arise because of the increasing complexity of am systems and a lack of support among vendors for interoperability. the result is that data cannot be readily shared among the components of that system. in an attempt to better homogenization this data, this paper aims to provide a reference model for data sharing of the activities to be under-taken in the am process, laser-based powder bed fusion (pbf). design/methodology/approach-the activity model identifies requirements for developing a process data model. the authors' approach begins by formally decomposing the pbf processes using an activity-modeling methodology. the resulting activity model is a means to structure process-related pbf data and align that data with specific pbf sub-processes. findings-this model in this paper provides the means to understand the organization of process activities and sub-activities and the flows among them in am pbf processes. research limitations/implications-the model is for modeling am activities and data associated with these activity. data modeling is not included in this work. social implications-after modeling the selected pbf process and its sub-processes as activities, the authors discuss requirements for developing the development of more advanced process data models. such models will provide a common terminology and new process knowledge that improve data management from various stages in am. originality/value-fundamental challenges in sharing/reusing data among heterogeneous systems include the lack of common data structures, vocabulary management systems and data interoperability methods. in this paper, the authors investigate these challenges specifically as they relate to process information for pbf-how it is captured, represented, stored and accessed. to do this, they focus on using methodical, information-modeling techniques in the context of design, process planning, fabrication, inspection and quality control.
parallel_computing	computing performance is one of the key problems in embedded systems for high-resolution face detection applications. to improve the computing performance of embedded high-resolution face detection systems, a novel parallel implementation of embedded face detection system was established based on a low power cpu-accelerator heterogeneous many-core architecture. first, a basic cpu version of face detection prototype was implemented based on the cascade classifier and local binary patterns operator. second, the prototype was extended to a specified embedded parallel computing platform that is called parallella and consists of xilinx zynq and adapteva epiphany. third, the face detection algorithm was optimized to adapt to the parallella architecture to improve the detection speed and the utilization of computing resources. finally, a face detection experiment was conducted to evaluate the computing performance of the proposal in this paper. the experimental results show that the proposed implementation obtained a very consistent accuracy as that of the dual-core arm, and achieved 7.8 times speedup than that of the dual-core arm. experiment results prove that the proposed implementation has significant advantages on computing performance.
parallel_computing	digital down converter (ddc) is a time-intensive and data-intensive computing task and considered as the key technology in software defined radio. this paper proposes a high-performance implementation of ddc on a graphics processing unit (gpu) using cuda, which is composed of a numerically controlled oscillator stage, a cascaded integrator-comb (cic) decimation filter stage, and a finite impulse response (fir) filter stage. the gpu implementation and optimizing of all the stages are studied in detail. additionally, for handling a long-duration signal, the signal data sequence is truncated into segments; the overlap-save and overlap-add mechanisms were applied in cic stage and fir stage, respectively. finally, experiments were conducted to evaluate the performance of gpu-based ddc with respect to a sequential version cpu implementation and an openmp implementation (16 threads). experimental results demonstrate that the ddc achieves significant improvements on the gpu; the maximum speed ups in numerically controlled oscillator stage, cic stage, and fir stage can achieve more than 1242, 527, and 179 times, including data-transfer, kernel execution, and other processing operations; the overall speed up of ddc can achieve more than 180. in the meantime, the speed ups of gpu implementation are far above the openmp implementation (about 2.5-6.4 times).
symbolic_computation	in this paper, the truncated painlev'e analysis and the consistent tanh expansion (cte) method are developed for the (2+1)-dimensional breaking soliton equation. as a result, the soliton-cnoidal wave interaction solution of the equation is explicitly given, which is difficult to be found by other traditional methods. when the value of the jacobi elliptic function modulus m = 1, the soliton-cnoidal wave interaction solution reduces back to the two-soliton solution. the method can also be extended to other types of nonlinear evolution equations in mathematical physics.
computer_graphics	in the oil and gas industry, processing and visualizing 3d models is of paramount importance for making exploratory and production decisions. hydrocarbons reservoirs are entities buried deep in the earth 's crust, and a simplified 3d geological model that mimics this environment is generated to run simulations and help understand geological and physical concepts. for the task of visually inspecting these models, we advocate the use of cutaways: an illustrative technique to emphasize important structures or parts of the model by selectively discarding occluding parts, while keeping the contextual information. however, the complexity of reservoir models imposes severe restrictions and limitations when using generic illustrative techniques previously proposed by the computer graphics community. to overcome this challenge, we propose an interactive cutaway method, strongly relying on screen-space gpu techniques, specially designed for inspecting 3d reservoir models represented as corner-point grids, the industry 's standard. (c) 2016 elsevier inc. all rights reserved.
distributed_computing	performance modeling for mapreduce applications with large-scale data is a very important issue in the study of optimization, evaluation, prediction and resource scheduling of the jobs over big data and cloud computing platforms. in this paper, we study the hadoop distributed computing framework, which is the current trend of big data solutions. we use the locally weighted linear regression (lwlr) algorithm and linear regression (lr) algorithm to establish three kinds of computing models based on different characteristics to estimate the execution time of the applications that have large-scale data and run on the hadoop framework, and at the same time we make comparison and improvement to the three models. by building different types of experimental environments, and running different types of jobs, we can draw a conclusion that all the three models have very good results in predicting the execution time and evaluating the performance of large-scale data applications with small-scale data.
network_security	attempting to educate practitioners of computer security can be difficult if for no other reason than the breadth of knowledge required today. the security profession includes widely diverse subfields including cryptography, network architectures, programming, programming languages, design, coding practices, software testing, pattern recognition, economic analysis, and even human psychology. while an individual may choose to specialize in one of these more narrow elements, there is a pressing need for practitioners that have a solid understanding of the unifying principles of the whole. we created the playground network simulation tool and used it in the instruction of a network security course to graduate students. this tool was created for three specific purposes. first, it provides simulation sufficiently powerful to permit rigorous study of desired principles while simultaneously reducing or eliminating unnecessary and distracting complexities. second, it permitted the students to rapidly prototype a suite of security protocols and mechanisms. finally, with equal rapidity, the students were able to develop attacks against the protocols that they themselves had created. based on our own observations and student reviews, we believe that these three features combine to create a powerful pedagogical tool that provides students with a significant amount of breadth and intense emotional connection to computer security in a single semester.
data_structures	this work presents a low-complexity audio coder-decoder (codec) based on fixed-point arithmetic to save the usage of system resources in a low-end embedded system. to reduce time complexity and memory usage, a simplified discrete wavelet transform (dwt) with only integer operations is developed. the simplified dwt reduces the computation that is required for the trigonometric operations in the fixed-point system. moreover, separating the even and odd orders of wavelet coefficients halves the computation time of the original dwt. additionally, a tri-mode zeroes recording algorithm (tzra) is proposed. the proposed tzra utilizes different encoding modes with their corresponding bit data structures to record the locations of zero wavelet coefficients, and determines the optimal encoding mode that uses the fewest bits. to further improve the compression performance, a dynamic wavelet level selecting algorithm is developed to decide the wavelet level for compressing each of frame. this algorithm can dynamically select the optimal wavelet level with the fewest bits by using the determined mode from the tzra. the experimental results herein reveal that the encoder in the proposed codec can multiply computation by factors of 164 and 112 and reduce the size of the execution file by approximately 95.61 and 98.42% relative to baseline audio compression codecs. such results indicate that the efficiency when used in real consumer products with fixed-point arithmetic.
bioinformatics	eva1a is an autophagy-related protein, which plays an important role in embryonic neurogenesis. in this study, we found that loss of eva1a could decrease neural differentiation in the brain of adult eva1a(-/-) mice. to determine the mechanism underlying this phenotype, we performed label-free quantitative proteomics and bioinformatics analysis using the brains of eva1a(-/-) and wild-type mice. we identified 11 proteins that were up-regulated and 17 that were down-regulated in the brains of the knockout mice compared to the wild-type counterparts. bioinformatics analysis indicated that biological processes, including atp synthesis, oxidative phosphorylation, and the tca cycle, are involved in the eva1a regulatory network. in addition, gene set enrichment analysis showed that neurodegenerative diseases, such as alzheimer 's disease and huntington 's disease, were strongly associated with eva1a knockout. western blot experiments showed changes in the expression of nicotinamide nucleotide transhydrogenase, an important mitochondrial enzyme involved in the tca cycle, in the brains of eva1a knockout mice. our study provides valuable information on the molecular functions and regulatory network of the eva1a gene, as well as new perspectives on the relationship between autography-related proteins and neural differentiation.
distributed_computing	due to technical bottlenecks and errors caused by artificial operation, the problem of incomplete data always exists in big data research. traditional data imputation algorithms incur high complexity and the accuracy cannot reach the desired level. at the same time, analysis and computation involved in mass data makes limitation of traditional algorithms and computing platform more noticeable. in this paper, we propose a data imputation method based on apriori algorithm, and implement the corresponding algorithm on the distributed computing system built with spark, the experimental results show that the proposed algorithm outperforms a traditional data imputation algorithm in terms of efficiency and accuracy.
parallel_computing	the robust conjugate direction search (rcds) method is used to optimize the collimation system for the rapid cycling synchrotron (rcs) of the china spallation neutron source (csns). the parameters of secondary collimators are optimized for a better performance of the collimation system. to improve the efficiency of the optimization, the objective ring beam injection and tracking (orbit) parallel module combined with matlab parallel computing is used, which can run multiple orbit instances simultaneously. this study presents a way to find an optimal parameter combination of the secondary collimators for a machine model in preparation for csns/rcs commissioning.
image_processing	the contact structure of asphalt mixtures has been considered as an important micromechanical mixture property related to the rutting performance. in this study, a two-dimensional (2d) image acquisition and processing procedure was utilized to acquire the contact structure of asphalt mixtures with varying compactness, gradation types, nominal maximum aggregate sizes (nmas) and binder types. new indices for contact structure of different asphalt mixtures were developed based on three aspects: contact distance distribution, contact length distribution and contact orientation, including average contact distance (d(ave)), proportion for contact distance no more than 0.5 mm (p-d <= 0.5mm), proportion for contact distance no more than 1 mm (p-d <= 1mm), total contact length (l-sum), number of contact (no. contact), average contact length (l-ave), the average contact angle of inclination 0 and the vector magnitude delta. flow number and strain rate from dynamic creep test were determined to be the rutting indicators for varying asphalt mixtures. a linear regression model was used to investigate the relationship between new contact indices and rutting indicators. the results indicated that the contact structure of varying mixtures could be successfully differed by the new indices proposed. new indices, such as l-sum, l-ave and d(ave), could be ranked in a similar trend as the flow number. furthermore, a more comprehensive index represented the contact structure of asphalt mixtures was developed to better uncover the mechanism of rutting in micromechanical perspective. (c) 2016 published by elsevier ltd.
relational_databases	in many institutions relational databases are used as a tool for managing information related to day to day activities. institutions may be required to keep the information stored in relational databases accessible because of many reasons including legal requirements and institutional policies. however, the evolution in technology and change in users with the passage of time put the information stored in relational databases in danger. in the long term the information may become inaccessible when the operating system, database management system or the application software is not available any more or the contextual information not stored in the database may be lost thus affecting the authenticity and understandability of the information. this paper presents an approach for preserving relational databases for the long-term. the proposal involves migrating a relational database to a dimensional model which is simple to understand and easy to write queries against. practical transformation rules are developed by carrying out multiple case studies. one of the case studies is presented as a running example in the paper. systematic implementation of the rules ensures no loss of information in the process except for the unwanted details. the database preserved using the approach is converted to an open format but may be reloaded to a database management system in the long-term.
data_structures	hashing is an important technique to achieve high code performance in a variety of data processing applications. the concept is emphasized in computer science curricula, and the it industry values graduates who can use hashing skillfully. although several implementation details of hashing are routinely handled by software libraries, it remains the responsibility of the programmer to choose a suitable hash function (a poor hash function can degrade performance). researchers have designed a number of generic hash functions, but environments for code development do not offer these as off-the-shelf solutions. it is also curious that textbooks and reference books do not point learners to this rich resource. our paper remedies this deficiency by providing learners with an easy way to compare their own hash function 's performance with these alternatives. our assistive tool is an eclipse plugin for java programs, and we have focused only on traditional hash functions (non-cryptographic hash functions that do not preserve distance). however, our approach can be extended easily to other programming languages, development environments and hash function classes.
symbolic_computation	in this paper is presented a criterion for identification of heat transfer regime through convection ( natural, forced or mixed) by making use of the mathematica system symbolic computation capabilities. the criterion is based on a comparison of buoyancy and viscous forces. the analysis is realized at the interior of two vertical plates submitted to uniform heat flux density, in steady and laminar state in a fully developed flow. thus, it was proposed the dimensionless numbers product rire to be the only representative parameter in order to identify the convective thermal regime, instead of the idea based only on the dimensionless number richardson or grashof. (c) 2015 the authors. published by elsevier ltd.
bioinformatics	cancer transcriptome analysis is one of the leading areas of big data science, biomarker, and pharmaceutical discovery, not to forget personalized medicine. yet, cancer transcriptomics and postgenomic medicine require innovation in bioinformatics as well as comparison of the performance of available algorithms. in this data analytics context, the value of network generation and algorithms has been widely underscored for addressing the salient questions in cancer pathogenesis. analysis of cancer trancriptome often results in complicated networks where identification of network modularity remains critical, for example, in delineating the druggable molecular targets. network clustering is useful, but depends on the network topology in and of itself. notably, the performance of different network-generating tools for network cluster (nc) identification has been little investigated to date. hence, using gastric cancer (gc) transcriptomic datasets, we compared two algorithms for generating pathway versus gene regulatory network-based ncs, showing that the pathway-based approach better agrees with a reference set of cancer-functional contexts. finally, by applying pathway-based nc identification to gc transcriptome datasets, we describe cancer ncs that associate with candidate therapeutic targets and biomarkers in gc. these observations collectively inform future research on cancer transcriptomics, drug discovery, and rational development of new analysis tools for optimal harnessing of omics data.
cryptography	cooperative spectrum sensing, despite its effectiveness in enabling dynamic spectrum access, suffers from location privacy threats, merely because secondary users (sus)' sensing reports that need to be shared with a fusion center to make spectrum availability decisions are highly correlated to the users' locations. it is therefore important that cooperative spectrum sensing schemes be empowered with privacy preserving capabilities so as to provide sus with incentives for participating in the sensing task. in this paper, we propose privacy preserving protocols that make use of various cryptographic mechanisms to preserve the location privacy of sus while performing reliable and efficient spectrum sensing. we also present cost-performance tradeoffs. the first consists on using an additional architectural entity at the benefit of incurring lower computation overhead by relying only on symmetric cryptography. the second consists on using an additional secure comparison protocol at the benefit of incurring lesser architectural cost by not requiring extra entities. our schemes can also adapt to the case of a malicious fusion center as we discuss in this paper. we also show that not only are our proposed schemes secure and more efficient than existing alternatives, but also achieve fault tolerance and are robust against sporadic network topological changes.
network_security	with the tremendous growth of internet, large amounts of data are generated and create big challenges for nowadays computing technologies and systems. however, on the other hand, it also sheds new light on the areas of data analytics and mining which enables uncovering the patterns and laws beneath the big data. in recent years, big data analytics have been successfully applied to many areas, such as e-commerce, healthcare, and industry. as the same time, security analytics based on big data also receive great attention from both academic and industry. in this paper, we give a comprehensive sketch of techniques about the applications of big data in network security analytics. the existing research works are classified into three types: supervised, unsupervised and hybrid approaches. then we elaborate the technical issues of the three kinds of approaches and compare their advantages and disadvantages. finally we outlook the potentials and research directions in the future.
computer_vision	saliency detection is the task of locating informative regions in an image, which is a challenging task in computer vision. in contrast to the existing saliency detection models that focus on either local or global image property, an effective salient object detection method is introduced based on joint modeling global shape and local consistency. to this end, restricted boltzmann machine (rbm) is utilized to model salient object shape as global image property and conditional random field (crf), on the other hand, is adopted to achieve its local consistency. in order to obtain the final saliency map, a universal framework is introduced to combine the results of rbm and crf. experimental results on five benchmark datasets demonstrate that the proposed saliency detection method performs favorably against the existing state-of-the-art algorithms.
computer_vision	automatic image annotation has been an active topic of research in the field of computer vision and pattern recognition for decades. in this paper, we present a new method for automatic image annotation based on gaussian mixture model (gmm) considering cross-modal correlations. to be specific, we first employ gmm fitted by the rival penalized expectation-maximization (rpem) algorithm to estimate the posterior probabilities of each annotation keyword. next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity by seamlessly integrating the information from both image low level visual features and high level semantic concepts together, which can effectively avoid the phenomenon that different images with the same candidate annotations would obtain the same refinement results. followed by the rank-two relaxation heuristics over the built label similarity graph is applied to further mine the correlation of the candidate annotations so as to capture the refining annotation results, which plays a crucial role in the semantic based image retrieval. the main contributions of this work can be summarized as follows: (1) exploiting gmm that is trained by the rpem algorithm to capture the initial semantic annotations of images. (2) the label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels. (3) refining the candidate set of annotations generated by the gmm through solving the max-bisection based on the rank-two relaxation algorithm over the weighted label graph. compared to the current competitive model sgmm-rw, we can achieve significant improvements of 4% and 5% in precision, 6% and 9% in recall on the corel5k and mirflickr25k, respectively. (c) 2017 elsevier inc. all rights reserved.
computer_vision	intersections are known for their integral and complex nature due to a variety of the participants' behaviors and interactions. this paper presents a review of recent studies on the behavior at intersections and the safety analysis for three types of participants at intersections: vehicles, drivers, and pedestrians. this paper emphasizes on techniques which are strong candidates for automation with visual sensing technology. a new behavior and safety classification is presented based on key features used for intersection design, planning, and safety. in addition, performance metrics are introduced to evaluate different studies, and insights are provided regarding the state of the art, inputs, algorithms, challenges, and shortcomings.
cryptography	bent functions are maximally nonlinear boolean functions with an even number of variables. they have attracted a lot of research for four decades because of their own sake as interesting combinatorial objects, and also because of their relations to coding theory, sequences and their applications in cryptography and other domains such as design theory. in this paper we investigate explicit constructions of bent functions which are linear on elements of spreads. after presenting an overview on this topic, we study bent functions which are linear on elements of presemifield spreads and give explicit descriptions of such functions for known commutative presemifields. a direct connection between bent functions which are linear on elements of the desarguesian spread and oval polynomials over finite fields was proved by carlet and the second author. very recently, further nice extensions have been made by carlet in another context. we introduce oval polynomials for semifields which are dual to symplectic semifields. in particular, it is shown that from a linear oval polynomial for a semifield one can get an oval polynomial for transposed semifield.
bioinformatics	backgroundorofacial clefts are congenital malformations of the orofacial region, with a global incidence of one per 700 live births. interferon regulatory factor 6 (irf6) (omim:607199) gene has been associated with the etiology of both syndromic and nonsyndromic orofacial clefts. the aim of this study was to show evidence of potentially pathogenic variants in irf6 in orofacial clefts cohorts from africa. methodswe carried out sanger sequencing on dna from 184 patients with nonsyndromic orofacial clefts and 80 individuals with multiple congenital anomalies that presented with orofacial clefts. we sequenced all the nine exons of irf6 as well as the 5 and 3 untranslated regions. in our analyses pipeline, we used various bioinformatics tools to detect and describe the potentially etiologic variants. resultswe observed that potentially etiologic exonic and splice site variants were nonrandomly distributed among the nine exons of irf6, with 92% of these variants occurring in exons 4 and 7. novel variants were also observed in both nonsyndromic orofacial clefts (p.glu69lys, p.asn185thr, c.175-2a>c and c.1060+26c>t) and multiple congenital anomalies (p.gly65val, p.lys320asn and c.379+1g>t) patients. our data also show evidence of compound heterozygotes that may modify phenotypes that emanate from irf6 variants. conclusionsthis study demonstrates that exons 4 and 7 of irf6 are mutational hotspots' in our cohort and that irf6 mutants-induced orofacial clefts may be prevalent in the africa population, however, with variable penetrance and expressivity. these observations are relevant for detection of high-risk families as well as genetic counseling. in conclusion, we have shown that there may be a need to combine both molecular and clinical evidence in the grouping of orofacial clefts into syndromic and nonsyndromic forms.
computer_programming	personalized e-learning environment is desirable in computer programming education. an important issue on personalized e-learning environment is to know the learning status of each student. this article proposes a method, skp-based student learning status description(skp-based slsd), to help instructors to know student individual 's learning status in c programming. skp-based slsd focus on the syntactic knowledge called syntax knowledge point(skp) extracted from program source code. firstly, it gathers all syntactic knowledge that should be learned by the students by extracting skp from the source code in teaching materials or exercises' model answers. then, for each student, it collects his learning activities on each skp by extracting skp from the source code the student have read or taught at lectures and wrote at exercises or tests. finally, for each student, his understanding of each skp is estimated based on the collected data. student learning status can be described by his understanding of all skps. by skp-based slsd, the information used to describe student learning status can be more detail, be better-defined and better-handled by computer systems. we have also conducted experiments and proved that skp-based slsd is effective and feasible.
cryptography	in this paper, a simple and efficient watermarking method is proposed by using visual cryptography, singular value decomposition and chaotic maps. the proposed scheme uses a gray-level image as watermark instead of binary logo or bit sequence. the proposed scheme is a zero-watermarking scheme, where the watermark is not embedded directly in the host image. the host image is encrypted with secret watermark image by constructing two shares- master share and ownership share. the two shares separately do not give any information about the watermark but when stacked together, the watermark is revealed. singular value decomposition has been used to select the robust features of the host image and chaotic maps have been used to improve the security. experimental study is conducted to evaluate the robustness of the algorithm against various signal processing and geometrical attacks.
cryptography	through generating the d-dimensional ghz state in the z-basis and measuring it in the x-basis, a dynamic quantum secret sharing scheme is proposed. in the proposed scheme, multiple participants can be added or deleted in one update period, and the shared secret does not need to be changed. the participants can be added or deleted by themselves, and the dealer does not need to be online. compared to the existing schemes, the proposed scheme is more efficient and more practical.
parallel_computing	the next-generation high speed wireless technologies, such as wirelesshd, bring the concept of several gigabits per second data communication. however, forward error correction that takes place at the receiver side has become a real computational challenge. so far, only hardware solutions have offered such high-speed convolutional decoding, which could not be achieved by software solutions. our paper aims to fill this gap and proposes a software level solution offering such multi-gbps convolutional decoding. for this intend, we use the massively parallel computing power of nvidia gpgpus and cuda programming environment to implement a solution. as a decoding method, the fano algorithm is selected for its relatively low memory requirements and computational complexity. a look-ahead mechanism and compact history in the form of circular queue is presented to support such high throughput. conducted tests showed that our gpu based algorithm can decode up to 9.25 gbps. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	a common way to learn is by studying written step-by-step tutorials such as worked examples. however, tutorials for computer programming can be tedious to create since a static text-based format cannot convey what happens as code executes. we created a system called codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization. using codepourri, we developed a novel crowdsourcing workflow where learners who are visiting an educational website (www.pythontutor.com) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations. since there are far more learners than experts, using learners as a crowd is a potentially more scalable way of creating tutorials. our experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook python code show the learner crowd 's annotations to be accurate, informative, and containing some insights that even experts missed.
data_structures	vector and raster are two types of spatial data structures used in a geographic information system (gis). with the development of gis and remote sensing (rs) technologies, how to rapidly convert raster to vector data and establish topological relations among vectorized polygons is becoming a bottleneck in data integration between gis and rs. based on the previous work, an improved vectorization method is proposed to vectorize classified rs raster data quickly and automatically establish topological relations. in accordance with the connection information of arcs and nodes and both-sides polygonal attributes of arcs, the next arc can be searched directly by attribute matching when constructing polygons, thereby improving search efficiency. moreover, our method addressed the problems of self-intersecting polygons, shared-boundary, and multi-nested islands and gave corresponding solutions, which can establish the topological relations of an entire image quickly. two experiments, one for comparison between before and after vectorization of two different classified rs raster maps, and the other for comparison with several methods, are carried out to test the accuracy and efficiency of our method. results show that the method solves the self-intersecting polygons, shared-boundary, and multi-nested islands problems. in addition, its vectorization speed is more than double that of commercial software arcgis, and the advantage of our method becomes more obvious as the number of polygons increases. thus, our method can vectorize large and complex classified rs raster data with sufficient efficiency for practical use and establish topological relations among vectorized polygons.
computer_programming	in this paper we introduce blockimpress, a visual, graphical block programming language for making web presentations by computer programming and we show some practical examples how to use it.
software_engineering	design defects are symptoms of design decay, which can lead to several maintenance problems. to detect these defects, most of existing research is based on the definition of rules that represent a combination of software metrics. these rules are sometimes not enough to detect design defects since it is difficult to find the best threshold values; the rules do not take into consideration the programming context, and it is challenging to find the best combination of metrics. as an alternative, we propose in this paper to identify design defects using a genetic algorithm based on the similarity/distance between the system under study and a set of defect examples without the need to define detection rules. we tested our approach on four open-source systems to identify three potential design defects. the results of our experiments confirm the effectiveness of the proposed approach.
distributed_computing	nowadays, it is more and more important to diagnose several kinds of pathologies at their early stage, in order to take the necessary countermeasures before having permanent consequences. unfortunately, though many pathologies are widespread, there does not exist a unique standardized reference or gold standard according to which it is possible to evaluate the patients, mainly when the pathology is in the early stages or is not very noticeable, and the doctor is not sufficiently expert in the problem domain. in this work, we deal with this problem, by envisioning new healthcare services supporting a collaborative clinical analysis of symptoms collected from the patients and forwarded to a group of experts, which are geographically distributed. the experts return back their assessment and diagnosis and the system combines these by means of the theory of the evidence, in order to provide a single response. the above services can be easily implemented on top of state-of-the-art distributed computing facilities such as grids or clouds, providing a connected environment for medical data distributed over different sites and allowing medical experts to collaborate without being co-located, thereby providing transparent access to data and computing resources. additionally, such services can provide feedbacks to each expert, in order to improve its own knowledge and experience in the case of divergence between the expert response and the global combined diagnosis in recognizing and classifying the received symptomatic indexes from the patient. we have considered the craniofacial pathologies in infant population as a practical example for better explaining the proposed solution. (c) 2016 elsevier b.v. all rights reserved.
symbolic_computation	in this article, we apply the singularity structure analysis to test an extended 2+1-dimensional fifth-order kdv equation for integrability. it is proven that the generalized equation passes the painleve test for integrability only in three distinct cases. two of those cases are in agreement with the known results, and a new integrable equation is first given. then, for the new integrable equation, we employ the bell polynomial method to construct its bilinear forms, bilinear backlund transformation, lax pair, and infinite conversation laws systematically. the n-soliton solutions of this new integrable equation are derived, and the propagations and collisions of multiple solitons are shown by graphs.
image_processing	varied spatial resolution of isochromatic fringes over the domain influences the accuracy of fringe order estimation using tfp/rgb photoelasticity. this has been brought out in the first part of the work. the existing scanning schemes do not take this into account, which leads to the propagation of noise from the low spatial resolution zones. in this paper, a method is proposed for creating a whole field map which represents the spatial resolution of the isochromatic fringe pattern. a novel scanning scheme is then proposed whose progression is guided by the spatial resolution of the fringes in the isochromatic image. the efficacy of the scanning scheme is demonstrated using three problems an inclined crack under biaxial loading, a thick ring subjected to internal pressure and a stress frozen specimen of an aerospace component. the proposed scheme has use in a range of applications. the scanning scheme is effective even if the model has random zones of noise which is demonstrated using a plate subjected to concentrated load. this aspect is well utilised to extract fringe data from thin slices cut from a stereo lithographic model that has characteristic random noise due to layered manufacturing. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	in this article we make a critical assessment of the relation between online and print design, focusing on the graphic language of newspaper infographics. a lot of the work done in this area consists in adapting print newspaper infographics to online versions. the problem with many of these adaptations is that there are losses in reading strategy and structure of their online versions, offering readers a mainly linear reading experience. to understand this fact, we compare print infographics and their digital versions through the analysis of layout and cognitive load. in a time when the knowledge of computer programming seems to be crucial to editorial design, we reflect on the importance of layout, which is the principle design structure to help readers access and understand information.
cryptography	elliptic curve cryptosystems proved to be well suited for securing systems with constrained resources like embedded and portable devices. in a fault-based attack, errors are induced during the computation of a cryptographic primitive, and the results are collected to derive information about the secret key safely stored in the device. we introduce a novel attack methodology to recover the secret key employed in implementations of the elliptic curve digital signature algorithm. our attack exploits the information leakage induced when altering the execution of the modular arithmetic operations used in the signature primitive and does not rely on the underlying elliptic curve mathematical structure, thus being applicable to all standardized curves. we provide both a validation of the feasibility of the attack, even employing common off-the-shelf hardware to perform the required computations, and a low-cost countermeasure to counteract it.
distributed_computing	this paper reports our research in developing a cyberinfrastructure platform to support multivariate visualization of data collected from distributed sensor network. three new techniques were introduced in this platform: (1) a hybrid data caching strategy that takes advantages of a scalable and distributed time series database, opentsdb, to realize efficient data retrieval; (2) a hyper-dimensional data cube is established to map and translate multivariate and heterogeneous sensor data into a common data structure to support location-aware visual analysis; and (3) a data-driven visualization module is implemented to support interactive and dynamic visualization on a simulated virtual globe. a series of experiments were conducted to demonstrate the good runtime performance of the proposed system. we expect this work to make a major contribution to both the visualization building block development in cyberinfrastructure research and the advancement of visual presentation and analysis of sensor data in domain sciences.
computer_vision	discovering kinship relations from face images in the wild has become an interesting and important problem in multimedia and computer vision. despite the rapid advances in face analysis in unconstrained environment, kinship verification still remains a challenging problem as the subtle kinship relation is difficult to discover and changes in pose and lighting condition further complicate this task. in this paper, we propose a kinship verification approach based on multi-linear coherent space learning. local image patches at different scales are independently projected into their corresponding coherent spaces learned by robust canonical correlation analysis such that patch pairs with kinship relations have improved correlation. in addition, most discriminative patches for verification are selected via constrained linear programming. experimental results on two widely used kinship verification datasets show that the proposed method can effectively identify different kinship relations in image pairs. compared to state-of-the-art techniques, the proposed method achieves very competitive performance with the use of simple feature descriptors.
algorithm_design	wireless sensor networks (wsns) consists of large number of spatially distributed configurable sensors, to meet the requirements of industrial, military, precision agriculture and health monitoring applications with ease of implementation and maintenance cost. transmission of data requires both energy and quality of service (qos) aware routing to ensure efficient use of the sensors and effective access of the gathered information. design of wsns considering its issues is challenging task and leads to complex algorithm design which are difficult to analyze by analytical methods and by physical measurements. deploying test-beds supposes a huge effort. in deed computer aided simulation is the feasible approach for analysis of wsns. we addressed different types of simulators along with their key features and their applicability for simulation in numerous application areas.
computer_vision	in photogrammetry, remote sensing, computer vision and robotics, a topic of major interest is represented by the automatic analysis of 3d point cloud data. this task often relies on the use of geometric features amongst which particularly the ones derived from the eigenvalues of the 3d structure tensor (e.g. the three dimensionality features of linearity, planarity and sphericity) have proven to be descriptive and are therefore commonly involved for classification tasks. although these geometric features are meanwhile considered as standard, very little attention has been paid to their accuracy and robustness. in this paper, we hence focus on the influence of discretization and noise on the most commonly used geometric features. more specifically, we investigate the accuracy and robustness of the eigenvalues of the 3d structure tensor and also of the features derived from these eigenvalues. thereby, we provide both analytical and numerical considerations which clearly reveal that certain features are more susceptible to discretization and noise whereas others are more robust. (c) 2017 international society for photogrammetry and remote sensing, inc. (isprs). published by elsevier b.v. all rights reserved.
distributed_computing	we develop several parallel algorithms for shortest distance queries in planar graphs that use graph partitioning in the preprocessing phase to precompute and store distances between selected pairs of vertices. in the query phase, given a pair of arbitrary vertices v and w, the stored information is used to find the distance between v and w fast. the algorithms are implemented and tested on a high performance cluster with upto 256 16-core cpus and their performances are analyzed and compared.
image_processing	the present work introduces a curvelet-like directional filter and discusses its application to edge detection in general images and fracture detection in gpr data. the filter is essentially a curvelet of adjustable anisotropy and orientation that can be tuned on any given (target) wavenumber; while retaining the properties of curvelets, it is not bound to the scaling rules of the curvelet frame but is individually steerable to any local trait of the data, hence it is dubbed ""curveletiform"". curveletiforms can be used in single-or multi-directional modes in a manner simple, computationally inexpensive and demonstrably efficient. gpr data generally contains straight or curved edge-like objects comprising reflections from planar interfaces and is notoriously susceptible to broadband noise. fractures are an important class of interfaces as they determine the health state of rocks or man-made structures and are primary targets of gpr surveys in geotechnical, engineering and environmental applications. as demonstrated with examples, curveletiforms can efficiently recover information of specific scale and geometry from straight or curved edges in general images. in gpr data they may distinguish reflections from small and large fractures, discriminate between groups of fractures, resolve fracture density and aid the assessment of damage in rocks and structures. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	software integration testing plays an increasingly important role as the software industry has experienced a major change from isolated applications to highly distributed computing environments. conducting integration testing is a challenging task because it is often very difficult to replicate a real enterprise environment. emulating testing environment is one of the key solutions to this problem. however, existing specification-based emulation techniques require manual coding of their message processing engines, therefore incurring high development cost. in this paper, we present a suite of domain-specific visual modeling languages to describe emulated testing environments at a high abstraction level. our solution allows domain experts to model a testing environment from abstract interface layers. these layer models are then transformed to runtime environment for application testing. our user study shows that our visual languages are easy to use, yet with sufficient expressive power to model complex testing applications.
image_processing	a look back at skeletal radiology in 2016 reveals a sizable number of publications that significantly advanced the state of knowledge about diseases of the musculoskeletal system. this review summarizes the content of some of the most intriguing papers of the year.
structured_storage	many large-scale online services use structured storage to persist metadata and sometimes data. the structured storage is typically provided by standard database servers such as microsoft 's sql server. it is important to understand the workloads seen by these servers, both for provisioning server hardware as well as to exploit opportunities for energy savings and server consolidation. in this paper we analyze disk i/o traces from production servers in four internet services as well as servers running tpc benchmarks. we show using a range of load metrics that the services differ substantially from each other and from standard tpc benchmarks. online services also show significant diurnal patterns in load that can be exploited for energy savings or consolidation. we argue that tpc benchmarks do not capture these important characteristics and argue for developing benchmarks that can be parameterized with workload features extracted from live production workload traces.
machine_learning	most diseases, including those of genetic origin, express a continuum of severity. clinical interventions for numerous diseases are based on the severity of the phenotype. predicting severity due to genetic variants could facilitate diagnosis and choice of therapy. although computational predictions have been used as evidence for classifying the disease relevance of genetic variants, special tools for predicting disease severity in large scale are missing. here, we manually curated a dataset containing variants leading to severe and less severe phenotypes and studied the abilities of variation impact predictors to distinguish between them. we found that these tools cannot separate the two groups of variants. then, we developed a novel machine-learning-based method, pon-ps (), for the classification of amino acid substitutions associated with benign, severe, and less severe phenotypes. we tested the method using an independent test dataset and variants in four additional proteins. for distinguishing severe and nonsevere variants, pon-ps showed an accuracy of 61% in the test dataset, which is higher than for existing tolerance prediction methods. pon-ps is the first generic tool developed for this task. the tool can be used together with other evidence for improving diagnosis and prognosis and for prioritization of preventive interventions, clinical monitoring, and molecular tests. (c) 2017 wiley periodicals, inc.
relational_databases	the dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. in order to apply the rationality result of belief dynamics theory to various practical problems, it should be generalized in two respects: first it should allow a certain part of belief to be declared as immutable; and second, the belief state need not be deductively closed. such a generalization of belief dynamics, referred to as base dynamics, is presented in this paper, along with the concept of a generalized revision algorithm for knowledge bases (horn or horn logic with stratified negation). we show that knowledge base dynamics has an interesting connection with kernel change via hitting set and abduction. in this paper, we show how techniques from disjunctive logic programming can be used for efficient (deductive) database updates. the key idea is to transform the given database together with the update request into a disjunctive (datalog) logic program and apply disjunctive techniques (such as minimal model reasoning) to solve the original update problem. the approach extends and integrates standard techniques for efficient query answering and integrity checking. the generation of a hitting set is carried out through a hyper tableaux calculus and magic set that is focused on the goal of minimality.
image_processing	the deficiency in rapid and in-field detection methods and portable devices that are reliable, easy-to-use, and low cost, results in the difficulties to uphold the high safety standards in china. in this study, we introduce a rapid and cost-effective smartphone-based method for point-of-need food safety inspection, which employs aptamer-conjugated aunps as the colorimetric indicator, and a battery-powered optosensing accessory attached to the camera of a smartphone for transmission images capture. a userfriendly and easy-to-use android application is developed for automatic digital image processing and result reporting. streptomycin (str) is selected as the proof-of-concept target, and its specific quantitation can be realized with a lod of 12.3 nm (8.97 mg kg(-1)) using the reported smartphone- based method. the quantitation of str in honey, milk and tap water confirm the reliability and applicability of the reported method. the extremely high acceptance of smartphone in remote and metropolitan areas of china and ease-of-use of the reported method facilitate active food contaminant and toxicant screening, thus making the implementation of the whole food supply chain monitoring and surveillance possible and hence significantly improving the current chinese food safety control system. (c) 2017 elsevier b.v. all rights reserved.
machine_learning	we evaluated the underlying causes of differences between latent heat (le) fluxes measured with two enclosed-path eddy covariance systems (ec) at two measurement levels and independent estimates in an open oak-tree grass savannah over almost one year. estimates of le of the well-stablished underlying grass by replicated weighable tension-controlled lysimiters (lelye) provided a robust baseline against which to compare ec le measured at 1.6 m above ground (le_1.6). similarly and at the ecosystem level, le up-scaled using independent measurements (leupscaled =sap flow+ lysimeter) was benchmarked with 3 ec-derived le estimates: 1) le measured by a ec tower at 15 m above ground (le-1.6), 2) le-1.5 adjusted to close the energy balance by using the bowen ratio method (lebowen = (rn - g)/(1 + beta)) and 3) le derived from the energy budget residual (leresidual = r-n - g - h-1.5). the sensitivity of ec le to the correction method applied (i.e. corrections for low-pass filtering effects on water vapor fluctuations and the so-called angle-of-attack correction) and its impact on the energy balance closure (ebc) were also evaluated. comparison of ec le between 1.6 m- and 15m-heights showed that grass dominated annual evaporative loss from 69 to 87% depending upon the spectral correction method applied. results revealed substantial underestimation of le-1.6 (up to 35%) compared to lelys, which mostly occurred during the growing season. however those differences were remarkably lower when likening le-1.5 versus leupscaled (14%) suggesting that the dampening of the water vapor fluctuations due to low-pass filtering effects is more pronounced near the surface. interestingly, a diagnostic evaluation of the errors with a random forest model showed that differences followed quite structured patterns and were associated with certain atmospheric conditions: turbulent mixing deficiencies and or stable atmospheric stratification. in addition, the model showed that differences increased with increasing relative humidity (rh) and soil moisture. our results revealed that the degree of ebc is highly sensitive to the flux correction method applied, in particular when correcting for flow distortion effects. typically, turbulent fluxes fell below the measured available energy (slope 0.92) but the slope switched abruptly when the angle-of-attack correction was applied (slope 1.07). consistent with the ebc, independent le estimates matched well with lebowen and the ebc gap decreased when leupscaied was used (slope 0.96). the use of independent estimates of le together with machine learning methods are proposed as a powerful means to diagnose the complexity behind le errors and give insights into the energy imbalance problem. in addition to inherent randomness of ec le data, accounting for uncertainties associated with the appropriateness of the correction method applied is highly recommended. 2017 elsevier b.v. all rights reserved.
distributed_computing	we present jump, a practical system for capturing high resolution, omnidirectional stereo (ods) video suitable for wide scale consumption in currently available virtual reality (vr) headsets. our system consists of a video camera built using off-the-shelf components and a fully automatic stitching pipeline capable of capturing video content in the ods format. we have discovered and analyzed the distortions inherent to ods when used for vr display as well as those introduced by our capture method and show that they are small enough to make this approach suitable for capturing a wide variety of scenes. our stitching algorithm produces robust results by reducing the problem to one of pairwise image interpolation followed by compositing. we introduce novel optical flow and compositing methods designed specifically for this task. our algorithm is temporally coherent and efficient, is currently running at scale on a distributed computing platform, and is capable of processing hours of footage each day.
relational_databases	data conversion has become an emerging topic in bigdata era. to face the challenge of rapid data growth, legacy or existing relational databases have the need to convert into nosql column-family database in order to achieve better scalability. the conversion from sql to nosql databases requires combining small, normalized sql data tables into larger nosql data tables; a process called denormalization. a challenging issues in data conversion is how to group the denormalized columns in a large data table into ""families"" in order to ensure the performance of query processing. in this paper, we propose an efficient heuristic algorithm, gpa (graph-based partition algorithm), to address this problem. we use tpc-c and tpc-h benchmarks to demonstrate that, the column-families produced by gpa is very efficient for large scale data processing.
distributed_computing	this paper presents a novel two-step method for automated design of self-stabilization. the first step enables the specification of legitimate states and an intuitive (but imprecise) specification of the desired functional behaviors in the set of legitimate states (hence the term ""shadow""). after creating the shadow specifications, we systematically introduce the main variables and the topology of the desired self-stabilizing system. subsequently, we devise a parallel and complete backtracking search towards finding a self-stabilizing solution that implements a precise version of the shadow behaviors, and guarantees recovery to legitimate states from any state. to the best of our knowledge, the shadow/puppet synthesis is the first sound and complete method that exploits parallelism and randomization along with the expansion of the state space towards generating self-stabilizing systems that cannot be synthesized with existing methods. we have validated the proposed method by creating both a sequential and a parallel implementation in the context of a software tool, called protocon. moreover, we have used protocon to automatically design three new self-stabilizing protocols that we conjecture to require the minimal number of states per process to achieve stabilization (when processes are deterministic): 2-state maximal matching on bidirectional rings, 5-state token passing on unidirectional rings, and 3-state token passing on bidirectional chains.
cryptography	while performing cryptanalysis, it is of interest to approximate a boolean function in n variables f : f-2(n) ->f-2 by affine functions. usually, it is assumed that all the input vectors to a boolean function are equiprobable while mounting affine approximation attack or fast correlation attacks. in this paper we consider a more general case when each component of the input vector to f is independent and identically distributed bernoulli variates with the parameter p. since our scope is within the area of cryptography, we initiate an analysis of cryptographic boolean functions under the previous considerations and derive expression of the analogue of walsh-hadamard transform and nonlinearity in the case under consideration. we observe that if we allow p to take up complex values then a framework involving quantum boolean functions can be introduced, which provides a connection between walsh-hadamard transform, nega-hadamard transform and boolean functions with biased inputs.
image_processing	three-dimensional (3d) vision based scanning for metrology and inspection applications is an area that has attracted increasing interest in the industry. this interest is driven by the recent advances in 3d technologies, which enable high precision measurements at an affordable cost. 3d vision allows for the modelling and inspection of the visible surface of objects. when it is necessary to detect subsurface defects, active infrared (ir) thermography is one of the most commonly used tools today for the non-destructive testing and evaluation (ndt&e) of materials. fusion of these two modalities allows the simultaneous detection of surface and subsurface defects and the visualisation of these defects overlaid on the 3d model of the scanned and modelled parts or their 3d computer-aided design (cad). in this work, we present a framework for automatically fusing 3d data (scanned or cad) with the infrared thermal images for an ndt&e process in 3d space. the captured 3d images and their thermal infrared counterparts are aligned and fused using automatically detected features. this fusion is undergone on 3d space, thus allowing 3d visualisation of subsurface defects on a 3d model (or cad). additionally, the defects are extracted using image processing techniques and overlaid over their virtual position in 3d space. their positioning at a certain distance from the part 's 3d surface is proportional to the computed depth using phase image analysis in the fourier domain. this depth represents the real position of the detected subsurface defect and is extracted using thermograms (temporal sequence of thermal images). the results obtained are promising and show how this new technology can be used efficiently in a combined ndt&e-metrology analysis of manufactured parts, in areas such as aerospace and automotive, among others.
computer_programming	we extended the current density convolution finite-difference time-domain (jec-fdtd) method to plasma photonic crystals using the crank-nicolson -difference scheme and derived the one-dimensional jec-crank-nicolson (cn)-fdtd iterative equation of plasma photonic crystals. the method eliminated the courant-friedrich-levy (cfl) stability constraint and became completely unconditional stable form. the incomplete cholesky conjugate gradient (iccg) algorithm is proposed to solve the equation with a large sparse matrix in the cn-fdtd method as the iccg method improves the speed of convergence, enhances stability, and reduces memory consumption. the jec-cn-fdtd method is applied to study the characteristics of time domain and frequency domain in the plasma photonic crystal objects. the high accuracy and efficiency of the jec-cn-fdtd method are confirmed by computing the characteristic parameters of plasma photonic crystals under different conditions such as the electric field distribution of electromagnetic wave, reflection coefficients, and transmission coefficients. simulation study showed that the algorithm performed stably and could reduce memory consumption and facilitate computer programming.
algorithm_design	to improve the energy conversion ability and well utilize renewable resources, j. rifkin first put forward the concept of internet of energy ioe). although a peer-to-peer energy sharing mechanism is achieved through bi-directional energy transportation, the approach to solving cooperative energy transportation and storage still needs improving. traditionally, the redundant energy will be wasted if it cannot be consumed by power load. in fact, the redundant energy can be stored to supply power loads in the future. for this end, we investigate cooperative energy transportation and storage for ioes in terms of problem analysis, algorithm design, and platform development. after demonstrating the feasibility condition and proving the np-hard of our problem, we derive the optimal solution by the reduction from a classic knapsack problem. we also design novel heuristics followed by different energy storage strategies. in addition, based on software-defined networking (sdn), a complementary platform is developed to make an effective decision for cooperative energy transportation and storage using heuristics above. both simulation and experimental results demonstrate the effectiveness of our solutions.
network_security	with continuous development of science and technology, actual data integration and operating path also change greatly. to better improve transmission accuracy of overall data information, and guarantee optimal establishment of computer network security system, accuracy of overall system can improve fundamentally and more efficient computer security treatment measures can be established only when efficient network model operates. this paper simply analyzes the connotation of computer network security risk assessment model, intensively interprets the principle of fuzzy theory and composition of neural network model and finally discusses neural network model of fuzzy theory and fusion system of computer network security. this paper aims to verify system security performance through effective data analysis.
cryptography	the key escrow problem and high computational cost are the two major problems that hinder the wider adoption of hierarchical identity-based signature (hibs) scheme. hibs schemes with either escrow-free (ef) or online/offline (oo) model have been proved secure in our previous work. however, there is no much ef or oo scheme that has been evaluated experimentally. in this letter, several ef/oo hibs schemes are considered. we study the algorithmic complexity of the schemes both theoretically and experimentally. scheme performance and practicability of ef and oo models are discussed.
machine_learning	feature selection problem in data mining is addressed here by proposing a bi-objective genetic algorithm based feature selection method. boundary region analysis of rough set theory and multivariate mutual information of information theory are used as two objective functions in the proposed work, to select only precise and informative data from the data set. data set is sampled with replacement strategy and the method is applied to determine non-dominated feature subsets from each sampled data set. finally, ensemble of such bi-objective genetic algorithm based feature selectors is developed with the help of parallel implementations to produce much generalized feature subset. in fact, individual feature selector outputs are aggregated using a novel dominance based principle to produce final feature subset. proposed work is validated using repository especially for feature selection datasets as well as on uci machine learning repository datasets and the experimental results are compared with related state of art feature selection methods to show effectiveness of the proposed ensemble feature selection method. (c) 2017 elsevier b.v. all rights reserved.
computer_graphics	this paper presents a virtual try-on system to correctly visualize 3d objects (e.g., glasses) in the face of a given user. by capturing the image and depth information of a user through a low-cost rgb-d camera, we apply a face tracking technique to detect specific landmarks in the facial image. these landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3d facial morphable model that fits as good as possible to the user 's head and face. at the end, we deform the chosen 3d objects from its rest shape to a deformed shape matching the specific facial shape of the user. the last step projects and renders the 3d object into the original image, with enhanced precision and in proper scale, showing the selected object in the user 's face. we validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. our results demonstrate that, by fitting a facial model to the user 's face, the rendered virtual 3d objects look more realistic.
computer_vision	the estimation of nutrient content of plants is considerably important in agricultural practices, especially in enabling the application of precision farming. a plethora of methods has been used to estimate nitrogen amount in plants, including the utilization of computer vision. however, most of the image-based nitrogen estimation methods are conducted in controlled environments. these methods are not so practical, time consuming, and require many equipment. therefore, there is a crucial need to develop a method to estimate nitrogen content of plants based on leaves images captured on field. it is a very challenging task since the intensity of sunlight is always changing and this leads to an inconsistent image capturing problem. in this paper, we develop a low-cost, simple, and accurate approach image-based nitrogen amount estimation. plant images are captured directly under sunlight by using a conventional digital camera and are subject to a variation in lighting conditions. we propose a color constancy method using neural networks fusion and a genetic algorithm to normalize various plant images due to different sunlight intensities. a macbeth color checker is utilized as the reference to normalize the color of the images. we also develop a combination of neural networks using a committee machine to estimate the nitrogen content in wheat leaves. twelve statistical rgb color features are used as the input parameters for the nutrient estimation. the obtained result shows considerable better performance than the conventional gray-world and scale-by-max approaches, as well as linear model and single neural network methods. finally, we show that our nutrient estimation approach is superior to the commonly used soil-plant analysis development meter based prediction.
computer_programming	terrestrial laser scanners are frequently used in most of measurement application, particularly in documentation and restoration studies of indoor historical structures, and in acquiring facade reliefs. when compared to a photogrammetric method, terrestrial laser scanners have the ability to give three dimensional point cloud data directly in a fast and detailed way. high data density of point cloud data is a challenging factor in texture-map operations during documentation and restoration of historical artifacts with more indoor spaces. when coordinate information for terrestrial laser scanner point cloud data is documented, it is seen that there is no regular order and classification for the data. the aim of this study is to suggest the mathematical filtering algorithm for segmentation work towards separation of planar surfaces which have different depths and parallel to each other and which can be frequently encountered in the indoor spaces from the data of terrestrial laser scanner. filtering function for segmentation used, is based on the distance of a point to the plane. this algorithm has been chosen for the advantage of the rapid and easy results for extracting 3d coordinate data in texture mapping process. the matlab interface has been developed for using this method and analyzing the results for application which is detected how many different surfaces exist according to the statistical deviation amount. in the application, test data with 21932 points was segmented by separating it into 16 points in total with four different planes and four corner points per plane. surfaces with four different depths were obtained as the result of the research. each of them included four points. these segmented surfaces consisting of four points will facilitate integrated data production by integrating vectorial terrestrial laser scanner data into raster camera data, without the need to conventional measurements that accelerate particularly documentation and modeling in the fields of historical indoor areas. (c) 2014 elsevier ltd. all rights reserved.
cryptography	the notion internet of things (iot) means all things in the global network can be interconnected and accessed. wireless sensor network (wsn) is one of the most important applications of the notion and is widely used in nearly all scopes. in 2014, hsieh et al. presented an improved authentication scheme for wsns. but it has several weaknesses, including no session key, lack of mutual authentication and under the insider attack, the off-line guessing attack, the user forgery attack and the sensor capture attack. to avoid the weaknesses, we present a new authentication scheme which is also for wsns. then we employ the random oracle model to show the formal proof, and use the protocol analyzing tool proverif to list the formal verification process. compared with some recent schemes for wsns via the aspects of security properties, the proposed scheme overcomes the common problems and fits for the security properties of iot.
symbolic_computation	the multiple exp-function method is a new approach to obtain multiple-wave solutions of nonlinear partial differential equations (nlpdes). by this method, one can obtain multi-soliton solutions of nlpdes. hence, in this paper, using symbolic computation, we apply the multiple exp-function method to construct the exact multiple-wave solutions of a (3 + 1)-dimensional soliton equation. based on this application, we obtain mobile single-wave, double-wave and multi-wave solutions for this equation. in addition, we employ the straightforward and algebraic hirota bilinearization method to construct the multi-soliton solutions of nlpdes, and we reveal the remarkable property of soliton-soliton collision through this approach. further, we investigate the one- and two-soliton solutions of a (3 + 1)-dimensional soliton equation using the hirota 's method. we explore the particle-like behavior or elastic interaction of solitons, which has potential application in optical communication systems and switching devices.
bioinformatics	background: aberrant activation of fibroblast growth factor receptor 3 (fgfr3) is frequently observed in bladder cancer, but how it involved in carcinogenesis is not well understood. the current study was aimed to investigate the underlying mechanism on the progression of bladder cancer. methods: the gse41035 dataset downloaded from gene expression omnibus was used to identify the differentially expressed genes (degs) between bladder cancer cell line rt112 with or without depletion of fgfr3, and gene ontology enrichment analysis was performed. then, fgfr3-centered protein-protein interaction (ppi) and regulatory networks were constructed. combined with the data retrieved from gse31684, prognostic makers for bladder cancer were predicted. results: we identified a total of 2855 degs, and most of them were associated with blood vessel morphogenesis and cell division. in addition, kiaa1377, pola2, fgfr3, and epha4 were the hub genes with high degree in the fgfr3-centered ppi network. besides, 17 micrornas (mirnas) and 6 transcriptional factors (tfs) were predicted to be the regulators of the nodes in ppi network. moreover, cstf2, pola1, hmox2, and efnb2 may be associated with the prognosis of bladder cancer patient. conclusions: the current study may provide some insights into the molecular mechanism of fgfr3 as a mediator in bladder cancer.
computer_programming	in today 's industrial environment, it is common to see the use of pipes or vessels to transfer mixtures of materials or products along them. however, the measurement of the amount delivered can only be done in one way: isolate the components first; then meter, weigh or measure the volume for each individual component. this measuring procedure often stalls the production rate. this work implements a noninvasive, real time monitoring system to measure the flow concentration and velocity distributions of a two-phase (liquid-gas) flow. the system uses the combination of a specially designed twin-plane segmented electrical capacitance tomography (ect) sensor with 16 portable electrodes and the cross-correlation method as the velocity measurement technique. the measured values or data from the ect sensor will be manipulated to reconstruct the cross sectional image of the pipeline by computer programming. these images are then cross-correlated to obtain the velocity profile of the multiphase flow. the visualization results deliver information regarding the flow regime, superficial velocity and concentration distribution in two-phase flow rate measurement system. the information obtained is able to improve: the process equipment 's design, verification of existing computational modeling and simulation techniques. this technique has the possibility to promote an excellent opportunity to build methods for measuring the velocity field in multiphase flow by using noninvasive technique and cross-correlating.
computer_programming	due to that conventional teaching method mainly focus on the impartment and the memorization of knowledge points, the initiative of student cannot be activated and there is still substantial room for improving the professional abilities, including independent learning ability and problem solving capability, hands-on practical ability, etc. aiming at improving the teaching results of computer programming courses, a practical mode of classroom instruction with diverse collaboration is proposed in this paper. in this teaching mode, the theoretical teaching and learning is closely integrated with professional practice, opening lab problems and scientific research project. the course assessment emphasizes the project evaluation instead of paper test for proper evaluation of actual practice level of students.
symbolic_computation	a family of conservative, truly nonlinear, oscillators with integer or non-integer order nonlinearity is considered. these oscillators have only one odd power-form elastic-term and exact expressions for their period and solution were found in terms of gamma functions and a cosine-ateb function, respectively. only for a few values of the order of nonlinearity, is it possible to obtain the periodic solution in terms of more common functions. however, for this family of conservative truly nonlinear oscillators we show in this paper that it is possible to obtain the fourier series expansion of the exact solution, even though this exact solution is unknown. the coefficients of the fourier series expansion of the exact solution are obtained as an integral expression in which a regularized incomplete beta function appears. these coefficients are a function of the order of nonlinearity only and are computed numerically. one application of this technique is to compare the amplitudes for the different harmonics of the solution obtained using approximate methods with the exact ones computed numerically as shown in this paper. as an example, the approximate amplitudes obtained via a modified ritz method are compared with the exact ones computed numerically. (c) 2014 elsevier b.v. all rights reserved.
network_security	wireless sensor networks are installed in hostile areas. the security issues in wireless sensor networks are very important. getting secure links between nodes is a challenging problem in wsns. they are more vulnerable to security attacks than wired networks. in order to protect the sensitive data in wsn can be protected using secret keys to encrypt the exchanged messages between communicating nodes. key management is essential for many security services such as confidentiality and authentication. the symmetric or asymmetric key cryptography or trusted-server schemes are used to solve this problem. asymmetric key cryptography increases network security but it increases computational, memory, and energy overhead. symmetric key cryptography provides less security and it is efficient key management scheme. trusted server schemes use key management server. because there is usually no trusted infrastructure it is not very suitable for sensor networks. in this paper, we have proposed mobile agent (ma) based key distribution (makd). in makd, mobile agents are used for dissemination of public keys and update of shared keys. each sensor node constructs different symmetric keys with its neighbors, and communication security is achieved by data encryption and mutual authentication with these keys. simulation results show that makd is scalable and with less memory overhead.
relational_databases	functional dependencies (fds) represent potentially novel and interesting patterns existent in relational databases. the discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. there has been growing interest in the problem of functional dependencies discovery in the last ten years. however, existing functional dependencies discovery algorithms are mainly applied to centralized small data. it is far more challenging to discover functional dependencies from big data. in this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. we prune candidate fds at each node by local fragmented data and batch verify candidate fds in parallel. load balance is taken into account when discovering functional dependencies. experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.
computer_programming	since the introduction of spice non-linear controlled voltage and current sources, they have become a central feature in the interactive development of behavioural device models and circuit macromodels. the current generation of spice-based open source general public license circuit simulators, including qucs, ngspice and xyce (c), implements a range of mathematical operators and functions for modelling physical phenomena and system performance. the qucs equation-defined device is an extension of the spice style non-linear b type controlled source which adds dynamic charge properties to behavioural sources, allowing for example, voltage and current dependent capacitance to be easily modelled. following, the standardization of verilog-a, it has become a preferred hardware description language where analogue models are written in a netlist format combined with more general computer programming features for sequencing and controlling model operation. in traditional circuit simulation, the generation of a verilog-a model from a schematic, with embedded non-linear behavioural sources, is not automatic but is normally undertaken manually. this paper introduces a new approach to the generation of verilog-a compact device models from qucs circuit schematics using a purpose built analogue module synthesizer. to illustrate the properties and use of the qucs verilog-a module synthesiser, the text includes a number of semiconductor device modelling examples and in some cases compares their simulation performance with conventional behavioural device models. copyright (c) 2016 john wiley & sons, ltd.
bioinformatics	background: next generation sequencing (ngs) technologies provide exciting possibilities for whole genome sequencing of a plethora of organisms including bacterial strains and phages, with many possible applications in research and diagnostics. no streptomyces flavovirens phages have been sequenced to date; there is therefore a lack in available information about s. flavovirens phage genomics. we report biological and physiochemical features and use ngs to provide the complete annotated genomes for two new strains (sf1 and sf3) of the virulent phage streptomyces flavovirens, isolated from egyptian soil samples. results: the s. flavovirens phages (sf1 and sf3) examined in this study show higher adsorption rates (82 and 85%, respectively) than other actinophages, indicating a strong specificity to their host, and latent periods (15 and 30 min.), followed by rise periods of 45 and 30 min. as expected for actinophages, their burst sizes were 1.95 and 2.49 virions per ml. both phages were stable and, as reported in previous experiments, showed a significant increase in their activity after sodium chloride (nacl) and magnesium chloride (mgcl2.6h(2)o) treatments, whereas after zinc chloride (zncl2) application both phages showed a significant decrease in infection. the sequenced phage genomes are parts of a singleton cluster with sizes of 43,150 bp and 60,934 bp, respectively. bioinformatics analyses and functional characterizations enabled the assignment of possible functions to 19 and 28 putative identified orfs, which included phage structural proteins, lysis components and metabolic proteins. thirty phams were identified in both phages, 10 (33.3%) of them with known function, which can be used in cluster prediction. comparative genomic analysis revealed significant homology between the two phages, showing the highest hits among sf1, sf3 and the closest streptomyces phage (vwb phages) in a specific 13kb region. however, the phylogenetic analysis using the major capsid protein (mcp) sequences highlighted that the isolated phages belong to the bg streptomyces phage group but are clearly separated, representing a novel sub-cluster. conclusion: the results of this study provide the first physiological and genomic information for s. flavovirens phages and will be useful for pharmaceutical industries based on s. flavovirens and future phage evolution studies.
software_engineering	in this paper an analysis of a technical support data with the goal of identifying process improvement actions for reducing interrupts is presented. a technical support chat is established and used to provide internal developer support to other development teams, which use the software code developed by a core team. the paper shows how data analysis of a 6-month support time helped to identify gaps and action items for improving the technical support process to minimize interrupts from other developer teams. we show that developers are interrupted through the investigated technical support chat multiple times a day and the interruptions come during the peak working hours. we also show that these interruptions can be prevented with the introduction of multiple tools such as a dispatcher service policy, a dispatcher role, and an faq page among others. the paper also shows effects (advantages and drawbacks) of the technical support refactor actions taken on the basis of this analysis.
algorithm_design	games became popular, within the formal verification community, after their application to automatic synthesis of circuits from specifications, and they have been receiving more and more attention since then. this paper focuses on coding the ""sokoban"" puzzle, i.e., a very complex single-player strategy game. we show how its solution can be encoded and represented as a bounded model checking problem, and then solved with a sat solver. after that, to cope with very complex instances of the game, we propose two different ad-hoc divide-and-conquer strategies. those strategies, somehow similar to state-of-the-art abstraction-and-refinement schemes, are able to decompose deep bounded model checking instances into easier subtasks, trading-off between efficiency and completeness. we analyze a vast set of difficult hard-to-solve benchmark games, trying to push forward the applicability of state-of-the-art sat solvers in the field. those results show that games may provide one of the next frontier for the sat community.
symbolic_computation	we investigate the resonant davey-stewartson (ds) system. the resonant ds system is a natural (2 +1)-dimensional version of the resonant nonlinear schrodinger equation. traveling wave solutions were found. in this paper, we demonstrate the effectiveness of the analytical methods, namely, improved tan(phi/2)-expansion method (item) and generalized (g'/g)-expansion method for seeking more exact solutions via the resonant davey-stewartson system. these methods are direct, concise and simple to implement compared to other existing methods. the exact particular solutions containing four types of solutions, i.e., hyperbolic function, trigonometric function, exponential and solutions. we obtained further solutions comparing these methods with other methods. the results demonstrate that the aforementioned methods are more efficient than the multilinear variable separation method applied by tang et al. (chaos solitons fractals 42: 2707-2712, 2009). recently the item was developed for searching exact traveling wave solutions of nonlinear partial differential equations. abundant exact traveling wave solutions including solitons, kink, periodic and rational solutions have been found. these solutions might play important role in engineering and physics fields. it is shown that these methods, with the help of symbolic computation, provide a straightforward and powerful mathematical tool for solving the nonlinear problems.
network_security	nowadays, a significant part of all network accesses comes from embedded and battery-powered devices, which must be energy efficient. this paper demonstrates that a hardware (hw) implementation of network security algorithms can significantly reduce their energy consumption compared to an equivalent software (sw) version. the paper has four main contributions: (i) a new feature extraction algorithm, with low processing demands and suitable for hardware implementation; (ii) a feature selection method with two objectives-accuracy and energy consumption; (iii) detailed energy measurements of the feature extraction engine and three machine learning (ml) classifiers implemented in sw and hw-decision tree (dt), naive-bayes (nb), and k-nearest neighbors (knn); and (iv) a detailed analysis of the tradeoffs in implementing the feature extractor and ml classifiers in sw and hw. the new feature extractor demands significantly less computational power, memory, and energy. its sw implementation consumes only 22 percent of the energy used by a commercial product and its hw implementation only 12 percent. the dual-objective feature selection enabled an energy saving of up to 93 percent. comparing the most energy-efficient sw implementation (new extractor and dt classifier) with an equivalent hw implementation, the hw version consumes only 5.7 percent of the energy used by the sw version.
symbolic_computation	lie symmetry analysis is performed on a two-dimensional generalized sawada-kotera equation, which arises in various problems in mathematical physics. exact solutions are obtained using the lie point symmetries method and the simplest equation method.
network_security	software defined networking (sdn) has recently emerged to become one of the promising solutions for the future internet. with the logical centralization of controllers and a global network overview, sdn brings us a chance to strengthen our network security. however, sdn also brings us a dangerous increase in potential threats. in this paper, we apply a deep learning approach for flow-based anomaly detection in an sdn environment. we build a deep neural network (dnn) model for an intrusion detection system and train the model with the nsl-kdd dataset. in this work, we just use six basic features (that can be easily obtained in an sdn environment) taken from the fortyone features of nsl-kdd dataset. through experiments, we confirm that the deep learning approach shows strong potential to be used for flow-based anomaly detection in sdn environments.
cryptography	in last years, low-dimensional and high-dimensional chaotic systems have been implemented in cryptography. the efficiency and performance of these nonlinear systems play an important role in limited hardware implementations. in this context, low-dimensional chaotic systems are more attractive than high-dimensional chaotic systems to produce the pseudorandom key stream used for encryption purposes. although low-dimensional chaotic maps present some security disadvantages when they are used in cryptography, they are highly attractive due its simple structure, discrete nature, less arithmetic operations, high output processing, and relatively easy to implement in a digital system. in this paper, we proposed both a pseudorandomly enhanced logistic map (pelm) and its application in a novel pseudorandom number generator (prng) algorithm, which produces pseudorandom stream with excellent statistical properties. the proposed pelm is compared with logistic map by using histograms and lyapunov exponents to show its higher benefits in pseudorandom number generator. in contrast to recent schemes in the literature, we present a comprehensive security analysis over the proposed pseudorandom number generator based on pseudorandomly enhanced logistic map (prng-pelm) from a cryptographic point of view to show its potential use in secure communications. in addition, the randomness of the prng-pelm is verified with the most complete random test suit of national institute of standards and technology (nist 800-22) and with testu01. based on security results, few arithmetic operations required, and high output rate, the proposed prng-pelm scheme can be implemented in secure encryption applications, even in embedded systems with limited hardware resources.
operating_systems	information leakage and memory disclosures are significant threats to the security of modern operating systems. if an attacker is able to obtain the binary-code of a program, it is then possible to reverse engineer its source code, uncover vulnerabilities, craft exploits, and subsequently patch together code-segments to form code-reuse attacks. these activities are particularly concerning when the program is a device driver or the operating system kernel, since these facilitate privilege-escalation and the ability to persist and hide. while execute-only code is a way to inhibit memory disclosures, the current x86-64 bit virtual memory implementation does not provide the capability to enforce execute only access permissions. the authors present their implementation of exoshim: a novel, 325-line, lightweight shim hypervisor layer employing intel 's commodity virtualization features that can be dynamically inserted beneath a running kernel to prevent memory disclosures by marking its code execute-only. unlike alternative approaches that operate only on user level applications, exoshim utilizes self-protection and hiding techniques that guarantee its integrity even in the event that the attacker is able to gain root-level access. the technology can be combined with fine grained compile- and load-time diversity to mitigate the additional threat of indirect memory disclosures. these concepts have been integrated within an experimental minix-like 64-bit microkernel. while the concepts are general and could be applied to other operating systems, their implementation is subtle and requires a detailed understanding of the kernels interaction with its virtual memory layer and consideration at boot -time to load kernel code and kernel data on distinct pages of memory. early evaluations quantify exoshim 's code size and complexity, run time performance cost, and effectiveness in thwarting information leakage. exoshim provides complete multic-like execute only protection for kernel code at a runtime- performance overhead of only 0.86% due to the advanced modern caching techniques in the x86 architecture. overall, this paper contributes the presentation, implementation, and evaluation of a lightweight tool for enforcing execute only access control permissions on kernel code using the virtualization features of the modern x86-64 architecture.
symbolic_computation	the auxiliary equation method presents wide applicability to handling nonlinear wave equations. in this article, we establish new exact travelling wave solutions of the nonlinear zoomeron equation, coupled higgs equation, and equal width wave equation. the travelling wave solutions are expressed by the hyperbolic functions, trigonometric functions, and rational functions. it is shown that the proposed method provides a powerful mathematical tool for solving nonlinear wave equations in mathematical physics and engineering. throughout the article, all calculations are made with the aid of the maple packet program.
computer_vision	in viticulture, there are several applications where bud detection in vineyard images is a necessary task, susceptible of being automated through the use of computer vision methods. a common and effective family of visual detection algorithms are the scanning-window type, that slide a (usually) fixed size window along the original image, classifying each resulting windowed-patch as containing or not containing the target object. the simplicity of these algorithms finds its most challenging aspect in the classification stage. interested in grapevine buds detection in natural field conditions, this paper presents a classification method for images of grapevine buds ranging 100-1600 pixels in diameter, captured in outdoor, under natural field conditions, in winter (i.e., no grape bunches, very few leaves, and dormant buds), without artificial background, and with minimum equipment requirements. the proposed method uses well-known computer vision technologies: scale-invariant feature transform for calculating low-level features, bag of features for building an image descriptor, and support vector machines for training a classifier. when evaluated over images containing buds of at least 100 pixels in diameter, the approach achieves a recall higher than 0.9 and a precision of 0.86 over all windowed-patches covering the whole bud and down to 60% of it, and scaled up to window patches containing a proportion of 20-80% of bud versus background pixels. this robustness on the position and size of the window demonstrates its viability for use as the classification stage in a scanning-window detection algorithms. (c) 2017 elsevier b.v. all rights reserved.
computer_programming	this paper introduces a system for automatic evaluation of correctness and originality of source codes submitted by students enrolled in courses dealing with computer programming. automatic correctness checking consists of searching for plagiarisms in assignments submitted earlier and checking the correct implementation of algorithms. user interface is implemented as a moodle module using its plagiarism api. the complete system is published with gplv3 license; therefore other learning institutions can use it as well.
data_structures	shk toxin is a cysteine-rich 35-residue protein ion-channel ligand isolated from the sea anemone stichodactyla helianthus. in this work, we studied the effect of inverting the side chain stereochemistry of individual thr or ile residues on the properties of the shk protein. molecular dynamics simulations were used to calculate the free energy cost of inverting the side-chain stereochemistry of individual thr or ile residues. guided by the computational results, we used chemical protein synthesis to prepare three shk polypeptide chain analogues, each containing either an allo-thr or an alloile residue. the three allo-thr or allo-ile-containing shk polypeptides were able to fold into defined protein products, but with different folding propensities. their relative thermal stabilities were measured and were consistent with the md simulation data. structures of the three shk analogue proteins were determined by quasi-racemic x-ray crystallography and were similar to wild-type shk. all three shk analogues retained ion-channel blocking activity.
operating_systems	the complexity increase in the software and hardware necessary to support more and more advanced applications for wireless sensor networks conspicuously contribute to render them susceptible to security attacks. the nodes of most complex wsn applications sport desktop-level operating systems and this reliance on software make them ideal prey for traditional threats, like viruses and general malware. to address these problems, in this paper we devise a system for a dedicated mobile node to locate, track, access and cure the infected elements of a wsn, threatened by a proximity malware infection. in parallel, we provide a mathematical formulation for the aforementioned operations. we perform extended simulations, comparing our proposal against classic solutions in different network scenarios and we use the results of the mathematical formulation as a benchmark. furthermore, we introduce a variation of our proposal, capable to support the concurrent operation of multiple mobile nodes and implement cooperation. (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	the spiking neural p (sn p) system is defined as a type of parallel computing mechanism bio-inspired by the behavior of the soma. several authors have been employing these systems in order to create efficient arithmetic divisor circuits exploiting at maximum their intrinsic parallel processing. however, the current neural divisors expend a large amount of neurons with complex spiking rules to synchronize the input information to be processed by the soma. this work proposes a compact neural divisor that uses eight neurons and two type spiking rules per neuron. in addition, the proposed circuit includes the dendrite 's behavior as feedback connections, dendritic delays, reduction in the dendrite length and dendritic pruning into the conventional sn p systems in order to simplify the synchronization of the neural processing carried out by the soma. the results show that the proposed neural divisor can be implemented in embedded neuromorphic circuits. this, potentially allows its use in portable applications such as vision processing systems for mobile robots and cryptographic systems for mobile communication devices. (c) 2017 elsevier b.v. all rights reserved.
data_structures	this paper presents a concurrent garbage collection method for functional programs running on a multicore processor. it is a concurrent extension of our bitmap-marking non-moving collector with yuasa 's snapshot-at-the-beginning strategy. our collector is unobtrusive in the sense of the doligez-leroy-gonthier collector; the collector does not stop any mutator thread nor does it force them to synchronize globally. the only critical sections between a mutator and the collector are the code to enqueue/dequeue a 32 kb allocation segment to/from a global segment list and the write barrier code to push an object pointer onto the collector 's stack. most of these data structures can be implemented in standard lock-free data structures. this achieves both efficient allocation and unobtrusive collection in a multicore system. the proposed method has been implemented in sml#, a full-scale standard ml compiler supporting multiple native threads on multicore cpus. our benchmark tests show a drastically short pause time with reasonably low overhead compared to the sequential bitmap-marking collector.
distributed_computing	one of the challenging issues in a distributed computing system is to reach on a decision with the presence of so many faulty nodes. these faulty nodes may update the wrong information, provide misleading results and may be nodes with the depleted battery power. consensus algorithms help to reach on a decision even with the faulty nodes. every correct node decides some values by a consensus algorithm. if all correct nodes propose the same value, then all the nodes decide on that. every correct node must agree on the same value. faulty nodes do not reach on the decision that correct nodes agreed on. binary consensus algorithm and average consensus algorithm are the most widely used consensus algorithm in a distributed system. we apply binary consensus and average consensus algorithm in a distributed sensor network with the presence of some faulty nodes. we evaluate these algorithms for better convergence rate and error rate.
parallel_computing	as the need for faster power system dynamic simulations increases, it is essential to develop new algorithms that exploit parallel computing to accelerate those simulations. this paper proposes a parallel algorithm based on a two-level, schur-complement-based, domain decomposition method. the two-level partitioning provides high parallelization potential (coarse- and fine-grained). in addition, due to the schur-complement approach used to update the sub-domain interface variables, the algorithm exhibits high global convergence rate. finally, it provides significant numerical and computational acceleration. the algorithm is implemented using the shared-memory parallel programming model, targeting inexpensive multi-core machines. its performance is reported on a real system as well as on a large test system combining transmission and distribution networks.
image_processing	this paper proposed an energy efficient adder employing multistage latency and approximate computing technology. the delay of the adder decreases after the critical path of the adder is divided into multiple short stages with series of predictors, then the approximate computing technology is exploited to make a tradeoff between output quality and energy efficiency. the proposed design is applied into discrete cosine transformation (dct) in image processing and support vector machine (svm) algorithm in machine learning to verify its availability, the simulation results demonstrate that the proposed approximate adder provides 25.6% power delay-product (pdp) reduction and 2 orders of magnitude reduction in output error than the recent counterpart designs. compared with the conventional accurate ripple carry adder (rca) and kogge stone adde (ksa), the proposed design presents 66.5% to 37.6% pdp reduction, at the cost of negligible output quality reduction, which are qualified as peak signal-to-noise ratio (psnr) for dct (decreases from 33.88 db to 33.84 db) and classification accuracy for svm (decreases from 80.46% to 79.19%).
symbolic_computation	this paper deals with a (2+1)-dimensional nonlinear evolution equation (nlee) generated by the jaulent-miodek hierarchy for nonlinear water waves via the hirota 's bilinear method and pfaffian. first, we construct rational solutions for general bilinear equations, and then convert the target bilinear equations to the general ones to obtain their rational solutions. the pfaffian plays a role to simplifying the computations compared with the determinant way in the existing literatures. once the first-and second-order rational solutions have been obtained, the higher-order solutions can be derived by the same token. figures for the first-and second-order rational solutions are plotted and analyzed. as an application, the rational solutions for the modified kadomtsev-petviashvili equation have also been constructed. the method might be used for some other nlees to construct their rational solutions. (c) 2016 elsevier ltd. all rights reserved.
distributed_computing	we introduce an architecture for undertaking data processing across multiple layers of a distributed computing infrastructure, composed of edge devices (making use of internet-of-things (iot) based protocols), intermediate gateway nodes and large scale data centres. in this way, data processing that is intended to be carried out in the data centre can be pushed to the edges of the network - enabling more efficient use of data centre and in-network resources. we suggest the need for specialist data analysis and management algorithms that are resource-aware, and are able to split computation across these different layers. we propose a coordination mechanism that is able to combine different types of data processing capability, such as in-transit and in-situ. an application scenario is used to illustrate the concepts, subsequently evaluated through a multi-site deployment.
network_security	both network security and quality of service (qos) consume computational resource of it system and thus may evidently affect the application services. in the case of limited computational resource, it is important to model the mutual influence between network security and qos, which can be concurrently optimized in order to provide a better performance under the available computational resource. in this paper, an evaluation model is accordingly presented to describe the mutual influence of network security and qos, and then a multi-objective genetic algorithm nsga-ii is revised to optimize the multi-objective model. using the intrinsic information from the target problem, a new crossover approach is designed to further enhance the optimization performance. simulation results validate that our algorithm can find a set of pareto-optimal security policies under different network workloads, which can be provided to the potential users as the differentiated security preferences. these obtained pareto-optimal security policies not only meet the security requirement of the user, but also provide the optimal qos under the available computational resource. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	libraries are a collection of implementations of behavior written in a computer programming language providing a well-defined interface by which the behavior can be invoked. although a majority of the code in numerous applications comes from libraries, the risk of security vulnerabilities that comes with these libraries is often overlooked. in this regard, we seek to assess the threat landscape associated with software libraries and discuss mitigation strategies via security development lifecycle (sdl).
machine_learning	among the population of known galactic black hole x-ray binaries, grs 1915+105 stands out in multiple ways. it has been in continuous outburst since 1992, and has shown a wide range of different states that can be distinguished by their timing and spectral properties. these states, also observed in igr j17091-3624, have in the past been linked to accretion dynamics. here, we present the first comprehensive study into the long-term evolution of grs 1915+105, using the entire data set observed with rossi x-ray timing explorer over its 16-yr lifetime. we develop a set of descriptive features allowing for automatic separation of states, and show that supervised machine learning in the form of logistic regression and random forests can be used to efficiently classify the entire data set. for the first time, we explore the duty cycle and time evolution of states over the entire 16-yr time span, and find that the temporal distribution of states has likely changed over the span of the observations. we connect the machine classification with physical interpretations of the phenomenology in terms of chaotic and stochastic processes.
computer_programming	during an actual drilling process, the wellbore pressure might be below the critical pressure under which the rock surrounding the wellbore begins to fail. therefore, optimizing the wellbore trajectory based on the critical pressure to improve wellbore stability may be unreasonable, because the critical pressure can only reflect the degree of difficulty for the initial damage to occur at the wellbore rather than the extent of the wellbore damage. in accordance with the linear poroelastic rock mechanics theory, combined with the mogi-coulomb criterion, the shape of the initial shear failure zone of arbitrary wellbores is simulated. in order to predict the degree of the wellbore damage, the initial shear failure location, failure width, and failure depth of arbitrary wellbores are determined, and then a new model for calculating the initial collapse volume of a directional wellbore is derived in this paper. with the help of computer programming, the failure position, critical pressure, failure depth, failure width, and the initial collapse volume of arbitrary wellbores under different in-situ stresses are analysed. the results show that the wellbore trajectory optimized according to the critical pressure is significantly different to that optimized according to the degree of wellbore damage, and these trajectories can be completely opposite. a case from southwest sichuan shows that, when the wellbore pressure has to be below the critical pressure during a drilling process, the new model provided in this paper can be used for optimizing the wellbore trajectory to ensure the safety of the drilling operation. (c) 2016 elsevier b.v. all rights reserved.
algorithm_design	there is an increasing interest in the field of parallel and distributed data mining in grid environment over the past decade. as an important branch of spatial data mining, spatial outlier mining can be used to find out some interesting and unexpected spatial patterns in many applications. in this paper, a new parallel & distributed spatial outlier mining algorithm (pd-som) is proposed to simultaneously detect global and local outliers in a grid environment. pd-som is a delaunay triangulation (d-tin) based approach, which was encapsulated and deployed in a distributed platform to provide parallel and distributed spatial outlier mining service. subsequently, a distributed system framework for pd-som is designed on top of a geographical knowledge service grid (geoksgrid) developed by our research group, a two-step strategy for spatial outlier detection is put forward to support the encapsulation and distributed deployment of the geographical knowledge service, and two key techniques of the geographical knowledge service: parallel and distributed computing of delaunay triangulation and the implementation of pd-som algorithm are discussed. finally, the efficiency of the spatial outlier mining service is analyzed in theory, the practicality is confirmed by a demonstrative application on the abnormality analyzing of soil geochemical investigation samples from fujian eastern coastal zone area in china, and the effectiveness and superiority of pd-som in a balanced, scalable grid environment are verified through the comparison with the popular spatial outlier mining algorithm slom, for the involvement of large amount of computing cores.
algorithm_design	with the increasingly growing amount of service requests from the world-wide customers, the cloud systems are capable of providing services while meeting the customers' satisfaction. recently, to achieve the better reliability and performance, the cloud systems have been largely depending on the geographically distributed data centers. nevertheless, the dollar cost of service placement by service providers (sp) differ from the multiple regions. accordingly, it is crucial to design a request dispatching and resource allocation algorithm to maximize net profit. the existing algorithms are either built upon energy-efficient schemes alone, or multi-type requests and customer satisfaction oblivious. they cannot be applied to multi-type requests and customer satisfaction-aware algorithm design with the objective of maximizing net profit. this paper proposes an ant-colony optimization-based algorithm for maximizing sp 's net profit (amp) on geographically distributed data centers with the consideration of customer satisfaction. first, using model of customer satisfaction, we formulate the utility (or net profit) maximization issue as an optimization problem under the constraints of customer satisfaction and data centers. second, we analyze the complexity of the optimal requests dispatchment problem and rigidly prove that it is an np-complete problem. third, to evaluate the proposed algorithm, we have conducted the comprehensive simulation and compared with the other state-of-the-art algorithms. also, we extend our work to consider the data center 's power usage effectiveness. it has been shown that amp maximizes sp net profit by dispatching service requests to the proper data centers and generating the appropriate amount of virtual machines to meet customer satisfaction. moreover, we also demonstrate the effectiveness of our approach when it accommodates the impacts of dynamically arrived heavy workload, various evaporation rate and consideration of power usage effectiveness. copyright (c) 2014 john wiley & sons, ltd.
algorithm_design	multicast routing that meets multiple quality of service constraints is important for supporting multimedia com-munications in the internet of things (iot). existing multicast routing technologies for iot mainly focus on ad hoc sensor net-working scenarios; thus, are not responsive and robust enough for supporting multimedia applications in an iot environment. in order to tackle the challenging problem of multicast routing for multimedia communications in iot, in this paper, we pro-pose two algorithms for the establishing multicast routing tree for multimedia data transmissions. the proposed algorithms leverage an entropy-based process to aggregate all weights into a comprehensive metric, and then uses it to search a multicast tree on the basis of the spanning tree and shortest path tree algorithms. we conduct theoretical analysis and extensive simulations for evaluating the proposed algorithms. both analytical and experimental results demonstrate that one of the proposed algorithms is more efficient than a representative multiconstrained multicast routing algorithm in terms of both speed and accuracy; thus, is able to support multimedia communications in an iot environ-ment. we believe that our results are able to provide in-depth insight into the multicast routing algorithm design for multimedia communications in iot.
computer_programming	pedagogical agents are animated avatars that stimulate cognitive and socio-emotive aspects of instructional activity in virtual learning environment. in the context of visual design, a pedagogical agent simulating human tutor can either be realistic (3d-rendered and naturalistic) or unrealistic (2d-rendered and stylized/cartoon). though design factors such as agent stereotypes, aesthetics, roles and gender has been studied extensively, the issue of pedagogical agent 's realism has not receive sufficient attention from the research community. this is unfortunate, as decision regarding agent 's visual realism must be faced by instructional designers when implementing pedagogical agent in virtual learning environment. hence, the objective of this study is to investigate the relationship among agent 's visual realism, learners' cognition and learner 's socio-emotive behaviours in a virtual learning environment. one hundred forty-four university sophomores at an asian university interacted with either a realistic agent (3d-rendered and naturalistic) or an unrealistic agent (2d-rendered and stylized/cartoon) in a virtual learning system that delivers a lesson on basic computer programming. results showed that the unrealistic (cartoonish) agent imposed extraneous cognitive load and hindered learning transfer, particularly for male learners. mediation analysis demonstrated that the effects of agent 's design (realistic versus cartoonish) on learning transfer among males were fully mediated by learner 's extraneous load. the hypothesis that a cartoonish agent will elicit higher socio-emotive responses from learners was not supported. on the contrary, the realistic agent induced a higher level of positive affect in learners than the unrealistic agent theoretical and design considerations regarding agents' visual realism are discussed in this paper.
distributed_computing	mobile cloud computing (mcc) is the state-of-the-art mobile distributed computing model that incorporates multitude of heterogeneous cloud-based resources to augment computational capabilities of the plethora of resource-constraint mobile devices. in mcc, execution time and energy consumption are significantly improved by transferring execution of resource-intensive tasks such as image processing, 3d rendering, and voice recognition from the hosting mobile to the cloud-based resources. however, accessing and exploiting remote cloud-based resources is associated with numerous security and privacy implications, including user authentication and authorization. user authentication in mcc is a critical requirement in securing cloud-based computations and communications. despite its critical role, there is a gap for a comprehensive study of the authentication approaches in mcc which can provide a deep insight into the state-of-the-art research. this paper presents a comprehensive study of authentication methods in mcc to describe mcc authentication and compare it with that of cloud computing. the taxonomy of the state-of-the-art authentication methods is devised and the most credible efforts are critically reviewed. moreover, we present a comparison of the state-of-the-art mcc authentication methods considering five evaluation metrics. the results suggest the need for futuristic authentication methods that are designed based on capabilities and limitations of mcc environment. finally, the design factors deemed could lead to effective authentication mechanisms are presented, and open challenges are highlighted based on the weaknesses and strengths of existing authentication methods. (c) 2015 elsevier ltd. all rights reserved.
computer_vision	supermarkets nowadays are equipped with barcode scanners to speed up the checkout process. nevertheless, most of the agricultural products cannot be pre-packaged and thus must be weighted. the development of produce recognition system based on computer vision could help the cashiers in the supermarkets with the pricing of these weighted products. this work proposes a hybrid approach of object classification and attribute classification for the produce recognition system which involves the cooperation and integration of statistical approaches and semantic models. the integration of attribute learning into the produce recognition system was proposed due to the fact that attribute learning has emerged as a promising paradigm for bridging the semantic gap and assisting in object recognition in many fields of study. this could tackle problems occurred when less training data are available, i.e. less than 10 samples per class. the experiments show that the correct classification rate of the hybrid approach were 60.55, 75.37 and 86.42% with 2, 4 and 8 training examples, respectively, which were higher than other individual classifiers. a well-balanced specificity, sensitivity and f-1 score were achieved by the hybrid approach for each produce type.
computer_programming	this article contains a description of the structure, the software and functional capabilities, and the scope and purposes of application of the group profile (gp) computer technique. this technique rests on a conceptual basis (the microgroup theory), includes 16 new and modified questionnaires, and a unique algorithm, tied to the questionnaires, for identification of informal groups. the gp yields a wide range of data about the group as a whole (47 indices), each informal group (43 indices), and each group member (16 indices). the gp technique can be used to study different types of groups: production (work groups, design teams, military units, etc.), academic (school classes, student groups), and sports.
computer_graphics	this contribution presents the functionalities and multimedia contents of the e-learning module on virtual prototyping of garments within the erasmus+ project entitled e-learning course for innovative textile fields - advan2tex. use of advanced information technologies and systems can assure the textile and garment manufacturing companies the competitive advantages, such as high and constant quality of products, productivity, flexibility, and quick response to the requirements of fashion and market. a wide range of new technologies, above all those using fascinating possibilities of computer graphics, together with a new generation of computer-based systems, assure the textile companies the ability to react extremely fast to the customer demands offering quality and future-oriented services. this enables greater commercial presence and contributes to company 's better marketing position. the universities, research institutions and software producers apply nowadays a whole range of new technologies to create the advanced computer solutions that will in the future support the whole cycle starting from the virtual design of fabric and garments through automated production up to virtual merchandising. therefore, the students and textile professionals, already working in textile industries, should be given the knowledge needed for successful work with the new technologies, which will contribute to developing the textile information society of the future. this module of the e-learning course for innovative textile field informs the readers and course participants about new emerging technologies, which have a great potential for the textile-related industries: virtual prototyping of garments and 3d scanning.
relational_databases	in this paper, we have discussed the challenges in handling real-time medical big data collection and storage in health information system (his). based on challenges, we have proposed a model for real-time analysis of medical big data. we exemplify the approach through spark streaming and apache kafka using the processing of health big data stream. apache kafka works very well in transporting data among different systems such as relational databases, apache hadoop and non-relational databases. however, apache kafka lacks analyzing the stream, spark streaming framework has the capability to perform some operations on the stream. we have identified the challenges in current real-time systems and proposed our solution to cope with the medical big data streams.
image_processing	microalgae are one of the most suitable subjects for testing the potentiality of light microscopy and image analysis, because of the size of single cells, their endogenous chromaticity, and their metabolic and physiological characteristics. microscope observations and image analysis can use microalgal cells from lab cultures or collected from water bodies as model to investigate metabolic processes, behavior/reaction of cells under chemical or photic stimuli, and dynamics of population in the natural environment in response to changing conditions. in this paper we will describe the original microscope we set up together with the image processing techniques we improved to deal with these topics. our system detects and recognizes in-focus cells, extracts their features, measures cell concentration in multi-algal samples, reconstructs swimming cell tracks, monitors metabolic processes, and measure absorption and fluorescent spectra of subcellular compartments. it can be used as digital microscopy station for algal cell biology and behavioral studies, and field analysis applications.
relational_databases	information sources such as relational databases, spreadsheets, xml, json, and web apis contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs. however, they rarely provide a semantic model to describe their contents. semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data. such models are the key ingredients to automatically publish the data into knowledge graphs. manually modeling the semantics of data sources requires significant effort and expertise, and although desirable, building these models automatically is a challenging problem. most of the related work focuses on semantic annotation of the data fields (source attributes). however, constructing a semantic model that explicitly describes the relationships between the attributes in addition to their semantic types is critical. we present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source. this model represents the semantics of the new source in terms of the concepts and relationships defined by the domain ontology. given some sample data from the new source, we leverage the knowledge in the domain ontology and the known semantic models to construct a weighted graph that represents the space of plausible semantic models for the new source. then, we compute the top k candidate semantic models and suggest to the user a ranked list of the semantic models for the new source. the approach takes into account user corrections to learn more accurate semantic models on future data sources. our evaluation shows that our method generates expressive semantic models for data sources and services with minimal user input. these precise models make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition. they also make it possible to automatically publish semantic data into knowledge graphs. (c) 2015 elsevier b.v. all rights reserved.
machine_learning	machine learning-based computational intelligence methods are widely used to analyze large-scale data sets in this age of big data. extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. with high levels of data availability (i.e., big data), automatic classification of them has become an important and complex task. hence, we explore the power of applying mapreduce-based distributed adaboosting of extreme learning machine (elm) to build a predictive bag of classification models. accordingly, (1) data set ensembles are created; (2) elm algorithm is used to build weak learners (classifier functions); and (3) builds a strong learner from a set of weak learners. we applied this training model to the benchmark knowledge discovery and data mining data sets.
bioinformatics	purpose: ketamine-induced cystitis (kc) among chronic ketamine young abusers has increased dramatically and it has brought attention for urologists. the underlying pathophysiological mechanism(s) of kc is still unclear. therefore, the purpose of this study is to elucidate the possible pathophysiological mechanism(s) of kc through proteomic techniques. experimental design: bladder tissues are obtained from seven patients with kc, seven patients with interstitial cystitis/bladder pain syndrome, and five control subjects who underwent videourodynamic study followed by augmentation enterocystoplasty to increase bladder capacity. 2de/ms/ms-based approach, functional classifications, and network analyses are used for proteomic and bioinformatics analyses and protein validation is carried out by western blot analysis. results: among the proteins identified, bioinformatics analyses revealed that several actin binding related proteins such as cofilin-1, myosin light polypeptide 9, filamin a, gelsolin, lamin a are involved in the apoptosis. besides, the contractile proteins and cytoskeleton proteins such as myosin light polypeptide 9, filamin a, and calponin are found downregulated in kc bladders. conclusions and clinical relevance: increased apoptosis in kc might be mediated by actinbinding proteins and a ca2+ -activated protease. rapid detrusor contraction in kc might be induced by contractile proteins and cytoskeleton proteins.
bioinformatics	background: hepatocellular carcinoma (hcc) is the most common liver malignancy worldwide. however, present studies of its multiple gene interaction and cellular pathways still could not explain the initiation and development of hcc perfectly. to find the key genes and mirnas as well as their potential molecular mechanisms in hcc, microarray data gse22058, gse25097, and gse57958 were analyzed. methods: the microarray datasets gse22058, gse25097, and gse57958, including mrna and mirna profiles, were downloaded from the geo database and were analyzed using geo2r. functional and pathway enrichment analyses were performed using the david database, and the protein-protein interaction (ppi) network was constructed using the cytoscape software. finally, mirdb was applied to predict the targets of the differentially expressed mirnas (dems). results: a total of 115 differentially expressed genes (degs) were found in hcc, including 52 up-regulated genes and 63 down-regulated genes. the gene ontology (go) and kyoto encyclopedia of genes and genomes (kegg) pathway enrichment analyses from david showed that up-regulated genes were significantly enriched in chromosome segregation and cell division, while the down-regulated genes were mainly involved in complement activation, protein activation cascades, carboxylic acid metabolic processes, oxoacid metabolic processes, and the immune response. from the ppi network, the 18 nodes with the highest degree were screened as hub genes. among them, esr1 was found to have close interactions with foxo1, cxcl12, and gnao1. in addition, a total of 64 dems were identified, which included 58 up-regulated mirnas and 6 down-regulated mirnas. esr1 was potentially targeted by five mirnas, including hsa-mir-18a and hsa-mir-221. conclusions: the roles of dems like hsa-mir-221 in hcc through interactions with degs such as esr1 and cxcl12 may provide new clues for the diagnosis and treatment of hcc patients.
symbolic_computation	studied in this paper are the bright-dark vector soliton solutions for a generalized coupled hirota system which describes the propagation for the high-intensity ultrashort pulses in the optical glass fiber. beyond the existing bilinear forms, using an auxiliary function, we obtain the improved bilinear forms and bright-dark soliton solutions under two integrable constraints through the hirota method and symbolic computation. with the help the analytic and graphic analysis, we study the soliton properties including the amplitudes, velocities and phase shifts, and show that the interactions for the bright-dark two solitons are elastic. for the bright-dark one soliton, parametric conditions that the dark component is ""black"" or ""gray"" are obtained. for the bright-dark two solitons, we find that the bright component is affected by the dark component background parameters during such an interaction, while the dark component is not affected by the bright component background parameters. velocities for the bright-dark two solitons are inversely proportional to the higher-order effect parameter, but amplitudes and phase shifts are independent of it. besides, the bound-state bright-dark two solitons are also presented. (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	in the course of the last decade, fast and qualitative computing power developments have undoubtedly permitted for a better and more realistic modeling of complex physiological processes. due to this favorable environment, a fast, generic and reliable model for high density surface electromyographic (hd-semg) signal generation with a multilayered cylindrical description of the volume conductor is presented in this study. its main peculiarity lies in the generation of a high resolution potential map over the skin related to active motor units (mus). indeed, the analytical calculus is fully performed in the frequency domain. hd-semg signals are obtained by surfacic numerical integration of the generated high resolution potential map following a variety of electrode shapes. the suggested model is implemented using parallel computing techniques as well as by using an object-oriented approach which is comprehensive enough to be fairly quickly understood, used and potentially upgraded. to illustrate the model abilities, several simulation analyses are put forward in the results section. these simulations have been performed on the same muscle anatomy while varying the number of processes in order to show significant speed improvement. accuracy of the numerical integration method, illustrating electrode shape diversity, is also investigated in comparison to analytical transfer functions definition. an additional section provides an insight on the volume detection of a circular electrode according to its radius. furthermore, a large scale simulation is introduced with 300 mus in the muscle and a hd-semg electrode grid composed of 16 x 16 electrodes for three constant isometric contractions in 12 s. finally, advantages and limitations of the proposed model are discussed with a focus on perspective works. (c) 2016 elsevier ltd. all rights reserved.
software_engineering	the inherent uncertainty to factors such as technology and creativity in evolving software development is a major challenge for the management of software projects. to address these challenges the project manager, in addition to examining the project progress, may cope with problems such as increased operating costs, lack of resources, and lack of implementation of key activities to better plan the project. software cost estimation (sce) models do not fully cover new approaches. and this lack of coverage is causing problems in the consumer and producer ends. in order to avoid these problems, many methods have already been proposed. model-based methods are the most familiar solving technique. but it should be noted that model-based methods use a single formula and constant values, and these methods are not responsive to the increasing developments in the field of software engineering. accordingly, researchers have tried to solve the problem of sce using machine learning algorithms, data mining algorithms, and artificial neural networks. in this paper, a hybrid algorithm that combines coa-cuckoo optimization and k-nearest neighbors (knn) algorithms is used. the so-called composition algorithm runs on six different data sets and is evaluated based on eight evaluation criteria. the results show an improved accuracy of estimated cost.
computer_programming	algorithmic thinking development is a difficulty that students have to confront when they learn programming the right use of selection and control structures is a big challenge. in this research were used generative learning objects for algorithmic thinking development in the programming foundations course that is offered to new students of computer systems career. research methodological approach, was quantitative, quasi-experimental design and were applied pretest and posttest. the obtained results determined that the use of generative learning objects was relevant.
software_engineering	smart farming is a management style that includes smart monitoring, planning and control of agricultural processes. this management style requires the use of a wide variety of software and hardware systems from multiple vendors. adoption of smart farming is hampered because of a poor interoperability and data exchange between ict components hindering integration. software ecosystems is a recent emerging concept in software engineering that addresses these integration challenges. currently, several software ecosystems for farming are emerging. to guide and accelerate these developments, this paper provides a reference architecture for farm software ecosystems. this reference architecture should be used to map, assess design and implement farm software ecosystems. a key feature of this architecture is a particular configuration approach to connect ict components developed by multiple vendors in a meaningful, feasible and coherent way. the reference architecture is evaluated by verification of the design with the requirements and by mapping two existing farm software ecosystems using the farm software ecosystem reference architecture. this mapping showed that the reference architecture provides insight into farm software ecosystems as it can describe similarities and differences. a main conclusion is that the two existing farm software ecosystems cad improve configuration of different ict components. future research is needed to enhance configuration in farm software ecosystems. (c) 2016 the authors. published by elsevier b.v.
symbolic_computation	as a result of the application of a technique of multistep processes stochastic models construction the range of models, implemented as a self-consistent differential equations, was obtained. these are partial differential equations (master equation, the fokker-planck equation) and stochastic differential equations (langevin equation). however, analytical methods do not always allow to research these equations adequately. it is proposed to use the combined analytical and numerical approach studying these equations. for this purpose the numerical part is realized within the framework of symbolic computation. it is recommended to apply stochastic runge-kutta methods for numerical study of stochastic differential equations in the form of the langevin. under this approach, a program complex on the basis of analytical calculations metasystem sage is developed. for model verification logarithmic walks and black-scholes two-dimensional model are used. to illustrate the stochastic ""predator-prey"" type model is used. the utility of the combined numerical-analytical approach is demonstrated.
distributed_computing	we present fabsim, a toolkit developed to simplify a range of computational tasks for researchers in diverse disciplines. fabsim is flexible, adaptable, and allows users to perform a wide range of tasks with ease. it also provides a systematic way to automate the use of resources, including hpc and distributed machines, and to make tasks easier to repeat by recording contextual information. to demonstrate this, we present three use cases where fabsim has enhanced our research productivity. these include simulating cerebrovascular bloodflow, modelling clay-polymer nanocomposites across multiple scales, and calculating ligand-protein binding affinities.
machine_learning	gene regulation modulates rna expression via transcription factors. posttranscriptional gene regulation in turn influences the amount of protein product through, for example, micrornas (mirnas). experimental establishment of mirnas and their effects is complicated and even futile when aiming to establish the entirety of mirna target interactions. therefore, computational approaches have been proposed. many such tools rely on machine learning (ml) which involves example selection, feature extraction, model training, algorithm selection, and parameter optimization. different ml algorithms have been used for model training on various example sets, more than 1,000 features describing pre-mirnas have been proposed and different training and testing schemes have been used for model establishment. for pre-mirna detection, negative examples cannot easily be established causing a problem for two class classification algorithms. there is also no consensus on what ml approach works best and, therefore, we set forth and established the impact of the different parts involved in ml on model performance. furthermore, we established two new negative datasets and analyzed the impact of using them for training and testing. it was our aim to attach an order of importance to the parts involved in ml for pre-mirna detection, but instead we found that all parts are intricately connected and their contributions cannot be easily untangled leading us to suggest that when attempting ml-based pre-mirna detection many scenarios need to be explored.
bioinformatics	although many changes have been discovered during heart maturation, the genetic mechanisms involved in the changes between immature and mature myocardium have only been partially elucidated. here, gene expression profile changed between the human fetal and adult heart was characterized. a human microarray was applied to define the gene expression signatures of the fetal (13-17 weeks of gestation, n = 4) and adult hearts (30-40 years old, n = 4). gene ontology analyses, pathway analyses, gene set enrichment analyses, and signal transduction network were performed to predict the function of the differentially expressed genes. ten mrnas were confirmed by quantificational real-time polymerase chain reaction. 5547 mrnas were found to be significantly differentially expressed. ""cell cycle"" was the most enriched pathway in the down-regulated genes. efgr, igf1r, and itgb1 play a central role in the regulation of heart development. egfr, igf1r, and fgfr2 were the core genes regulating cardiac cell proliferation. the quantificational real-time polymerase chain reaction results were concordant with the microarray data. our data identified the transcriptional regulation of heart development in the second trimester and the potential regulators that play a prominent role in the regulation of heart development and cardiac cells proliferation.
cryptography	with the rapid advancement of wireless technologies, distinct futuristic applications of wireless sensor networks (wsns) are evolving for both public and private domains. however, wireless sensor nodes experience the major challenges in terms of power supply, computing capabilities and bandwidth requirements during the data communication. apart from these, data confidentiality and integrity suffer due to various transmission errors caused by channel noise, channel bandwidth limitation, weak signals, limitation of transmitting and receiving devices as well as different security attacks. various researches have been conducted to resolve these issues in an integrated way. nevertheless, existing literature does not offer any effective solution to resolve these issues. hence, this paper proposes a unique elliptic curve based cryptography along with the diffie-hellman key exchange technique to address these issues. the experimental results show that the proposed technique consumes low power and utilizes cpu time effectively by offering better throughput and low cyclomatic complexity. it enhances confidentiality by producing higher avalanche effect and entropy value than the existing techniques. the capacity of generating higher signal to noise ratio and lower percentage of information loss justify its efficiency to produce higher data integrity as compared to the existing schemes in the literature.
machine_learning	autism spectrum disorder (asd) is a complex neurodevelopmental disorder mainly showed atypical social interaction, communication, and restricted, repetitive patterns of behavior, interests and activities. now clinic diagnosis of asd is mostly based on psychological evaluation, clinical observation and medical history. all these behavioral indexes could not avoid defects such as subjectivity and reporter-dependency. therefore researchers devoted themselves to seek relatively stable biomarkers of asd as supplementary diagnostic evidence. the goal of present study is to generate relatively stable predictive model based on anatomical brain features by using machine learning technique. forty-six asd children and thirty-nine development delay children aged from 18 to 37 months were evolved in. as a result, the predictive model generated by regional average cortical thickness of regions with top 20 highest importance of random forest classifier showed best diagnostic performance. and random forest was proved to be the optimal approach for neuroimaging data mining in small size set and thickness-based classification outperformed volume-based classification and surface area-based classification in asd. the brain regions selected by the models might attract attention and the idea of considering biomarkers as a supplementary evidence of asd diagnosis worth exploring. autism res2017, 0: 000-000. (c) 2016 international society for autism research, wiley periodicals, inc. autism res 2017, 10: 620-630. (c) 2016 international society for autism research, wiley periodicals, inc.
software_engineering	socially assistive robotics is an important emerging research area. socially assistive robotics is challenging as it is required to move robots out of laboratories and industrial settings to interact with ordinary human beings as peers, which requires social skills. the design process usually requires multidisciplinary research teams, which may comprise subject matter experts from various domains such as robotics, systems integration, medicine, psychology, gerontology, social and cognitive sciences, and neuroscience, among many others. unlike most other robotic applications, socially assistive robotics faces some unique software and systems integration challenges. in this paper, the healthbot robot architecture, which was designed to overcome these challenges, is presented. the presented architecture was implemented and used in several field trials. the details of the field trials are presented, and lessons learned are discussed with field trial results.
data_structures	trees are a fundamental structure in algorithmics. in this paper, we study the transformation of an arbitrary binary tree s with n vertices into a completely balanced tree t via rotations, a widely studied elementary tree operation. combining concepts on rotation distance and data structures, we give a basic algorithm that performs the transformation in i similar to(n) time and i similar to(1) space, making at most 2n - 2 log(2)n rotations and improving on known previous results. the algorithm is then improved, exploiting particular properties of s. finally, we show tighter upper bounds and obtain a close lower bound on the rotation distance between a zig-zag tree and a completely balanced tree. we also find the exact rotation distance of a particular almost balanced tree to a completely balanced tree, and thus show that their distance is quite large despite the similarity of the two trees.
computer_graphics	increasing networking performances as well as the emergence of mixed reality (mr) technologies make possible providing advanced interfaces to improve remote collaboration. in this paper, we present our novel interaction paradigm called vishnu that aims to ease collaborative remote guiding. we focus on collaborative remote maintenance as an illustrative use case. it relies on an expert immersed in virtual reality (vr) in the remote workspace of a local agent helped through an augmented reality (ar) interface. the main idea of the vishnu paradigm is to provide the local agent with two additional virtual arms controlled by the remote expert who can use them as interactive guidance tools. many challenges come with this: collocation, inverse kinematics (ik), the perception of the remote collaborator and gestures coordination. vishnu aims to enhance the maintenance procedure thanks to a remote expert who can show to the local agent the exact gestures and actions to perform. our pilot user study shows that it may decrease the cognitive load compared to a usual approach based on the mapping of 2d and de-localized informations, and it could be used by agents in order to perform specific procedures without needing to have an available local expert.
computer_graphics	in this paper, we conduct research on the computer graphics rendering technology based on gpu and parallel computing system. traditional geometry based on the polygon rendering technique, due to difficulty in modeling and rendering time is too long, has been more and more cannot adapt to the large-scale complex scene accurately modeling and real-time display. on the other hand, with the improvement of image acquisition equipment precision and falling prices is easy to get high precision of image information, based on image rendering technology gradually become one of the main means of scene rendering. under this basis circumstance we propose the new perspective of the methodology that will optimize the traditional ways and provide systematic modification.
operating_systems	one key aspect of synthetic biology is the development and characterization of modular biological building blocks that can be assembled to construct integrated cell-based circuits performing computational functions. likewise, the idea of extracting biological modules from the cellular context has led to the development of in vitro operating systems. this principle has attracted substantial interest to extend the repertoire of functional materials by connecting them with modules derived from synthetic biology. in this respect, synthetic biological switches and sensors, as well as biological targeting or structure modules, have been employed to upgrade functions of polymers and solid inorganic material. the resulting systems hold great promise for a variety of applications in diagnosis, tissue engineering, and drug delivery. this review reflects on the most recent developments and critically discusses challenges concerning in vivo functionality and tolerance that must be addressed to allow the future translation of such synthetic biology-upgraded materials from the bench to the bedside. (c) 2016 elsevier b.v. all rights reserved.
data_structures	there are billions of lines of sequential code inside nowadays' software which do not benefit from the parallelism available in modern multicore architectures. automatically parallelizing sequential code, to promote an efficient use of the available parallelism, has been a research goal for some time now. this work proposes a new approach for achieving such goal. we created a new parallelizing compiler that analyses the read and write instructions, and control-flow modifications in programs to identify a set of dependencies between the instructions in the program. afterwards, the compiler, based on the generated dependencies graph, rewrites and organizes the program in a task-oriented structure. parallel tasks are composed by instructions that cannot be executed in parallel. a work-stealing-based parallel runtime is responsible for scheduling and managing the granularity of the generated tasks. furthermore, a compile-time granularity control mechanism also avoids creating unnecessary data-structures. this work focuses on the java language, but the techniques are general enough to be applied to other programming languages. we have evaluated our approach on 8 benchmark programs against ooojava, achieving higher speedups. in some cases, values were close to those of a manual parallelization. the resulting parallel code also has the advantage of being readable and easily configured to improve further its performance manually.
symbolic_computation	under consideration in this paper is a volterra lattice system. through symbolic computation, the lax pair and conservation laws are derived, an integrable lattice hierarchy and an n-fold darboux transformation (dt) are constructed for this system. furthermore, n-soliton solutions in terms of determinant are generated with the resulting n-fold dt. structures of the one-, two-and three-soliton solutions are shown graphically. overtaking inelastic solitonic interactions between/among the two and three solitons are discussed by figures plotted.
computer_graphics	digital restoration of film content that has historical value is crucial for the preservation of cultural heritage. also, digital restoration is not only a relevant application area of various video processing technologies that have been developed in computer graphics literature but also involves a multitude of unresolved research challenges. currently, the digital restoration workflow is highly labor intensive and often heavily relies on expert knowledge. we revisit some key steps of this workflow and propose semiautomatic methods for performing them. to do that we build upon state-of-the-art video processing techniques by adding the components necessary for enabling (i) restoration of chemically degraded colors of the film stock, (ii) removal of excessive film grain through spatiotemporal filtering, and (iii) contrast recovery by transferring contrast from the negative film stock to the positive. we show that when applied individually our tools produce compelling results and when applied in concert significantly improve the degraded input content. building on a conceptual framework of film restoration ensures the best possible combination of tools and use of available materials. (c) 2017 spie and is&t
computer_programming	one of the major challenges related to teaching programming and algorithmics to amateur students is the time spent to explain a language 's syntax. also, students who undertake computer programming may find problems that hinder their understanding of concepts and the development of their problem-solving and programming skills. this paper presents the results of an experimental approach that evaluated the interaction of a group of colombian students with a web solution within the context of mobile robotics to learn programming and algorithmics. the designed web app is oriented towards autodidactic learning by using visual blocks programming through five interactive modules that include concepts to be learned by students such as the following: variables, sensors, conditionals, cycles, and functions. the solution is designed to present virtual scenarios for mobile robotics. this proposal was evaluated with middle school students from the colombian education system and was compared to the results obtained using scratch as a reference tool.
machine_learning	we present a methodology based on matrix factorization and gradient descent to predict the number of sessions established in the access points of a wi-fi network according to the users' behavior. as the network considered in this work is monitored and controlled by software in order to manage users and resources in real time, we may consider it as a cyber-physical system that interacts with the physical world through access points, whose demands can be predicted according to users' activity. these predictions are useful for relocating or reinforcing some access points according to the changing physical environment. in this work we propose a prediction model based on machine learning techniques, which is validated by comparing the prediction results with real user 's activity. our experiments collected the activity of 1095 users demanding 26,673 network sessions during one month in a wi-fi network composed of 10 access points, and the results are qualitatively valid with regard to the previous knowledge. we can conclude that our proposal is suitable for predicting the demand of sessions in access points when some devices are removed taking into account the usual activity of the network users. (c) 2017 elsevier b.v. all rights reserved.
relational_databases	use of cloud database-as-a-service (daas) is gradually increasing in private and government organizations. organizations are now considering outsourcing of their local databases to cloud database servers to minimize their operational and maintenance expenses. at the same time, users are apprehensive about the confidentiality breach of their vital data in cloud database. to achieve complete confidentiality of such data in outsourced databases, it is required to keep data in alwaysencrypted form in its entire life cycle i. e. at rest, in transition and while in operation in premises of cloud database services. searchable encryption is a scheme, which allows users to perform an encrypted keyword search on encrypted data stored in database server directly without decrypting it. in many applications, it requires to access the database by multiple users where data is written by different users using different encryption keys. in this paper, we propose schemes for multi user multi-key encryption search for cloud relational databases (mes-rd). it supports search operation on data encrypted under different keys by multiple users using a trusted proxy. these data may be stored in a shared table under one or other column of database server. to the best of our knowledge, the proposed schemes mes-rd are practical and first time proposed for databases.
network_security	data communication and network have changed the way business and other daily affair works. now, they rely on computer network and internetwork. computer network is a telecommunication channel through which we can share our data. it is also called data network. security for data transmission is one of the important aspects to be considered in modern communication system. in this paper data that is to be transferred is sent in certain pattern which is embedded in the huge amount of data that can be seen by everyone. the effectiveness of the proposed method is described in such a way to increase security of data. to hide data in a binary image, no key is needed here rather this algorithm is based on binary tree traversal through which the bits are plotted in msb (most significant bit), lsb(least significant bit) and middle bit of a byte. the proposed algorithm assures the data hiding and security.
operating_systems	we describe and evaluate a software-only implementation of a novel mechanism for accessing and streaming gpu-rendered content from the cloud to low-end user devices. the unique properties of our implementation enable the trivial cloud-deployment of graphics-intensive applications, even ones that were not originally intended to run in the cloud. we achieve this goal by creating virtual gpu nodes that appear to the application like hardware devices, but that do not incur the overhead of virtualization. the low-level access to the frame buffer maximizes the number of applications that work out-of-the-box without the system imposing any specific display manager or windowing system.
computer_vision	a microscope vision system to retrieve small metallic surface via micro laser line scanning and genetic algorithms is presented. in this technique, a 36 pm laser line is projected on the metallic surface through a laser diode head, which is placed to a small distance away from the target. the micro laser line is captured by a ccd camera, which is attached to the microscope. the surface topography is computed by triangulation by means of the line position and microscope vision parameters. the calibration of the microscope vision system is carried out by an adaptive genetic algorithm based on the line position. in this algorithm, an objective function is constructed from the microscope geometry to determine the microscope vision parameters. also, the genetic algorithm provides the search space to calculate the microscope vision parameters with high accuracy in fast form. this procedure avoids errors produced by the missing of references and physical measurements, which are employed by the traditional microscope vision systems. the contribution of the proposed system is corroborated by an evaluation via accuracy and speed of the traditional microscope vision systems, which retrieve micro-scale surface topography.
data_structures	genetic sequences of multiple genes are becoming increasingly common for a wide range of organisms including viruses, bacteria and eukaryotes. while such data may sometimes be treated as a single locus, in practice, a number of biological and statistical phenomena can lead to phylogenetic incongruence. in such cases, different loci should, at least as a preliminary step, be examined and analysed separately. the r software has become a popular platform for phylogenetics, with several packages implementing distance-based, parsimony and likelihood-based phylogenetic reconstruction, and an even greater number of packages implementing phylogenetic comparative methods. unfortunately, basic data structures and tools for analysing multiple genes have so far been lacking, thereby limiting potential for investigating phylogenetic incongruence. in this study, we introduce the new r package apex to fill this gap. apex implements new object classes, which extend existing standards for storing dna and amino acid sequences, and provides a number of convenient tools for handling, visualizing and analysing these data. in this study, we introduce the main features of the package and illustrate its functionalities through the analysis of a simple data set.
computer_vision	efficiently and effectively detecting shell-like structures of particular shapes is an important task in computer vision and image processing. this paper presents a generalized possibilistic c-means algorithm (pcm) for shell clustering based on the diversity index of degree-lambda proposed by patil and taillie [diversity as a concept and its measurement. j amer statist assoc. 1982;77:548-561]. experiments on various data sets in wang [possibilistic shell clustering of template-based shapes. ieee trans fuzzy syst. 2009;17:777-793] show that the the proposed generalized pcm performs better than wang 's [possibilistic shell clustering of template-based shapes. ieee trans fuzzy syst. 2009;17:777-793] possibilistic shell clustering method according two two criteria: (i) the 'grade of detection' g(d) for each target cluster; (ii) the amount of computation, denoted as k(c), required to attain a given g(d).
algorithm_design	suppose x is any exactly k-sparse vector in r-n. we present a class of sparse matrices a, and a corresponding algorithm that we call short and fast(1) (sho-fa) that, with high probability over a, can reconstruct x from ax. the sho-fa algorithm is related to the invertible bloom lookup tables recently introduced by goodrich et al., with two important distinctions-sho-fa relies on linear measurements, and is robust to noise. the sho-fa algorithm is the first to simultaneously have the following properties: 1) it requires only o(k) measurements; 2) the bit precision of each measurement and each arithmetic operation is o(log(n) + p) (here, 2(-p) corresponds to the desired relative error in the reconstruction of x); 3) the computational complexity of decoding is o(k) arithmetic operations and that of encoding is o(n) arithmetic operations; and 4) if the reconstruction goal is simply to recover a single component of x instead of all of x, with significant probability over a, this can be done in constant time. all the above constants are independent of all problem parameters other than the desired probability of success. for a wide range of parameters, these properties are information-theoretically order-optimal. in addition, our sho-fa algorithm works over fairly general ensembles of sparse random matrices, and is robust to random noise and (random) approximate sparsity for a large range of k. in particular, suppose the measured vector equals a(x + z) + e, where z and e correspond to the source tail and measurement noise, respectively. under reasonable statistical assumptions on z and e, our decoding algorithm reconstructs x with an estimation error of o(parallel to z parallel to(2) + parallel to e parallel to(2)). the sho-fa algorithm works with high probability over a, z, and e, and still requires only o(k) steps and o(k) measurements over o(log(n))-bit numbers. this is in contrast to most existing algorithms that focus on the worst case z model, where it is known that omega(k log(n/k)) measurements over o(log(n))-bit numbers are necessary. our algorithm has good empirical performance, as validated by simulations.
image_processing	sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing. conversely, enforcing the information to be spread uniformly over representation coefficients exhibits relevant properties in various applications such as robust encoding in digital communications. antisparse regularization can be naturally expressed through an l(infinity) -norm penalty. this paper derives a probabilistic formulation of such representations. anew probability distribution, referred to as the democratic prior, is first introduced. its main properties as well as three random variate generators for this distribution are derived. then this probability distribution is used as a prior to promote antisparsity in a gaussian linearmodel, yielding a fully bayesian formulation of antisparse coding. two markov chain monte carlo algorithms are proposed to generate samples according to the posterior distribution. the first one is a standard gibbs sampler. the second one uses metropolis-hastings moves that exploit the proximity mapping of the log-posterior distribution. these samples are used to approximate maximum a posteriori and minimum mean square error estimators of both parameters and hyperparameters. simulations on synthetic data illustrate the performances of the two proposed samplers, for both complete and over-complete dictionaries. all results are compared to the recent deterministic variational fitra algorithm.
algorithm_design	much ink has been spilled regarding the trials and tribulations of adapting formal methods to the needs of software engineering practitioners with the exception of computer scientists with a passion for algorithm design and optimization, a plethora of greek letters and symbols can be an anathema to those whose first love is writing code. the advent of graphical modeling languages such as uml, and supporting tools that generate production quality code, executable modeling behavioral simulations for bridging the gap between formalism and coding. this paper proposes, with illustrative examples, an exploratory learning modality, by which the practicing engineer can investigate and empirically learn the semantic mapping of uml syntax to the semantic domains of system instantiation and reactive behavior.
computer_graphics	a tangent vector field on a surface is the generator of a smooth family of maps from the surface to itself, known as the flow. given a scalar function on the surface, it can be transported, or advected, by composing it with a vector field 's flow. such transport is exhibited by many physical phenomena, e.g., in fluid dynamics. in this paper, we are interested in the inverse problem: given source and target functions, compute a vector field whose flow advects the source to the target. we propose a method for addressing this problem, by minimizing an energy given by the advection constraint together with a regularizing term for the vector field. our approach is inspired by a similar method in computational anatomy, known as lddmm, yet leverages the recent framework of functional vector fields for discretizing the advection and the flow as operators on scalar functions. the latter allows us to efficiently generalize lddmm to curved surfaces, without explicitly computing the flow lines of the vector field we are optimizing for. we show two approaches for the solution: using linear advection with multiple vector fields, and using non-linear advection with a single vector field. we additionally derive an approximated gradient of the corresponding energy, which is based on a novel vector field transport operator. finally, we demonstrate applications of our machinery to intrinsic symmetry analysis, function interpolation and map improvement.
data_structures	a novel dimensionality reduction method named nonlocal and local structure preserving projection (nllspp) is proposed and used for process monitoring. nllspp can simultaneously preserve the nonlocal structure (i.e., data variance) and the local structure (i.e., neighborhood relationships between data points) of the data set. according to nonadjacent or neighboring relationships of different pairs of data points, nllspp defines nonlocal or local similarity weight coefficients for pairwise data points on the basis of their distances. the nonlocal similarity weight coefficients force two nonadjacent data points to be projected far apart from each other. the local similarity weight coefficients force two neighboring data points to be projected near each other. in this way, nonlocal and local structures of the data set are naturally preserved and highlighted in a lower-dimensional space. because of this advantage, nllspp is more powerful than principal component analysis (pca) and locality preserving projections (lpp) in extracting important data characteristics. a process monitoring method is developed based on the nllspp algorithm. its advantages are illustrated by a case study on the tennessee eastman process. the results indicate that the nllspp-based method has better monitoring performance that the pca-based and lpp-based methods. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	3d mesh segmentation is considered an important process in the field of computer graphics. it is a fundamental process in different applications such as shape reconstruction in reverse engineering, 3d models retrieval, and cad/cam applications, etc. it consists of subdividing a polygonal surface into patches of uniform properties either from a geometrical point of view or from a perceptual/semantic point of view. in this paper, unsupervised clustering techniques for the 3d mesh segmentation problem are introduced. the k-means and the fuzzy c-means (fcm) clustering techniques are selected for the development of the proposed clustering-based 3d mesh segmentation techniques. since the mesh faces are considered the main element, the clustering technique is applied to the dual mesh. the 3d euclidean distance is used as the distance measure to compute matching between mesh elements. based on empirical results on a benchmark dataset of 3d mesh models, the fcm-based mesh segmentation technique outperforms the k-means-based one in terms of accuracy and consistency with human segmentations.
computer_vision	objectives: the impact of wearing lenses on visual and musculoskeletal complaints in vdu workers is currently unknown. the goal of this study was 1) to evaluate the impact of wearing vdu lenses on visual fatigue and self-reported neck pain and disability, compared to progressive lenses, and 2) to measure the effect of both lenses on head inclination and pressure pain thresholds during the performance of a vdu task. methods: thirty-five eligible subjects were randomly assigned to wear progressive vdu lenses (vdu group) (n = 18) or progressive lenses (p group) (n = 17). they were enquired about visual complaints (vfq), self-perceived pain (nrs) and disability (ndi) at baseline (with old lenses), and 1 week, 3 months and 6 months after wearing their new lenses. in addition, forward head angle (fha) and ppts were assessed during and after a vdu task before and 6 months after wearing the new lenses. a short questionnaire concerning the satisfaction about the study lenses was completed at the end of the study. results: in both groups, visual fatigue and neck pain was decreased at 3 and 6 months follow up, compared to baseline. all ppts were higher during the second vdu task, independent of the type of lenses. the vdu group reported a significantly higher suitability of the lenses for vdu work. conclusion: it can be concluded that there is little difference in effect of the different lenses on visual and musculoskeletal comfort. lenses should be adjusted to the task-specific needs and habits of the participant.
computer_vision	automatic computation of surface correspondence via harmonic map is an active research field in computer vision, computer graphics and computational geometry. it may help document and understand physical and biological phenomena and also has broad applications in biometrics, medical imaging and motion capture industries. although numerous studies have been devoted to harmonic map research, limited progress has been made to compute a diffeomorphic harmonic map on general topology surfaces with landmark constraints. this work conquers this problem by changing the riemannian metric on the target surface to a hyperbolic metric so that the harmonic mapping is guaranteed to be a diffeomorphism under landmark constraints. the computational algorithms are based on ricci flow and nonlinear heat diffusion methods. the approach is general and robust. we employ our algorithm to study the constrained surface registration problem which applies to both computer vision and medical imaging applications. experimental results demonstrate that, by changing the riemannian metric, the registrations are always diffeomorphic and achieve relatively high performance when evaluated with some popular surface registration evaluation standards.
image_processing	in image processing, it is often desirable to remove the noise and preserve image features. due to the strong edge preserving ability, the total variation (tv) based regularization has been widely studied. however, it produces undesirable staircase effect. to alleviate the staircase effect, the lot model proposed by lysaker et al. (ieee trans image process 13(10): 1345-1357, 2004) has been studied, which is called the two-step method. after that, this method has started to appear as one of the more effective methods for image denoising, which includes two energy functions: one is about the normal field, the other is about the reconstruction image using the normal field obtained in the first step. however, the smoothed normal field is only related to the original noisy image in the first step, which is not enough. in this paper, we proposed a modified lot model for image denoising, which lets the reconstruction vector field be related to the restored image. in addition, to compute the new model, we design a relaxed alternative direction method. the numerical experiments show that the new model can obtain the better results compared with some state-of-the art methods.
computer_vision	this paper focuses on learning a smooth skeleton structure from noisy data-an emerging topic in the fields of computer vision and computational biology. many dimensionality reduction methods have been proposed, but none are specially designed for this purpose. to achieve this goal, we propose a unified probabilistic framework that directly models the posterior distribution of data points in an embedding space so as to suppress data noise and reveal the smooth skeleton structure. within the proposed framework, a sparse positive similarity matrix is obtained by solving a box-constrained convex optimization problem, in which the sparsity of the matrix represents the learned neighborhood graph and the positive weights stand for the new similarity. embedded data points are then obtained by applying the maximum a posteriori estimation to the posterior distribution expressed by the learned similarity matrix. the embedding process naturally provides a probabilistic interpretation of laplacian eigenmap and maximum variance unfolding. extensive experiments on various datasets demonstrate that our proposed method obtains the embedded points that accurately uncover inherent smooth skeleton structures in terms of data visualization, and the method yields superior clustering performance compared to various baselines.
software_engineering	effective team communication is a prerequisite for software quality and project success. it implies correctly elicited customer requirements, conduction of occurring change requests and to adhere releases. team communication is a complex construct that consists of numerous characteristics, individual styles, influencing factors and dynamic intensities during a project. these elements are complicated to be measured or scheduled, especially in newly formed teams. according to software developers with few experiences in teams, it would be highly desirable to recognize dysfunctional or underestimated communication behaviors already in early project phases. otherwise, negative affects may cause delay of releases or even endanger software quality. we introduce an approach on the feasibility of forecasting team 's communication behavior in student software projects. we build a very first forecasting model that involves software engineering and industrial psychological terms to extract multi week communication forecasts with accurate results. the model consists of a k-nearest neighbor machine learning algorithm and is trained and evaluated with 34 student software projects from a previously taken field study. this study is an encouraging first step towards forecasting team communication to reveal potential miscommunications during a project. it is our aim to give young software developing teams an experience-based assistance about their information flow and enable adjustment for dysfunctional communication, to avoid fire fighting situation or even risks of alternating software qualities.
distributed_computing	cloud computing is an evolutionary model for distributed computing, which consists of centralized data centers providing resources for massive scalable computing. major security challenges are the generation, distribution, and usage of encryption keys in cloud systems. one of ways to provide security, proxy re-encryption scheme is proposed, in which a semi-trusted proxy transforms a cipher-text for data sender into a cipher-text for data receiver without seeing the underlying plaintext. this paper proposes a new re-encryption scheme for secure data sharing, which is based on a trusted authority. experimental results show that our scheme attains good performance than the other previous schemes.
image_processing	a key issue in fruit export is classification and sorting for acceptable marketing. in the present work, the image processing technique was employed to grade three varieties of oranges (bam, khooni and thompson) separately. the reason for choosing this fruit as the object of the study was its abundant consumption worldwide. in this study, 14 parameters were extracted: area, eccentricity, perimeter, length/ area, blue value, green value, red value, width, contrast, texture, width/ area, width/ length, roughness, and length. further, the anfis (adaptive network-based fuzzy inference system) method was utilized to estimate the orange mass from the data obtained using the image processing in three varieties. in anfis model, samples were divided into two sets, one with 70% for training set and the other one with 30% for testing set. the results of the present study demonstrated that the coefficient of determination (r-2) of the best model for bam, khooni and thompson measured 0.948, 0.99, and 0.98, respectively. in addition, the results indicated that the estimation accuracy of the best model for bam, khooni and thompson was measured as +/- 3.7 g, +/- 1.28 g, +/- 3.2 g, respectively. this result was very satisfactory for the application of anfis to estimate the orange mass.
data_structures	branch-and-bound (b&b) is a popular approach to accelerate the solution of the optimization problems, but its parallelization on graphics processing units (gpus) is challenging because of b&b 's irregular data structures and poor computation/communication ratio. the contributions of this paper are as follows: (1) we develop two cuda-based implementations (iterative and recursive) of b&b on systems with gpus for a practical application scenario-optimal design of multi-product batch plants, with a particular example of a chemical-engineering system (ces); (2) we propose and implement several optimizations of our cuda code by reducing branch divergence and by exploiting the properties of the gpu memory hierarchy; and (3) we evaluate our implementations and their optimizations on a modern gpu-based system and we report our experimental results.
cryptography	recently, he et al. proposed an anonymous two-factor authentication scheme following the concept of temporal-credential for wireless sensor networks (wsns), which is claimed to be secure and capable of withstanding various attacks. however, we reveal that the authentication phase of their scheme has several pitfalls. firstly, their scheme is susceptible to malicious user impersonation attack, in which a legal but malicious user can impersonate as other registered users. in addition, their scheme is also vulnerable to stolen smart card attack. furthermore, the scheme cannot provide untraceability and is prone to tracking attack. then we put forward an untraceable two-factor authentication scheme based on elliptic curve cryptography (ecc) for wsns. our new scheme makes up for the missing security features necessary for real-life applications while maintaining the desired features of the original scheme. we prove that the scheme fulfills mutual authentication in the burrows-abadi-needham (ban) logic. moreover, by way of informal security analysis, we show that the proposed scheme can resist a variety of attacks and provide more security features than he et al. 's scheme.
distributed_computing	cloud, fog and dew computing concepts offer elastic resources that can serve scalable services. these resources can be scaled horizontally or vertically. the former is more powerful, which increases the number of same machines (scaled out) to retain the performance of the service. however, this scaling is tightly connected with the existence of a balancer in front of the scaled resources that will balance the load among the end points. in this paper, we present a successful implementation of a scalable low-level load balancer, implemented on the network layer. the scalability is tested by a series of experiments for a small scale servers providing services in the range of dew computing services. the experiments showed that it adds small latency of several milliseconds and thus it slightly reduces the performance when the distributed system is underutilized. however, the results show that the balancer achieves even a super-linear speedup (speedup greater than the number of scaled resources) for a greater load. the paper discusses also many other benefits that the balancer provides.
relational_databases	rough set theory provides a powerful tool for dealing with uncertainty in data. application of variety of rough set models to mining data stored in a single table has been widely studied. however, analysis of data stored in a relational structure using rough sets is still an extensive research area. this paper proposes compound approximation spaces and their constrained versions that are intended for handling uncertainty in relational data. the proposed spaces are expansions of tolerance approximation ones to a relational case. compared with compound approximation spaces, the constrained version enables to derive new knowledge from relational data. the proposed approach can improve mining relational data that is uncertain, incomplete, or inconsistent. (c) 2016 elsevier inc. all rights reserved.
computer_programming	community editing is an effective tool for improving contributions in peer production communities like wikipedia and question-answer (q&a) communities. however, the mechanisms behind who edits and why is not well understood. previous studies have focused on the effectiveness of editing and emergent hierarchies in editing communities. what is unknown is how editing is executed in a system that contains gamified motivations for contributing edits. in this paper, we examine participants editing unfit questions on stack overflow (so), a large computer programming q&a community. the combination of so 's community and reputation system with the dynamics of unfit questions allows us to examine how different actors behave. we find that early edits come from high-reputation users who do not participate as a questioner or answerer, indicating that these users work to retain certain questions. these results suggest that high-reputation user actions can be used to identify bad questions that have archival quality.
computer_vision	end-to-end learning machines enable a direct mapping from the raw input data to the desired outputs, eliminating the need for hand-crafted features. despite less engineering effort than the hand-crafted counterparts, these learning machines achieve extremely good results for many computer vision and medical image analysis tasks. two dominant classes of end-to-end learning machines are massive-training artificial neural networks (mtanns) and convolutional neural networks (cnns). although mtanns have been actively used for a number of medical image analysis tasks over the past two decades, cnns have recently gained popularity in the field of medical imaging. in this study, we have compared these two successful learning machines both experimentally and theoretically. for that purpose, we considered two well-studied topics in the field of medical image analysis: detection of lung nodules and distinction between benign and malignant lung nodules in computed tomography (ct). for a thorough analysis, we used 2 optimized mtann architectures and 4 distinct cnn architectures that have different depths. our experiments demonstrated that the performance of mtanns was substantially higher than that of cnn when using only limited training data. with a larger training dataset, the performance gap became less evident even though the margin was still significant. specifically, for nodule detection, mtanns generated 2.7 false positives per patient at 100% sensitivity, which was significantly (p < 0.05) lower than the best performing cnn model with 22.7 false positives per patient at the same level of sensitivity. for nodule classification, mtanns yielded an area under the receiver-operating-characteristic curve (auc) of 0.8806 (95% ci: 0.8389-0.9223), which was significantly (p < 0.05) greater than the best performing cnn model with an auc of 0.7755 (95% ci: 0.7120-0.8270). thus, with limited training data, mtanns would be a suitable end-to-end machine-learning model for detection and classification of focal lesions that do not require high-level semantic features.
operating_systems	heterogeneous system architecture (hsa) is an architecture developed by the hsa foundation aiming at reducing programmability barriers as well as improving communication efficiency for heterogeneous computing. for example, hsa allows heterogeneous computing devices to share the same virtual address space. this feature allows programmers to bypass explicit data copying between devices, as was required in the past. hsa features such as job dispatching through user level queues and memory based signaling help to reduce communication latency between the host and other computing devices. while the new features in hsa enable more efficient heterogeneous computing, they also introduce new challenges to system virtualization, especially in memory virtualization and i/o virtualization. this work investigates the issues involved in hsa virtualization and implements a kvm-based hypervisor that supports the main features of hsa inside guest operating systems. furthermore, this work shows that with the newly introduced hypervisor for hsa, system resources in hsa-compliant amd kaveri can be effectively shared between multiple guest operating systems.
computer_graphics	combining high-resolution level set surface tracking with lower resolution physics is an inexpensive method for achieving highly detailed liquid animations. unfortunately, the inherent resolution mismatch introduces several types of disturbing visual artifacts. we identify the primary sources of these artifacts and present simple, efficient, and practical solutions to address them. first, we propose an unconditionally stable filtering method that selectively removes sub-grid surface artifacts not seen by the fluid physics, while preserving fine detail in dynamic splashing regions. it provides comparable results to recent error-correction techniques at lower cost, without substepping, and with better scaling behavior. second, we show how a modified narrow-band scheme can ensure accurate free surface boundary conditions in the presence of large resolution mismatches. our scheme preserves the efficiency of the narrow-band methodology, while eliminating objectionable stairstep artifacts observed in prior work. third, we demonstrate that the use of linear interpolation of velocity during advection of the high-resolution level set surface is responsible for visible grid-aligned kinks; we therefore advocate higher-order velocity interpolation, and show that it dramatically reduces this artifact. while these three contributions are orthogonal, our results demonstrate that taken together they efficiently address the dominant sources of visual artifacts arising with high-resolution embedded liquid surfaces; the proposed approach offers improved visual quality, a straightforward implementation, and substantially greater scalability than competing methods.
symbolic_computation	lump solutions are rationally localized in all directions in the space. a general class of lump solutions to the (2 + 1)-dimensional b-kadomtsev-petviashvili (bkp) equation is presented through symbolic computation with maple. the hirota bilinear form of the equation is the starting point in the computation process. like the kp equation, the resulting lump solutions contain six arbitrary parameters. two of the parameters are due to the translation invariances of the bkp equation with the independent variables, and the other four need to satisfy a nonzero determinant condition and the positivity condition, which guarantee analyticity and rational localization of the solutions.
computer_vision	this article proposes an automatic image processing method that can be an effective diagnostic tool to detect and grade the severity of diabetic retinopathy. this computer vision-based algorithm imitates the logic and medical sense used by ophthalmologist in detecting the abnormality and its location in the image for grading the severity of the disease. the detection is based on finding abnormalities, such as exudates and red lesions, and the grading is based on the location of these abnormalities in the image referred with its distance from the macula. accordingly the entire image has been divided into regions and occurrence of abnormalities in these regions indicates the severity of the disease. the methodology stresses on two major factors, accuracy of the results and computation time. it is found that both these design parameters are nicely achieved in the proposed method. the experimental results indicate that overall accuracy over 90% can be achieved in the proposed method and also significantly reduce the computational complexity in this region-based approach.
relational_databases	in recent years, many researchers have studied reversible watermarking techniques for relational databases. most of the developed schemes have been based on a primary key attribute in order to determine the selected tuples and attributes to carry the watermark bits. what happens, however, when the primary key attribute does not exist for a relational database? in this paper, we propose a blind robust reversible watermarking scheme for a textual relational database. this scheme does not rely on the primary key attribute. to avoid the absolute dependence on the primary key attribute, as in existing schemes, in the proposed scheme the content of textual attributes are used to generate the virtual primary attribute that is applied in tuple and attribute selections. moreover, the selection of attributes does not depend on the order of attributes in the relational database. model and robustness analysis demonstrate that our proposed scheme achieves a high resilience against different types of tuple attacks, i.e., tuple attacks and attribute attacks. the experimental results also confirm that the proposed scheme is more secure and robust than other existing schemes.
image_processing	the magnetic moment of magnetically labeled cells, microbubbles or microspheres is an important optimization parameter for many targeting, delivery or separation applications. the quantification of this property is often difficult, since it depends not only on the type of incorporated nanoparticle, but also on the intake capabilities, surface properties and internal distribution. we describe a method to determine the magnetic moment of those carriers using a microscopic set-up and an image processing algorithm. in contrast to other works, we measure the diversion of superparamagnetic nanoparticles in a static fluid. the set-up is optimized to achieve a homogeneous movement of the magnetic carriers inside the magnetic field. the evaluation is automated with a customized algorithm, utilizing a set of basic algorithms, including blob recognition, feature-based shape recognition and a graph algorithm. we present example measurements for the characteristic properties of different types of carriers in combination with different types of nanoparticles. those properties include velocity in the magnetic field as well as the magnetic moment. the investigated carriers are adherent and suspension cells, while the used nanoparticles have different sizes and coatings to obtain varying behavior of the carriers.
algorithm_design	sketching as a natural mode for human communication and creative processes presents opportunities for improving human-computer interaction in geospatial information systems. however, to use a sketch map as user input, it must be localized within the underlying spatial data set of the information system, the base metric map. this can be achieved by a matching process called qualitative map alignment in which qualitative spatial representations of the two input maps are used to establish correspondences between each sketched object and one or more objects in the metric map. the challenge is that, to the best of our knowledge, no method for matching qualitative spatial representations suggested so far is applicable in realistic scenarios due to excessively long runtimes, incorrect algorithm design or the inability to use more than one spatial aspect at a time. we address these challenges with a metaheuristic algorithm which uses novel data structures to match qualitative spatial representations of a pair of maps. we present the design, data structures and performance evaluation of the algorithm using real-world sketch and metric maps as well as on synthetic data. our algorithm is novel in two main aspects. firstly, it employs a novel system of matrices known as local compatibility matrices, which facilitate the computation of estimates for the future size of a partial alignment and allow several types of constraints to be used at the same time. secondly, the heuristic it computes has a higher accuracy than the state-of-the-art heuristic for this task, yet requires less computation. our algorithm is also a general method for matching labelled graphs, a special case of which is the one involving complete graphs whose edges are labelled with spatial relations. the results of our evaluation demonstrate practical runtime performance and high solution quality.
computer_vision	different studies in the field of agricultural engineering have successfully related irrigation needs of plants with the percentage of green cover in crop images, by using simple allometric equations. therefore, the problem of segmenting plants from soil in digital images becomes a key component of many water management systems. the development of automatic computer vision algorithms avoids slow and expensive procedures which require the supervision of human experts. in this sense, color analysis techniques have shown to yield the best results in accuracy and efficiency. this paper describes the design and development of a new web application with two different color segmentation techniques to estimate the percentage of green cover. the system allows a remote monitoring of crops, including functionality to upload images, analyze images, database storage, and graphical visualization of the results. an extensive experimental validation of this tool has been carried out on a lettuce crop of variety 'little gem'. the two segmentation methods - based on probabilistic color models using histograms, and clustering in the rgb space using the fuzzy c-means algorithm - are compared with respect to a manual segmentation technique which allows the human expert to validate the outcome of the process for each image. the experimental results demonstrate the feasibility of these two automatic methods as substitutes of the supervised process. the first method achieves a relative error below 2.4% in the obtained segmentation, while the second method has an error below 4.8%. both techniques require less than 1 s of processing time in the server. equations to compute the crop coefficient parameter are also included and validated for the same kind of crop.(c) 2016 elsevier b.v. all rights reserved.
operating_systems	the social relationships graph i.e. sociogram allows a transparent view of people in their surroundings to whom we have a certain relationship. such options have already depicted many tools. they generally use data from social networks, e.g. facebook or google+. certainly, not every such a graph is entirely transparent and usable. a sociogram contains a large number of vertices and edges when there are a larger number of people in their surroundings. then it is not possible to simply work with the graph and subsequently describe it. in this article, we introduce options that will make the sociogram transparent, thus offering easier work with it. we work with a software tool from the ihmc company, called cmaptools, which normally operates on desktop operating systems. in our case, we are working with a version for the android os, which has been externally developed for the company for a long time, but has not yet been publicly introduced.
machine_learning	we present a supervised machine learning approach for markerless estimation of human full-body kinematics for a cyclist from an unconstrained colour image. this approach is motivated by the limitations of existing marker-based approaches restricted by infrastructure, environmental conditions, and obtrusive markers. by using a discriminatively learned mixture-of-parts model, we construct a probabilistic tree representation to model the configuration and appearance of human body joints. during the learning stage, a structured support vector machine (ssvm) learns body parts appearance and spatial relations. in the testing stage, the learned models are employed to recover body pose via searching in a test image over a pyramid structure. we focus on the movement modality of cycling to demonstrate the efficacy of our approach. in natura estimation of cycling kinematics using images is challenging because of human interaction with a bicycle causing frequent occlusions. we make no assumptions in relation to the kinematic constraints of the model, nor the appearance of the scene. our technique finds multiple quality hypotheses for the pose. we evaluate the precision of our method on two new datasets using loss functions. our method achieves a score of 91.1 and 69.3 on mean probability of correct keypoint (pck) measure and 88.7 and 66.1 on the average precision of keypoints (apk) measure for the frontal and sagittal datasets respectively. we conclude that our method opens new vistas to robust user-interaction free estimation of full body kinematics, a prerequisite to motion analysis. (c) 2017 elsevier ltd. all rights reserved.
network_security	the rapid development of the internet, especially the emergence of the social networks, leads rumor propagation into a new media era. rumor propagation in social networks has brought new challenges to network security and social stability. this paper, based on partial differential equations (pdes), proposes a new sis rumor propagation model by considering the effect of the communication between the different rumor infected users on rumor propagation. the stabilities of a nonrumor equilibrium point and a rumor-spreading equilibrium point are discussed by linearization technique and the upper and lower solutions method, and the existence of a traveling wave solution is established by the cross-iteration scheme accompanied by the technique of upper and lower solutions and schauder 's fixed point theorem. furthermore, we add the time delay to rumor propagation and deduce the conditions of hopf bifurcation and stability switches for the rumor-spreading equilibrium point by taking the time delay as the bifurcation parameter. finally, numerical simulations are performed to illustrate the theoretical results.
machine_learning	the grasslands of western jilin province in china have experienced severe degradation during the last 50 years. radial basis function neural networks (rbfnn) and support vector machines (svm) were used to predict the carbon, nitrogen, and phosphorus contents of leymus chinensis (l. chinensis) and explore the degree of grassland degradation using the matter-element extension model. both rbfnn and svm demonstrated good prediction accuracy. the results indicated that there was severe degradation, as samples were mainly concentrated in the 3rd and 4th levels. the growth of l chinensis was shown to be limited by either nitrogen, phosphorus, or both during different stages of degradation. the soil chemistry changed noticeably as degradation aggravated, which represents a destabilization of l chinensis community homeostasis. soil salinization aggravates soil nutrient loss and decreases the bioavailability of soil nutrients. this, along with the destabilization of c/n, c/p and n/p ratios, weakens the photosynthetic ability and productivity of l chinensis. this conclusion was supported by observations that l. chinensis is gradually being replaced by a chloris virgata, puccinellia tenuiflora and suaeda acuminate mixed community. (c) 2017 elsevier ltd. all rights reserved.
computer_vision	although the introduction of commercial rgb-d sensors has enabled significant progress in the visual navigation methods for mobile robots, the structured-light-based sensors, like microsoft kinect and asus xtion pro live, have some important limitations with respect to their range, field of view, and depth measurements accuracy. the recent introduction of the second- generation kinect, which is based on the time-of-flight measurement principle, brought to the robotics and computer vision researchers a sensor that overcomes some of these limitations. however, as the new kinect is, just like the older one, intended for computer games and human motion capture rather than for navigation, it is unclear how much the navigation methods, such as visual odometry and slam, can benefit from the improved parameters. while there are many publicly available rgb-d data sets, only few of them provide ground truth information necessary for evaluating navigation methods, and to the best of our knowledge, none of them contains sequences registered with the new version of kinect. therefore, this paper describes a new rgb-d data set, which is a first attempt to systematically evaluate the indoor navigation algorithms on data from two different sensors in the same environment and along the same trajectories. this data set contains synchronized rgb-d frames from both sensors and the appropriate ground truth from an external motion capture system based on distributed cameras. we describe in details the data registration procedure and then evaluate our rgb-d visual odometry algorithm on the obtained sequences, investigating how the specific properties and limitations of both sensors influence the performance of this navigation method.
network_security	the proliferation of low-cost ieee 802.15.4 zigbee wireless devices in critical infrastructure applications presents security challenges. network security commonly relies on bit-level credentials that are easily replicated and exploited by hackers. unauthorized access can be mitigated by physical layer (phy) security measures that exploit device-dependent emission characteristics that are sufficiently unique to discriminate devices. rf distinct native attribute (rf-dna) fingerprinting is a phy-based security measure, which computes statistical features extracted from such device emissions. however, the rf-dna fingerprints can be numerous, correlated, and noisy, therefore, a dimensional reduction analysis (dra) via feature selection is, therefore, of interest. device classification with dra feature subsets is evaluated using a multiple discriminant analysis (mda) classifier. determining feature relevance from mda was generally dismissed in prior rf fingerprinting work and is seldom considered in other applications. here, the mda feature relevance is revisited using a proposed eigen-based mda loadings fusion (mlf) methodology. the mda classification models are adopted and used to assess device identification (id) classification and verification performance for both the authorized and unauthorized (rogue) devices using a claimed versus actual biometric methodology. performance is compared for six dra methods using: 1) a two-sample kolmogorov-smirnov test; 2) one-way analysis of variance f-test statistics; 3) a wilk 's lambda ratio; 4) generalized relevance learning vector quantized-improved relevance; 5) randomly selected; and 6) the proposed mlf method. quantitative and qualitative dimensionality assessment methods are compared and contrasted to establish upper bounds on the number of retained features. experimentally collected zigbee emissions are considered and zigbee device classification and id verification performance using dra subsets are compared with a full-dimensional feature set. results show that dra via the proposed mlf method is superior and more robust than competing methods.
relational_databases	database models for road inventories are based on classical schemes for relational databases: many related tables, in which the database designer establishes, a priori, every detail that they consider relevant for inventory management. this kind of database presents several problems. first, adapting the model and its applications when new database features appear is difficult. in addition, the different needs of different sets of road inventory users are difficult to fulfil with these schemes. for example, maintenance management services, road authorities and emergency services have different needs. in addition, this kind of database cannot be adapted to new scenarios, such as other countries and regions (that may classify roads or name certain elements differently). the problem is more complex if the language used in these scenarios is not the same as that used in the database design. in addition, technicians need a long time to learn to use the database efficiently. this paper proposes a flexible, multi-language and multipurpose database model, which gives an effective and simple solution to the aforementioned problems. (c) 2016 the authors. published by elsevier b.v.
bioinformatics	egfr-mutated nsclc is a genetically heterogeneous disease that includes more than 200 distinct mutations. the implications of mutational subtype for both prognostic and predictive value are being increasingly understood. although the most common egfr mutations exon 19 deletions or l858r mutations predict sensitivity to egfr tyrosine kinase inhibitors (tkis), it is now being recognized that outcomes may be improved in patients with exon 19 deletions. additionally, 10% of patients will have an uncommon egfr mutation, and response to egfr tki therapy is highly variable depending on the mutation. given the growing recognition of the genetic and clinical variation seen in this disease, the development of comprehensive bioinformatics-driven tools to both analyze response in uncommon mutation subtypes and inform clinical decision making will be increasingly important. clinical trials of novel egfr tkis should prospectively account for the presence of uncommon mutation subtypes in study design. (c) 2016 international association for the study of lung cancer. published by elsevier inc. all rights reserved.
computer_graphics	despite the recent advent of various radiographic imaging techniques, it is still very difficult to correctly distinguish a pediatric osteolytic lesion in the occipital condyle, which makes it further complicated to decide on the necessity of and the adequate timing for radical resection and craniocervical fusions. to establish a legitimate therapeutic strategy for this deep-seated lesion, surgical biopsy is a reasonable choice for first-line intervention. the choice of surgical approach becomes very important because a sufficient amount of histological specimen must be obtained to confirm the diagnosis but, ideally, the residual bony structures and the muscular structures should be preserved so as not to increase craniocervical instability. in this report, we present our experience with a case of solitary langerhans cell histiocytosis (lch) involving the occipital condyle that was successfully treated with minimally invasive surgical biopsy with a far lateral condylar approach supported by preoperative 3d computer graphic simulation. an 8-year-old girl presented with neck pain. magnetic resonance imaging and computed tomography (ct) revealed an osteolytic lesion of the left occipital condyle. at surgery, the patient was placed in the prone position. a 3-cm skin incision was made in the posterior auricular region, and the sternocleidomastoid and splenius capitis muscles were dissected in the middle of the muscle bundle along the direction of the muscle fiber. under a navigation system, we approached the occipital condyle through the space between the longissimus capitis muscle and the posterior belly of the digastric muscle and lateral to the superior oblique muscle, verifying each muscle at each depth of the surgical field and, finally, obtained sufficient surgical specimen. after the biopsy, her craniocervical instability had not worsened, and chemotherapy was performed. twelve weeks after chemotherapy, her neck pain had gradually disappeared along with her torticollis, and ct showed remission of the lesion and marked regeneration of the left occipital condyle. within our knowledge, this is the first reported case of lch involving the occipital condyle. although very rare, our case indicated that lch can be an alternative in the differential diagnosis of osteolytic lesions in the craniocervical junction, in which early bone regeneration with sufficient cervical stability is expected after chemotherapy. in cases of pediatric osteolytic lesions, when they initially presented with apparent cervical instability, craniocervical fusion may possibly become unnecessary after a series of treatments. thus, the effort to maximally preserve the musculoskeletal structure should be made until its histological diagnosis is finally confirmed.
computer_programming	this article presents a proposal for the detection of programming source code similitude in academic environments. the objective of this proposal is to provide support to professors in detecting plagiarism in student homework assignments in introductory computer programming courses. the developed tool, codesight, is based on a modification of the greedy string tiling algorithm. the tool was tested in one theoretical and three real scenarios, obtaining similitude detections for assignments ranging from those that contained code without modifications to assignments containing insertions of procedural instructions inside the main code. the results verified the efficiency of the tool at the first five levels of the plagiarism spectrum for programming code, in addition to supporting suspicions of plagiarism in real scenarios. (c) 2013 wiley periodicals, inc. comput appl eng educ 23:13-22, 2015; view this article online at ; doi
computer_graphics	providing optimal mechanical ventilation to critically-ill children remains a challenge. patient-ventilator dyssynchrony results frequently with numerous deleterious consequences on patient outcome including increased requirement for sedation, prolonged duration of ventilation, and greater imposed work of breathing. most currently used ventilators have real-time, continuously-displayed graphics of pressure, volume, and flow versus time (scalars) as well as pressure, and flow versus volume (loops). a clear understanding of these graphics provides a lot of information about the mechanics of the respiratory system and the patient ventilator interaction in a dynamic fashion. using this information will facilitate tailoring the support provided and the manner in which it is provided to best suit the dynamic needs of the patient. this paper starts with a description of the scalars and loops followed by a discussion of the information that can be obtained from each of these graphics. a review will follow, on the common types of dyssynchronous interactions and how each of these can be detected on the ventilator graphics. the final section discusses how graphics can be used to optimize the ventilator support provided to patients.
operating_systems	background: hypertension or high blood pressure is on the rise. not only does it affect the elderly but is also increasingly spreading to younger sectors of the population. treating this condition involves exhaustive monitoring of patients. the current mobile health services can be improved to perform this task more effectively. objective: to develop a useful, user-friendly, robust and efficient app, to monitor hypertensive patients and adapted to the particular requirements of hypertension. methods: this work presents bpcontrol, an android and ios app that allows hypertensive patients to communicate with their health-care centers, thus facilitating monitoring and diagnosis. usability, robustness and efficiency factors for bpcontrol were evaluated for different devices and operating systems (android, ios and system-aware). furthermore, its features were compared with other similar apps in the literature. results: bpcontrol is robust and user-friendly. the respective start-up efficiency of the android and ios versions of bpcontrol were 2.4 and 8.8 times faster than a system-aware app. similar values were obtained for the communication efficiency (7.25 and 11.75 times faster for the android and ios respectively). when comparing plotting performance, bpcontrol was on average 2.25 times faster in the android case. most of the apps in the literature have no communication with a server, thus making it impossible to compare their performance with bpcontrol. conclusions: its optimal design and the good behavior of its facilities make bpcontrol a very promising mobile app for monitoring hypertensive patients.
computer_programming	tabulation method is usually adopted to describe the mapping relation of the analytic function of the local rules of cellular automaton. although tabulation method is quite suitable to describe the mapping relation among discrete variables, there are numerous if-else sentences for the state decision of independent variables in computer programming. in this paper, we define an analytic function of the local rules of an elementary cellular automaton by constructing the recognition function of the states of the cell and the neighbors. therefore, a huge number of calculations by if-else sentences are transferred to the function calculation due to the adoption of the function of the local rules during the evolution of the eca. finally, the simulation results show that the programming structure is optimized and the programming efficiency is enhanced. in addition, the proposed method can also be extended to other 2-state cellular automata. (c) 2015 elsevier inc. all rights reserved.
computer_graphics	3d digital visualization technology is a new research field along with the rapid development of computer technology. it is a multi-scale technology which consists of computer graphics, image information processing, computer aided design. this paper is about the research and development of products innovation design method, thereby realize the innovation of product design rapidly. at the same time to verify the feasibility and practicality and broad application prospect of applying 3d scan technology to the rapid development, design and manufacture of products.
software_engineering	purpose: this article aims to the evaluation of a prototypal assistive technology for alzheimer 's disease (ad) patients that helps them to remember personal details of familiar people they meet in their daily lives. method: an architecture is proposed for a personal information system powered by face recognition, where the main ad patient 's interaction is performed in a smart watch device and the face recognition is carried out on the cloud. a prototype was developed to perform some tests in a real-life scenario. results: the prototype showed correct results as a personal information system based on face recognition. however, usability flaws were identified in the interaction with the smart watch. conclusions: our architecture showed correct performance and we realized that it could be introduced in other fields, apart from assistive technology. however, when being targeted to patients with dementia some usability problems appeared, such as difficulties to read information in a small screen or take a proper photo. these problems should be addressed in further research.
symbolic_computation	a web-based knowledge database and computing platform for nonlinear differential equations is presented, which could provide computing and graphing based on symbolic computing system maple and some of its built-in packages. users can not only calculate specific types of analytical solutions of nonlinear differential systems by calling the packages, but also carry out any symbolic computations associated with equations and other kinds of simple computations in an interactive mode with visual output. the knowledge database of differential equations has all functions of the general database. furthermore, each equation has a web page to show its properties and research results. in addition, each mathematica formula is stored in its infix form in the knowledge database and can be displayed visually.
computer_vision	the woven fabric is a flexible object and to specify its parameters, applying inflexible and ordinary methods of image processing ever have considerable errors. in this regards, proposing an adaptable method to fabric image properties is concentrated to detect the yarns position. in this research, a flexible algorithm is proposed containing two stages: first, the inexact ranges of fabric parameters are determined by preprocessing colored fabric images using wavelet transform and clustering methods. then, the hybrid genetic and imperialist competitive algorithm is applied to optimize the obtained ranges and detect the yarns position. to achieve better results, the parameters of the hybrid ica-ga are calibrated using the taguchi method. results indicate that in this new method, the error value of detecting structural fabric parameters has considerably decreased to 5% as compared with common gray-scale projection method. the proposed method is capable of detecting the exact yarns position in colored fabric images with uneven color intensity and low-density weave with mean precision value of 96.2%. in the fabric images with high density weaves, the mean precision value is more than 94.72%.
bioinformatics	n-6-methyladenosine (m(6)a) is a prevalent modification present in the mrnas of higher eukaryotes. yth domain family 2 (ythdf2), an m(6)a reader protein, can recognize mrna m(6)a sites to mediate mrna degradation. however, the regulatory mechanism of ythdf2 is poorly understood. to this end, we investigated the post-transcriptional regulation of ythdf2. bioinformatics analysis suggested that the microrna mir-145 might target the 3-untranslated region (3-utr) of ythdf2 mrna. the levels of mir-145 were negatively correlated with those of ythdf2 mrna in clinical hepatocellular carcinoma (hcc) tissues, and immunohistochemical staining revealed that ythdf2 was closely associated with malignancy of hcc. interestingly, mir-145 decreased the luciferase activities of 3-utr of ythdf2 mrna. mutation of predicted mir-145 binding sites in the 3-utr of ythdf2 mrna abolished the mir-145-induced decrease in luciferase activity. overexpression of mir-145 dose-dependently down-regulated ythdf2 expression in hcc cells at the levels of both mrna and protein. conversely, inhibition of mir-145 resulted in the up-regulation of ythdf2 in the cells. dot blot analysis and immunofluorescence staining revealed that the overexpression of mir-145 strongly increased m(6)a levels relative to those in control hcc cells, and this increase could be blocked by ythdf2 overexpression. moreover, mir-145 inhibition strongly decreased m(6)a levels, which were rescued by treatment with a small interfering rna-based ythdf2 knockdown. thus, we conclude that mir-145 modulates m(6)a levels by targeting the 3-utr of ythdf2 mrna in hcc cells.
algorithm_design	in this paper, an algorithm is designed for shooting games under strong background light. six leds are uniformly distributed on the edge of a game machine screen. they are located at the four corners and in the middle of the top and the bottom edges. three leds are enlightened in the odd frames, and the other three are enlightened in the even frames. a simulator is furnished with one camera, which is used to obtain the image of the leds by applying inter-frame difference between the even and odd frames. in the resulting images, six led are six bright spots. to obtain the leds' coordinates rapidly, we proposed a method based on the area of the bright spots. after calibrating the camera based on a pinhole model, four equations can be found using the relationship between the image coordinate system and the world coordinate system with perspective transformation. the center point of the image of leds is supposed to be at the virtual shooting point. the perspective transformation matrix is applied to the coordinate of the center point. then we can obtain the virtual shooting point 's coordinate in the world coordinate system. when a game player shoots a target about two meters away, using the method discussed in this paper, the calculated coordinate error is less than ten mm. we can obtain 65 coordinate results per second, which meets the requirement of a real-time system. it proves the algorithm is reliable and effective.
computer_graphics	as a result of the relief image surface pattern shapes are often very complicated, no rules of geometry and detailed, with traditional computer graphics methods is difficult to achieve realistic embossed reproduction effect and the practical processing effect. at present, there are few new theoretical achievements in this field in china.this paper designed a simple method to get the 3 d relief effect fast, color images based on cellular neural networks (cnn) embossment effect processing method, through simulation experiment, this method has proved its convenient operation and implementation, and its application in the hail cloud layer image pre-processing,achieved good results.
symbolic_computation	studied in this paper is a ()-dimensional nonlinear schrodinger equation with the group velocity dispersion, fiber gain-or-loss and nonlinearity coefficient functions, which describes the evolution of a slowly varying wave packet envelope in the inhomogeneous optical fiber. with the hirota method and symbolic computation, the bilinear form and dark multi-soliton solutions under certain variable-coefficient constraint are derived. interactions between the different-type dark two solitons have been asymptotically analyzed and presented. both velocities and amplitudes of the two linear-type dark solitons do not change before and after the interaction. the two parabolic-type dark solitons propagating with the opposite directions both change their directions after the interaction. interaction between the two periodic-type dark solitons is also presented. interactions between the linear-, parabolic- and periodic-type dark two solitons are elastic.
machine_learning	twitter spam has become a critical problem nowadays. recent works focus on applying machine learning techniques for twitter spam detection, which make use of the statistical features of tweets. in our labeled tweets data set, however, we observe that the statistical properties of spam tweets vary over time, and thus, the performance of existing machine learning-based classifiers decreases. this issue is referred to as ""twitter spam drift"". in order to tackle this problem, we first carry out a deep analysis on the statistical features of one million spam tweets and one million non-spam tweets, and then propose a novel lfun scheme. the proposed scheme can discover ""changed"" spam tweets from unlabeled tweets and incorporate them into classifier 's training process. a number of experiments are performed to evaluate the proposed scheme. the results show that our proposed lfun scheme can significantly improve the spam detection accuracy in real-world scenarios.
computer_programming	data structures and algorithms are important foundation topics in computer science education. however, they are considered to be hard to teach and learn because usually model complicated concepts, refer to abstract mathematical notions, or describe complex dynamic changes in data structures. many students in programming courses have difficulties to master all required competencies and skills especially at introductory level. in the literature there are different ways to enhance learning programming and deal with the important dropout rate. recently, games are increasingly being used for education in various fields. we hypothesize that games have the potential to be an important teaching tool for their interactive, engaging and immersive activities. so they can improve student engagement, motivation, and consequently learning. to this end, we are developing a game to teach basic algorithmic concepts and algorithms. we aim to initially investigate the educational games developed for and used in the computer programming domain and review to which level they address the aforementioned difficulties. then, we propose a role playing game called algogame based on existing solutions and incorporates new elements. a pre validation of the game with novice students was very encouraging and demonstrates that learning programming can be enhanced by playing with algogame.
computer_programming	computer programming is an important competence for engineering and computer science students. teaching and learning programming concepts and skills have been recognized as being a big challenge to both teachers and students. accordingly, researchers have indicated that 'learning strategy,' lack of study' and 'lack of practice' are the causal attributes of success or failure in a computer programming course. therefore, to cope with this problem, an interactive test system is proposed to enhance students' learning outcomes in computer programming courses in this study. an experiment has been conducted on a college computer programming course to evaluate the effectiveness of the proposed method. the experimental results show that the proposed approach can benefit the students in enhancing their programming skills. in addition, the students' attitudes toward learning computer programming were improved.
distributed_computing	time-triggered architectures form an important component of many distributed computing platforms for safety-critical real-time applications such as avionics and automotive control systems. tta, flexray, and ttcan are examples of such time-triggered architectures that have been popular in recent times. these architectures involve a number of algorithms for synchronizing a set of distributed computing nodes for meaningful exchange of data among them. the algorithms include a startup algorithm whose job is to integrate one or more nodes into the group of communicating nodes. the startup algorithm runs on every node when the system is powered up, and again after a failure occurs. some critical issues need to be considered in the design of the startup algorithms, for example, the algorithms should be robust under reasonable assumptions of failures of nodes and channels. the safety-critical nature of the applications where these algorithms are used demands rigorous verification of these algorithms, and there have been numerous attempts to use formal verification techniques for this purpose. this paper focuses on various formal verification efforts carried out for ensuring the correctness of the startup algorithms. in particular, the verification of different startup algorithms used in three time-triggered architectures, tta, flexray, and ttcan, is studied, compared, and contrasted. besides presenting the various verification approaches for these algorithms, the gaps and possible improvements on the verification efforts are also indicated.
relational_databases	purpose - the purpose of this paper is to present a four-level architecture that aims at integrating, publishing and retrieving ecological data making use of linked data (ld). it allows scientists to explore taxonomical, spatial and temporal ecological information, access trophic chain relations between species and complement this information with other data sets published on the web of data. the development of ecological information repositories is a crucial step to organize and catalog natural reserves. however, they present some challenges regarding their effectiveness to provide a shared and global view of biodiversity data, such as data heterogeneity, lack of metadata standardization and data interoperability. ld rose as an interesting technology to solve some of these challenges. design/methodology/approach - ecological data, which is produced and collected from different media resources, is stored in distinct relational databases and published as rdf triples, using a relational-resource description format mapping language. an application ontology reflects a global view of these datasets and share with them the same vocabulary. scientists specify their data views by selecting their objects of interest in a friendly way. adata view is internally represented as an algebraic scientific workflow that applies data transformation operations to integrate data sources. findings - despite of years of investment, data integration continues offering scientists challenges in obtaining consolidated data views of a large number of heterogeneous scientific data sources. the semantic integration approach presented in this paper simplifies this process both in terms of mappings and query answering through data views. social implications - this work provides knowledge about the guanabara bay ecosystem, as well as to be a source of answers to the anthropic and climatic impacts on the bay ecosystem. additionally, this work will enable evaluating the adequacy of actions that are being taken to clean up guanabara bay, regarding the marine ecology. originality/value - mapping complexity is traded by the process of generating the exported ontology. the approach reduces the problem of integration to that of mappings between homogeneous ontologies. as a byproduct, data views are easily rewritten into queries over data sources. the architecture is general and although applied to the ecological context, it can be extended to other domains.
computer_vision	rgb-d sensors have been widely used in various areas of computer vision and graphics. a good descriptor will effectively improve the performance of operation. this article further analyzes the recognition performance of shape features extracted from multi-modality source data using rgb-d sensors. a hybrid shape descriptor is proposed as a representation of objects for recognition. we first extracted five 2d shape features from contour-based images and five 3d shape features over point cloud data to capture the global and local shape characteristics of an object. the recognition performance was tested for category recognition and instance recognition. experimental results show that the proposed shape descriptor outperforms several common global-to-global shape descriptors and is comparable to some partial-to-global shape descriptors that achieved the best accuracies in category and instance recognition. contribution of partial features and computational complexity were also analyzed. the results indicate that the proposed shape features are strong cues for object recognition and can be combined with other features to boost accuracy.
machine_learning	high performance, parallel applications with irregular data accesses are becoming a critical workload class for modern systems. in particular, the execution of such workloads on emerging many-core systems is expected to be a significant component of applications in data mining, machine learning, scientific computing and graph analytics. however, power and energy constraints limit the capabilities of individual cores, memory hierarchy and on-chip interconnect of such systems, thus leading to architectural and software trade-offs that must be understood in the context of the intended application 's behavior. irregular applications are notoriously hard to optimize given their data-dependent access patterns, lack of structured locality and complex data structures and code patterns. we have ported two irregular applications, graph community detection using the louvain method (grappolo) and high-performance conjugate gradient (hpccg), to the tilera many-core system and have conducted a detailed study of platform-independent and platform-specific optimizations that improve their performance as well as reduce their overall energy consumption. to conduct this study, we employ an auto-tuning based approach that explores the optimization design space along three dimensions memory layout schemes, gcc compiler flag choices and openmp loop scheduling options. we leverage mit 's opentuner auto-tuning framework to explore and recommend energy optimal choices for different combinations of parameters. we then conduct an in-depth architectural characterization to understand the memory behavior of the selected workloads. finally, we perform a correlation study to demonstrate the interplay between the hardware behavior and application characteristics. using auto-tuning, we demonstrate whole-node energy savings and performance improvements of up to 49.6% and 60% relative to a baseline instantiation, and up to 31% and 45.4% relative to manually optimized variants. (c) 2016 elsevier inc. all rights reserved.
software_engineering	this paper presents a controlled experiment in which the goodness of using theory belbin roles for the integration of software development teams is explored. the study takes place in an academic environment with students from the engineering of software and compares the quality of the readability of the code generated by integrated teams with roles compatible - according to the theory of belbin- and traditional teams, in our case, integrated teams with students selected randomly. the results provide positive evidence on the use of this theory and motivate researchers to continue studies in other activities related to the software development process; on the other hand, from a pedagogical perspective, the results obtained with the experiment, allow proposing as an alternative to integration teams, learning scenarios related courses software engineering, theory of roles of belbin.
algorithm_design	information retrieval and web search present a challenging question to researches. today users urge for accurate and precise hands on information from search machine. interpreting of user query goal is major challenge in past and present. numerous algorithms and frameworks have be proposed, but fail to incorporate user aims, as query without proper intent processing retrieves irrelevant information pattern discovery has ability to solve in limitations of keyword and image disambiguates with phrase learning ie, pattern discovery. today 's search machines are based on ranking model eliminating boolean retrieval constraint and boosting natural language use. even though word sense and concept extraction is major challenge which comes up with keywords. information can be presented in better way with image presentation, which is been used in news portals to communicate fastly happing news and social websites instagram facebook, flicker. user purchase goods by sighting product images on flipkart. so today uses have sifted their approach from text based information to image based, which has given rise to research domain of image information retrieval (iir) but large number of image attributes also give rise to image classification ambiguity. relevance is major factor that influence information retrieval system performance with impact precision and recall. relevance re-ranking is methodology opted in to retrieve most optimized relevant results eliminating non-relevant. large amount of image with associated word annotations are present on different web portals. in this research we build a semantic search engine which selects network design pattern and integrate reinformant learning approach (agent based learning) that help in selecting information from various networks and help in network structuring with wair (web agents for information retrieval) architecture at core. agent helping in retrieving precise objects from different portals and linking them. a optimized procedure e-simrank is been implemented to count in link semantic in network and content based knowledge learning for reinforcing better results. performance evaluation show that proposed architecture and algorithm design present faster and relevance result. a image based recommendation system is our research outcome which contributes to image retrieval domain. the research work is been developed by studying 24 core vital articles on image retrieval and find research scope with major challenges which have common ground and need to be addressed. the found research analysis query (raq) help in directing to study better techniques to overcome problem. our research innovation is reinforment learning algorithm agent based system development. existing state of art of present algorithms have been optimized with this innovation integration. future scope of research lies to image to image base retrieval or video recommendation system.
structured_storage	micro-blog as an important part of the network media has become an important access to information for netizens. some hot topics have an immeasurable impact on the public opinion formation and dissemination, and also potential security risks are absolutely not underestimated. collecting the relevant data in the micro-blog can provide data base for public opinion analysis. the study, social topic detection system based on micro-blog is mainly centered on the key technologies in the information capture and the chinese handling of micro-blog. this study invokes the api of sina micro-blog through three kinds of capture strategy, and then uses rmm reverse maximum matching algorithm to capture the information of micro-blog, after this using classification of self-build public opinion to do information classification of micro-blog for structured storage. at last this study finishes the research and implement of the system based on micro-blog. the system has realized the hot topics of timely capture, collection, classification, storage and retrieval which lay the foundation for further public opinion analysis.
computer_graphics	texture synthesis is a well-established area, with many important applications in computer graphics and vision. however, despite their success, synthesis techniques are not used widely in practice because the creation of good exemplars remains challenging and extremely tedious. in this paper, we introduce an unsupervised method for analyzing texture content across multiple scales that automatically extracts good exemplars from natural images. unlike existing methods, which require extensive manual tuning, our method is fully automatic. this allows the user to focus on using texture palettes derived from their own images, rather than on manual interactions dictated by the needs of an underlying algorithm. most natural textures exhibit patterns at multiple scales that may vary according to the location (non-stationarity). to handle such textures many synthesis algorithms rely on an analysis of the input and a guidance of the synthesis. our new analysis is based on a labeling of texture patterns that is both (i) multi-scale and (ii) unsupervised - that is, patterns are labeled at multiple scales, and the scales and the number of labeled clusters are selected automatically. our method works in two stages. the first builds a hierarchical extension of superpixels and the second labels the superpixels based on random walk in a graph of similarity between superpixels and a nonnegative matrix factorization. our label-maps provide descriptors for pixels and regions that benefit state-of-the-art texture synthesis algorithms. we show several applications including guidance of non-stationary synthesis, content selection and texture painting. our method is designed to treat large inputs and can scale to many megapixels. in addition to traditional exemplar inputs, our method can also handle natural images containing different textured regions.
data_structures	we introduce transactions into libraries of concurrent data structures; such transactions can be used to ensure atomicity of sequences of data structure operations. by focusing on transactional access to a well-defined set of data structure operations, we strike a balance between the ease-ofprogramming of transactions and the efficiency of customtailored data structures. we exemplify this concept by designing and implementing a library supporting transactions on any number of maps, sets (implemented as skiplists), and queues. our library offers efficient and scalable transactions, which are an order of magnitude faster than state-of-theart transactional memory toolkits. moreover, our approach treats stand-alone data structure operations (like put and enqueue) as first class citizens, and allows them to execute with virtually no overhead, at the speed of the original data structure library.
image_processing	a microstructure-based modeling method is developed to predict the mechanical behaviors of lithium ion battery separators. existing battery separator modeling methods cannot capture the structural features on the microscale. to overcome this issue, we propose an image-based microstructure representative volume element (rve) modeling method, which facilitates the understanding of the separators complex macro mechanical behaviors from the perspective of microstructural features. a generic image processing workflow is developed to identify different phases in the microscopic image. the processed rve image supplies microstructural information to the finite element analysis (fea). both mechanical behavior and microstructure evolution are obtained from the simulation. the evolution of microstructure features is quantified using the stochastic microstructure characterization methods. the proposed method successfully captures the anisotropic behavior of the separator under tensile test, and provides insights into the microstructure deformation, such as the growth of voids. we apply the proposed method to a commercially available separator as the demonstration. the analysis results are validated using experimental testing results that are reported in literature. (c) 2017 elsevier b.v. all rights reserved.
cryptography	mobile cloud computing (mcc) combines the features of mobile computing, cloud computing, and wireless networks to create the healthy computational resources to mobile cloud users. the aim of mcc is to execute the highly attractive mobile applications on a plethora of mobile cellular telephones, with highly rich user experience. from the perspective of mobile computing, quality of service (qos) provisioning depends on the efficiency of the handoff process. thus, it is highly important to introduce an energy efficient and secure handoff process to improve the performance. in this paper, we propose a secure seamless fast handoff (ssfh) scheme to improve the energy efficiency and the qos in the mcc. the proposed scheme consists of four layers: application layer, service layer, infrastructure layer, and media layer. these four layers collectively handle the security, energy-efficiency, and the qos. existing service-oriented architectures designed for the mcc are based on the symmetric encryption protocols to support the application layer. however, it is much easier for an adversary to expose the symmetric key and gain access to the confidential data. the application layer is secured using a combination of both attribute-based encryption and an asymmetric encryption cryptography. to extend the mobile lifetime, energy detection (ed) model is deployed at the infrastructure layer to detect the energy level of the mobile devices prior to the pre-registration process. furthermore, a dual authentication process is performed on the service and at the application layer to minimize the possibility of identity high jacked or impersonation attack. the media layer supports the secure handoff process using policy enforcement module that allows only legitimate users to complete the re registration process after initiating the handoff. thus, a significant amount of the bandwidth and energy could be preserved. finally, the secure service-oriented architecture is programmed using c++ platform and the results are compared with other well-known existing service-oriented architectures. the experimental results confirm the validity and the effectiveness of our proposed architecture. (c) 2017 elsevier ltd. all rights reserved.
symbolic_computation	the integrability and multi-shock wave solutions of the djkm equation are studied by means of bell polynomials scheme, hirota bilinear method, and symbolic computation. a more generalized bilinear system of the djkm equation is constructed via bell polynomials scheme. moreover, lax pair and infinite conservation laws of this equation are first obtained via its corresponding bell-polynomials-type backlund transformation. furthermore, the multi-shock wave solutions are also obtained by applying standard hirota bilinear method, and the propagation and collision of shock waves are graphically demonstrated by graphs.
distributed_computing	mapreduce is the most widely used distributed computing framework due to its excellent parallelism and scalability in dealing with large-scale data. it is one of the most important research point in distributed computing field to improve the performance of mapreduce application in datacenter network. openflow protocol makes it possible to schedule network resource dynamically to provide better link bandwidth for shuffle traffic. current openflow-based scheduling method runs on a single controller, which cannot meet the needs of excessive switch requests in large scale data center networks. the performance of those scheduling method will decrease obviously due to some conflict problem when they run on distributed controllers. this paper proposed dscheduler, a dynamic network scheduling method for distributed controllers. dscheduler is running as an application on each sdn controller and avoid a majority of conflict problems in scheduling with small cost by using lock and communication between each controller. we implement a prototype system on floodlight to demonstrate our design and test the performance. experimental results show that dscheduler has a significant effect on decreasing the occurrence times of conflict situations and improving the performance of openflow-based scheduling method on distributed controllers.
software_engineering	evolution of systems during their operational life is mandatory and both updates and upgrades should not impair their dependability properties. dependable systems must evolve to accommodate changes, such as new threats and undesirable events, application updates or variations in available resources. a system that remains dependable when facing changes is called resilient. in this paper, we present an innovative approach taking advantage of component-based software engineering technologies for tackling the online adaptation of fault tolerance mechanisms. we propose a development process that relies on two key factors: designing fault tolerance mechanisms for adaptation and leveraging a reflective component based middleware enabling fine-grained control and modification of the software architecture at runtime. we thoroughly describe the methodology, the development of adaptive fault tolerance mechanisms and evaluate the approach in terms of performance and agility. (c) 2017 elsevier b.v. all rights reserved.
distributed_computing	there is a strong relationship between scientific research and technology advancement. the former generally focuses on studying phenomena happening in the real world, the latter improves tools that are at the basis of this research. from this perspective, information and communication technologies allow the implementation of ever faster tools for analyzing data generated by experiments. the aim of demogrape project is to study the interaction of the upper earth atmosphere and the gnss ( global navigation satellite systems) signals received at ground, in critical environments such as the polar regions. this paper describes the ict infrastructure used to manage the software applications used to analyzed data collected during project experimental campaigns, taking into account the following constraints: (i) the intellectual property of the applications must be protected; (ii) the underlying infrastructure resembles a cloud-federation; (iii) data are over-sized; (iv) provide a unified vision of the available resources to the user (i.e., what are the available applications, and where experimental data reside). leveraging on a lightweight virtualization system, we proposed a management system that copes with all these four constraints. a case study is used to show the process of deploying an application through the proposed system on a specific node where data of interest reside.
machine_learning	emerging technologies are often not part of any official industry, patent or trademark classification systems. thus, delineating boundaries to measure their early development stage is a nontrivial task. this paper is aimed to present a methodology to automatically classify patents concerning service robots. we introduce a synergy of a traditional technology identification process, namely keyword extraction and verification by an expert community, with a machine learning algorithm. the result is a novel possibility to allocate patents which (1) reduces expert bias regarding vested interests on lexical query methods, (2) avoids problems with citation approaches, and (3) facilitates evolutionary changes. based upon a small core set of worldwide service robotics patent applications, we derive apt n-gram frequency vectors and train a support vector machine, relying only on titles, abstracts, and ipc categorization of each document. altering the utilized kernel functions and respective parameters, we reach a recall level of 83% and precision level of 85%.
network_security	there is an enormous growth of industrial applications using internet communication. secure network is a prime objective for the survival of any organization. network monitoring and defence systems have become an integral part of network security for identifying and preventing potential attacks. intrusion detection and prevention systems (idps) are network based defence systems which combines intrusion detection system (ids) and a firewall. in contrast to ids, idps is a proactive technique which provides both quick reactions to potential threats and attacks in a network as well as preventing the attacks from entering the network. current generation idps have their limitations on their performance and effectiveness. some studies have proven that the modern idps have difficulties in dealing with high-speed network traffic. meeting the current network requirements there exist several research approaches to find an efficient idps. nevertheless, serious security and privacy breaches still occur every day, creating an absolute necessity to provide secure and safe information security systems. this survey provides an up-to-date comprehensive review on state of the art of idps based on different accelerating techniques, different detection algorithms, types of hardware and optimizing algorithms to match the demand requirements of high speed network. a detailed overview on high performance ids and idps along with pros and cons of individual techniques will be given. this paper also highlights and discusses the requirement for developing a new idps to detect the known and unknown threats.
computer_graphics	this paper presents analysis on two 3d mesh to 2d map strategies applied to unwrap images of rock tunnels and facilitate visualization of large datasets. first, we examined mesh parameterization algorithms which are used in computer graphics to convert a 3d mesh model to a 2d representation. we found that while these methods were automatic and could provide 2d maps with minimal metric distortion (ie: conservation of lengths in 3d when mapped to 2d), they exhibited twisted shapes and were not intuitive to interpret. second, we proposed two novel approaches, combining mesh deformation algorithms, which are used in computer animation to reshape a 3d mesh to resemble a 3d plane, and projection onto a 2d plane. we found that while these methods required user interaction and introduced a greater amount of metric distortion, their outputs were fairly intuitive to interpret. to compare the relative merits of mesh parameterization and mesh deformation and projection, the different strategies are applied to a 8.2 m wide by 41 m long by 6.7 m high subsection of a mining tunnel. the metric distortion produced was calculated and their respective output 20 maps are presented and discussed. (c) 2016 elsevier ltd. all rights reserved.
computer_graphics	reversible logic has various applications in fields of computer graphics, optical information processing, quantum computing, dna computing, ultra low power cmos design and communication. as our day to day life is demanding more and more portable electronic devices, challenging focus on technology is demanding great system performance without any compromise in power consumption. it is obvious to find tradeoff between processing power and heat generation. as decreased processing speed leads to reduced power consumption but obviously compromise in performance is not acceptable for sophisticated applications. thus power consumption is a prime target now days. needless to say, researchers will now look at reversible logic in this vein. primitive component of reversible logic synthesis are reversible logic gates. thus it is very important for a new researcher to look into extensive literature survey of reversible logic gates. many papers have been reported with review of reversible logic gates. this paper aims on updates in reversible logic gates which are stepping stones in design and synthesis of any complex reversible logic based synthesis.
image_processing	background and objective: the manual transformation of dna fingerprints of dominant markers into the input of tools for population genetics analysis is a time-consuming and error-prone task; especially when the researcher deals with a large number of samples. in addition, when the researcher needs to use several tools for population genetics analysis, the situation worsens due to the incompatibility of data formats across tools. the goal of this work consists in automating, from banding patterns of gel images, the input-generation for the great diversity of tools devoted to population genetics analysis. methods: after a thorough analysis of tools for population genetics analysis with dominant markers, and tools for working with phylogenetic trees; we have detected the input requirements of those systems. in the case of programs devoted to phylogenetic trees, the newick and nexus formats are widely employed; whereas, each population genetics analysis tool uses its own specific format. in order to handle such a diversity of formats in the latter case, we have developed a new xml format, called popxml, that takes into account the variety of information required by each population genetics analysis tool. moreover, the acquired knowledge has been incorporated into the pipeline of the ceti system - a tool for analysing dna fingerprint gel images - to reach our automatisation goal. results: we have implemented, in the gel.j system, a pipeline that automatically generates, from gel banding patterns, the input of tools for population genetics analysis and phylogenetic trees. such a pipeline has been employed to successfully generate, from thousands of banding patterns, the input of 29 population genetics analysis tools and 32 tools for managing phylogenetic trees. conclusions: gelj has become the first tool that fills the gap between gel image processing software and population genetics analysis with dominant markers, phylogenetic reconstruction, and tree editing software. this has been achieved by automating the process of generating the input for the latter software from gel banding patterns processed by gelj. (c) 2016 elsevier ireland ltd. all rights reserved.
network_security	as networks applications are growing fast, data are more generated and transmitted, leaving it vulnerable to modification. besides that, the significance, sensitivity and preciseness of that information cause a big security issue, increasing the needs to keep it safe. one proper solution for this problem is cryptography, which is a technique used to transform data to unrecognizable information and useless to any unauthorized person. by being the main core of network security and since new attacks techniques are daily invented, ciphers are constantly under test by cryptanalysis attacks to harden their safety. this paper contains a fair comparison between serval ciphers in speed test under different platforms, also in performance and risk to globalize the vision about the most fast safe algorithm.
software_engineering	context: component-based software engineering is aimed at managing the complexity of large-scale software development by composing systems from reusable parts. to understand or validate the behavior of such a system, one needs to understand the components involved in combination with understanding how they are configured and composed. this becomes increasingly difficult when components are implemented in various programming languages, and composition is specified in external artifacts. moreover, tooling that supports in-depth system-wide analysis of such heterogeneous systems is lacking. objective: this paper contributes a method to analyze and visualize information flow in a component based system at various levels of abstraction. these visualizations are designed to support the comprehension needs of both safety domain experts and software developers for, respectively, certification and evolution of safety-critical cyber-physical systems. method: we build system-wide dependence graphs and use static program slicing to determine all possible end-to-end information flows through and across a system 's components. we define a hierarchy of five abstractions over these information flows that reduce visual distraction and cognitive overload, while satisfying the users' information needs. we improve on our earlier work to provide interconnected views that support both systematic, as well as opportunistic navigation scenarios. results: we discuss the design and implementation of our approach and the resulting views in a prototype tool called flowtracker. we summarize the results of a qualitative evaluation study, carried out via two rounds of interview, on the effectiveness and usability of these views. we discuss a number of improvements, such as more selective information presentations, that resulted from the evaluation. conclusion: the evaluation shows that the proposed approach and views are useful for understanding and validating heterogeneous component-based systems, and address information needs that could earlier only be met by manual inspection of the source code. we discuss lessons learned and directions for future work. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	with the proliferation of application specific accelerators, the use of heterogeneous clusters is rapidly increasing. consisting of processors with different architectures, a heterogeneous cluster aims at providing different performance and cost tradeoffs for different types of workloads. in order to achieve peak performance, software running on heterogeneous cluster needs to be designed carefully to provide enough flexibility to explore its variety. we propose a design methodology to modularize complex software applications with data dependencies. the software application designed in this way have the flexibility to be reconfigured for different hardware platforms to facilitate resource management, and features high scalability and parallelism. using a neuromorphic application as a case study, we present the concept of modularization and discuss the management, scheduling and communication of the modules. we also present experimental results demonstrating the improvements and effects of system scaling on throughput.
relational_databases	dbmask is a system that implements encrypted query processing with support for complex queries and fine grained access control with create, update, delete and cryptographically enforced read (crud) operations for data stored on an untrusted database server hosted in a public cloud. past research efforts have not adequately addressed flexible access control on encrypted data at different granularity levels which is critical for data sharing among different users and applications. dbmask proposes a novel technique that separates fine grained access control from encrypted query processing when evaluating sql queries on encrypted data and enforces fine grained access control at the granularity level of a column, row and cell based on an expressive attribute-based group key encryption scheme. dbmask does not require modifications to the database engine, and thus maximizes the reuse of the existing dbms infrastructures. our experiments evaluate the performance of an encrypted database, managed by dbmask, using queries from tpc-h benchmark in comparison to plain-text postgres. we further evaluate the functionality of our prototype using a policy simulator and a multi-user web application. the results show that dbmask is efficient and scalable to large datasets.
network_security	honeypots and honeynets are popular tools in the area of network security and network forensics. the deployment and usage of these tools are influenced by a number of technical and legal issues, which need to be carefully considered. in this paper, we outline the privacy issues of honeypots and honeynets with respect to their technical aspects. the paper discusses the legal framework of privacy and legal grounds to data processing. we also discuss the ip address, because by eu law, it is considered personal data. the analysis of legal issues is based on eu law and is supported by discussions on privacy and related issues.
relational_databases	recently, there has been a renovated interest in functional dependencies due to the possibility of employing them in several advanced database operations, such as data cleaning, query relaxation, record matching, and so forth. in particular, the constraints defined for canonical functional dependencies have been relaxed to capture inconsistencies in real data, patterns of semantically related data, or semantic relationships in complex data types. in this paper, we have surveyed 35 of such functional dependencies, providing a classification criteria, motivating examples, and a systematic analysis of them.
symbolic_computation	in this paper, we employ the extended variable-coefficient homogeneous balance method (evchb), the painlev,-backlund transformation and the simplified hirota 's method to derive auto-backlund transformation, multiple soliton solutions and multiple singular soliton solutions of the (3+1)-dimensional potential-ytsf equation. we also obtain a variety of traveling wave solutions, soliton-type solutions and rational solutions of distinct physical structures.
machine_learning	the present paper proposes a novel cluster-basedmethod, named as agglomerative concentric hypersphere (ach), to detect structural damage in engineering structures. continuous structural monitoring systems often require unsupervised approaches to automatically infer the health condition of a structure. however, when a structure is under linear and nonlinear effects caused by environmental and operational variability, data normalization procedures are also required to overcome these effects. the proposed approach aims, through a straightforward clustering procedure, to discover automatically the optimal number of clusters, representing the main state conditions of a structural system. three initialization procedures are introduced to evaluate the impact of deterministic and stochastic initializations on the performance of this approach. the ach is compared to state-of-the-art approaches, based on gaussian mixture models and mahalanobis squared distance, on standard data sets from a post-tensioned bridge located in switzerland: the z-24 bridge. the proposed approach demonstrates more efficiency in modeling the normal condition of the structure and its corresponding main clusters. furthermore, it reveals a better classification performance than the alternative ones in terms of false-positive and false-negative indications of damage, demonstrating a promising applicability in real world structural health monitoring scenarios.(c) 2017 elsevier ltd. all rights reserved.
parallel_computing	this paper presents a distributed computing architecture for solving a distribution optimal power flow (dopf) model based on a smart grid communication middleware (sgcm) system. the system is modeled as an unbalanced three-phase distribution system, which includes different kind of loads and various components of distribution systems. in this paper, fixed loads are modeled as constant impedance, current and power loads, and neural network models of controllable smart loads are integrated into the dopf model. a genetic algorithm is used to determine the optimal solutions for controllable devices, in particular load tap changers, switched capacitors, and smart loads in the context of an energy management system for practical feeders, accounting for the fact that smart loads consumption should not be significantly affected by network constraints. since the number of control variables in a realistic distribution power system is large, solving the dopf for real-time applications is computationally expensive. hence, to reduce computational times, a decentralized system with parallel computing nodes based on an sgcm system is proposed. using a ""mapreduce"" model, the sgcm system runs the dopf model, communicates between master and worker computing nodes, and sends/receives data among different parts of parallel computing system. compared to a centralized approach, the proposed architecture is shown to yield better optimal solutions in terms of reducing energy losses and/or energy drawn from the substation within adequate practical run-times for a realistic test feeder.
structured_storage	in face of high partial and complete disk failure rates and untimely system crashes, the executions of low-priority background tasks become increasingly frequent in large-scale data centers. however, the existing algorithms are all reactive optimizations and only exploit the temporal locality of workloads to reduce the user i/o requests during the low-priority background tasks. to address the problem, this paper proposes intelligent data outsourcing (ido), a zone-based and proactive data migration optimization, to significantly improve the efficiency of the low-priority background tasks. the main idea of ido is to proactively identify the hot data zones of raid-structured storage systems in the normal operational state. by leveraging the prediction tools to identify the upcoming events, ido proactively migrates the data blocks belonging to the hot data zones on the degraded device to a surrogate raid set in the large-scale data centers. upon a disk failure or crash reboot, most user i/o requests addressed to the degraded raid set can be serviced directly by the surrogate raid set rather than the much slower degraded raid set. consequently, the performance of the background tasks and user i/o performance during the background tasks are improved simultaneously. our lightweight prototype implementation of ido and extensive trace-driven experiments on two case studies demonstrate that, compared with the existing state-of-the-art approaches, ido effectively improves the performance of the low-priority background tasks. moreover, ido is portable and can be easily incorporated into any existing algorithms for raid-structured storage systems.
cryptography	in wireless sensor networks (wsns), a large number of nodes are densely deployed in an open environment to gather some useful required information. these nodes are small in size, operating on limited processing capabilities with scarce working memory and battery life and not very powerful radio transceivers. they can only communicate with each other through wireless media. radio waves are insecure in nature; therefore, by using such waves for communication there are always opportunities for different attacks on the network. most wireless techniques are founded on the cluster-based sensor network. forwarding cluster head 's (chs) data in a secure manner is very important because chs collect data from the cluster members and send it to the sink node or base station. for securing ch 's data, we propose a mechanism termed icmds (inter-cluster multiple key distribution scheme for wireless sensor networks), which enables the securing of the entire network. in icmds, we use two phases of security implementations for the sensor node 's authenticity while communicating with the ch. a recovery phenomena is also stated at the time when a ch ceases to function due to its high energy consumption. (c) 2016 elsevier b.v. all rights reserved.
computer_programming	normal flow depth is an important parameter in design of open channels and analysis of gradually varied flow. in open channels with parabolic and rectangular cross-sections, the governing equations are nonlinear in terms of the normal depth and thus solution of the implicit equations involves numerical methods. in current research explicit solutions for these channels have been obtained using asymptote matching technique. for the parabolic channel, the maximum error of proposed equation for normal depth is less than 0.07% (near exact solution). but, in rectangular channels, the maximum error of proposed equation for normal depth is less than 1.94% which is not very accurate. the efficiency of the asymptote matching technique can be considerably improved by adding a power-law function between two asymptotes. for rectangular channel a new solution for normal flow depth is developed using the improved asymptote matching technique proposed in this research. the maximum error of this full range solution is less than 0.12%. the results showed that the improvement in proposed solution is substantial. proposed full range solutions have definite physical concept, high accuracy and easy calculation and are well-suited for manual calculations and computer programming. (c) 2015 elsevier ltd. all rights reserved.
operating_systems	control applications are implemented using real-time operating systems. digital control theory is based on sampling intervals that have to be strictly met in order to get predictable behaviors. however, a real-time system may introduce execution jitters that may cause unpredictable effects on the control application. stability, overshoot and settling time may be affected when an inadequate real-time system is used. several papers have proposed different mechanisms to measure the jitter that a real time system produces. however, jitter can not be translated as a performance criterion in control theory. on the other hand, frequency domain techniques are widely applied in control theory for designing control strategies, as well as analyzing and measuring the performance of control mechanisms. in this paper, frequency domain analysis is used to measure the perturbations that a real-time operating system may produce on a control application. harmonic distortion is defined as a criterion to evaluate the control performance of the the application. experiments show that higher priority tasks are likely to be used for control tasks since the perturbation produced up to an utilization factor of 70% is adequate for most control applications.
parallel_computing	this paper describes a learning parallel constraint programming (cp) solver designed for solving cp problems with several instances on massively parallel computing platforms comprising multi-core parallel machines or many integrated cores. the cp solver proposed in this work is based on a portfolio parallelization that employs a linear reward inaction learning algorithm in order to obtain the best possible performance for a large set of instances of the same problem. the linear reward inaction algorithm enables the prediction of the number of cores to be assigned to each search strategy based on previous experiments, reducing the computing time required to solve constraint satisfaction and optimization problems. the underlying principle of the portfolio approach is to run n sequential search strategies using n computing cores (n to n portfolio) where each core uses its own strategy in order to perform a search that is different from strategies used by the other cores. the first strategy that finds a solution stops all other strategies. the problem with the n to n portfolio approach is that the number of search strategies is very small compared with the current number of computing cores used by the parallel machines. however, using an internal parallelization for each search strategy, it is possible to run n parallel search using p computing cores with p >>n (n to p portfolio). this n to p portfolio performs suboptimally for solving different cp problems because many computing resources are wasted. to improve this portfolio model, an adaptive n to p portfolio was proposed, which tries to privilege the strategy that is most likely to find a solution first in order to give it more computing cores than the other strategies. however, the main problem with the adaptive portfolio is that it loses all the learned information at the end of each search; it is designed to solve just one cp problem. furthermore, many computational resources are wasted by both portfolio solvers, the non-adaptive (n to n and n to p) and the adaptive n to p portfolio, when employed in some industrial projects, such as the pajero project, which always solves different instances of the same cp problem. to minimize the amount of wasted resources and to learn the most efficient search strategies, we propose a new learning portfolio solver that uses a learning algorithm that configures automatically number of cores to each search. the performance obtained using the different portfolio solvers is compared and illustrated by solving cp problems using as example the google or-tools solver. copyright (c) 2016 john wiley & sons, ltd.
distributed_computing	distributed systems for big data management very often face the problem of load imbalance among nodes. to address this issue, there exist almost as many load balancing strategies as there are different systems. when designing a scalable distributed system geared towards handling large amounts of information, it is often not so easy to anticipate which kind of strategy will be the most efficient to maintain adequate performance regarding response time, scalability, and reliability at any time. based on this observation, we describe a generic api to implement and experiment any strategy independently from the rest of the code, prior to a definitive choice for instance. we then show how existing load balancing strategies used by famous systems could be implemented with this api. we also present how this work has helped us implement load balancing on our distributed system and modify the behavior of our strategy in a few lines of code. this led us to easily perform various experiments to determine the most efficient scheme for our system. this paper is an extension to our work presented at workshop on parallel and distributed computing for big data applications (wpba) 2014. we detail here more experiments and extend the use of the api to a broad class of big data storage systems. copyright (c) 2015 john wiley & sons, ltd.
distributed_computing	in mapreduce environment, problem of data redundancy, a large number of tasks to be processing and mass data storage come up, in order to solve these problems, we put forward the way of data prefetching, preprocessing of hid the remote data access latency, by adjusting the allocation of resources to reduce the business, we put forward the method of than before, caused by the reduce tasks of remote data access performance problems caused by time delay and resource competition system, the hidden by prefetching data the method reduce task of remote data access latency, and reduce task control through the resource allocation, to reduce resource competition caused the reduce task, the experiment results show that with the default hadoop graphs and hadoop online prototype (hop), compared with the method the system performance can be improved by more than 10%.
software_engineering	knowledge management within companies and institutions is one of the main problems when defining and developing new solutions to improve information systems. currently, information systems have evolved into technological ecosystems that are a set of different components related to each other through information flows in a physical environment that supports these flows, where users are part of the ecosystem. particularly, during the last several years, the open source software has been used to develop these technological solutions. the aim of this paper is to formalize an architectural pattern to support the right description and implementation of elearning ecosystems. the theoretical basis has been provided in previous works by using a comparative analysis of the strengths, weaknesses, opportunities and threats of several real case studies developed in different contexts. the problems detected through the previous analysis have been modelled using the business process model and notation. next, the pattern has been formalized with the service oriented architecture modelling language. finally, the pattern has been tested in a real context, namely, an institutional development for the spanish public administration, which has demonstrated that the pattern works properly. as a result, we have obtained a set of business process model and notation diagrams that provide a high abstraction level for the main detected problems in elearning ecosystems. we have also obtained an architectural pattern composed of several layers and a set of external elements that provides a solution to these problems. (c) 2016 elsevier b.v. all rights reserved.
network_security	the failure on all homogeneous devices due to the same reason is called homogeneous fault in networks. in contrast, heterogeneous platforms deployed simultaneously in the network are more robust against homogeneous faults. one of the challenging problems is how to design survivable networks that against homogeneous faults. this paper utilizes edge-colored graphs to investigate the network topology with homogeneous faults, in order to guarantee network connectivity using minimum number of links. two types of network topologies are proposed on the edge-colored graph. one type of networks is characterized by the fact that all the edges of the same color form a hamiltonian path or a hamiltonian cycle. an upper bound on the number of colors used in the proposed network topologies is obtained. the network topologies of the second type have edges colored with at most five colors. additionally, the subnetworks induced by the edges of two colors contain a hamiltonian path, or a hamilton cycle in some cases.
computer_programming	a research has been carried out on the dynamic analysis and simulation of parallel robots due to the situation where parallel robots tend to break down on account of large stress during movements. in order to study the force condition of the robot on the move, a mathematical model, based on the kane method, has been built and the driving force of each electric cylinder has been calculated by computer programming. in the meantime, a dynamic simulation model and a simulation control system are established on the basis of adams and matlab and through analyzing the joint simulation, we can obtain the driving force of each cylinder. the gap between the simulation results and the results obtained by the mathematical model is within 7%, which verifies the correctness of the kane dynamic system and gives provide the foundation for future studies of dynamics and structure optimization.
parallel_computing	this paper presents the features and functions of a software tool called fatigue prediction utility for abaqus/cae (fpu). it is designed as a plugin for the abaqus commercial fe code allowing for fatigue predictions based on the results of abaqus fe analyses. it also contains an interface for data transfer between abaqus and pragtic, a standalone fatigue post-processor. the program contains a simple and intuitive graphical interface, which makes preparation of a fatigue analysis straightforward. the fpu fatigue solver may be executed on multiple cpu cores independently from abaqus/cae, which allows large-scale problems to be solved on server machines in reasonable time. an overview of the functionality of the plugin, available prediction models and the concept of data exchange between abaqus and pragtic is given. illustrative examples are given, where appropriate, in order to introduce features of the tool. possible ways of adapting the code for interconnection with an fe solver other than abaqus are discussed. (c) 2016 elsevier ltd. all rights reserved.
algorithm_design	overlapping community detection algorithm research is one of hot topics in current social network analysis. in this paper, we applied the idea of weighted label propagation to overlapping community detection algorithm design, and propose a weighted label propagation algorithm (wlpa). moreover, in order to evaluate the performance results of various overlapping community detection algorithms, we put forward a series of evaluation criteria based on error distribution curve of overlapping vertices. the experiment results show that the algorithm has a faster speed and better community detection results, and the evaluation criteria is in line with the inherent characteristics of the social network overlapping community structure.
software_engineering	this paper presents a formal model of a decision making system for public transport routes. the approach focuses on (1) environmental and societal sustainability aspects of green software engineering, (2) spatial planning and optimisation for smarter sustainable cities, and (3) user satisfaction with this information system for the various contexts of passenger, driver and overall system view.
algorithm_design	automated systems based on programmable logic controllers (plc) are still applied in discrete event systems (des) for controlling and monitoring of industrial processes signals. plc-based control systems are characterized for having physical input and output signals coming from and going to sensors and actuators, respectively, which they are in direct contact with the production or manufacturing process. the input subsystem to plc consists of sensor-wiring-physical inputs module, and it can present two kinds of faults: short circuit or open circuit, in one or more signals of the process physical inputs, which it causes faults in the control and/or in the control algorithms behavior. ladder diagram (ld) is one of the five programming languages supported by the international electrotechnical commission (iec) through the iec-61131-3 standard, and it remains being used at industry for control algorithm design of plc-based systems. this paper proposes the simulation and validation of control algorithms developed in ld by using petri nets (pn) in order to deal with the possible fault options (short circuit and/or open circuit) in the physical inputs subsystem of a plc-based control system. one control algorithms in ld have been analyzed in order to show the advantages of the proposed approach.
cryptography	we explain how to share photons between two distant parties using concatenated entanglement swapping and assess performance according to the two-photon visibility as the figure of merit. from this analysis, we readily see the key generation rate and the quantum bit error rate as figures of merit for this scheme applied to quantum key distribution (qkd). our model accounts for practical limitations, including higher-order photon pair events, dark counts, detector inefficiency, and photon losses. our analysis shows that compromises are needed among the runtimes for the experiment, the rate of producing photon pairs, and the choice of detector efficiency. from our quantitative results, we observe that concatenated entanglement swapping enables secure qkd over long distances but at key generation rates that are far too low to be useful for large separations. we find that the key generation rates are close to both the takeoka-guha-wilde and the pirandola-laurenza-ottaviani-banchi bounds. (c) 2017 society of photo-optical instrumentation engineers (spie)
image_processing	countercurrent flow limitation (ccfl) was experimentally investigated in a 1/3.9 downscaled collider facility with a 190 mm pipe 's diameter using air/water at 1 atmospheric pressure. previous investigations provided knowledge over the onset of ccfl mechanisms. in current article, ccfl characteristics at the collider facility are measured and discussed along with time-averaged distributions of the air/water interface for a selected matrix of liquid/gas velocities. the article demonstrates the time-averaged interface as a useful method to identify ccfl characteristics at quasi-stationary flow conditions eliminating variations that appears in single images, and showing essential comparative flow features such as: the degree of restriction at the bend, the extension and the intensity of the two-phase mixing zones, and the average water level within the horizontal part and the steam generator. consequently, making it possible to compare interface distributions obtained at different investigations. the distributions are also beneficial for cfd validations of ccfl as the instant chaotic gas/liquid interface is impossible to reproduce in cfd simulations. the current study shows that final ccfl characteristics curve (and the corresponding ccfl correlation) depends upon the covered measuring range of water delivery. it also shows that a hydraulic diameter should be sufficiently larger than 50 mm in order to obtain ccfl characteristics comparable to the 1:1 scale data (namely the uptf data). finally, the study shows that the change of the flow condition inside the hot-leg is not only related to the water and air inlet velocities, but is also dependent upon the existent interface distribution within the hot-leg, and that several ccfl cases of identical inlet flow conditions can exist with different interface distribution and pressure difference. the last result is of a special importance to the investigation of this phenomenon during sbloca accidents, since the entire phenomenon is driven by pressure difference between the steam generator and reactor vessel, as well as by gravity. this result show also that ccfl characteristics cannot be investigated using 1d codes, as the interface distribution within the hot-leg during a sbloca accident will depend upon flow history or previous interface distribution. current investigations support the effort to provide more knowledge over ccfl in order to extrapolate results obtained in downscaled models into the 1:1 scale. (c) 2017 elsevier ltd. all rights reserved.
distributed_computing	in many application fields such as social networks, e-commerce and content delivery networks there is a constant production of big amounts of data in geographically distributed sites that need to be timely elaborated. distributed computing frameworks such as hadoop (based on the mapreduce paradigm) have been used to process big data by exploiting the computing power of many cluster nodes interconnected through high speed links. unfortunately, hadoop was proved to perform very poorly in the just mentioned scenario. we designed and developed a hadoop framework that is capable of scheduling and distributing hadoop tasks among geographically distant sites in a way that optimizes the overall job performance. we propose a hierarchical approach where a top-level entity, by exploiting the information concerning the data location, is capable of producing a smart schedule of low-level, independent mapreduce sub-jobs. a software prototype of the framework was developed. tests run on the prototype showed that the job scheduler makes good forecasts of the expected job 's execution time.
distributed_computing	it is now several years since scientists in poland can use the resources of the distributed computing infrastructure - plgrid. it is a flexible, large-scale e-infrastructure, which offers a homogeneous, easy to use access to organizationally distributed, heterogeneous hardware and software resources. it is built in accordance with good organizational and engineering practices, taking advantage of international experience in this field. since the scientists need assistance and close collaboration with service providers, the e-infrastructure is relied on users' requirements and needs coming from different scientific disciplines, being equipped with specific environments, solutions and services, suitable for various disciplines. all these tools help to lowering the barriers that hinder researchers to use the infrastructure.
computer_graphics	filters with slowly decaying impulse responses have many uses in computer graphics. recursive filters are often the fastest option for such cases. in this paper, we derive closed-form formulas for computing the exact initial feedbacks needed for recursive filtering infinite input extensions. we provide formulas for the constant-padding (e.g. clamp-to-edge), periodic (repeat) and even-periodic (mirror or reflect) extensions. these formulas were designed for easy integration into modern block-parallel recursive filtering algorithms. our new modified algorithms are state-of-the-art, filtering images faster even than previous methods that ignore boundary conditions.
computer_vision	one of the first steps in numerous computer vision tasks is the extraction of keypoints in images. despite the large number of works proposing image keypoint detectors, only a few methodologies are able to efficiently use both visual and geometrical information. in this work we introduce kvd (keypoints from visual and depth data), a novel keypoint detector which is scale invariant and combines intensity and geometrical data using a decision tree. we present results from several experiments showing that the detector created with our methodology outperforms state-of-the-art methods, both in repeatability scores for rotations, translations and scale changes, as well as in robustness to corrupted visual or geometric data. additionally, as far as processing time is concerned, kvd yields the best time performance among the methods that also use depth and visual data. (c) 2016 elsevier b.v. all rights reserved.
software_engineering	with over 10 million git repositories, github is becoming one of the most important sources of software artifacts on the internet. researchers mine the information stored in github 's event logs to understand how its users employ the site to collaborate on software, but so far there have been no studies describing the quality and properties of the available github data. we document the results of an empirical study aimed at understanding the characteristics of the repositories and users in github; we see how users take advantage of github 's main features and how their activity is tracked on github and related datasets to point out misalignment between the real and mined data. our results indicate that while github is a rich source of data on software development, mining github for research purposes should take various potential perils into consideration. for example, we show that the majority of the projects are personal and inactive, and that almost 40 % of all pull requests do not appear as merged even though they were. also, approximately half of github 's registered users do not have public activity, while the activity of github users in repositories is not always easy to pinpoint. we use our identified perils to see if they can pose validity threats; we review selected papers from the msr 2014 mining challenge and see if there are potential impacts to consider. we provide a set of recommendations for software engineering researchers on how to approach the data in github.
relational_databases	one of the biggest challenges to health data sharing is regulations that prohibit the transmission and distribution of personal health information (phi) even among collaborating organizations. this impedes research and reduces the utility of these datasets. anonymization can address this issue by hiding phi while maintaining the analytical utility of the data. much research has focused on data that is static, independent and complete. unfortunately, this is not typical of health data. instead of static, independent tables, health data is in relational databases with multiple high-dimensional tables that are transactional and constantly changing. data recipients usually receive multiple versions of the database over time. this study reviews literature on anonymization methodologies for large and fast changing high-dimensional datasets, especially health data. relevant papers are analyzed, categorized and compared in terms of scope, and contributions. finally, we used the extracted details from our analysis to outline possible research direction for developing a realistic anonymization framework for health data sharing. (c) 2015 the authors. published by elsevier b.v.
bioinformatics	background: japanese encephalitis virus (jev) is a mosquito-borne flavivirus that causes japanese encephalitis (je) and acute encephalitis syndrome (aes) in humans. genotype-i (as co-circulating cases with genotype-iii) was isolated in 2010 (jev28, jev21) and then in 2011 (jev45) from midnapur district, west bengal (wb) for the first time from clinical patients who were previously been vaccinated with live attenuated sa14-14-2 strain. we apply bioinformatics and immunoinformatics on sequence and structure of e protein for analysis of crucial substitutions that might cause the genotypic transition, affecting protein-function and altering specificity of epitopes. results: although frequency of substitutions in e glycoprotein of jev28, jev21 and jev45 isolates vary, its homologous patterns remain exactly similar as earlier japan isolate (ishikawa). sequence and 3d model-structure based analyses of e protein show that only four of all substitutions are critical for genotype-i specific effect of which n103k is common among all isolates indicating its role in the transition of genotype-iii to genotype-i. predicted b-cell and t-cell epitopes are seen to harbor these critical substitutions that affect overall conformational stability of the protein. these epitopes were subjected to conservation analyses using a large set of the protein from asian continent. conclusions: the study identifies crucial substitutions that contribute to the emergence of genotype-i. predicted epitopes harboring these substitutions may alter specificity which might be the reason of reported failure of vaccine. conservation analysis of these epitopes would be useful for design of genotype-i specific vaccine.
distributed_computing	the ability to design effective solutions using parallel processing should be a required competency for every computing student. however, teaching parallel concepts is sometimes challenging and costly, specially at early stages of a computer science degree. for such reasons we present a set of modules to teach parallel computing paradigms using as examples problems that are computationally intensive, but easy to understand and can be easily implemented using the python parallelization libraries mpi for python and disco.
computer_graphics	the article describes efficient methods to visualize the results from finite element analysis and implementation of these methods in post-processing results. the work is based on premise that computer memory and performance are limited and amount of data processed by complex finite element analysis is enormous. therefore, some kind of simplification and approximation of resulting data has to be used. multigrid method was the inspiration for research work and development of post-processor. the stored data from finite element analysis are discrete values. the paper deals with several ways of replacing them by continuous functions suitable for representation in computer graphics, which are different from the approximation functions used in finite element method. special attention is devoted to approximation errors-difference between these functions. finite element mesh is decomposed into sub domains with respect to approximation errors. the ways of creating mesh hierarchy are described in details and also the possibilities of nodal value interpolations in simplified mesh are discussed in the text. besides the approximation of data in space, also the approximation in time is used. pseudo-code of the approximation algorithm key parts is shown. various types of approximation functions were investigated to reach the lowest approximation error and the highest compression factor. results are summarized in the article. (c) 2016 elsevier ltd. all rights reserved.
distributed_computing	host cardinality is defined as the number of distinct peers that a host communicates with in the network. there have been several algorithms proposed to monitor network traffic and identify high-cardinality hosts at a centralized network operation center (noc). due to massive amounts of distributed data and limitations on transforming and processing them at the noc, it is desirable to design mergable and reversible data structures summarizing traffic measurements in a distributed network monitoring system. a mergable data structure summarizes traffic measurements at each local monitor, and these summaries from different monitors can be merged at the noc, while preserving the error guarantee without increasing space. a reversible data structure can report interested (high-cardinality) hosts efficiently using compressed information without querying every single host in the network. in this paper, we propose a new data streaming algorithm to identify high-cardinality hosts over the network-wide traffic measurements. our algorithm introduces a new mergable and reversible data structure for the distributed network monitoring system, which is designed by noisy group testing. we have theoretically analyzed our algorithm and evaluated it against real-world data sets.
bioinformatics	alternative splicing provides a major mechanism to generate protein diversity. increasing evidence suggests a link of dysregulation of splicing associated with cancer. genome-wide alternative splicing profiling in lung cancer remains largely unstudied. we generated alternative splicing profiles in 491 lung adenocarcinoma (wad) and 471 lung squamous cell carcinoma (lusc) patients in tcga using rna-seq data, prognostic models and splicing networks were built by integrated bioinformatics analysis. a total of 3691 and 2403 alternative splicing events were significantly associated with patient survival in luad and lusc, respectively, including egfr, cd44, pik3c3, rras2, mapkapi and fgfr2. the area under the curve of the receiver-operator characteristic curve for prognostic predictor in nsclc was 0.817 at 2000 days of overall survival which were also over 0.8 in luad and lusc, separately. interestingly, splicing correlation networks uncovered opposite roles of splicing factors in luad and lusc. we created prognostic predictors based on alternative splicing events with high performances for risk stratification in nsclc patients and uncovered interesting splicing networks in luad and lusc which could be underlying mechanisms. (c) 2017 elsevier b.v. all rights reserved.
computer_graphics	the increased availability of consumer-grade virtual reality (vr) head-mounted displays (hmd) has created significant demand for affordable and reliable 3d input devices that can be used to control 3d user interfaces. accurate positioning of a user 's body within the virtual environment is essential in order to provide users with convincing and interactive vr experiences. existing full-body motion tracking systems from academia and industry have suffered from problems of occlusion and accumulated sensor error while often lacking absolute positional tracking. this paper describes a wireless sensor array system that uses multiple inertial measurement units (imus) for calculating the complete pose of a user 's body. the system corrects gyroscope errors by using magnetic sensor data. the sensor array system is augmented by a positional tracking system that consists of a rotary-laser base station and a photodiode-based tracked object worn on the user 's torso. the base station emits horizontal and vertical laser lines that sweep across the environment in sequence. with the known configuration of the photodiode constellation, the position and orientation of the tracked object can be determined with high accuracy, low latency, and low computational overhead. as will be shown, the sensor fusion algorithms used result with a full-body tracking system that can be applied to a wide variety of 3d applications and interfaces.
software_engineering	context: the trustworthiness of research results is a growing concern in many empirical disciplines. aim: the goals of this paper are to assess how much the trustworthiness of results reported in software engineering experiments is affected by researcher and publication bias, given typical statistical power and significance levels, and to suggest improved research practices. method: first, we conducted a small-scale survey to document the presence of researcher and publication biases in software engineering experiments. then, we built a model that estimates the proportion of correct results for different levels of researcher and publication bias. a review of 150 randomly selected software engineering experiments published in the period 2002-2013 was conducted to provide input to the model. results: the survey indicates that researcher and publication bias is quite common. this finding is supported by the observation that the actual proportion of statistically significant results reported in the reviewed papers was about twice as high as the one expected assuming no researcher and publication bias. our models suggest a high proportion of incorrect results even with quite conservative assumptions. conclusion: research practices must improve to increase the trustworthiness of software engineering experiments. a key to this improvement is to avoid conducting studies with unsatisfactory low statistical power. (c) 2015 elsevier inc. all rights reserved.
image_processing	incident angle of light source is a key factor that affects imaging resolution of direct optical imaging for a reflected target. for a large incident angle, it is easy to form the ""blind area"" in which the image of the object cannot be distinguished clearly. using classical statistical optics, we study ghost imaging (gi) for a reflected object numerically and experimentally and show that by measuring the second-order correlation of light fields, a ghost-image with good quality can be retrieved in the ""blind area."" unlike transmitted ghost imaging, which is affected greatly by the transverse size of the test detector, the size has almost no effect on the resolution of reflective ghost imaging when the blind area appears.
network_security	nowadays, denial of service (dos) attacks have become a major security threat to networks and the internet. therefore, even a naive hacker can launch a large-scale dos attack to the victim from providing internet services. this article deals with the evaluation of the snort ids in terms of packet processing performance and detection. this work describes the aspect involved in building campus network security system and then evaluates the campus network security risks and threats, mainly analyses the attacks dos and ddos, and puts forward new approach for snort campus network security solutions. the objective is to analyze the functional advantages of the solution, deployment and configuration of the open source based on snort intrusion detection system. the evaluation metrics are defined using snort namely comparison between basic rules with new ones, available bandwidth, cpu loading and memory usage.
cryptography	card-based protocols enable us to easily perform cryptographic tasks such as secure multiparty computation using a deck of physical cards. since the first card-based protocol appeared in 1989, many protocols have been designed. a protocol is usually described with a series of somewhat intuitive and verbal descriptions, such as ""turn over this card,"" ""shuffle these two cards,"" ""apply a random cut to these five cards,"" and so on. on the other hand, a formal computational model of card-based protocols via abstract machine was constructed in 2014. by virtue of the formalization, card-based protocols can be treated more rigorously; for example, it enables one to discuss the lower bounds on the number of cards required for secure computations. in this paper, an overview of the computational model with its applications to designing protocols and a survey of the recent progress in card-based protocols are presented.
relational_databases	remote sensing of resource in a geographic space at regular temporal intervals has paved way for the evolution of geo-spatial information processing. knowledge engineering of facts acquired through this technology primarily aims at qualitative results to support human in solving complex tasks that cannot be solved through quantitative relational query processing methods with database management systems (dbms). this necessitates the need for automated inference mechanism to be built over relational databases. automated reasoning, a systematic process of formal symbolic representation to codify the acquired facts enables the system to infer new knowledge which can further update the facts. a formal representation of event attributed spatial entity (ease) knowledge base is proposed using the theory of allen 's interval calculus and randel 's rcc-8. the objective of the proposed knowledge base is to formalize spatial entities in a geographic region whose temporal attributes are events occurring in an interval, at time instant and over successive intervals to qualitatively answer the event-based queries on prediction of spatial process. the significance of this formal approach is shown using query evaluation on real datasets. the working of proposed knowledge base is explained with illustrative results. towards the end of this work, the direction for enhancement of ease to explore its use is discussed.
parallel_computing	we report on the first application of the graphics processing units (gpus) accelerated computing technology to improve performance of numerical methods used for the optical characterization of evaporating microdroplets. single microdroplets of various liquids with different volatility and molecular weight (glycerine, glycols, water, etc.), as well as mixtures of liquids and diverse suspensions evaporate inside the electrodynamic trap under the chosen temperature and composition of atmosphere. the series of scattering patterns recorded from the evaporating microdroplets are processed by fitting complete mie theory predictions with gradientless lookup table method. we showed that computations on gpus can be effectively applied to inverse scattering problems. in particular, our technique accelerated calculations of the mie scattering theory on a single-core processor in a matlab environment over 800 times and almost 100 times comparing to the corresponding code in c language. additionally, we overcame problems of the time-consuming data post-processing when some of the parameters (particularly the refractive index) of an investigated liquid are uncertain. our program allows us to track the parameters characterizing the evaporating droplet nearly simultaneously with the progress of evaporation.
bioinformatics	pax3 functions at the nodal point in neural stem cell maintenance and differentiation. using bioinformatics methods, we identified pax3 as a potential regulator of beta-tubulin-iii (tubb3) gene transcription, and the results indicated that pax3 might be involved in neural stem cell (nsc) differentiation by orchestrating the expression of cytoskeletal proteins. in the present study, we reported that pax3 could inhibit the differentiation of nscs and the expression of tubb3. further, using luciferase and electrophoretic mobility shift assays, we demonstrated that pax3 could bind to the promoter region of tubb3 and inhibit tubb3 transcription. finally, we confirmed that pax3 could bind to the promoter region of endogenous tubb3 in the native chromatin of nscs. these findings indicated that pax3 is a pivotal factor targeting various molecules during differentiation of nscs in vitro. (c) 2017 elsevier inc. all rights reserved.
network_security	while all cloud based platforms possess security vulnerabilities, the additional security challenges with container systems stem from the sharing of host os among independent containers. if a malicious application was to break into the root of container daemon, it could gain root access into the host kernel thereby compromising the entire system. it could create denial-of-service attack for other user applications, rejecting service to other applications. in this paper, we propose a quantum network security framework for the cloud. we devise a means by which quantum particles, denoted entangled bell pairs, are routed to network nodes. this enables teleportation of quantum information between source and destination only when root privileges are required by an application. the secure quantum channel works on a use-once only policy, so the key data cannot be easily copied, regenerated or spoofed without detection. a network framework for multiple pre-staged channels is devised and we illustrate that policy for network routing of entangle particles formulated as a multi-tenant teleportation network, capable of disseminating key data to servers hosting docker container applications. the framework can achieve provably high levels of security and is capable of integration into a cloud data center for securing applications using docker containers. we also describe quantum network layer protocols for cloud container security that leverage the unique properties of quantum entanglement. to resolve security concerns, this layer would control access between application and container daemon, thereby facilitating restricted communication with proper authentication.
data_structures	this article describes a new open source scientific workflow system, the timestudio project, dedicated to the behavioral and brain sciences. the program is written in matlab and features a graphical user interface for the dynamic pipelining of computer algorithms developed as timestudio plugins. timestudio includes both a set of general plugins (for reading data files, modifying data structures, visualizing data structures, etc.) and a set of plugins specifically developed for the analysis of event-related eyetracking data as a proof of concept. it is possible to create custom plugins to integrate new or existing matlab code anywhere in a workflow, making timestudio a flexible workbench for organizing and performing a wide range of analyses. the system also features an integrated sharing and archiving tool for timestudio workflows, which can be used to share workflows both during the data analysis phase and after scientific publication. timestudio thus facilitates the reproduction and replication of scientific studies, increases the transparency of analyses, and reduces individual researchers' analysis workload. the project website (http://timestudioproject.com) contains the latest releases of timestudio, together with documentation and user forums.
algorithm_design	in view of the special structure of isolated grid, the backtracking algorithm which is widely used in black-start path optimization has its defect, such as time-consuming and blind optimizing. in this paper, minimal spanning tree integrated with backtracking algorithm is proposed to realize the path optimization for black-start. firstly, this paper builds the weight factors of transmission line and unit to identify significant nodes. then, the optimal black start topology is built with the maximum weight factors of transmission line. after that, backtracking algorithm is introduced to calculate the specific black-start path from the optimal tree topology, where the target is to maximize the weight factor of path. the optimization algorithm can effectively reduce the path search space and has high robustness. finally, new england to-unit 39-bus system is used as an example to verify the effectiveness of the proposed algorithm.
computer_graphics	city models by 3d cg (computer graphics) are important in promoting public participation for smart city, which will use solar photovoltaic (pv) generation. but, creating city models are labor intensive. in order to automate laborious steps, we proposed new technology by integration of a gis (geographic information system) and cg. the proposed integrated system automatically generates 3d building models, based on building polygons or building footprints on digital maps, which show most building polygons' edges meet at right angles (orthogonal polygon). a complicated orthogonal polygon can be partitioned into a set of rectangles. the proposed integrated system partitions orthogonal building polygons into a set of rectangles and places rectangular roofs and box-shaped building bodies on these rectangles. in this paper, we propose to automatically generate 3d building models topped with double shed roofs attached by pv arrays. the sizes and positions, slopes of roof boards and under roof constructions are made clear by designing the top view and side view of a double shed roof house. for the application example of the developed system, we simulate the solar photovoltaic generation change of a city block by performing land readjustment and changing shape of buildings, that is, ordinary roof house or double shed roof house suitable for greater pv generation. our simulation reveals that double shed roof houses have greatly improved the solar photovoltaic generation.
operating_systems	in the ""design4all"" project, a hardware and software architecture is under development for the implementation of adaptable and adaptive applications aimed to support all people in carrying out an independent life at home. in this paper, the problems of interactions with applications implemented in the android platform, chosen for the experiments of interaction with the developed applications, are discussed with main emphasis on two main aspects: (i) the use of facilities supporting accessibility available in the most commonly used operating systems (mainstreaming) and (ii) the portability of solutions across different platforms.
algorithm_design	movement primitive segmentation enables long sequences of human movement observation data to be segmented into smaller components, termed movement primitives, to facilitate movement identification, modeling, and learning. it has been applied to exercise monitoring, gesture recognition, humanmachine interaction, and robot imitation learning. this paper proposes a segmentation framework to categorize and compare different segmentation algorithms considering segment definitions, data sources, application-specific requirements, algorithm mechanics, and validation techniques. the framework is applied to human motion segmentation methods by grouping them into online, semionline, and offline approaches. among the online approaches, distance-based methods provide the best performance, while stochastic dynamic models work best in the semionline and offline settings. however, most algorithms to date are tested with small datasets, and algorithm generalization across participants and to movement changes remains largely untested.
algorithm_design	dynamic optimization problems (dops) have attracted increasing attention in recent years. analyzing the fitness landscape is essential to understand the characteristics of dops and may provide guidance for the algorithm design. existing measures for analyzing the dynamic fitness landscape, such as the dynamic fitness distance correlation and the severity of change, cannot give a comprehensive evaluation of the landscape and have many disadvantages. in this paper, we used discrete-time fourier transform( dtft) and dynamic time warping (dtw) distance to acquire information of fitness landscape from frequency and time domains. five measures are proposed, including the stationarity of amplitude change, the keenness, the periodicity, the change degree of average fitness and the similarity. they can reflect the features of fitness landscape from the aspects of outline, keenness, period, fitness value and similarity degree, respectively. these criteria can obtain essential information that cannot be acquired by existing criteria, and do not depend on the distribution of variables, the prior information of solutions and algorithms. to illustrate the performance of the five measures, experiments are conducted based on three types of standard dops with a two-peak function. in addition, we also apply these criteria on the test task scheduling problem for illustrating the fairness and adaptability. the experiment results show that these criteria can reflect the change characteristics of dynamic fitness landscape, and are consistent with the theoretical analysis. (c) 2016 elsevier b.v. all rights reserved.
computer_programming	there are repetitive patterns in strategies of manipulating source code. for example, modifying source code before acquiring knowledge of how a code works is a depth-first style and reading and understanding before modifying source code is a breadth-first style. to the extent we know there is no study on the influence of personality on them. the objective of this study is to understand the influence of personality on programming styles. we did a correlational study with 65 programmers at the university of stuttgart. academic achievement, programming experience, attitude towards programming and five personality factors were measured via self-assessed survey. the programming styles were asked,in the survey or mined from the software repositories. performance in programming was composed of bug-proneness of programmers which was mined from software repositories, the grades they got in a software project course and their estimate of their own programming ability. we did statistical analysis and found that openness to experience has a positive association with breadth-first style and conscientiousness has a positive association with depth-first style. we also found that in addition to having more programming experience and better academic achievement, the styles of working depth-first and saving coarse-grained revisions improve performance in programming. (c) 2015 elsevier inc. all rights reserved.
image_processing	document image segmentation into text lines is one of the stages in unconstrained handwritten document recognition. this paper presents a new algorithm for text line separation in handwriting. the developed algorithm is based on a method using the projection profile. it employs thresholding, but the threshold value is variable. this permits determination of low or overlapping peaks of the graph. the proposed technique is shown to improve the recognition rate relative to traditional methods. the algorithm is robust in text line detection with respect to different text line lengths.
image_processing	detailed information on the distribution of airway diameters during bronchoconstriction in situ is required to understand the regional response of the lungs. imaging studies using computed tomography (ct) have previously measured airway diameters and changes in response to bronchoconstricting agents, but the manual measurements used have severely limited the number of airways measured per subject. hence, the detailed distribution and heterogeneity of airway responses are unknown. we have developed and applied dynamic imaging and advanced image-processing methods to quantify and compare hundreds of airways in vivo. the method, based on ct, was applied to house dust-mite-sensitized and control mice during intravenous methacholine (mch) infusion. airway diameters were measured pre-and post-mch challenge, and the results compared demonstrate the distribution of airway response throughout the lungs during mechanical ventilation. forced oscillation testing was used to measure the global response in lung mechanics. we found marked heterogeneity in the response, with paradoxical dilation of airways present at all airway sizes. the probability of paradoxical dilation decreased with decreasing baseline airway diameter and was not affected by pre-existing inflammation. the results confirm the importance of considering the lung as an entire interconnected system rather than a collection of independent units. it is hoped that the response distribution measurements can help to elucidate the mechanisms that lead to heterogeneous airway response in vivo. new & noteworthy information on the distribution of airway diameters during bronchoconstriction in situ is critical for understanding the regional response of the lungs. we have developed an imaging method to quantify and compare the size of hundreds of airways in vivo during bronchoconstriction in mice. the results demonstrate large heterogeneity with both constriction and paradoxical dilation of airways, confirming the importance of considering the lung as an interconnected system rather than a collection of independent units.
software_engineering	context: several research efforts have been targeted to support architecture centric development and evolution of software for robotic systems for the last two decades. objective: we aimed to systematically identify and classify the existing solutions, research progress and directions that influence architecture-driven modeling, development and evolution of robotic software. research method: we have used systematic mapping study (sms) method for identifying and analyzing 56 peer-reviewed papers. our review has (i) taxonomically classified the existing research and (ii) systematically mapped the solutions, frameworks, notations and evaluation methods to highlight the role of software architecture in robotic systems. results and conclusions: we have identified eight themes that support architectural solutions to enable (i) operations, (ii) evolution and (iii) development specific activities of robotic software. the research in this area has progressed from object-oriented to component-based and now to service-driven robotics representing different architectural models that emerged overtime. an emerging solution is cloud robotics that exploits the foundations of service-driven architectures to support an interconnected web of robots. the results of this sms facilitate knowledge transfer- benefiting researchers and practitioners- focused on exploiting software architecture to model, develop and evolve robotic systems. (c) 2016 elsevier inc. all rights reserved.
network_security	security in terms of networks have turn out to be more significant to organizations, military and personal computer user 's. since various kinds of threats are for data from sending it from sender side over internet till it reaches to receiver. here we will focus on ssl it is a technique used to give client and server authentication, data confidentiality and data integrity. it transform our data into unintelligible form, data which we will be sending can be text or no text form, by encrypting our data we can save it from attacks like eavesdropping, in which interception of communication by unauthorized person, he can either listen or can add malicious information in our data which can lead to catastrophic results. this technique of secure data transmission is very useful in securing the integrity of data sent by the unmanned aerial vehicles in military application to commercially used electricity meter. since the above mentioned devices uses microcontroller to send data through internet hence this data is always going to be susceptible to above mentioned threats so it is important to ensure that it does n't fall in wrong hands, our objective is that our microcontroller sends the data to remote location has authenticity, confidentiality and integrity. first we will send some meaningful text already stored in controller of stm3240g eval-board then that data will be sent to server. these encrypted packets will be sending to remote server through ethernet. at the receiver end this data will be received and decrypted to get the original captured data.
algorithm_design	the increase in the number of cores per processor and the complexity of memory hierarchies make cache coherence key for programmability of current shared memory systems. however, ignoring its detailed architectural characteristics can harm performance significantly. in order to assist performance-centric programming, we propose a methodology to allow semi-automatic performance tuning with the systematic translation from an algorithm to an analytic performance model for cache line transfers. for this, we design a simple interface for cache line aware optimization, a translation methodology, and a full performance model that exposes the block-based design of caches to middleware designers. we investigate two different architectures to show the applicability of our techniques and methods: the many-core accelerator intel xeon phi and a multi-core processor with a numa configuration (intel sandy bridge). we use mathematical optimization techniques to tune synchronization algorithms to the microarchitectures, identifying three techniques to design and optimize data transfers in our model: single-use, single-step broadcast, and private cache lines.
data_structures	we propose a new sharing analysis of object-oriented programs based on abstract interpretation. two variables share when they are bound to data structures which overlap. we show that sharing analysis can greatly benefit from linearity analysis. we propose a combined domain including aliasing, linearity and sharing information. we use a graph-based representation of aliasing information which naturally encodes sharing and linearity information, and define all the necessary operators for the analysis of a java-like language.
operating_systems	in recent years smart mobile devices have bolstered new interaction scenarios that require more sophisticated human-machine interfaces. the leading developers of operating systems for these devices now provide apis (application programming interface) for developers to implement their own applications, including different solutions for developing graphical interfaces, control sensors and providing oral interaction. despite the usefulness of these resources, defined strategies are still needed for developing multimodal interfaces to take greater advantage of these devices for identifying and meeting the needs of users. currently, these applications are typically ad-hoc and facilitate oral communication only through simple commands. in this paper we propose the practical application of context-sensitive multimodal conversational agents to provide advanced library services that dynamically consider specific user needs and preferences, as well as the specific characteristics of the environment in which the interaction occurs. such agents would improve and customize the service provided by a mobile device with internet access. our proposal integrates features of android apis on a modular architecture emphasizing the management of interactions and context awareness in order to create robust applications that can be easily updated and adapted to the user.
symbolic_computation	under investigation in this paper are the (1+1)-dimensional and (2+1)-dimensional ito equations. with the help of the bell polynomials method, hirota bilinear method and symbolic computation, the bilinear representations, n-soliton solutions, bilinear backlund transformations and lax pairs of these two equations are obtained, respectively. in particular, we obtain a new bilinear form and n-soliton solutions of the (2+1)-dimensional ito equation. the bilinear backlund transformation and lax pair of the (2+1)-dimensional ito equation are also obtained for the first time. copyright (c) 2014 john wiley & sons, ltd.
network_security	future home networks are expected to become extremely sophisticated, yet only the most technically adept persons are equipped with skills to secure them. in this paper, we provide a novel solution to detect and prevent attacks on home routers based on anomalous power consumption. we developed a means of measuring power consumption that could be used in a wide variety of home networks, although our primary focus on is on profiling homenet-based residential routers, specifically to detect attacks against homenet routing infrastructure. several experimental results are presented when the infrastructure is exposed to various types of attacks, which show strong evidence of the feasibility of our approach.
computer_graphics	implicit representations have gained an increasing popularity in geometric modeling and computer graphics due to their ability to represent shapes with complicated geometry and topology. however, the storage requirement, e.g. memory or disk usage, for implicit representations of complex models is relatively large. in this paper, we propose a compact representation for multilevel rational algebraic spline (mras) surfaces using low-rank tensor approximation technique, and exploit its applications in surface reconstruction. given a set of 3d points equipped with oriented normals, we first fit them with an algebraic spline surface defined on a box that bounds the point cloud. we split the bounding box into eight sub-cells if the fitting error is greater than a given threshold. then for each sub-cell over which the fitting error is greater than the threshold, an offset function represented by an algebraic spline function of low rank is computed by locally solving a convex optimization problem. an algorithm is presented to solve the optimization problem based on the alternating direction method of multipliers (admm) and the candecomp/parafac (cp) decomposition of tensors. the procedure is recursively performed until a certain accuracy is achieved. to ensure the global continuity of the mras surface, quadratic b-spline weight functions are used to blend the offset functions. numerous experiments show that our approach can greatly reduce the storage of the reconstructed implicit surface while preserve the fitting accuracy compared with the state-of-the-art methods. furthermore, our method has good adaptability and is able to produce reconstruction results with high quality. (c) 2016 elsevier ltd. all rights reserved.
network_security	a secure and safe authentication on a vehicular ad-hoc network (vanet) is essential for network security. a safe data transmission requires integrity, availability and privacy protection features as well as efficient communication in diverse settings. recently proposed security protocols for vehicular communication cannot authenticate in a complex way in areas of heavy traffic due to the increasing number of messages in proportion to the number of vehicles. for efficient communication, data volume need be reduced and communication should be safe against a range of attacks. hence, the present paper proposes a protocol that accelerates message processing by sending a low data volume for communication in areas of heavy traffic and that blocks replay attacks by checking timestamps. in addition, casper/fdr (lowe in casper: a compiler for the analysis of security protocols. user manual and tutorial, version 1.12, 2009; formal systems (europe) ltd in failures-divergence renement. fdr2 user manual, 2010) is used to verify the proposed protocol for its security and efficiency against any security vulnerabilities.
distributed_computing	assembling and simultaneously using different types of distributed computing infrastructures (dci) like grids and clouds is an increasingly common situation. because infrastructures are characterized by different attributes such as price, performance, trust, and greenness, the task scheduling problem becomes more complex and challenging. in this paper we present the design for a fault-tolerant and trust-aware scheduler, which allows to execute bag-of-tasks applications on elastic and hybrid dci, following user defined scheduling strategies. our approach, named promethee scheduler, combines a pull-based scheduler with multi-criteria promethee decision making algorithm. because multi-criteria scheduling leads to the multiplication of the possible scheduling strategies, we propose soft, a methodology that allows to find the optimal scheduling strategies given a set of application requirements. the validation of this method is performed with a simulator that fully implements the promethee scheduler and recreates an hybrid dci environment including internet desktop grid, cloud and best effort grid based on real failure traces. a set of experiments shows that the promethee scheduler is able to maximize user satisfaction expressed accordingly to three distinct criteria: price, expected completion time and trust, while maximizing the infrastructure useful employment from the resources owner point of view. finally, we present an optimization which bounds the computation time of the promethee algorithm, making realistic the possible integration of the scheduler to a wide range of resource management software. (c) 2015 elsevier b.v. all rights reserved.
relational_databases	advances in high-throughput proteomics have led to a rapid increase in the number, size, and complexity of the associated data sets. managing and extracting reliable information from such large series of data sets require the use of dedicated software organized in a consistent pipeline to reduce, validate, exploit, and ultimately export data. the compilation of multiple mass-spectrometry-based identification and quantification results obtained in the context of a large-scale project represents a real challenge for developers of bioinformatics solutions. in response to this challenge, we developed a dedicated software suite called heidi to manage and combine both identifications and semiquantitative data related to multiple lcms/ms analyses. this paper describes how, through a user-friendly interface, heidi can be used to compile analyses and retrieve lists of nonredundant protein groups. moreover, heidi allows direct comparison of series of analyses, on the basis of protein groups, while ensuring consistent protein inference and also computing spectral counts. heidi ensures that validated results are compliant with miape guidelines as all information related to samples and results is stored in appropriate databases. thanks to the database structure, validated results generated within heidi can be easily exported in the pride xml format for subsequent publication. heidi can be downloaded from http://biodev.extra.cea.fr/docs/heidi.
bioinformatics	background: next-generation sequencing (ngs) allows ultra-deep sequencing of nucleic acids. the use of sequence-independent amplification of viral nucleic acids without utilization of target-specific primers provides advantages over traditional sequencing methods and allows detection of unsuspected variants and co-infecting agents. however, ngs is not widely used for small rna viruses because of incorrectly perceived cost estimates and inefficient utilization of freely available bioinformatics tools. methods: in this study, we have utilized ngs-based random sequencing of total rna combined with barcode multiplexing of libraries to quickly, effectively and simultaneously characterize the genomic sequences of multiple avian paramyxoviruses. thirty libraries were prepared from diagnostic samples amplified in allantoic fluids and their total rnas were sequenced in a single flow cell on an illumina miseq instrument. after digital normalization, data were assembled using the mira assembler within a customized workflow on the galaxy platform. results: twenty-eight avian paramyxovirus 1 (apmv-1), one apmv-13, four avian influenza and two infectious bronchitis virus complete or nearly complete genome sequences were obtained from the single run. the 29 avian paramyxovirus genomes displayed 99.6% mean coverage based on bases with phred quality scores of 30 or more. the lower and upper quartiles of sample median depth per position for those 29 samples were 2984 and 6894, respectively, indicating coverage across samples sufficient for deep variant analysis. sample processing and library preparation took approximately 25-30 h, the sequencing run took 39 h, and processing through the galaxy workflow took approximately 2-3 h. the cost of all steps, excluding labor, was estimated to be 106 usd per sample. conclusions: this work describes an efficient multiplexing ngs approach, a detailed analysis workflow, and customized tools for the characterization of the genomes of rna viruses. the combination of multiplexing ngs technology with the galaxy workflow platform resulted in a fast, user-friendly, and cost-efficient protocol for the simultaneous characterization of multiple full-length viral genomes. twenty-nine full-length or near-full-length apmv genomes with a high median depth were successfully sequenced out of 30 samples. the applied de novo assembly approach also allowed identification of mixed viral populations in some of the samples.
relational_databases	time is pervasive of reality, and many relational database approaches have been developed to cope with it. in practical applications, facts can repeat several times, and only the overall period of time containing all the repetitions may be known (consider, e.g., on january, john attended five meetings of the bioinformatics project). while some temporal relational databases have faced facts repeated at (known) periodic time, or single facts occurred at temporally indeterminate time, the conjunction of non-periodic repetitions and temporal indeterminacy has not been faced yet. coping with this problem requires an in-depth extension of current techniques. in this paper, we have introduced a new data model, and new definitions of relational algebraic operators coping with the above issues. we have studied the properties of the new model and algebra (with emphasis on the reducibility property), and how it can be integrated with other models in the literature.
software_engineering	in mexico, the small and medium size enterprises (smes) are key for the software development industry. for them, having highly qualified personal for the development of high quality software products is a fundamental piece to guarantee their permanency in the market. therefore, matching the software industry requirements with the academy training represents a significant problem that must be addressed for both sectors benefit. this paper presents an analysis of the coverage between the moprosoft norm, standard developed to be used for software industry to ensure quality in software engineering practices, and ten academic curricular programs of higher education related to computer science and informatics; to get an overview of the knowledge and skills that computer science students acquire at universities, regarding knowledge required in organizations that work under process models. in addition, a survey to 32 smes was conducted to contrast the coverage results with their hired, recently graduated, personal.
parallel_computing	modelling of multi-million atomic semiconductor structures is important as it not only predicts properties of physically realizable novel materials, but can accelerate advanced device designs. this work elaborates a new technology-computer-aided-design (tcad) tool for nanoelectronics modelling, which uses a sp(3)d(5)s(*) tight-binding approach to describe multi-million atomic structures, and simulate electronic structures with high performance computing (hpc), including atomic effects such as alloy and dopant disorders. being named as quantum simulation tool for advanced nanoscale devices (q-and), the tool shows nice scalability on traditional multi-core hpc clusters implying the strong capability of large-scale electronic structure simulations, particularly with remarkable performance enhancement on latest clusters of intel xeon phi (tm) coprocessors. a review of the recent modelling study conducted to understand an experimental work of highly phosphorus-doped silicon nanowires, is presented to demonstrate the utility of q-and. having been developed via intel parallel computing center project, q-and will be open to public to establish a sound framework of nanoelectronics modelling with advanced hpc clusters of a many-core base. with details of the development methodology and exemplary study of dopant electronics, this work will present a practical guideline for tcad development to researchers in the field of computational nanoelectronics. (c) 2016 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license.
computer_vision	the ability to automatically recognize human faces based on dynamic facial images is important in security, surveillance and the health/independent living domains. specific applications include access control to secure environments, identification of individuals at a particular place and intruder detection. this research proposes a real-time system for surveillance using cameras. the process is broken into two steps: (1) face detection and (2) face recognition to identify particular persons. for the first step, the system tracks and selects the faces of the detected persons. an efficient recognition algorithm is then used to recognize detected faces with a known database. the proposed approach exploits the viola-jones method for face detection, the kanade-lucas-tomasi algorithm as a feature tracker and principal component analysis (pca) for face recognition. this system can be implemented at different restricted areas, such as at the office or house of a suspicious person or at the entrance of a sensitive installation. the system works almost perfectly under reasonable lighting conditions and image depths.
algorithm_design	in the real world, it is not uncommon to face an optimization problem with more than three objectives. such problems, called many-objective optimization problems (maops), pose great challenges to the area of evolutionary computation. the failure of conventional pareto-based multi-objective evolutionary algorithms in dealing with maops motivates various new approaches. however, in contrast to the rapid development of algorithm design, performance investigation and comparison of algorithms have received little attention. several test problem suites which were designed for multi-objective optimization have still been dominantly used in many-objective optimization. in this paper, we carefully select (or modify) 15 test problems with diverse properties to construct a benchmark test suite, aiming to promote the research of evolutionary many-objective optimization (emao) via suggesting a set of test problems with a good representation of various real-world scenarios. also, an open-source software platform with a user-friendly gui is provided to facilitate the experimental execution and data observation.
relational_databases	flexible business processes can often be modelled more easily using a declarative rather than a procedural modelling approach. process mining aims at automating the discovery of business process models. existing declarative process mining approaches either suffer from performance issues with real-life event logs or limit their expressiveness to a specific set of constaint types. lately, relationalxes, a relational database architecture for storing event log data, has been introduced. in this paper, we introduce a mining approach that directly works on relational event data by querying the log with conventional sql. by leveraging database performance technology, the mining procedure is fast without limiting itself to detecting certain control-flow constraints. queries can be customised and cover process perspectives beyond control flow, e.g., organisational aspects. we evaluated the performance and the capabilities of our approach with regard to several real-life event logs.
computer_graphics	artists and animators have observed that children 's movements are quite different from adults performing the same action. previous computer graphics research on human motion has primarily focused on adult motion. there are open questions as to how different child motion actually is, and whether the differences will actually impact animation and interaction. we report the first explicit study of the perception of child motion (ages 5 to 9 years old), compared to analogous adult motion. we used markerless motion capture to collect an exploratory corpus of child and adult motion, and conducted a perceptual study with point light displays to discover whether naive viewers could identify a motion as belonging to a child or an adult. we find that people are generally successful at this task. this work has implications for creating more engaging and realistic avatars for games, online social media, and animated videos and movies.
algorithm_design	the fully polynomial-time approximation scheme (fptas) is a class of approximation algorithms for optimisation problems that is able to deliver an approximate solution within any chosen ratio in polynomial time. by generalising bird and de moor 's thinning theorem to a property between three orderings, we come up with a datatype-generic strategy for constructing fold-based fptass. greedy, thinning, and approximation algorithms can thus be seen as a series of generalisations. components needed in constructing an fptas are often natural extensions of those in the thinning algorithm, design of complex fptass is thus made easier, and some of the resulting algorithms turn out to be simpler than those in previous works. (c) 2014 elsevier b.v. all rights reserved.
operating_systems	silent data corruptions (sdcs) are errors that corrupt the system or falsify results while remaining unnoticed by firmware or operating systems. in numerical integration solvers, sdcs that impact the accuracy of the solver are considered significant. detecting sdcs in high-performance computing is necessary because results need to be trustworthy and the increase of the number and complexity of components in emerging large-scale architectures makes sdcs more likely to occur. until recently, sdc detection methods consisted in replicating the processes of the execution or in using checksums (for example algorithm-based fault tolerance). recently, new detection methods have been proposed relying on mathematical properties of numerical kernels or performing data analysis of the results modified by the application. none of those methods, however, provide a lightweight solution guaranteeing that all significant sdcs are detected. we propose a new method called hot rod as a solution to this problem. it checks and potentially corrects the data produced by numerical integration solvers. our theoretical model shows that all significant sdcs can be detected. we present two detectors and conduct experiments on streamline integration from the wrf meteorology application. compared with the algorithmic detection methods, the accuracy of our first detector is increased by 52% with a similar false detection rate. the second detector has a false detection rate one order of magnitude lower than these detection methods while improving the detection accuracy by 23 %. the computational overhead is lower than 5% in both cases. the model has been developed for an explicit runge-kutta method, although it can be generalized to other solvers.
computer_programming	this paper presents a systematic literature review in the internet of things and ambient intelligence areas. the goal was to identify the best software tools that allow end users, namely people without competencies in computer programming, to manage and configure the behaviors of a smart home. the review selected 48 papers out of 1049 papers found through automatic and manual search. from these papers, 11 tools have been identified and analyzed by means of eight technical characteristics. finally, among the eleven tools, six tools have been chosen for a qualitative comparison on the basis of seven design principles for smart home control proposed in a literature paper.
symbolic_computation	in this paper, we show an algorithmic procedure to compute abelian subalgebras and ideals of a given finite-dimensional leibniz algebra, starting from the non-zero brackets in its law. in order to implement this method, the symbolic computation package maple 12 is used. moreover, we also show a brief computational study considering both the computing time and the memory used in the two main routines of the implementation. finally, we determine the maximal dimension of abelian subalgebras and ideals for 3-dimensional leibniz algebras and 4-dimensional solvable ones over .
computer_vision	aim: alzheimer 's disease patients are increasing rapidly every year. scholars tend to use computer vision methods to develop automatic diagnosis system. (background) in 2015, gorji et al. proposed a novel method using pseudo zernike moment. they tested four classifiers: learning vector quantization neural network, pattern recognition neural network trained by levenberg-marquardt, by resilient backpropagation, and by scaled conjugate gradient. method: this study presents an improved method by introducing a relatively new classifier-linear regression classification. our method selects one axial slice from 3d brain image, and employed pseudo zernike moment with maximum order of 15 to extract 256 features from each image. finally, linear regression classification was harnessed as the classifier. results: the proposed approach obtains an accuracy of 97.51%, a sensitivity of 96.71%, and a specificity of 97.73%. conclusion: our method performs better than gorji 's approach and five other state-of-the-art approaches. therefore, it can be used to detect alzheimer 's disease.
network_security	the frequency transformation methods like fast fourier transform algorithms can be competently used in realization of discrete fourier transforms over galois field, which have broad applications in network security and digital communication in error correcting codes. the cyclotomic fast fourier transform (cfft) is a type of fast fourier transform algorithm over finite fields this method utilizes the benefit of cyclic decomposition. the cyclotomic breakdown of input data is used to reduce the number of operations which can be equally exploited to get a set by set treatment of the input sequence. common subexpression elimination (cse) is an useful optimization process to solve the multiple constant multiplication problems. in this paper, common subexpression elimination algorithm for cyclotomic fast fourier transform over fixed field 23 is designed. using cse algorithm, we reduce the additive complexities of cyclotomic fast fourier transform. the design of cse is to spot regular patterns that are present in expressions more than once and replace them with a single variable. using above method every regular pattern calculates only once, thus minimizing the area of cfft architecture required in vlsi implementation.
symbolic_computation	in this paper, a modified discrete g'/g-expansion method is used to construct exact solutions of toda lattice equation and ablowitz-ladik lattice equations. with the aid of computer symbolic computation, we obtained in a uniform way hyperbolic function solutions, trigonometric function solutions and rational solutions of these two nonlinear lattice equations. when the parameters are taken as special values, some known solutions are recovered. it is shown that the modified method with symbolic computation provides a more effective mathematical tool for solving nonlinear lattice equations in science and enginnering.
software_engineering	this paper introduces the potential for reusing ui elements in the context of model-based ui development (mbuid) and provides guidance for future mbuid systems with enhanced reutilization capabilities. our study is based upon the development of six inter-related projects with a specific mbuid environment which supports standard techniques for reuse such as parametrization and sub-specification, inclusion or shared repositories. we analyze our experience and discuss the benefits and limitations of each technique supported by our mbuid environment. the system architecture, the structure and composition of ui elements and the models specification languages have a decisive impact on reusability. in our case, more than 40% of the elements defined in the ul specifications were reused, resulting in a reduction of 55% of the specification size. inclusion, parametrization and sub-specification have facilitated modularity and internal reuse of ui specifications at development time, whereas the reuse of ul elements between applications has greatly benefited from sharing repositories of ui elements at run time. (c) 2015 elsevier ltd. all rights reserved.
computer_vision	we have proposed an effective machine learning method to analyze multimedia content addressing gesture event detection and recognition. our machine learning method is based on well-studied techniques such that procrustes analysis, combination of local and global representations, linear shape model, and application to smart tv virtual keyboard. in this paper, we address gesture event detection specially fingertip gesture detection to get smart and advanced usage of technology. our modern vision keyboard could be a good next generation replacement of smart tv remote control. it can be more economical as we do n't need physical object like traditional keyboard, remote control and their energy resources like batteries. more information and demonstrations of the proposed keyboard can be accessed at http://video.minelab.tw/mcaoged/.
software_engineering	nowadays environmental protection has become a valuable asset for the entire society. the information based society can provide more efficient solutions in its risks mitigation. there are already in place complex infrastructures used to monitor and control the natural environment, but most of them have lower or no interdependency. a solution to gain this integration level is represented by the ""smart cities"" concept. this study will present an approach used to integrate almost every existing system related to environmental monitoring and control into one common view. this is required in order to quickly integrate the produced data or commands for control instruments in the main big data stream. as a result, the ability to solve environmental specific issues will be further increased due to their superior analytical ability.
bioinformatics	preeclampsia presents serious risk of both maternal and fetal morbidity and mortality. biomarkers for the detection of preeclampsia are critical for risk assessment and targeted intervention. the goal of this study is to screen potential biomarkers for the diagnosis of preeclampsia and to illuminate the pathogenesis of preeclampsia development based on the differential expression network. two groups of subjects, including healthy pregnant women, subjects with preeclampsia, were recruited for this study. the metabolic profiles of all of the subjects' serum were obtained by liquid chromatography quadruple time of -flight mass spectrometry. correlation between metabolites was analyzed by bioinformatics technique. results showed that the pc(14:0/00), proline betaine and proline were potential sensitive and specific biomarkers for preeclampsia diagnosis and prognosis. perturbation of corresponding biological pathways, such as inos signaling, nitric oxide signaling in the cardiovascular system, mitochondrial dysfunction were responsible for the pathogenesis of preeclampsia. this study indicated that the metabolic profiling had a good clinical significance in the diagnosis of preeclampsia as well as in the study of its pathogenesis. (c) 2017 elsevier inc. all rights reserved.
image_processing	computer-assisted orthopedic surgery allows clinicians to have better results and decreases the number of early prosthetic replacements. nevertheless, the patient follow-up from pre-operative diagnosis to post-operative control cannot be assessed in a constant referential. in this paper, a real-time algorithm that extracts bone edges from images and, then, derives bony landmarks from these edges is proposed. indeed, we assess in real-time the bone structure positions via ultrasound imaging to create a useful referential for pre-operative, intra-operative and post-operative measurements. to assist the clinician while acquiring bony anatomical landmarks, the extraction of the bone-soft tissue interface and bony landmarks from ultrasound images is done automatically. the experimentations were performed on a database of images from healthy volunteers, and the obtained results showed the efficiency and the stability of the performance of the proposed method.
computer_graphics	with the development of computer graphics technology, machine vision and virtual reality technology in recent years, 3d reconstruction method through the sequence of images of outdoor scenes has become a key research direction in computer vision and graphics. during the acquisition process of image, due to the measurement equipment and environment, single shot sequence of photos may not be able to extract enough surface information and lead to unable to complete the reconstruction of 3d objects. to solve this problem, fusion method of point clouds from multi-group images is adopted in the thesis. firstly the color histogram matching is used to complete the supplement image sequence. next the point cloud from the supplement image sequence is solely calculated. then the transform parameters in overlap area of different point clouds are computed by using the improved iterative closest point algorithm. finally the registration and fusion among the point cloud data of different photos is conducted. the experiments show that this method can effectively supplement point cloud data for reconstruction.
bioinformatics	signaling pathways driven by protein and lipid kinases are altered in most human diseases. therefore, pharmacological inhibitors of cell signaling are one of the most intensively pursued therapeutic approaches for the treatment of diseases such as cancer, neurodegeneration, and metabolic syndromes. phosphoproteomics is a technique that measures the products of kinase activities and, with the appropriate bioinformatics techniques, the methodology can also provide measures of kinase pathway activation and network circuitry. hence, due to recent technological advantages, lc-ms-based quantitative phosphoproteomics provides relevant information for the design and implementation of kinase inhibitor based therapies. here, we review how phosphoproteome profiling is being used in translational research as a means to identify drug targets and biomarkers for personalizing therapies based on kinase inhibitors.
computer_programming	in this paper, we propose an encryption scheme based on phase-shifting digital interferometry. according to the original system framework, we add a random amplitude mask and replace the fourier transform by the fresnel transform. we develop a mathematical model and give a discrete formula based on the scheme, which makes it easy to implement the scheme in computer programming. the experimental results show that the improved system has a better performance in security than the original encryption method. moreover, it demonstrates a good capability of anti-noise and anti-shear robustness.
image_processing	this exploratory work is concerned with generation of natural language descriptions that can be used for video retrieval applications. it is a step ahead of keyword-based tagging as it captures relations between keywords associated with videos. firstly, we prepare hand annotations consisting of descriptions for video segments crafted from a trec video dataset. analysis of this data presents insights into human 's interests on video contents. secondly, we develop a framework for creating smooth and coherent description of video streams. it builds on conventional image processing techniques that extract high-level features from individual video frames. natural language description is then produced based on high-level features. although feature extraction processes are erroneous at various levels, we explore approaches to putting them together to produce a coherent, smooth and well-phrased description by incorporating spatial and temporal information. evaluation is made by calculating rouge scores between human-annotated and machine-generated descriptions. further, we introduce a task-based evaluation by human subjects which provides qualitative evaluation of generated descriptions.
algorithm_design	gnss-based navigation technology for lunar mission with more than 60,000 km above earth, which is still lack of relevant research and simulation. in this paper, ""chang-e one"" lunar mission is used with three stages of orbiting the earth, earth-moon transfer, and around moon. a detailed analysis of feasibility of autonomous navigation using gps, galileo navigation system alone and combination under sidelobe signals is given. the number of visible satellites gnss receiver to receive, dop value, receiving signal level and dynamic, etc. are analyzed, and autonomous navigation algorithm design under different observation conditions were also considered. as can be seen from the theoretical analysis and simulation results, the use of gnss signals fully meet user needs that realize autonomous navigation around earth, earth-moon transfer, and around moon three-stages, which can be considered as the foundation for future engineering applications.
relational_databases	clustering is an unsupervised learning algorithm. k-means algorithm is one of the well-known and promising clustering algorithms that can converge to a local optimum in few iterative. in our work, we will be hybridizing k-means algorithm with genetic algorithms to look for the solution in the global search space in order to converge to a global optima. the problem for clustering is that when the number of clusters increases up to the same number of total records in the dataset, it leads to a scenario in which a cluster only contains a single record, and thus the cluster purity is maximized to the maximum value, 1. however, it will be useless since the common regularities among records will not be seen. therefore, choosing the best number of clusters is trivial. instead of choosing an inappropriate number of clusters and risking the main purpose of the clustering process, a genetic algorithm based k-means ensemble is proposed in order to find the consensus result of several runs of clustering task using different number of clusters, k.
data_structures	this paper presents an analytical study on parsec benchmark suite in order to examine the auto-vectorization potential of emerging workloads by icc and gcc compilers. for investigating auto-vectorization potential, we have analyzed the amount of vectorized and non-vectorized loops and the number of vector instructions of application. we have found most of the time-consuming loops of the applications have not been vectorized. then, we have modified the applications and profiled them again. we have shown applying the modifications have a considerable effect on the amount of vectorized loops but the number of instructions has not reduced to what we expect because of the limited size of simd-width of current processors. as a result, in addition to applying some algorithmic methods such as loop unrolling, splitting large loops, definition of data structures, replacing function calls in loops with function bodies removing control flows from the loops in possible cases and so on to help the compilers for auto-vectorization, increasing the simd-width of the vector extension of cpus is an important issue in order to improve the speed and performance. (c) 2016 elsevier b.v. all rights reserved.
operating_systems	current day networks operate on multiple hardware devices assisted by numerous numbers of operating systems. systems may have vulnerabilities. these are explored and then exploited. among the overall malicious activity countries, india ranked third after us and china. this paper has made an attempt to explore possibility of quantifying various probabilities. cyber system can be modeled in different ways. there are various attack vectors that make cyber network vulnerable. a compromised employee is an insider to any network and contributes significantly to the network vulnerability. keeping in mind various random variables that affect the safety of cyber random space, there may be a need to quantify the probability associated with different cyber exploitation related activities.
computer_vision	due to the edge-preserving ability, the bilateral filter is considered as the fundamental tool in computer vision and computer graphics. however, its computational complexity has a close connection with the size of the box window. this drawback leads that the bilateral filter is inappropriate for the computational insensitive application. one way to accelerate the bilateral filter is to approximate the gaussian range kernel by trigonometric functions and synthesise final results from a set of filtering results of fast convolutions. a novel approximation that can be applied to any range kernel is proposed. specifically, first the z transformation of the range kernel is obtained, then approximate the z transformation of the range kernel using the pade approximation. finally, inverse the transformation of the pade approximation and obtain an exponential sum to approximate original range kernel, where the coefficients of the exponential basis are computed by solving a set of linear equations. experiments show the method achieves state-of-the-art results in terms of accuracy and speed.
network_security	with the development and popularization of internet, computer network has been widely used in various trades and fields. computer network has been used frequently in daily office, management, life and service. for example, government departments, institutions, enterprises, etc., the subsequent application of internet technology has encountered some difficulties in practice, it is necessary for network problems in the application to improve, especially in network maintenance and security risks therefore, the research and analysis on the maintenance and security management of the network are of great significance to further guarantee the network operation and work. in this paper, the author first describes the concept of network maintenance and security management, and then analyzes the application of network security management technology and the corresponding multi-level protection content; finally, on this basis, pointed out that the current network maintenance and security management problems, and pointed out the perfect strategy for the new era of network development and maintenance to provide new ideas.
computer_programming	computer programming remains a difficult discipline to teach. e-learning can help improve student engagement and outcomes but offerings designed to teach programming in a university context are rudimentary when compared to publicly available sites such as code academy. this paper describes nooblab, an e-learning platform for teaching programming. the environment provides a complete suite of features surpassing prior work, and has successfully been used in a number of undergraduate modules to improve student outcomes, satisfaction, and inform pedagogy.
data_structures	this paper presents a novel automated procedure for discovering expressive shape specifications for sophisticated functional data structures. our approach extracts potential shape predicates based on the definition of constructors of arbitrary user-defined inductive data types, and combines these predicates within an expressive first-order specification language using a lightweight data-driven learning procedure. notably, this technique requires no programmer annotations, and is equipped with a type-based decision procedure to verify the correctness of discovered specifications. experimental results indicate that our implementation is both efficient and effective, capable of automatically synthesizing sophisticated shape specifications over a range of complex data types, going well beyond the scope of existing solutions.
image_processing	introduction: huntington 's disease (hd) is a genetic neurodegenerative disorder that primarily affects striatal neurons. striatal volume loss is present years before clinical diagnosis; however, white matter degradation may also occur prior to diagnosis. diffusion-weighted imaging (dwi) can measure microstructural changes associated with degeneration that precede macrostructural changes. dwi derived measures enhance understanding of degeneration in prodromal hd (pre-hd). methods: as part of the predict-hd study, n=191 pre-hd individuals and 70 healthy controls underwent two or more (baseline and 1-5 year follow-up) dwi, with n=649 total sessions. images were processed using cutting-edge dwi analysis methods for large multicenter studies. diffusion tensor imaging (dti) metrics were computed in selected tracts connecting the primary motor, primary somato-sensory, and premotor areas of the cortex with the subcortical caudate and putamen. pre-hd participants were divided into three cag-age product (cap) score groups reflecting clinical diagnosis probability (low, medium, or high probabilities). baseline and longitudinal group differences were examined using linear mixed models. results: cross-sectional and longitudinal differences in dti measures were present in all three cap groups compared with controls. the high cap group was most affected. conclusions: this is the largest longitudinal dwi study of pre-hd to date. findings showed dti differences, consistent with white matter degeneration, were present up to a decade before predicted hd diagnosis. our findings indicate a unique role for disrupted connectivity between the premotor area and the putamen, which may be closely tied to the onset of motor symptoms in hd. (c) 2017 wiley periodicals, inc.
parallel_computing	image enhancement and edge-preserving denoising are relevant steps before classification or other postprocessing techniques for remote sensing images. however, multisensor array systems are able to simultaneously capture several low-resolution images from the same area on different wavelengths, forming a high spatial/spectral resolution image and raising a series of new challenges. in this paper, an open computing language based parallel implementation approach is presented for near real-time enhancement based on bayesian maximum entropy (bme), as well as an edge-preserving denoising algorithm for remote sensing imagery, which uses the local linear stein 's unbiased risk estimate (llsure). bme was selected for its results on synthetic aperture radar image enhancement, whereas llsure has shown better noise removal properties than other commonly used methods. within this context, image processing methods are algorithmically adapted via parallel computing techniques and efficiently implemented using cpus and commodity graphics processing units (gpus). experimental results demonstrate the reduction of computational load of real-world image processing for near real-time gpu adapted implementation.
relational_databases	the main aspect of database protection is to prove the ownership of data that describes who is the originator of data. it is of particular importance in the case of electronic data, as data sets are often modified and copied without proper citation or acknowledgement of originating data set. we present a novel method for watermarking relational databases for identification and proof of ownership based on the secure embedding of blind and multi-bit watermarks using bacterial foraging algorithm (bfa). feasibility of bfa implementation is shown in the framed watermarking databases application. identification of owner is cryptographically made secure and used as an embedded watermark. an improved hash partitioning approach is used that is independent of primary key of the database to secure ordering of the tuples. strength of bfa is explored to make the technique robust, secure and imperceptible. bfa is implemented to give nearly global optimal values bounded by data usability constraints and thus makes database fragile to any attack. the parameters of bfa are tuned to reduce the execution time. bfa is experimentally proved to be better solution than genetic algorithm (ga). the technique proposed is experimentally proved to be resilient against malicious attacks.
network_security	an attack graph depicts multiple-step attack and provides a description of system security vulnerabilities. it illustrates critical information necessary to identify potential weaknesses and areas for enhanced defense. attack graphs include multiple attack paths, which are a focus for further detailed analysis and risk mitigation. considering that different vulnerabilities have different probabilities of being exploited, this paper proposes an algorithm to dynamically generate the top k attack paths with maximum probabilities for every node of a system. the proposed algorithm does not require generation of the full attack graph to calculate the k attack paths. instead, it directly processes and analyzes the system input data and dynamically identifies the k attack paths. the computational time, based upon the complexity of the attack paths, can be constrained by the parameter k. experimental results show that the algorithm is scalable and efficient.
computer_vision	we consider the problem of automatically re-identifying a person of interest seen in a ""probe"" camera view among several candidate people in a ""gallery"" camera view. this problem, called person re-identification, is of fundamental importance in several video analytics applications. while extracting knowledge from high dimensional visual representations based on the notions of sparsity and regularization has been successful for several computer vision problems, such techniques have not been fully exploited in the context of the re-identification problem. here, we develop a principled algorithm for the re-identification problem in the general framework of learning sparse visual representations. given a set of feature vectors for a person in one camera view (corresponding to multiple images as they are tracked), we show that a feature vector representing the same person in another view approximately lies in the linear span of this feature set. furthermore, under certain conditions, the associated coefficient vector can be characterized as being block sparse. this key insight allows us to design an algorithm based on block sparse recovery that achieves stateof-the-art results in multi-shot person re-identification. we also revisit an older feature transformation technique, fisher discriminant analysis, and show that, when combined with our proposed formulation, it outperforms many sophisticated methods. additionally, we show that the proposed algorithm is flexible and can be used in conjunction with existing metric learning algorithms, resulting in improved ranking performance. we perform extensive experiments on several publicly available datasets to evaluate the proposed algorithm. (c) 2016 elsevier b.v. all rights reserved.
computer_vision	in order to reduce the security risk of commercial aircraft, passengers are not allowed to take certain items in their carry-on baggage. for this reason, human operators are trained to detect prohibited items using a manually-controlled baggage screening process. in this paper, the use of an automated method based on multiple x-ray views is proposed to recognise certain regular objects with highly-defined shapes and sizes. the method consists of two steps: 'monocular analysis', to obtain possible detections in each view of a sequence, and 'multiple view analysis', to recognise the objects of interest using matching in all views. the search for matching candidates is efficiently performed using a look-up table that is computed offline. in order to illustrate the effectiveness of the proposed method, experimental results on recognising regular objects (clips, springs and razor blades) in pencil cases are shown achieving high precision and recall (p-r = 95.7%, r-e = 92.5%) for 120 objects. we believe that it would be possible to design an automated aid in a target detection task using the proposed algorithm.
computer_graphics	limmersive displays for virtual reality systems can be roughly classified into spatially immersive displays (similar to cave-like displays or large-screen simulators) or head-mounted displays. the former type is usually static in spatial configuration and configured to support a small group of users. the latter supports only a single user. we propose a new class of actuated, reconfigurable display that can support both small groups and individual users: in particular we suggest a robotic display that can change shape. the display can change shape to support different usage conditions, and can also move rapidly to give a larger apparent field of view for an individual user. we explore the potential advantages of a display that can move independently from its user(s), and we present a prototype that demonstrates some of the potential use scenarios.
software_engineering	much of software engineering research needs to provide an implementation as proof of -concept. often such implementations are created as exploratory prototypes without polished user interfaces, making it difficult to (1) run user studies to validate the tool 's contribution, (2) validate the author 's claim by fellow scientists, and (3) demonstrate the utility and value of the research contribution to any interested parties. however, turning an exploratory prototype into a ""proper"" tool for end-users often entails great effort. heavyweight mainstream frameworks such as eclipse do not address this issue; their steep learning curves constitute substantial entry barriers to such ecosystems. in this paper, we present the model analyzer/checker (mach), a stand-alone tool with a command-line interpreter. mach integrates a set of research prototypes for analyzing uml models. by choosing a simple command line interpreter rather than (costly) graphical user interface, we achieved the core goal of quickly deploying research results to a broader audience while keeping the required effort to an absolute minimum. we analyze mach as a case study of how requirements and constraints in an academic environment influence design decisions in software tool development. we argue that our approach while perhaps unconventional, serves its purpose with a remarkable cost-benefit ratio. (c) 2015 elsevier b.v. all rights reserved.
relational_databases	a comprehensive ontology can ease the discovery, maintenance and popularization of knowledge in many domains. as a means to enhance existing ontologies, attribute extraction has attracted tremendous research attentions. however, most existing attribute extraction techniques focus on exploring a single type of sources, such as structured (e.g., relational databases), semi-structured (e.g., extensible markup language (xml)) or unstructured sources (e.g., web texts, images), which leads to the poor coverage of knowledge bases (kbs). this paper presents a framework for ontology augmentation by extracting attributes from four types of sources, namely existing knowledge bases (kbs), query stream, web texts, and document object model (dom) trees. in particular, we use query stream and two major kbs, dbpedia and freebase, to seed the attribute extraction from web texts and dom trees. we specially focus on exploring the extraction technique from dom trees, which is rarely studied in previous works. algorithms and a series of filters are developed. experiments show the capability of our approach in augmenting existing kb ontology.
operating_systems	optimization can be defined as the operation of finding the best solution for a problem. this operation is performed by changing the initial parameters using existing data. there are various optimization algorithms to solve these kinds of problems; however, it cannot be expected that all optimization algorithms offer a proper and effective solution to all optimization problems. therefore, it is necessary to select the proper algorithm by using similar benchmark functions to the problem, and to determine the best parameter values for the selected algorithm. in this study, a test tool that can run on the devices using the windows, os x, android, and ios operating systems was developed for eight different optimization algorithms: genetic, artificial immune, differential evolution, particle swarm optimization, simulated annealing, tabu search, artificial bee colony, and ant colony optimization algorithms. six hump camel back, rastrigin, shubert, schwefel, and drop wave were the preferred benchmark functions. special user defined special functions were also permitted. the developed test tool was tested for all the optimization algorithms in different platforms. in spite of the small differences in the running times, the results show that the tool can easily be used in windows, os x, android, and ios devices. (c) 2016 wiley periodicals, inc.
software_engineering	agility is a concept and practice with significant importance in managing projects and organizations, although it can also be very risky due to its degree of fuzziness if not properly defined. this research re-defines agility, emphasizes the need for ontologies for its management, and creates an application to measure the degree of agility inside an organization. in this research, various definitions of agility were gathered for the creation of ontology through a mind map revealing the characteristics of agility. as part of the co-evolute theory and methodology, the first agility ontology was developed as well as an application that evaluates the degree of agility in an organization. the application includes statements on which the respondents give opinions concerning the current and future desired states of agility and its importance in an evaluative way. the application has proven to operate well and extensive validation and verification of the tests runs will follow.
computer_vision	gaussian noise is an important problem in computer vision. the novel methods that become popular in recent years for gaussian noise reduction are bayesian techniques in wavelet domain. in wavelet domain, the bayesian techniques require a prior distribution of wavelet coefficients. in general case, the wavelet coefficients might be better modeled by non-gaussian density such as laplacian, two-sided gamma, and pearson type vii densities. however, statistical analysis of textural image is gaussian model. so, we require flexible model between non-gaussian and gaussian models. indeed, gumbel density is a suitable model. so, we present new bayesian estimator for gumbel random vectors in awgn (additive white gaussian noise). the proposed method is applied to dual-tree complex wavelet transform (dt-cwt) as well as orthogonal discrete wavelet transform (dwt). the simulation results show that our proposed methods outperform the state-of-the-art methods qualitatively and quantitatively.
network_security	cloud computing is an internet based computing where virtual shared servers provide software, infrastructure, platform and other resources to the customer on pay-as-you-use basis. cloud computing is increasingly becoming popular as many enterprise applications and data are moving into cloud platforms. however, with the enormous use of cloud, the probability of occurring intrusion also increases. there is a major need of bringing security, transparency and reliability in cloud model for client satisfaction. one of the security issues is how to reduce the impact of any type of intrusion in this environment. to address this issue, a security solution is proposed in this paper. we provide a collaborative framework between our hybrid intrusion detection system (hy-ids) based on mobile agents and virtual firewalls. therefore, our hybrid intrusion detection system consists of three types of ids namely ids-c, ids-cr and ids-m, which are dispatched over three layer of cloud computing. in the first layer, we use ids-c over our framework to collect, analyze and detect malicious data using mobile agents. in case of attack, we collect at the level of the second layer all the malicious data detected in the first layer for the generation of new signatures using ids-cr, which is based on a signature generation algorithm (sga) and network intrusion detection system (nids). finally, through an ids-m placed in the third layer, the new signatures will be used to update the database nids belonging to ids-cr, then the database to nids belonging of ids-cr the cluster neighboring and also their ids-c. hardware firewall is unable to control communication between virtual machines on the same hypervisor. moreover, they are blind to virtual traffic. mostly, they are deployed at virtual machine monitor-level (vmm) under cloud provider 's control. equally, the mobile agents play an important role in this collaboration. they are used in our framework for investigation of hosts, transfer data malicious and transfer update of a database of neighboring ids in the cloud. with this technique, the neighboring ids will use these new signatures to protect their area of control against the same type of attack. by this type of close-loop control, the collaborative network security management framework can identify and address new distributed attacks more quickly and effectively.
computer_graphics	this demonstration illustrates the possibilities of new 3d technologies in conveying large scale historical photographic databases in interactive 3d virtual environments. we illustrate the visualization of the state library of western australia (slwa) 's photographic collection containing over 1 million photographs dating back to the 1850s utilizing curtin 's hub for immersive visualization and eresearch (hive). our application was intended to explore the possibilities in visualizing cultural data sets on the hive 's cylinder, a 3 m high, eight-meter diameter, and 180 degrees cylindrical projection surface. our demonstration illustrated the potentials of virtual environments in creating interactive information designs for photographic imagery, which can be explored according location, time-period, creator, and subject.
distributed_computing	this paper describes a multi-institution effort to develop a ""data science as a service"" platform. this platform integrates advanced federated data management for small to large datasets, access to high performance computing, distributed computing and advanced networking. the goal is to develop a platform that is flexible and extensible while still supporting domain research and avoiding the walled garden problem. some preliminary lessons learned and next steps will also be outlined.
structured_storage	current computer programs for intracellular recordings often lack advanced data management, are usually incompatible with other applications and are also difficult to adapt to new experiments. we have addressed these shortcomings in e-phys, a suite of electrophysiology applications for intracellular recordings. the programs in e-phys use component object model (com) technologies available in the microsoft windows operating system to provide enhanced data storage, increased interoperability between e-phys and other com-aware applications, and easy customization of data acquisition and analysis thanks to a script-based integrated programming environment. data files are extensible, hierarchically organized and integrated in the windows shell by using the structured storage technology. data transfers to and from other programs are facilitated by implementing the activex automation standard and distributed com (dcom). activex scripting allows experimenters to write their own event-driven acquisition and analysis programs in the vbscript language from within e-phys. scripts can reuse components available from other programs on other machines to create distributed meta-applications. this paper describes the main features of e-phys and how this package was used to determine the effect of the atypical antipsychotic drug clozapine on synaptic transmission at the neuromuscular junction. (c) 2003 elsevier b.v. all rights reserved.
data_structures	when compared to earlier programming and data structure experiences that our students might have, the perspective changes on computers and programming when introducing theoretical computer science into the picture. underlying computational models need to be addressed, and mathematical tools employed, to understand the quality criteria of theoretical computer science. focus shifts from doing to proving. over several years, we have tried to make this perspective transition smoother for the students of a third-year mandatory algorithms, data structures, and computational complexity course. the concepts receiving extra attention in this work are np-completeness, one of the most central concepts in computer science, and dynamic programming, an algorithm construction method that is powerful but somewhat unintuitive for some students. the major difficulties that we attribute to np-completeness are that the tasks look similar but have a different purpose than in algorithm construction exercises. students do not immediately see the usefulness of the concept, and hence motivation could be one issue. one line of attacking np-completeness has been to emphasize its algorithmic aspects using typical tools for teaching algorithms. some potential difficulties associated with dynamic programming are that the method is based on a known difficult concept-recursion-and that there are many ingredients in a dynamic programming solution to a problem. for both dynamic programming and np-completeness, we have invented several new activities and structured the teaching differently, forcing students to think and adopt a standpoint, and practice the concepts in programming assignments. student surveys show that these activities are appreciated by the students, and our evaluations indicate that they have positive effects on learning. we believe that these activities could be useful in any similar course. the approach to improving the course is action research, and the evaluation has been done using course surveys, self-efficacy surveys, rubrics-like grading protocols, and grades. we have also interviewed teaching assistants about their experiences.
software_engineering	with the rapid development of computer technology, people pay more attention to the security of computer data and the computer virus has become a chief threat to computer data security. by using an antivirus system that can identify randomly generated computer viruses and on the basis of the basic characteristics of the computer code, this paper investigates the heuristic scanning technique. this paper proposes the minimum distance classifier and detection model through the analysis of the malicious code. this model can identify unknown feature codes of illegal procedures and construct a healthy network environment by using a combination of model and experimental method, which can intercept the illegal virus program in the installation and operation stages. (c) 2016 elsevier b.v. all rights reserved.
data_structures	1. while phylogenies have been getting easier to build, it has been difficult to reuse, combine and synthesize the information they provide because published trees are often only available as image files, and taxonomic information is not standardized across studies. 2. the open tree of life (otl) project addresses these issues by providing a digital tree that encompasses all organisms, built by combining taxonomic information and published phylogenies. the project also provides tools and services to query and download parts of this synthetic tree, as well as the source data used to build it. here, we present rot1, an r package to search and download data fromthe open tree of life directly in r. 3. rot1 uses common data structures allowing researchers to take advantage of the rich set of tools and methods that are available in r to manipulate, analyse and visualize phylogenies. here, and in the vignettes accompanying the package, we demonstrate how rot1 can be used with other r packages to analyse biodiversity data. 4. as phylogenies are being used in a growing number of applications, rot1 facilitates access to phylogenetic data and allows their integration with statistical methods and data sources available inr.
algorithm_design	battery recovery effect is a phenomenon that the available capacity of a battery could increase if the battery can sleep for a certain period of time since its last discharging. accordingly, the battery can work for a longer time when it takes some rests between consecutive discharging processes than when it works all the time. however, this effect has not been considered in the design of energy-efficient topology control algorithms for wireless sensor networks. in this paper, we propose a distributed battery recovery effect aware connected dominating set constructing algorithm (bre-cds) for wireless sensor networks. in bre-cds, each network node periodically decides to join the connected dominating set or not. nodes that have slept in the preceding round have priority to join the connected dominating set in the current round while nodes that have worked in the preceding round are encouraged to take sleep in the current round for battery recovery. detailed algorithm design is presented. the computational complexity of bre-cds is deduced to be o(d-2), where d is node degree. simulation results show that bre-cds can significantly prolong the network lifetime as compared with existing work. copyright (c) 2016 john wiley & sons, ltd.
network_security	ipv6 provides more address space, improved address design, and greater security than ipv4. different transition mechanisms can be used to migrate from ipv4 to ipv6 which includes dual stack networks, tunnels and translation technologies. within all of this, network security is an essential element and therefore requires special attention. this paper analyses two transition technologies which are dual stack and tunnel. both technologies are implemented using cisco packet tracer and gns3. this work will also analyse the security issues of ipv6 to outline the most common vulnerabilities and security issues during the transition. finally, the authors will design and implement the dual stack, automatic and manual tunnelling transition mechanisms using riverbed modeler simulation tool to analyse the performance and compare with the native ipv4 and ipv6 networks.
network_security	in this paper, we conduct research on the wireless sensor network management methods based on the runtime model. with the deepening of the research, scalability and maintainability of wireless sensor network has become an important target of its application promotion. consider that the nodes randomly distributed monitoring area, looking for a complete coverage of this area several disjoint nodes which uses genetic algorithm to optimize the network survival time nodes and corresponding coverage. from the point of view of software engineering, most of the specific software system knowledge hidden in the program and document, the model as the main content of the document and procedures of high-level abstractions. the management of the network is urgently needed. as the additional research, we also conduct theoretical analysis on the wireless sensor network security enhancement methodology with the tradition game theory and mathematical optimization approaches which will be meaningful. game theory is on the interaction between much of decision-making behavior has, according to the different subjects in the control information and the cognition of their own capabilities which will be a novel method for the analysis. the numerical simulation shows that our method performed better compared with other related approaches. in the future, more research will be conducted to polish the current method.
distributed_computing	with the rapid development of embedded system and internet of things technology, embedded system and smart device based on embedded system is collecting huge amounts of data, and corresponding data processing and application method have been greatly changed, different from the traditional big data and cloud computing focus, local processing and application become important trend. this paper tries to take cortex-a7 as the main system with the stronger ability, hadoop distributed computing system is deployed in embedded system and could meet the demand of the future data process with directly managing the resource-constrained sensors, embedded systems and smart devices. successfully deploying over 20gb data in the test, the system is verified that it can complete most of the functions of data processing cluster, and can also manage the collected sensors and embedded system terminal, with better research and market promotional value.
symbolic_computation	the modeling of wave propagation in microstructured materials should be able to account for various scales of microstructure. based on the proposed new exponential expansion method, we obtained the multiple explicit and exact traveling wave solutions of the strain wave equation for describing different types of wave propagation in microstructured solids. the solutions obtained in this paper include the solitary wave solutions of topological kink, singular kink, non-topological bell type solutions, solitons, compacton, cuspon, periodic solutions, and solitary wave solutions of rational functions. it is shown that the new exponential method, with the help of symbolic computation, provides an effective and straightforward mathematical tool for solving nonlinear evolution equations arising in mathematical physics and engineering. (c) 2014 faculty of engineering, ain shams university. production and hosting by elsevier b. v. this is an open access article under the cc by-nc-nd license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
computer_graphics	volume rendering has been a relevant topic in scientific visualization for the last decades. however, the exploration of reasonably big volume datasets requires considerable computing power, which has limited this field to the desktop scenario. but the recent advances in mobile graphics hardware have motivated the research community to overcome these restrictions and to bring volume graphics to these ubiquitous handheld platforms. this survey presents the past and present work on mobile volume rendering, and is meant to serve as an overview and introduction to the field. it proposes a classification of the current efforts and covers aspects such as advantages and issues of the mobile platforms, rendering strategies, performance and user interfaces. the paper ends by highlighting promising research directions to motivate the development of new and interesting mobile volume solutions.
network_security	achieving efficient security solutions for wireless sensor networks (wsn) is a daring challenge due to vulnerable nature of wireless medium. routing is a major threat to security. an adversary can inject bogus routing to en-route nodes causing false decision and drain the sensors energy in the network system. to identify and prune the adversaries, pruning route modifiers (prm) algorithm based on the collaborative authentication system has been proposed. the algorithm considers the random deployment of sensor nodes. based on the collaborative authentication scheme, the prm algorithm can save energy by pruning the malicious nodes at early stage. the simulation results and theoretical analysis reveals that prm algorithm is effective in terms of efficient and secure routing. this algorithm reduces the consumption of energy by pruning and establishes the shortest path that leads to efficient network and enables security.
machine_learning	objective. automated behavioral state classification can benefit next generation implantable epilepsy devices. in this study we explored the feasibility of automated awake (aw) and slow wave sleep (sws) classification using wide bandwidth intracranial eeg (ieeg) in patients undergoing evaluation for epilepsy surgery. approach. data from seven patients (age $34\pm 12$ , 4 women) who underwent intracranial depth electrode implantation for ieeg monitoring were included. spectral power features (0.1600 hz) spanning several frequency bands from a single electrode were used to train and test a support vector machine classifier. main results. classification accuracy of 97.8 +/- 0.3% (normal tissue) and 89.4 +/- 0.8% (epileptic tissue) across seven subjects using multiple spectral power features from a single electrode was achieved. spectral power features from electrodes placed in normal temporal neocortex were found to be more useful (accuracy 90.8 +/- 0.8%) for sleep-wake state classification than electrodes located in normal hippocampus (87.1 +/- 1.6%). spectral power in high frequency band features (ripple (80250 hz), fast ripple (250600 hz)) showed comparable performance for aw and sws classification as the best performing berger bands (alpha, beta, low gamma) with accuracy ?90% using a single electrode contact and single spectral feature. significance. automated classification of wake and sws should prove useful for future implantable epilepsy devices with limited computational power, memory, and number of electrodes. applications include quantifying patient sleep patterns and behavioral state dependent detection, prediction, and electrical stimulation therapies.
bioinformatics	helicases play a critical role in processes such as replication or recombination by unwinding double-stranded dna; mutations of these genes can therefore have devastating biological consequences. in humans, mutations in genes of three members of the recq family helicases (blm, wrn, and recq4) give rise to three strikingly distinctive clinical phenotypes: bloom syndrome, werner syndrome, and rothmund-thomson syndrome, respectively. however, the molecular basis for these varying phenotypic outcomes is unclear, in part because a full mechanistic description of helicase activity is lacking. because the helicase core domains are highly conserved, it has been postulated that functional differences among family members might be explained by significant differences in the n-terminal domains, but these domains are poorly characterized. to help fill this gap, we now describe bioinformatics, biochemical, and structural data for three vertebrate blm proteins. we pair high resolution crystal structures with saxs analysis to describe an internal, highly conserved sequence we term the dimerization helical bundle in n-terminal domain (dhbn). we show that, despite the n-terminal domain being loosely structured and potentially lacking a defined three-dimensional structure in general, the dhbn exists as a dimeric structure required for higher order oligomer assembly. interestingly, the unwinding amplitude and rate decrease as blm is assembled from dimer into hexamer, and also, the stable dhbn dimer can be dissociated upon atp hydrolysis. thus, the structural and biochemical characterizations of n-terminal domains will provide new insights into how the n-terminal domain affects the structural and functional organization of the full blm molecule.
algorithm_design	mobile hotspots have made the dream of ubiquitous internet access come true, while the widespread applications are still hindered by the limited power of smart phones. to address this issue, we propose a novel distributed cooperative data transmission scheme for energy-rechargeable mobile devices. in particular, we not only let a mobile phone help the nearby client devices connect to the internet via its cellular accessing, but also let those clients replenish the mobile hotspot energy via wireless power transfer. we mathematically formulate the mutually beneficial relationship between mobile hotspots and clients into an optimization problem, with the objective of conducting the cooperative wireless data and energy transmission to maximize the system utility. resorting to methods from combinatorics and matching theory, we develop a near optimal solution for many-to-one matching when there is a single mobile hotspot and a distributed matching strategy for the general case by considering the nature of data communication and the characteristic of wireless power transfer. by extensive simulation, we show that the proposed distributed solution achieves a performance close to the centralized method, and it outperforms the greedy matching strategy and the classic gale-shapley matching strategy in different scenarios.
relational_databases	in this paper the process and program model of universal microelectromechanical systems data extracting and update mechanism from different relational databases to nosql database mongodb is described.
software_engineering	game development is an interdisciplinary concept that embraces software engineering, business, management, and artistic disciplines. this research facilitates a better understanding of the business dimension of digital games. the main objective of this research is to investigate empirically the effect of business factors on the performance of digital games in the market and to answer the research questions asked in this study. game development organizations are facing high pressure and competition in the digital game industry. business has become a crucial dimension, especially for game development organizations. the main contribution of this paper is to investigate empirically the influence of key business factors on the business performance of games. this is the first study in the domain of game development that demonstrates the interrelationship between key business factors and game performance in the market. the results of the study provide evidence that game development organizations must deal with multiple business key factors to remain competitive and handle the high pressure in the digital game industry. furthermore, the results of the study support the theoretical assertion that key business factors play an important role in game business performance. (c) 2015 elsevier b.v. all rights reserved.
software_engineering	this paper presents a literature review in the field of summarizing software artifacts, focusing on bug reports, source code, mailing lists and developer discussions artifacts. from jan. 2010 to apr. 2016, numerous summarization techniques, approaches, and tools have been proposed to satisfy the ongoing demand of improving software performance and quality and facilitating developers in understanding the problems at hand. since aforementioned artifacts contain both structured and unstructured data at the same time, researchers have applied different machine learning and data mining techniques to generate summaries. therefore, this paper first intends to provide a general perspective on the state of the art, describing the type of artifacts, approaches for summarization, as well as the common portions of experimental procedures shared among these artifacts. moreover, we discuss the applications of summarization, i.e., what tasks at hand have been achieved through summarization. next, this paper presents tools that are generated for summarization tasks or employed during summarization tasks. in addition, we present different summarization evaluation methods employed in selected studies as well as other important factors that are used for the evaluation of generated summaries such as adequacy and quality. moreover, we briefly present modern communication channels and complementarities with commonalities among different software artifacts. finally, some thoughts about the challenges applicable to the existing studies in general as well as future research directions are also discussed. the survey of existing studies will allow future researchers to have a wide and useful background knowledge on the main and important aspects of this research field.
image_processing	in this paper a novel and effective approach for automated audio classification is presented that is based on the fusion of different sets of features, both visual and acoustic. a number of different acoustic and visual features of sounds are evaluated and compared. these features are then fused in an ensemble that produces better classification accuracy than other state-of-the-art approaches. the visual features of sounds are built starting from the audio file and are taken from images constructed from different spectrograms, a gammatonegram, and a rhythm image. these images are divided into sub windows from which a set of texture descriptors are extracted. for each feature descriptor a different support vector machine (svm) is trained. the svms outputs are summed for a final decision. the proposed ensemble is evaluated on three well-known databases of music genre classification (the latin music database, the ismir 2004 database, and the gtzan genre collection), a dataset of bird vocalization aiming specie recognition, and a dataset of right whale calls aiming whale detection. the mat lab code for the ensemble of classifiers and for the extraction of the features will be publicly available (https://www.delunipclit/node/2357 +pattern recognition and ensemble classifiers). (c) 2017 elsevier b.v. all rights reserved.
parallel_computing	we propose a novel parallel computing framework for a nonlinear finite element method (fem)-based cell model and apply it to simulate avascular tumor growth. we derive computation formulas to simplify the simulation and design the basic algorithms. with the increment of the proliferation generations of tumor cells, the fem elements may become larger and more distorted. then, we describe a remesh and refinement processing of the distorted or over large finite elements and the parallel implementation based on message passing interface to improve the accuracy and efficiency of the simulation. we demonstrate the feasibility and effectiveness of the fem model and the parallelization methods in simulations of early tumor growth.
distributed_computing	searching for new efficient algorithms to solve complex optimization problems in big data scenarios is a priority, especially when the search space increases exponentially with the problem size, making impossible to find a solution through a mere blind search. networks of evolutionary processors (nep) is a formal framework formed of highly parallel and distributed computing models inspired and abstracted from biological evolution that is able to solve hard problems in an efficient way. however, nep is not expressive enough to model quantitative aspects present in many problems. in this paper we propose nepo, a new model based on the nep evolutionary processors. nepo deals with a class of data that is able to solve hard optimization problems and defines a novel selection process based on a quantitative filtering strategy. we present a linear time solution to a well known np-complete optimization problem (the 0/1 knapsack problem) in order to demonstrate nepo advantages. this result suggests that nepo 's quantitative filtering is more suitable to tackle practical solutions to optimization problems in order to deploy them on highly scalable distributed computational platforms. (c) 2016 elsevier b.v. all rights reserved.
cryptography	many identity-based proxy signature (ibps) schemes have been proposed, but most were proved to be secure using a random oracle model, which has attracted considerable criticism. cao and cao proposed an ibps scheme using the standard model, but their scheme was shown to be insecure because it could not resist a delegator attack. in order to overcome this weakness, gu et al. proposed a new ibps scheme in 2013 that uses the standard model and they also provided a detailed security model for ibps. however, in this study, we demonstrate that gu et al. 's scheme is still vulnerable to delegator attack. in order to correct this problem, we propose an improvement of the ibps scheme described by gu et al. we also present an efficiency analysis for our scheme and a detailed security proof based on the computational diffie-hellman assumption.
bioinformatics	background: variants of unknown significance (vuss) have been identified in brca1 and brca2 and account for the majority of all identified sequence alterations. notably, vuss occur disproportionately in people of african descent hampering breast cancer (bca) management and prevention efforts in the population. our study sought to identify and characterize mutations associated with increased risk of bca at young age. methods: in our study, the spectrum of mutations in brca1 and brca2 was enumerated in a cohort of 31 african american women of early age at onset breast cancer, with a family history of breast or cancer in general and/or with triple negative breast cancer. to improve the characterization of the brca1 and brca2 variants, bioinformatics tools were utilized to predict the potential function of each of the variants. results: using next generation sequencing methods and in silico analysis of variants, a total of 197 brca1 and 266 brca2 variants comprising 77 unique variants were identified in 31 patients. of the 77 unique variants, one (1.3%) was a pathogenic frameshift mutation (rs80359304; brca2 met591ile), 13 (16.9%) were possibly pathogenic, 34 (44.2%) were benign, and 29 (37.7%) were vuss. genetic epidemiological approaches were used to determine the association with variant, haplotype, and phenotypes, such as age at diagnosis, family history of cancer and family history of breast cancer. there were 5 brca1 snps associated with age at diagnosis; rs1799966 (p=. 045; log additive model), rs16942 (p=. 033; log additive model), rs1799949 (p=. 058; log additive model), rs373413425 (p=. 040 and.023; dominant and log additive models, respectively) and rs3765640 (p=. 033 log additive model). additionally, a haplotype composed of all 5 snps was found to be significantly associated with younger age at diagnosis using linear regression modeling (p=.023). specifically, the haplotype containing all the variant alleles was associated with older age at diagnosis (or= 5.03 95% ci=. 91-9.14). conclusions: knowing a patient 's brca mutation status is important for prevention and treatment decision-making. improving the characterization of mutations will lead to better management, treatment, and bca prevention efforts in african americans who are disproportionately affected with aggressive bca and may inform future precision medicine genomic-based clinical studies.
computer_programming	this paper analyzes the advantages and trends of holographic projection technology and the characteristics of large sports performance arrangement using the method of literature, aiming at providing new ideas for the performance arrangement, thus facilitating new artistic system of large sports performance in modern times. results show that the technology of holographic projection gets rid of the tedious steps in the past computer programming, overcomes the design flaws of 3d computer graphic simulation with its unique 360 - degree holographic imaging, and presents a more realistic three - dimensional image whose effect is closer to actual practice. it provides with a better idea of arrangement of large opening and closing ceremonies as well as large sports performances.
cryptography	a new secure key distribution scheme based on the dynamic chaos synchronization of two cascaded semiconductor laser systems (cslss) subject to common chaotic injection and random phase-modulated optical feedback is demonstrated. in this scheme, alice and bob adopt two independent random sequences to control the phase modulators of cslss, which induces a dynamic perturbation to the chaos synchronization. we thoroughly investigate the chaos synchronization performance under different phase-shift conditions with cross-correlation function, and systematically discuss the feasibility and security of the system. the results show that, with proper injection and feedback strength, the correlation coefficient gap between phase shift match and mismatch is clear and robust to the parameter mismatches in the cslss and those between the two cslss. based on this, high-quality key distribution can be performed by picking out the identical random bits from the two independent random sequences according to the computational correlation. moreover, the investigations on the information theoretic security and rate of the key distribution show that the security of the key distribution scheme can be further enhanced by properly increasing the number of layers in the cslss or employing highorder modulation format.
computer_vision	understanding continuous human actions is a non-trivial but important problem in computer vision. although there exists a large corpus of work in the recognition of action sequences, most approaches suffer from problems relating to vast variations in motions, action combinations, and scene contexts. in this paper, we introduce a novel method for semantic segmentation and recognition of long and complex manipulation action tasks, such as ""preparing a breakfast"" or ""making a sandwich"". we represent manipulations with our recently introduced ""semantic event chain"" (sec) concept, which captures the underlying spatiotemporal structure of an action invariant to motion, velocity, and scene context. solely based on the spatiotemporal interactions between manipulated objects and hands in the extracted sec, the framework automatically parses individual manipulation streams performed either sequentially or concurrently. using event chains, our method further extracts basic primitive elements of each parsed manipulation. without requiring any prior object knowledge, the proposed framework can also extract object-like scene entities that exhibit the same role in semantically similar manipulations. we conduct extensive experiments on various recent datasets to validate the robustness of the framework.
software_engineering	a plethora of multi agent systems (mas) development methodologies exists and all compete for prominence. this paper advocates unification of best of breed activities from these methodologies and examines two existing approaches for unifying access to them. it proposes an alternative approach that focusses on the use of domain knowledge through ontologies as offering the best potential for unifying access to them. the reliance on ontologies will provide flexibility in the process and workproducts use within the methodology. the focus on domain knowledge will reduce the number of mandatory methodological tasks and at the same time create scope for reuse with respect to both system designs and components. the paper will further sketch and argue for a full software development lifecycle for mas where ontologies expressing domain knowledge are the central artifacts.
bioinformatics	background: tuberculosis remains a major global threat. two billion of the world 's population is latently infected with mycobacterium tuberculosis and is at the risk of progression to active disease. bacillus calmette-guerin (bcg), as the only licensed vaccine, has prophylaxis strategy, which protects children from disseminated form of tuberculosis. therefore, postexposure vaccine strategy, which targets individuals with latent tuberculosis infection, is an important strategy to control this disease globally. objectives: in the present study, we designed a novel postexposure multi-epitope dna construct based on 3 latency-associated antigens of rv2029c, rv2031c, and rv2627c and microtubule-associated protein light chain 3 (lc3) as a hallmark protein of the autophagy system. methods: a mouse construct was designed based on predicted mhc class i-and class ii-restricted t-cell epitopes that fused together tandemly. mhc class i-and class ii-restricted epitopes were linked by aay and gpgpg motifs, respectively. lc3 directly fused to the mhc class ii-restricted epitopes at the c-terminus of the peptide. the varieties of expressed construct features were analyzed by bioinformatics tools. finally, construct codons were optimized and mrna structure of optimized construct was analyzed. results: mhc class i-and class ii-predicted epitopes showed a high potential to binding to human hlas alleles, with global broad-spectrum population coverage. the construct had no allergenicity, and the analysis indicated a desirable antigenicity of the construct. the construct had several posttranslational modifications, no signal peptide, and cytoplasmic localization with high score. also, mrna analysis showed low delta g which demonstrated high stability and efficient translation. conclusions: the results revealed that the novel multi-epitope dna construct could be an effective candidate in tuberculosis vaccine development, and it is qualified to investigate its potential to induce cd4 and cd8 t-cell immune response in the experimental animal model.
operating_systems	operating systems interface between hardware and the user, random numbers are useful for security and simulation, and file systems form the program access to them in a modern operating system. blending these items into a remotely accessed infrastructure forms the basis for supporting operating systems projects. this work describes the hardware, software, and communication infrastructure to support student projects by sharing remote hardware to acquire background radiations events with a geiger counter, transforming those events into random numbers, and providing those numbers through a custom file system. collectively, the hardware and software provide an inexpensive remote laboratory experience for computing students.
operating_systems	memory diagnostics are important to improving the resilience of dram main memory. as bit cell size reaches physical limits, dram memory will be more likely to suffer both transient and permanent errors. memory diagnostics that operate online can be a component of a comprehensive strategy to allay errors. this paper presents a novel approach, asteroid, to integrate online memory diagnostics during workload execution. the approach supports diagnostics that adapt at runtime to workload behavior and resource availability to maximize test quality while reducing performance overhead. we describe asteroid 's design and how it can be efficiently integrated with a hierarchical memory allocator in modern operating systems. we also present how the framework enables control policies to dynamically configure a diagnostic. using an adaptive policy, in a 16-core server, asteroid has modest overhead of 1-4 % for workloads with low to high memory demand. for these workloads, asteroid 's adaptive policy has good error coverage and can thoroughly test memory.
data_structures	graphs are considered to be one of the best studied data structures in discrete mathematics and computer science. hence, data mining on graphs has become quite popular in the past few years. the problem of finding frequent itemsets in conventional data mining on transactional databases, thus transformed to the discovery of subgraphs that frequently occur in the graph dataset containing either single graph or multiple graphs. most of the existing algorithms in the field of frequent subgraph discovery adopts an apriori approach based on generation of candidate set and test approach. the problem with this approach is the costlier candidate set generation, particularly when there exist more number of large subgraphs. the research goals in frequent subgraph discovery are to evolve (i) mechanisms that can effectively generate candidate subgraphs excluding duplicates and (ii) mechanisms that find best processing techniques that generate only necessary candidate subgraphs in order to discover the useful and desired frequent subgraphs. in this paper, a two phase approach is proposed by integrating apriori algorithm on graphs to frequent subgraph (fs) tree to discover frequent subgraphs in graph datasets.
software_engineering	one of the most important parts in developing any kind of software is to check the sustainability of the software which is done by the software testing by the software engineers. a varied number of test cases have been taken into account and the test is done by using the unified modelling language (uml). by using the combination of the hybrid revised genetic algorithm along with the bee colony optimization, the test has come out to be in the favour of finding the maximum and the minimum coverage. this novel approach has been tested over the application of a smart card users/security token user. in order to increase the security of the smart card/security token (sc/st), a novel approach for smart card/security token (sc/st) has been proposed,is a decision making structure for self- organising systems based on software engineering. by this secured details can be obtained from the total number of smart card users. by this approach, the total flaws present in the system can be easily detected along with the maximum possible paths. the novel technique for software testing guarantees that minimum time has been considered for maximum coverage. (c) 2016 elsevier ltd. all rights reserved.
bioinformatics	background: autophagy is a conserved molecular pathway involved in the degradation and recycling of cellular components. it is active either as response to starvation or molecular damage. evidence is emerging that autophagy plays a key role in the degradation of damaged cellular components and thereby affects aging and lifespan control. in earlier studies, it was found that autophagy in the aging model podospora anserina acts as a longevity assurance mechanism. however, only little is known about the individual components controlling autophagy in this aging model. here, we report a biochemical and bioinformatics study to detect the protein-protein interaction (ppi) network of p. anserina combining experimental and theoretical methods. results: we constructed the ppi network of autophagy in p. anserina based on the corresponding networks of yeast and human. we integrated paatg8 interaction partners identified in an own yeast two-hybrid analysis using atg8 of p. anserina as bait. additionally, we included age-dependent transcriptome data. the resulting network consists of 89 proteins involved in 186 interactions. we applied bioinformatics approaches to analyze the network topology and to prove that the network is not random, but exhibits biologically meaningful properties. we identified hub proteins which play an essential role in the network as well as seven putative sub-pathways, and interactions which are likely to be evolutionary conserved amongst species. we confirmed that autophagy-associated genes are significantly often up-regulated and co-expressed during aging of p. anserina. conclusions: with the present study, we provide a comprehensive biological network of the autophagy pathway in p. anserina comprising ppi and gene expression data. it is based on computational prediction as well as experimental data. we identified sub-pathways, important hub proteins, and evolutionary conserved interactions. the network clearly illustrates the relation of autophagy to aging processes and enables further specific studies to understand autophagy and aging in p. anserina as well as in other systems.
machine_learning	clustering is one of the basic tasks in data mining and machine learning which aims at discovering hidden structure in the data. for many real-world applications, there often exist many different yet meaningful clusterings while most of existing clustering methods only produce a single clustering. to address this limitation, multiple clustering, which tries to generate clusterings that are high quality and different from each other, has emerged recently. in this paper, we propose a novel alternative clustering method that generates non-redundant multiple clusterings sequentially. the algorithm is built upon nonnegative matrix factorization, and we take advantage of the nonnegative property to enforce the non-redundancy. specifically, we design a quadratic term to measure the redundancy between the reference clustering and the new clustering, and incorporate it into the objective. the optimization problem takes on a very simple form, and can be solved efficiently by multiplicative updating rules. experimental results demonstrate that the proposed algorithm is comparable to or outperforms existing multiple clustering methods.
computer_vision	motion segmentation and non-rigid structure from motion are two challenging computer vision problems that have attracted numerous research interests. while the previous works handle these two problems separately, we present a general motion segmentation framework in this paper for solving these two seemingly different problems in a unified manner. at the heart of our general motion segmentation framework is a model selection mechanism based on finding the minimal basis subspace representation, by seeking the joint sparse representation of the data matrix. however, such formulation is np-hard and we solve the convex proxy instead. unlike other compressive sensing related works, this convex proxy solution is insufficient for our problem. the convex relaxation artefacts and noise yield multiple subspace representations, making identification of the exact number of motion subspaces challenging. we solve for the right number of subspaces by transforming this problem into a facility location problem with global cost and solve the factor graph formulation using max product belief propagation message passing.
cryptography	in this paper, we propose a new public key scheme, which is a combination of rsa variant namely the drsa and the generalization of generalized discrete logarithm problem (generalized gdlp). the security of this scheme depends equally on the integer factorization of n and the discrete logarithm problem (dlp) on z(n)*, where n is the product of two large primes and z(n)* is the multiplicative group modulo n. the scheme is a randomized algorithm. it is at least as secure as the drsa and elgamal schemes. we also compare the encryption-decryption performance of the proposed scheme with the rsa and drsa schemes.
distributed_computing	the suites of numerical models used for simulating climate of our planet are usually run on dedicated high-performance computing (hpc) resources. this study investigates an alternative to the usual approach, i.e. carrying out climate model simulations on commercially available cloud computing environment. we test the performance and reliability of running the cesm (community earth system model), a flagship climate model in the united states developed by the national center for atmospheric research (ncar), on amazon web service (aws) ec2, the cloud computing environment by amazon.com, inc. starcluster is used to create virtual computing cluster on the aws ec2 for the cesm simulations. the wall-clock time for one year of cesm simulation on the aws ec2 virtual cluster is comparable to the time spent for the same simulation on a local dedicated high-performance computing cluster with infiniband connections. the cesm simulation can be efficiently scaled with the number of cpu cores on the aws ec2 virtual cluster environment up to 64 cores. for the standard configuration of the cesm at a spatial resolution of 1.9 degrees latitude by 2.5 degrees longitude, increasing the number of cores from 16 to 64 reduces the wall-clock running time by more than 50% and the scaling is nearly linear. beyond 64 cores, the communication latency starts to outweigh the benefit of distributed computing and the parallel speedup becomes nearly unchanged.
network_security	this paper designs and implements an embedded security gateway based on double-homed structure which composed of software and hardware parts. the core of hardware platform is based on two s3c6410 processors and one ep1c18f4620 fpga. the software is based on reduced linux kernel 3.0.1. the gateway uses net-filter/ip-tables firewall, ipsec vpn and network isolation technologies. and it can effectively reduce the risk of transmitting information by public network and improve the defensive capability. so it can be applied to the business with high security level.
operating_systems	latency jitter is a pressing problem in virtual reality (vr) applications. this paper analyzes latency jitter caused by typical interprocess communication (ipc) techniques commonly found in today 's computer systems used for vr. test programs measure the seal ability and latencies for various ipc techniques, where increasing number of threads are performing the same task concurrently. we use four different implementations on a vanilla linux kernel as well as on a real-time (rt) linux kernel to further assess if a rt variant of a multiuser multiprocess operating system can prevent latency spikes and how this behavior would apply to different programming languages and ipc techniques. we found that linux rt can limit the latency jitter at the cost of throughput for certain implementations. further, coarse grained concurrency should be employed to avoid adding up of scheduler latencies, especially for native system space ipc, while actor systems are found to support a higher degree of concurrency granularity and a higher level of abstraction.
symbolic_computation	in this type functional variable method has been used to private type of nonlinear fractional differential equations. the main property of the method demonstrate in its flexibility and ability to solve nonlinear equations accurately, efficiency and conveniently. the fractional derivatives are described in the modified riemann-liouville sense. three examples, are presented to show the application of the present technique. as a result, periodic and hyperbolic solutions are obtained.
image_processing	nucleic acids are responsible for the storage, transfer and realization of genetic information in the cell, which provides correct development and functioning of organisms. dna interaction with ligands ensures the safety of this information. over the past 10 years, advances in electron microscopy and image processing allowed to obtain the structures of key dna-protein complexes with resolution below 4 a. however, radiation damage is a limiting factor to the potentially attainable resolution in cryo-em. the prospect and limitations of studying protein-dna complex interactions using cryo-electron microscopy are discussed here. we reviewed the ways to minimize radiation damage in biological specimens and the possibilities of using radiation damage (so-called 'bubblegrams') to obtain additional structural information. (c) 2017 elsevier ltd. all rights reserved.
computer_vision	clinical decisions are sometimes based on a variety of patient 's information such as: age, weight or information extracted from image exams, among others. depending on the nature of the disease or anatomy, clinicians can base their decisions on different image exams like mammographies, positron emission tomography scans or magnetic resonance images. however, the analysis of those exams is far from a trivial task. over the years, the use of image descriptors-computational algorithms that present a summarized description of image regions-became an important tool to assist the clinician in such tasks. this paper presents an overview of the use of image descriptors in healthcare contexts, attending to different image exams. in the making of this review, we analyzed over 70 studies related to the application of image descriptors of different natures-e.g., intensity, texture, shape-in medical image analysis. four imaging modalities are featured: mammography, pet, ct and mri. pathologies typically covered by these modalities are addressed: breast masses and microcalcifications in mammograms, head and neck cancer and alzheimer 's disease in the case of pet images, lung nodules regarding cts and multiple sclerosis and brain tumors in the mri section.
computer_vision	salmon gelatin and boldine as a natural antioxidant were used to prepare edible films by a cold casting method. the concentration of each component was optimised by applying a box-behnken experimental design (bbd) with the goal of maximising radical scavenging capacity of film forming suspensions (ffs) measured by the 2,2-dipheny1-1-picrylhydrazyl (dpph) free radical assay. the results showed synergistic effect between gelatin and boldine for the antioxidant capacity (radical scavenging of over 80%) and antimicrobial activity of gelatin against escherichia coli atcc 25922 and listeria monocytogenes isp 6508. the release of boldine into the food simulant was faster for films containing 2 % gelatin than for those containing 4 %. kinetic data for boldine release from films fitted to the weibull model (r = 0,99). possible molecular interactions between gelatin and boldine were observed in the ftir spectrum of the composite films. within the range of 1638 to 1628 cm(-1) a strong interference caused by boldine in the hydrogen bonding between water and imide residues was observed. owing to the simultaneous antioxidant and antimicrobial activities displayed for the gelatinboldine films, there is potential for application in the preservation of some perishable fresh food such as fish, meat and cheese. (c) 2016 elsevier ltd. all rights reserved.
computer_vision	the proliferation of large number of images has made it necessary to develop systems for indexing and organizing images for easy access. this has made content-based image retrieval (cbir) an important area of research in computer vision. this paper proposes a combination of features in multiresolution analysis framework for image retrieval. in this work, the concept of multiresolution analysis has been exploited through the use of wavelet transform. this paper combines local binary pattern (lbp) with legendre moments at multiple resolutions of wavelet decomposition of image. first, lbp codes of discrete wavelet transform (dwt) coefficients of images are computed to extract texture feature from image. the legendre moments of these lbp codes are then computed to extract shape feature from texture feature for constructing feature vectors. these feature vectors are used to search and retrieve visually similar images from large database. the proposed method has been tested on five benchmark datasets, namely, corel-1k, olivia-2688, corel-5k, corel-10k, and ghim-10k, and performance of the proposed method has been measured in terms of precision and recall. the expetimental results demonstrate that the proposed method outperforms some of the other state-of-the-art methods in terms of precision and recall. (c) 2016 elsevier inc. all rights reserved.
relational_databases	supply chain management is a critical domain for fast moving consumer goods (fmcgs). this domain is known for its complexity. new standards and regulations regarding energy efficiency and environmental aspects in general, as well as customer demand, make the analysis, modeling and design of the supply chain more and more complicated. partners involved in these processes are numerous and of diverse background. to help solving this problem, common understanding of the domain and exchange of information among partners involved in the supply chain is of high importance. an ontology capturing the knowledge of the domain was created. to achieve maximum efficiency of the domain operations in terms of cost, quality of service and environmental impact, concept definitions from multiple sources were gathered. an advanced software solution that leverages semantic web technologies, enables users to link data from multiple excel spreadsheets and relational databases together in real-time for data collection, collaboration, and reporting. in this framework, a new way for collaboration throughout the supply chain with the use of an underlying ontology, semantic technologies and visualization technics is introduced. the proposed approach is applied in the context of the fp7 european project e-save. (c) 2015 the authors. published by elsevier ltd.
data_structures	we describe a general framework c2i for generating an invariant inference procedure from an invariant checking procedure. given a checker and a language of possible invariants, c2i generates an inference procedure that iteratively invokes two phases. the search phase uses randomized search to discover candidate invariants and the validate phase uses the checker to either prove or refute that the candidate is an actual invariant. to demonstrate the applicability of c2i , we use it to generate inference procedures that prove safety properties of numerical programs, prove non-termination of numerical programs, prove functional specifications of array manipulating programs, prove safety properties of string manipulating programs, and prove functional specifications of heap manipulating programs that use linked list data structures.
operating_systems	in the new era of cyber-physical systems, software must adapt itself to ever-changing environmental conditions and situations. this is currently not reflected in the design of embedded operating systems, since they are primarily optimized for fixed usage scenarios with tight resource constraints. we discuss the idea of interpreted operating system kernels, which can form a new foundation for highly reconfigurable embedded systems. the paper elaborates reasonable use cases, shows comparable approaches from the past and sketches an implementation strategy that is based on a bare-metal python interpreter.
symbolic_computation	in this article, the prolongation structure technique is applied to a generalised inhomogeneous gardner equation, which can be used to describe certain physical situations, such as the stratified shear flows in ocean and atmosphere, ion acoustic waves in plasmas with a negative ion, interfacial solitary waves over slowly varying topographies, and wave motion in a non-linear elastic structural element with large deflection. the lax pairs, which are derived via the prolongation structure, are more general than the lax pairs published before. under the painleve conditions, the linear-damping coefficient equals to zero, the quadratic non-linear coefficient is proportional to the dispersive coefficient c(t), the cubic non-linear coefficient is proportional to c(t), leaving no constraints on c(t) and the dissipative coefficient d(t). we establish the prolongation structure through constructing the exterior differential system. we introduce two methods to obtain the lax pairs: (a) based on the prolongation structure, the lax pairs are obtained, and (b) via the lie algebra, we can derive the pfaffian forms and lax pairs when certain parameters are chosen. we set d(t) as a constant to discuss the influence of c(t) on the pfaffian forms and lax pairs, and to discuss the influence of d(t) on the pfaffian forms and lax pairs, we set c(t) as another constant. then, we get different prolongation structure, pfaffian forms and lax pairs.
distributed_computing	cloud computing which uses outsourcing and remote processing of applications first appeared about ten years ago. cloud computing built on research in virtualization, distributed computing, utility computing, and web services. it reduces the information technology overhead for starting a new business and it can be accessed from anywhere. one of the concepts used for constructing cloud computing is virtualization, which has its own security risks, but they are not specific to the cloud. the key drawback to adopting cloud computing is security since clients use someone else 's cpu and hard disk for processing and storing data. this paper proposes a security framework to secure virtual machine images in a virtualization layer in the cloud environment. securing the virtual machine image is significant as it will most probably affect the security of cloud computing.
data_structures	we study the problem of indexing a text t[1...n] such that whenever a pattern p[1...p] and an interval [alpha, beta] come as a query, we can report all pairs (i, j) of consecutive occurrences of p in t with alpha <= j - i <= beta. we present an o (n logn) space data structure with optimal o (p + k) query time, where k is the output size. (c) 2016 elsevier b.v. all rights reserved.
cryptography	in this paper, we propose two new algorithms and their hardware implementations for the normal basis multiplication over gf(p(m)), where p is an element of {2, 3}. in this case, the proposed multipliers are designed using serial and digit-serial hardware architectures. the normal basis multipliers over gf(2(m)) and gf(3(m)) are based on two proposed algorithms to compute the multiplication matrices t-k in order to speed-up the execution time and to reduce the area resources. it can be seen that the new hardware architecture for the nb multiplier over gf(2(m)) has the best characteristics of area complexity presented by reyhani [16] and time complexity presented by azarderakhsh and reyhani [31]. the proposed hardware architectures for the normal basis multipliers over gf(2(163)), gf(2(233)), gf(2(283)), gf(2(409)), gf(3(89)) and gf(3(233)) were described in vhdl, and simulated and synthesized using modelsim and quartus prime v16, respectively.
operating_systems	many-core architectures trade single-thread performance for a larger number of cores. scalable throughput can be attained only by a high degree of parallelism and minimized synchronization. whilst this is achievable for many applications, the operating system still introduces bottlenecks through non-local sharing, synchronization, and message passing. a particular challenge for highly dynamic applications, for example invasive hpc applications and elastic compute clouds, is the management of short-living application threads and processes. this paper discusses os architecture choices based on microkernel, multikernel and distributed systems designs and our development experience in the context of the mythos project. initial experiments show a much faster thread creation and activation compared to monolithic systems like linux while providing a more flexible protection and threading model that is better suited for dynamic scenarios. however, despite significant progress in the overall domain of operating systems, the design space for scalable many-core operating systems is yet to be fully explored.
network_security	against multipoint eavesdropping attack in elastic optical networks, new objective criterion, security strategy and extended routing and spectrum allocation algorithm is proposed the simulation results shows that better security guarantee and resource utilization are provided.
distributed_computing	this paper presents a cloud-based system framework based on bigtable and mapreduce as the data storage and processing paradigms for providing a web-based service for viewing, storing, and analyzing massive building information models (bims). cloud and web 3d technologies were utilized to develop a bim data center that can handle the big data of massive blms using multiple servers in a distributed manner and can be accessed by multiple users to concurrently submit and view bims online in 3d. traditional bim include only static information such as the geometric parameters, physical properties, and spatial relations for modeling a physical space. in this study, bim was extended to dynamic bim, which includes dynamic data such as historical records from the monitoring of the facility environment and usage. owing to this extension, a dynamic bim became a parametric model, which can be used to simulate user behaviors. on the client side, this study applied webgl in the web interface development to achieve the display of bims in 3d on browsers. users can access the services via various online devices anytime and anywhere to view the 3d model online. on the server side, this study used apache hadoop, which can utilize multiple servers to provide mass storage spaces in a distributed manner with bigtable-like structured storage, to establish the bim data center. a schema for storing the big data of massive dynamic bims in bigtables was proposed. mapreduce, a hadoop component for the parallel processing of large data sets, was utilized to process big data from dynamic bims. a big data analysis framework to effectively retrieve and calculate required information from dynamic bims in the data center for various applications by mapreduce distributed computing was proposed this study. we provide principle and architecture of the proposed framework along with its experimental assessment. the results confirmed that scalable and reliable management of massive bims can be achieved using the proposed framework. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	in medical decision support systems, both the accuracy (i.e., the ability to adequately represent the decision making processes) as well as the transparency and interpretability (i.e., the ability to provide a domain user with compact and understandable explanation and justification of the proposed decisions) play essential roles. this paper presents an approach for automatic design of fuzzy rule-based classification systems (frbcss) from medical data using multi-objective evolutionary optimization algorithms (moeoas). our approach generates, in a single run, a collection of solutions (medical frbcss) characterized by various levels of accuracy-interpretability trade-off. we propose a new complexity-related interpretability measure and we address the semantics-related interpretability issue by means of efficient implementation of the so-called strong fuzzy partitions of attribute domains. we also introduce a special coding-free representation of the rule base and original genetic operators for its processing as well as we implement our ideas in the context of well-known and one of the presently most advanced moe0a5, i.e., non-dominated sorting genetic algorithm ii (nsga-ii). an important part of the paper is devoted to a broad comparative analysis of our approach and as many as 26 alternative techniques arranged in 32 experimental set-ups and applied to three well-known benchmark medical data sets (breast cancer wisconsin (original), pima indians diabetes, and heart disease (cleveland)) available from the uci repository of machine learning databases (http://archive.ics.uci.edu/ml). a number of useful in medical applications performance measures including accuracy, sensitivity, specificity, and several interpretability measures are employed. the results of such a broad comparative analysis demonstrate that our approach significantly outperforms the alternative methods in terms of the interpretability of the obtained frbcss while remaining either competitive or superior in terms of their accuracy. it is worth stressing that the overwhelming majority of the existing medical classification methods concentrate almost exclusively on the accuracy issues. (c) 2016 elsevier ltd. all rights reserved.
bioinformatics	chinese herbal medicine (chm) plays a significant role in breast cancer treatment. we conduct the study to ascertain the relative molecular targets of effective chinese herbs in treating stage iv breast cancer. survival benefit of chm was verified by kaplan-meier method and cox regression analysis. a bivariate correlation analysis was used to find and establish the effect of herbs in complex chm formulas. a network pharmacological approach was adopted to explore the potential mechanisms of chm. patients in the chm group had a median survival time of 55 months, which was longer than the 23 months of patients in the non-chm group. cox regression analysis indicated that chm was an independent protective factor. correlation analysis showed that 10 herbs were strongly correlated with favorable survival outcomes (p<0.01). bioinformatics analyses suggested that the 10 herbs might achieve anti-breast cancer activity primarily through inhibiting hsp90, er alpha and top-ii related pathways.
bioinformatics	decidualization of endometrial stromal cells is an important feature of implantation and pregnancy. the molecular mechanism underlying decidualization remains unclear, particularly regarding the microrna (mirna/mir) regulation of this process. the present study revealed the temporal and spatial distribution of mmu-mir-96 in the mouse uterus during early pregnancy by reverse transcription-quantitative polymerase chain reaction and in situ hybridization. in addition, primary stromal cells were isolated from the mouse uterus and used to explore the role of mmu-mir-96 in decidualization. the results demonstrated that mmu-mir-96 was highly expressed in stromal cells during pregnancy, and was upregulated at implantation sites. in addition, mmu-mir-96 was strongly expressed during decidualization, which indicates that it may serve a role in the decidualization of stromal cells. based on existing reports, mmu-mir-96 participates in apoptosis; therefore the present study investigated its effects on the apoptosis of primary endometrial stromal cells. the results indicated that overexpression of mmu-mir-96 may induce apoptosis of stromal cells. in further studies regarding the underlying mechanism, the target genes of mmu-mir-96 were screened by bioinformatics analysis, and it was confirmed that b-cell lymphoma 2, an anti-apoptotic gene, was the target of mmu-mir-96, as determined using a reporter gene assay. in conclusion, the present study suggested that mmu-mir-96 participates in the decidualization of endometrial stromal cells in mice, thereby serving a key role in pregnancy.
network_security	protecting control planes in networking hardware from high rate packets is a critical issue for networks under operation. one common approach for conventional networking hardware is to offload expensive functions onto hard-wired offload engines as asics. this approach is inadequate for openflow networks because it restricts a certain amount of flexibility for network control that openflow tries to provide. therefore, we need a control plane protection mechanism in openflow switches as a last resort, while preserving flexibility for network control. in this paper, we propose a mechanism to filter out packet-in messages, which include packets handled by the control plane in openflow networks, without dropping important ones for network control. switches record values of packet header fields before sending packet-in messages, and filter out packets that have the same values as the recorded ones. the controllers set the header fields in advance whose values must be recorded, and the header fields are selected based on controller design. we have implemented and evaluated the proposed mechanism on a prototype software switch, concluding that it dramatically reduces cpu loads on switches while passes important packet-in messages for network control.
symbolic_computation	through symbolic computation with maple, the rational solutions and the lump solutions of the generalized (3 + 1)-dimensional shallow water-like equation are presented by using the generalized bilinear operator when the parameter p = 3. it is pointed that these rational solutions are classified into five classes because they are obtained mainly by depending on the polynomial solutions. the resulting lump solutions which are rationally localized in all directions in the space are acquired by making use of the quadratic function. however, not every nonlinear partial differential equation has the lump solutions, if any, the quantity of the lump solutions is fewer than the rational solutions. only one class of lump solutions of the generalized (3 + 1)-dimensional shallow water-like equation is gotten and three 3d plots with specific values of the involved parameters are plotted. (c) 2016 elsevier ltd. all rights reserved.
image_processing	in this study, the temperature and viscosity-dependent methods were used to identify the main heat conduction mechanism in nanofluids. three sets of experiments were conducted to investigate the effects of brownian motion and aggregation. image processing approach was used to identify detailed configurations of different nanofluids microstructures. the thermal conductivity of the nanofluids was measured with respect to the dynamic viscosity in the temperature range between 0 and 55 degrees c. the results clearly indicated that the nanoparticle brownian motion did not play a significant role in heat conduction of nanofluids, which was also supported by the observation that a more viscous sample rendered a higher thermal conductivity. moreover, the microscopic pictures and the differences in the viscosity between theoretical and experimental values suggested the major role of particle aggregation and clustering. (c) 2017 elsevier ltd. all rights reserved.
distributed_computing	attention is a very important cognitive and behavioral process, by means of which an individual is able to focus on a single aspect of information, while ignoring others. in a time in which we are drawn in notifications, beeps, vibrations and blinking messages, the ability to focus becomes increasingly important. this is true in many different domains, from the workplace to the classroom. in this paper we present a non-intrusive distributed system for monitoring attention in teams of people. it is especially suited for teams working at the computer. the presented system is able to provide real-time information about each individual as well as information about the team. it can be very useful for team managers to identify potentially distracting events or individuals, as well as to detect the onset of mental fatigue or boredom, which significantly influence attention. in the overall, this tool may prove very useful for team managers to implement better human resources management strategies.
bioinformatics	the ex vivo challenge assay is being increasingly used as an efficacy endpoint during early human clinical trials of hiv prevention treatments. there is no standard methodology for the ex vivo challenge assay, although the use of different data collection methods and analytical parameters may impact results and reduce the comparability of findings between trials. in this analysis, we describe the impact of data imputation methods, kit type, testing schedule and tissue type on variability, statistical power, and ex vivo hiv growth kinetics. data were p24 antigen (pg/ml) measurements collected from clinical trials of candidate microbicides where rectal (n = 502), cervical (n = 88), and vaginal (n = 110) tissues were challenged with hiv-1(bal) ex vivo. imputation of missing data using a nonlinear mixed effect model was found to provide an improved fit compared to imputation using half the limit of detection. the rectal virus growth period was found to be earlier and of a relatively shorter duration than the growth period for cervical and vaginal tissue types. on average, only four rectal tissue challenge assays in each treatment and control group would be needed to find a one log difference in p24 to be significant (alpha = 0.05), but a larger sample size was predicted to be needed for either cervical (n = 21) or vaginal (n = 10) tissue comparisons. overall, the results indicated that improvements could be made in the design and analysis of the ex vivo challenge assay to provide a more standardized and powerful assay to compare efficacy of microbicide products.
operating_systems	configuration options are widely used for customizing the behavior and initial settings of software applications, server processes, and operating systems. their distinctive property is that each option is processed, defined, and described in different parts of a software project - namely in code, in configuration file, and in documentation. this creates a challenge for maintaining project consistency as it evolves. it also promotes inconsistencies leading to misconfiguration issues in production scenarios. we propose an approach for detection of inconsistencies between source code and documentation based on static analysis. our approach automatically identifies source code locations where options are read, and for each such location retrieves the name of the option. inconsistencies are then detected by comparing the results against the option names listed in documentation. we evaluated our approach on multiple components of apache hadoop, a complex framework with more than 800 options. our tool orplocator was able to successfully locate at least one read point for 93% to 96% of documented options within four hadoop components. a comparison with a previous state-of-the-art technique shows that our tool produces more accurate results. moreover, our evaluation has uncovered 4 previously unknown, real-world inconsistencies between documented options and source code.
symbolic_computation	the algebraic and algorithmic study of integro-differential algebras and operators has only started in the past decade. integro-differential operators allow us in particular to study initial value and boundary problems for linear odes from an algebraic point of view. differential operators already provide a rich algebraic structure with a wealth of results and algorithmic methods. adding integral operators and evaluations, many new phenomena appear, including zero devisors and non-finitely generated ideals. in this tutorial, we give an introduction to symbolic methods for integro-differential operators and boundary problems developed over the last years. in particular, we discuss normal forms, basic algebraic properties, and the computation of polynomial solutions for ordinary integro-differential equations with polynomial coefficients. we will also outline methods for manipulating and solving linear boundary problems and illustrate them with an implementation.
software_engineering	the energy consumption of computer systems has become an important economic and environmental issue. many researchers have focused on the energy consumption of hardware, but what about the software? software energy consumption is widely adopted for green computation of practical experimentation in research laboratories. but current researchers fail to build a consistent concept base for software energy consumption of critical applications. while branch coverage and concolic testing are very critical practices to validate the safety critical systems, very little effort is given to measure their energy consumption. the computation of the energy consumption of these techniques is an important issue in green it and green software engineering. the contribution of this paper is to automate the computation and analysis of the energy consumption of the testing technique while enhancing the branch coverage using concolic testing. we implement our proposed automation framework in a tool, named green analysis of branch coverage enhancement. the empirical study with forty java programs and the evaluation results show that our developed tool achieves an average increase of 13.5 % in branch coverage. the average energy consumption of our automated tool is approximately 5.6 kj to compute the branch coverage for all the forty experimental programs.
computer_vision	bacterial nanocellulose (bnc) is an emerging nanomaterial with a morphologic structure of a 3-d network and unique properties produced by several species of bacteria. the objective of the present work was to evaluate whether the addition of bnc improved the baking quality of wheat flours, making a change in the viscoelastic behavior of the mass. a study of the rheological behavior of wheat bread dough containing bnc was performed by thermo-rheological and isothermal dynamic oscillatory experiments. the baking response and bread quality parameters were also analyzed. bnc increased specific volume, and moisture retention, decreasing browning index. although bnc produced both raw and heat-treated doughs with more elastic characteristics, textural studies revealed that the addition of bnc reduced firmness of bread crumb. confocal laser scanning microscopy observations showed differences in gluten filaments between control and bnc crumb samples that could explain the larger average porous size of bnc crumb. bnc could be used as improver in the bread-making performance. (c) 2017 elsevier ltd. all rights reserved.
cryptography	we have realized a multifunctional aerial display. an aerial image of a polarization-processing display is formed through aerial imaging by retro-reflection. by changing the polarization modulation patterns, we can switch between a three-layered display and a secure display.
network_security	cyber-physical embedded systems (cpess) are distributed embedded systems integrated with various actuators and sensors. when it comes to the issue of cpes security, the most significant problem is the security of embedded sensor networks (esns). with the continuous growth of esns, the security of transferring data from sensors to their destinations has become an important research area. due to the limitations in power, storage, and processing capabilities, existing security mechanisms for wired or wireless networks cannot apply directly to esns. meanwhile, esns are likely to be attacked by different kinds of attacks in industrial scenarios. therefore, there is a need to develop new techniques or modify the current security mechanisms to overcome these problems. in this article, we focus on intrusion detection (id) techniques and propose a new attack-defense game model to detect malicious nodes using a repeated game approach. as a direct consequence of the game model, attackers and defenders make different strategies to achieve optimal payoffs. importantly, error detection and missing detection are taken into consideration in intrusion detection systems (idss), where a game tree model is introduced to solve this problem. in addition, we analyze and prove the existence of pure nash equilibrium and mixed nash equilibrium. simulations show that the proposed model can both reduce energy consumption by up to 50% compared with the existing all monitor (am) model and improve the detection rate by up to 10% to 15% compared with the existing cluster head (ch) monitor model.
relational_databases	in this paper, we consider restricted data sharing between a set of parties that wish to provide some set of online services requiring such data sharing. each party is assumed to store its data in private relational databases, and is given a set of mutually agreed set of authorization rules that specify access to attributes over individual relations or joins over relations owned by one or more parties. the access restrictions introduce significant additional complexity in rule enforcement and query planning as compared with a traditional distributed database environment. we examine the problem of minimum cost rule enforcement which simultaneously checks for the enforceability of each rule and generation of minimum cost plan of its execution. however, the paper is not focused on specific cost functions, but instead of efficient methods for enforcing rules in the face of access restrictions and inter-party data transfer needs. we propose an efficient heuristic algorithm for this minimal enforcement since the exact problem is np-hard. in some cases, it is not possible to enforce the rules with the regular parties only. in such cases, we need help of trusted third parties (tps). if all parties trust a single tp, such a party can enforce all unenforced rules, but it is desirable to use the tp minimally. we also consider the extended case where multiple tps are required since not every regular party can trust a single tp.
parallel_computing	a review and comparative analyses of methods for restricting the range of molecular interactions within the concept of atom-atom potentials are presented. emphasis is placed on the problem of calculating the electrostatic energy in models with periodic boundary conditions. numerous calculations of the thermodynamic and structural characteristics of water using parallel monte carlo computations have shown that the use of functional forms simulating the electric potentials of ""screened charges"" provides very good results.
computer_vision	computer vision-based human activity recognition (har) has become very famous these days due to its applications in various fields such as smart home healthcare for elderly people. a video-based activity recognition system basically has many goals such as to react based on people 's behavior that allows the systems to proactively assist them with their tasks. a novel approach is proposed in this work for depth video based human activity recognition using joint-based motion features of depth body shapes and deep belief network (dbn). from depth video, different body parts of human activities are segmented first by means of a trained random forest. the motion features representing the magnitude and direction of each joint in next frame are extracted. finally, the features are applied for training a dbn to be used for recognition later. the proposed har approach showed superior performance over conventional approaches on private and public datasets, indicating a prominent approach for practical applications in smartly controlled environments.
computer_programming	this article introduces the educational functions of a computer program developed and put into practice for the computer-aided instruction of the finite element method. an interactive simulation with graphical visualization is used as a tool to teach and learn the concepts and procedures related to the construction and solution of finite element stiffness equations. the tool encompasses the stages of processing finite element equations from element modeling to equation solving. any operation of this educational tool is not an emulation of computational processes, but an actual execution of finite element processing. thus, instructors of the finite element method may use this educational program as a tool to present complex computational aspects in comprehensible graphic images. students may experience and explore each stage of the computation using the interactive simulation, and attain a better understanding of the concepts and procedures of the stiffness method, bypassing manual calculation or computer programming. the instructional tool has been implemented as a part of a finite element analysis program. (c) 2013 wiley periodicals, inc. comput appl eng educ 23:157-169, 2015; view this article online at ; doi
computer_programming	sorting is one of the important algorithms in computer programming. the ordinary 6 sorting algorithms are analyzed and compare from the algorithmic time complexity and stability. the executive efficiency of 6 sorting algorithms is verified by java program. the costing time and stability of 6 sorting algorithms are compared to provide certain reference for sorting algorithm.
symbolic_computation	in this paper, we construct soliton solutions for a generalized variable-coefficient coupled hirota-maxwell-bloch system, which can describe the ultrashort optical pulse propagation in a nonlinear, dispersive fiber doped with two-level resonant atoms. under certain transformations and constraints, one- and two-soliton solutions are obtained via the hirota method and symbolic computation, and soliton collisions are graphically presented and analyzed. one soliton is shown to maintain its amplitude and shape during the propagation. soliton collision is elastic, while bright two-peak solitons and dark two-peak solitons are also observed. we discuss the influence of the coefficients for the group velocity, group-velocity dispersion (gvd), self-phase modulation, distribution of the dopant, and stark shift on the soliton propagation and collision features, with those coefficients are set as some constants and functions, respectively. we find the group velocity and self-phase modulation can change the solitons' amplitudes and widths, and the solitons become curved when the gvd and distribution of the dopant are chosen as some functions. when the stark shift is chosen as a certain constant, the two peaks of bright two-peak solitons and dark two-peak solitons are not parallel. in addition, we observe the periodic collision of the two solitons.
computer_programming	computer programming is essential in engineering education. we are developing a programming education support tool pgtracer in order to facilitate learning process of computer programming. pgtracer utilizes fill-in-the-blank question composed of a pair of a source program and a trace table. we propose a set of feedback functions for the students in this paper. pgtracer automatically collects learning log of the students when they fill a blank. the feedback functions provide various analysis result of the collected log so that students can easily understand their achievement level and weak points. many students highly appreciate the feedback functions through an experimental evaluation of the functions.
computer_programming	in this paper, we describe the development of a support system that facilitates the process of learning computer programming through the reading of computer program source code. reading code consists of two steps: reading comprehension and meaning deduction. in this study, we developed a tool that supports the comprehension of a program 's reading. the tool is equipped with an error visualization function that illustrates a learner 's mistakes and makes them aware of their errors. we conducted experiments using the learning support tool and confirmed that the system is effective.
computer_graphics	we present a method for computing ambient occlusion (ao) for a stack of images of a lambertian scene from a fixed viewpoint. ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. while ao has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. we use our derived ao measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the mit intrinsic images benchmark. we also demonstrate our method on several synthetic and real scenes, including 3d printed objects with known ground truth geometry.
symbolic_computation	fractional derivatives are powerful tools in solving the problems of science and engineering. in this paper, an analytical algorithm for solving fractional differential-difference equations in the sense of jumarie 's modified riemann-liouville derivative has been described and demonstrated. the algorithm has been tested against time-fractional differential-difference equations of rational type via symbolic computation. three examples are given to elucidate the solution procedure. our analyses lead to closed form exact solutions in terms of hyperbolic, trigonometric, and rational functions, which might be subject to some adequate physical interpretations in the future. copyright (c) 2013 john wiley & sons, ltd.
relational_databases	data provenance is the history associated with that data. it constitutes the origin, creation, processing, and archiving of data. in today 's internet era, it has gained significant importance for database analytics. most of the provenance models store provenance information in relational databases for further querying and analysis. although, querying of provenance in relational databases is very efficient for small data sets, it becomes inefficient as the provenance data grows and traversal depth of provenance query increases. this is mainly due to increase in number of join operations to search the entire provenance data. graph databases provide an alternative to rdbmss for storing and analyzing provenance data as it can scale to billions of nodes and at the same time traverse thousands of relationships efficiently. in this paper, we propose efficient multi-depth querying of provenance data using graph databases. the proposed solution allows efficient querying of provenance of current as well as historical queries. a comparison between relational and graph databases is presented for varying provenance data size and traversal depths. graph databases are found to scale well with increasing depth of provenance queries, whereas in relational databases the querying time increases exponentially.
image_processing	composite data sets measured on different objects are usually affected by random errors, but may also be influenced by systematic (genuine) differences in the objects themselves, or the experimental conditions. if the individual measurements forming each data set are quantitative and approximately normally distributed, a correlation coefficient is often used to compare data sets. however, the relations between data sets are not obvious from the matrix of pairwise correlations since the numerical value of the correlation coefficient is lowered by both random and systematic differences between the data sets. this work presents a multidimensional scaling analysis of the pairwise correlation coefficients which places data sets into a unit sphere within low-dimensional space, at a position given by their cc* values [as defined by karplus & diederichs (2012), science, 336, 1030-1033] in the radial direction and by their systematic differences in one or more angular directions. this dimensionality reduction can not only be used for classification purposes, but also to derive data-set relations on a continuous scale. projecting the arrangement of data sets onto the subspace spanned by systematic differences (the surface of a unit sphere) allows, irrespective of the random-error levels, the identification of clusters of closely related data sets. the method gains power with increasing numbers of data sets. it is illustrated with an example from low signal-to-noise ratio image processing, and an application in macromolecular crystallography is shown, but the approach is completely general and thus should be widely applicable.
symbolic_computation	in this paper, the fractional derivatives in the sense of modified riemann-liouville derivative and the exp-function method, the (g'/g)-expansion method and the generalized kudryashov method are used to construct exact solutions for (3 + 1)-dimensional space-time fractional modified kdv-zakharov-kuznetsov equation. this fractional equation can be turned into another nonlinear ordinary differential equation by fractional complex transformation and then these three methods are applied to solve it. as a result, some new exact solutions are obtained. the three methods demonstrate power, reliability and efficiency. (c) 2016 elsevier ltd. all rights reserved.
parallel_computing	in the industry 4.0, factories around the world grow automated and intelligent, and where smart camera plays an important role. smart camera is equipped with processor, memory, communication interface, and operating system, so it can process large amounts of data in advance to assist follow-up automatic inspection and judgment. additionally, since smart camera is an independent system, it will not affect the original system of factories, which is an immense advantage in troubleshooting. besides, thanks to technology breakthroughs in recent years, using graphics processing unit (gpu) to implementing tons of parallel computing helps to significantly boost the overall efficiency. therefore, when a rising number of factories consider improving production capacity of production lines, how to use gpu to assist the improvement is an important issue. based on this scenario, this paper used nvidia tegra tx1 platform with 256 gpu cuda cores and quad-core arm cortex a57 processor and basler usb 3.0 industrial camera to simulate a smart industrial camera, which has gpu and can perform a myriad of complex computations. this paper designed how to recognize and count objects in a real time manner in a highspeed industrial inspection environment with large volumes of data, so as to verify the concept (smart camera with gpu cores) we proposed. the experimental results proved our ideas, and the software design architecture provided in this paper is a simple and efficient design. in the future application in the internet of things or the internet of everything, this structure can be a valuable reference.
machine_learning	in many research and application areas, such as information retrieval and machine learning, we often encounter dealing with a probability distribution that is mixed by one distribution that is relevant to our task in hand and the other that is irrelevant and that we want to get rid of. thus, it is an essential problem to separate the irrelevant distribution from the mixture distribution. this article is focused on the application in information retrieval, where relevance feedback is a widely used technique to build a refined query model based on a set of feedback documents. however, in practice, the relevance feedback set, even provided by users explicitly or implicitly, is often a mixture of relevant and irrelevant documents. consequently, the resultant query model (typically a term distribution) is often a mixture rather than a true relevance term distribution, leading to a negative impact on the retrieval performance. to tackle this problem, we recently proposed a distribution separation method (dsm), which aims to approximate the true relevance distribution by separating a seed irrelevance distribution from the mixture one. while it achieved a promising performance in an empirical evaluation with simulated explicit irrelevance feedback data, it has not been deployed in the scenario where one should automatically obtain the irrelevance feedback data. in this article, we propose a substantial extension of the basic dsm from two perspectives: developing a further regularization framework and deploying dsm in the automatic irrelevance feedback scenario. specifically, in order to avoid the output distribution of dsm drifting away from the true relevance distribution when the quality of seed irrelevant distribution (as the input to dsm) is not guaranteed, we propose a dsm regularization framework to constrain the estimation for the relevance distribution. this regularization framework includes three algorithms, each corresponding to a regularization strategy incorporated in the objective function of dsm. in addition, we exploit dsm in automatic (i.e., pseudo) irrelevance feedback, by automatically detecting the seed irrelevant documents via three different document reranking methods. we have carried out extensive experiments based on various trec datasets, in order to systematically evaluate the proposed methods. the experimental results demonstrate the effectiveness of our proposed approaches in comparison with various strong baselines.
data_structures	we present a new version of the core structural package of our application programming interface, apinetworks, for the treatment of complex networks in arbitrary computational environments. the new version is written in java and presents several advantages over the previous c++ version: the portability of the java code, the easiness of object-oriented design implementations, and the simplicity of memory management. in addition, some additional data structures are introduced for storing the sets of nodes and edges. also, by resorting to the different garbage collectors currently available in the jvm the java version is much more efficient than the c++ one with respect to memory management. in particular, the g1 collector is the most efficient one because of the parallel execution of g1 and the java application. using g1, apinetworks java outperforms the c++ version and the well-known networkx and jgrapht packages in the building and bfs traversal of linear and complete networks. the better memory management of the present version allows for the modeling of much larger networks.
machine_learning	energy prediction of machine tools can deliver many advantages to a manufacturing enterprise, ranging from energy-efficient process planning to machine tool monitoring. physics-based energy prediction models have been proposed in the past to understand the energy usage pattern of a machine tool. however, uncertainties in both the machine and the operating environment make it difficult to predict the energy consumption of the target machine reliably. taking advantage of the opportunity to collect extensive, contextual, energy-consumption data, we discuss a data-driven approach to develop an energy prediction model of a machine tool in this paper. first, we present a methodology that can efficiently and effectively collect and process data extracted from a machine tool and its sensors. we then present a data-driven model that can be used to predict the energy consumption of the machine tool for machining a generic part. specifically, we use gaussian process (gp) regression, a nonparametric machine-learning technique, to develop the prediction model. the energy prediction model is then generalized over multiple process parameters and operations. finally, we apply this generalized model with a method to assess uncertainty intervals to predict the energy consumed by any part of the machine using a mori seiki nvd1500 machine tool. furthermore, the same model can be used during process planning to optimize the energy-efficiency of a machining process.
network_security	we address the issue of large scale network security. it is known that traditional game theory becomes intractable when considering a large number of players, which is a realistic situation in today 's networks where a centralized administration is not available. we propose a new model, based on mean field theory, that allows us to obtain optimal decentralised defence policy for any node in the network and optimal attack policy for an attacker. in this way we settle a promising framework for the development of a mean field game theory of large scale network security. we also present a case study with experimental results.
computer_programming	this article addresses the problem of testing the difference between two correlated agreement coefficients for statistical significance. a number of authors have proposed methods for testing the difference between two correlated kappa coefficients, which require either the use of resampling methods or the use of advanced statistical modeling techniques. in this article, we propose a technique similar to the classical pairwise t test for means, which is based on a large-sample linear approximation of the agreement coefficient. we illustrate the use of this technique with several known agreement coefficients including cohen 's kappa, gwet 's ac(1), fleiss 's generalized kappa, conger 's generalized kappa, krippendorff 's alpha, and the brenann-prediger coefficient. the proposed method is very flexible, can accommodate several types of correlation structures between coefficients, and requires neither advanced statistical modeling skills nor considerable computer programming experience. the validity of this method is tested with a monte carlo simulation.
software_engineering	the goal of software testing should go beyond simply finding defects. ultimately, testing should be focused on increasing customer satisfaction. defects that are detected in areas of the software that the customers are especially interested in can cause more customer dissatisfaction. if these defects accumulate, they can cause the software to be shunned in the marketplace. therefore, it is important to focus on reducing defects in areas that customers consider valuable. this article proposes a value-driven v-model (v-2 model) that deals with customer values and reflects them in the test design for increasing customer satisfaction and raising test efficiency.
software_engineering	the successful use of intelligent agents in healthcare has attracted researchers to apply this emerging software engineering paradigm in more advanced and complex applications. main success factor is the natural mapping of real world medical problems into cyber world. multi-agent architecture can easily model the heterogeneous, distributed and autonomous health care systems. the multi agent systems have been applied from single healthcare activity like knowledge based medical system to complex, multi-component based systems like complete healthcare unit. the use of multi agent systems in health care domain has also opened the ways to find out new applications like personalized and socialized health care systems. this versatile use of multi agent systems has also posed new problems for researchers like; security, communication, and different social issues. this work reviews recent years' research and applications of multi agent systems in healthcare published in different research journals, international conferences, and implemented practically. we reviewed five subdomains and three systems in each subdomain. a set of common parameters of these systems has been extracted and compared to analyze systems' merits and deficiencies. based on our analysis, we have provided recommendations for multi agent systems applied in healthcare domain. future research directions for interested researchers and practitioners are also discussed. as our own future research work, we intend to study healthcare and multi agent systems in e-commerce.
machine_learning	bioinformatics has grown very quickly for the last 20 years, and it will grow even faster in the future. one of the long-standing open challenges in bioinformatics is biomarker identification and cancer diagnosis from gene expression. in this paper, the authors propose a novel hybrid wrapper/filter feature selection approach to identify the most informative genes for cancer diagnosis, named hwf-gs. it handles selection through two steps. the first one is an iterative filter-based mechanism to generate potential subsets of genes. the second step is the aggregation of the best-selected subsets by means of a wrapper-based consensus process that relies on a particle swarm optimization adapted to feature selection. an ensemble of classifiers (svm and knn) is employed to evaluate the selected genes. experiments on nine publicly available cancer dna microarray datasets have shown that hwf-gs selects robust signatures with high classification accuracy and competes with and even outperforms other methods in the literature.
bioinformatics	introduction: the aim of this study was to clarify the microrna (mirna) expression profiles of raw264.7 macrophages infected by candida albicans to elucidate the roles of differentially expressed mirnas and to further explore the mechanisms underlying the immune response to c. albicans infection. methods: high-throughput mirna microarray analysis was performed to detect differentially expressed mirnas in control and c. albicans-infected raw264.7 cells. quantitative real-time pcr analysis was used to verify the microarray results. target genes of differentially expressed mirnas were predicted with bioinformatics software. the cell biological processes and signaling pathways of these mirna-targeted genes involved in c. albicans infection were predicted by gene ontology (go) enrichment and pathway analyses. results: significant upregulation of eight mirnas (mmu-mir-140-5p, mmu-mir-96-5p, mmu-mir-8109, mmu-mir-466i-3p, mmu-mir-222-5p, mmu-mir-301b-3p, mmu-mir-466g, and mmu-mir-7235-5p) and downregulation of eight mirnas (mmu-mir-3154, mmu-mir-223-3p, mmu-mir-494-3p, mmu-mir-6908-5p, mmu-mir-188-5p, mmu-mir-6769b-5p, mmu-mir-7002-5p, and mmu-mir-1224-5p) were observed, as compared with the control (fold change >= 2.0 and p<0.05). go analysis revealed that both mmu-mir-140-5p and mmu-mir-223-3p participated in immune responses, inflammatory reactions, and cell apoptosis in c. albicans infection. also, the mapk signaling pathway was found to play an important role in the immune response against c. albicans infection. conclusions: this study revealed comprehensive expression and functional profiles of differentially expressed mirnas in macrophage raw264.7 cells infected by c. albicans. these findings should help to further elucidate the mechanisms underlying the immune response to c. albicans infection.
distributed_computing	a mapreduce algorithm can be described by a mapping schema, which assigns inputs to a set of reducers, such that for each required output there exists a reducer that receives all the inputs participating in the computation of this output. reducers have a capacity that limits the sets of inputs they can be assigned. however, individual inputs may vary in terms of size. we consider, for the first time, mapping schemas where input sizes are part of the considerations and restrictions. one of the significant parameters to optimize in any mapreduce job is communication cost between the map and reduce phases. the communication cost can be optimized by minimizing the number of copies of inputs sent to the reducers. the communication cost is closely related to the number of reducers of constrained capacity that are used to accommodate appropriately the inputs, so that the requirement of how the inputs must meet in a reducer is satisfied. in this work, we consider a family of problems where it is required that each input meets with each other input in at least one reducer. we also consider a slightly different family of problems in which each input of a list, x, is required to meet each input of another list, y, in at least one reducer. we prove that finding an optimal mapping schema for these families of problems is np-hard, and present a bin-packing-based approximation algorithm for finding a near optimal mapping schema.
bioinformatics	background: translational researchers need robust it solutions to access a range of data types, varying from public data sets to pseudonymised patient information with restricted access, provided on a case by case basis. the reason for this complication is that managing access policies to sensitive human data must consider issues of data confidentiality, identifiability, extent of consent, and data usage agreements. all these ethical, social and legal aspects must be incorporated into a differential management of restricted access to sensitive data. methods: in this paper we present a pilot system that uses several common open source software components in a novel combination to coordinate access to heterogeneous biomedical data repositories containing open data ( open access) as well as sensitive data ( restricted access) in the domain of biobanking and biosample research. our approach is based on a digital identity federation and software to manage resource access entitlements. results: open source software components were assembled and configured in such a way that they allow for different ways of restricted access according to the protection needs of the data. we have tested the resulting pilot infrastructure and assessed its performance, feasibility and reproducibility. conclusions: common open source software components are sufficient to allow for the creation of a secure system for differential access to sensitive data. the implementation of this system is exemplary for researchers facing similar requirements for restricted access data. here we report experience and lessons learnt of our pilot implementation, which may be useful for similar use cases. furthermore, we discuss possible extensions for more complex scenarios.
symbolic_computation	with the help of the symbolic computation system maple, the riccati equation mapping approach and a linear variable separation approach, a new family of complex solutions for the (2+1)-dimensional boiti-leon-pempinelli system (blp) is derived. based on the derived solitary wave solution, some novel complex wave localized excitations are obtained.
computer_vision	libcoopt is an open-source matlab code library which provides a general and convenient tool to approximately solve the combinatorial optimization problems on the set of partial permutation matrices, which are frequently encountered in computer vision, bioinformatics, social analysis, etc. to use the library, the user needs only to give the objective function and its gradient function associated with the problem. two typical problems, the subgraph matching problem and the quadratic assignment problem, are employed to illustrate how to use the library and also its flexibility on different types of problems.
computer_vision	x-ray screening systems have been used to safeguard environments in which access control is of paramount importance. security checkpoints have been placed at the entrances to many public places to detect prohibited items, such as handguns and explosives. generally, human operators are in charge of these tasks as automated recognition in baggage inspection is still far from perfect. research and development on x-ray testing is, however, exploring new approaches based on computer vision that can be used to aid human operators. this paper attempts to make a contribution to the field of object recognition in x-ray testing by evaluating different computer vision strategies that have been proposed in the last years. we tested ten approaches. they are based on bag of words, sparse representations, deep learning, and classic pattern recognition schemes among others. for each method, we: 1) present a brief explanation; 2) show experimental results on the same database; and 3) provide concluding remarks discussing pros and cons of each method. in order to make fair comparisons, we define a common experimental protocol based on training, validation, and testing data (selected from the public gdxray database). the effectiveness of each method was tested in the recognition of three different threat objects: 1) handguns; 2) shuriken (ninja stars); and 3) razor blades. in our experiments, the highest recognition rate was achieved by methods based on visual vocabularies and deep features with more than 95% of accuracy. we strongly believe that it is possible to design an automated aid for the human inspection task using these computer vision algorithms.
image_processing	deep learning (dl) is a powerful state-of-the-art technique for image processing including remote sensing (rs) images. this letter describes a multilevel dl architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery. the pillars of the architecture are unsupervised neural network (nn) that is used for optical imagery segmentation and missing data restoration due to clouds and shadows, and an ensemble of supervised nns. as basic supervised nn architecture, we use a traditional fully connected multilayer perceptron (mlp) and the most commonly used approach in rs community random forest, and compare them with convolutional nns (cnns). experiments are carried out for the joint experiment of crop assessment and monitoring test site in ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by landsat-8 and sentinel-1a rs satellites. the architecture with an ensemble of cnns outperforms the one with mlps allowing us to better discriminate certain summer crop types, in particular maize and soybeans, and yielding the target accuracies more than 85% for all major crops (wheat, maize, sunflower, soybeans, and sugar beet).
software_engineering	the use of computers and complex software is pervasive in archaeology, yet their role in the analytical pipeline is rarely exposed for other researchers to inspect or reuse. this limits the progress of archaeology because researchers cannot easily reproduce each other 's work to verify or extend it. four general principles of reproducible research that have emerged in other fields are presented. an archaeological case study is described that shows how each principle can be implemented using freely available software. the costs and benefits of implementing reproducible research are assessed. the primary benefit, of sharing data in particular, is increased impact via an increased number of citations. the primary cost is the additional time required to enhance reproducibility, although the exact amount is difficult to quantify.
software_engineering	we theorize a two-mind model of design thinking. mind 1 is about logical design reasoning, and mind 2 is about the reflection on our reasoning and judgments. the problem solving ability of mind 1 has often been emphasized in software engineering. the reflective mind 2, however, has not received much attention. in this study, we want to find out if mind 2, or reflection, can improve design discourse, a prerequisite of design quality. we conducted multiple case studies with 12 student groups, divided into test groups and control groups. we provided external reflections to the test groups. no reflections were given to the control groups. we analyzed the quality of the design discourse in both groups. we found that reflection (mind 2) improves the quality of design discourse (mind 1) under certain preconditions. the results highlight the significance of reflection as a mean to improve the quality of design discourse. we conclude that software designers need both mind 1 and mind 2 to obtain a higher quality design discourse, as a foundation for a good design. copyright (c) 2016 john wiley & sons, ltd.
distributed_computing	determination of conserved regions that plays vital roles on regulation of transcription and translation processes is one of the most challenging problems in bioinformatics. however, with the increasing power of distributed computing systems, solving these types of combinatorial problems by utilizing parallelized brute force or exhaustive search algorithms recently has gained popularity. in this paper, we investigated the parallelized implementation of a search tree based brute force technique to find motifs with different lengths. experimental studies showed that parallelization of the brute force techniques with less communication overhead is significantly increased the usability of them to analyze long nucleotide sequences.
parallel_computing	hypergraph partitioning is commonly used in solving very large scale integration (vlsi) placement problem, data mining, sparse matrix multiplication, and parallel computing. this paper presents a novel heuristic for hypergraph partitioning based on nonlinear programming. in our approach we consider adjacent one-dimensional bins. since the reduction of cuts is equivalent to reducing the net length across the two bins, the vertices are moved across the bins in such a way that the density of vertices in each bin is balanced as per the partitioning requirement and reduction in the wirelength. for the walshaw partitioning benchmarks, our tool naps' results are consistently comparable to that obtained by chaco. our tool nap produces better cuts than chaco on 22 instances out of 29 graph samples.
symbolic_computation	for a given septic bezier curve with a distinct ordered sequence of control points, how to determine whether it is a ph curve via exact symbolic computation in theory. this problem motivated the study of a necessary and sufficient condition for a planar septic bezier curve to possess a pythagorean hodograph (ph). based on the definition of a ph curve and the complex representation of a planar curve, we develop geometric conditions in terms of the leg-lengths and angles of a control polygon that must be separated to guarantee the ph property. the relation between the compatibility of solutions with respect to the complex coefficients of ph equations and geometric constraints is analyzed. moreover, ph septic curves with inflections are extended to construct s-shaped transition curves. (c) 2015 elsevier b.v. all rights reserved.
parallel_computing	mpj express is a messaging system that allows application developers to parallelize their compute-intensive sequential java codes on high performance computing clusters and multicore processors. in this paper, we extend mpj express software to provide two new communication devices. the first device-called hybrid-enables mpj express to exploit hybrid parallelism on cluster of multicore processors by sitting on top of existing shared memory and network communication devices. the second device-called native-uses jni wrappers in interfacing mpj express to native mpi implementations like mpich and open mpi. we evaluate performance of these devices on a range of interconnects including 1g/10g ethernet, 10g myrinet and 40g infiniband. in addition, we analyze and evaluate the cost of mpj express buffering layer and compare it with the performance numbers of other java mpi libraries. our performance evaluation reveals that the native device allows mpj express to achieve comparable performance to native mpi libraries-for latency and bandwidth of point-to-point and collective communications-which is a significant gain in performance compared to existing communication devices. the hybrid communication device-without any modifications at application level-also helps parallel applications achieve better speedups and scalability by exploiting multicore architecture. our performance evaluation quantifies the cost incurred by buffering and its impact on overall performance of software. we witnessed comparative performance as both new devices improve application performance and achieve upto 90 % of the theoretical bandwidth available without application rewriting effort-including nas parallel benchmarks, point-to-point and collective communication.
algorithm_design	vibrations with unknown and/or time-varying frequencies significantly affect the achievable performance of control systems, particularly in precision engineering and manufacturing applications. this paper provides an overview of disturbance-observer-based adaptive vibration rejection schemes; studies several new results in algorithm design; and discusses new applications in semiconductor manufacturing. we show the construction of inverse-model-based controller parameterization and discuss its benefits in decoupled design, algorithm tuning, and parameter adaptation. also studied are the formulation of recursive least squares and output-error-based adaptation algorithms, as well as their corresponding scopes of applications. experiments on a wafer scanner testbed in semiconductor manufacturing prove the effectiveness of the algorithm in high-precision motion control. copyright (c) 2015 john wiley & sons, ltd.
network_security	the development of wireless vehicular ad-hoc network (vanet) aimed to enhance road 's safety and provide comfortable driving environment by delivering early warning and infotainment messages. intentional jamming attacks target at undermining such a goal by disrupting wireless communications. while detecting jamming attacks is important towards enhancing road safety, it is challenging because vanet operates in outdoor environment (highly changeable road conditions and atmospheric phenomena), and encompasses volatile topology and high mobility of vehicles (traveling speed and directions). to overcome these challenges, in this work, we study jamming attack mobility and behaviors in ieee802.11p networks. in particular, we focus on analyzing jamming impact based on jammers behaviors, and mobility patterns. thus, in order to achieve reliable detection, first we identify the impact of vehicles' density on network performance. then, we study jamming effectiveness when adopting different mobility patterns (stationary, random, or targeting) and behaviors (constant, random, and reactive). finally, we propose a two phase detection algorithm and evaluate it in a simulation environment. our approach shows promising results to detect different types of jammers accurately in ieee802.11p networks.
relational_databases	due to the fact that existing database systems are increasingly more difficult to use, improving the quality and the usability of database systems has gained tremendous momentum over the last few years. in particular, the feature of explaining why some expected tuples are missing in the result of a query has received more attention. in this paper, we study the problem of explaining missing answers to top-k queries in the context of sql (i.e., with selection, projection, join, and aggregation). to approach this problem, we use the query-refinement method. that is, given as inputs the original top-k sql query and a set of missing tuples, our algorithms return to the user a refined query that includes both the missing tuples and the original query results. case studies and experimental results show that our algorithms are able to return high quality explanations efficiently.
computer_graphics	making decisions using judgements of multiple non-deterministic indicators is an important task, both in everyday and professional life. learning of such decision making has often been studied as the mapping of stimuli (cues) to an environmental variable (criterion); however, little attention has been paid to the effects of situation-by-person interactions on this learning. accordingly, we manipulated cue and feedback presentation mode (graphic or numeric) and task difficulty, and measured individual differences in working memory capacity (wmc). we predicted that graphic presentation, fewer cues, and elevated wmc would facilitate learning, and that person and task characteristics would interact such that presentation mode compatible with the decision maker 's cognitive capability (enhanced visual or verbal wmc) would assist learning, particularly for more difficult tasks. we found our predicted main effects, but no significant interactions, except that those with greater wmc benefited to a larger extent with graphic than with numeric presentation, regardless of which type of working memory was enhanced or number of cues. our findings suggest that the conclusions of past research based predominantly on tasks using numeric presentation need to be reevaluated and cast light on how working memory helps us learn multiple cue-criterion relationships, with implications for dual-process theories of cognition.
network_security	with the popularization and application of internet technology, it has brought great opportunities, challenges and has a significant change to various industries, which entered the internet age. computer network is double-edged sword, bringing convenience to people at the same time there are some security risks, seriously affecting the information security of the internet age. therefore, this article is to explore the main computer network security risks and to lower the risks of computer security management measures, thus providing an important guarantee for computer network security.
parallel_computing	purpose: this paper studies the influence of information and communication technologies on human reasoning and decision making. it investigates the potential impact of ambient intelligence on change in pedestrian mobility behavior, using a large city-scale scenario. methods: this work establishes an interplay between social and technological aspects of awareness by augmenting a model-driven framework of a realistic information eco-system. a distributed multi-agent system is developed to model a real-life urban mobility environment. the model is then simulated using a large scale parallel computing platform. results: evaluation results revealed that the quality of information in ambient-assisted environments increases when compared with those without ambient intelligence. conclusion: we conclude that there is a positive correlation between the extent of information being shared in a socially-inspired information eco-systems and the level of collective awareness in an urban mobility scenario.
bioinformatics	an important problem in the field of bioinformatics is to identify interactive effects among profiled variables for outcome prediction. in this paper, a logistic regression model with pairwise interactions among a set of binary covariates is considered. modeling the structure of the interactions by a graph, our goal is to recover the interaction graph from independently identically distributed (i.i.d.) samples of the covariates and the outcome. when viewed as a feature selection problem, a simple quantity called influence is proposed as a measure of the marginal effects of the interaction terms on the outcome. for the case when the underlying interaction graph is known to be acyclic, it is shown that a simple algorithm that is based on a maximum-weight spanning tree with respect to the plug-in estimates of the influences not only has strong theoretical performance guarantees, but can also outperform generic feature selection algorithms for recovering the interaction graph from i.i.d. samples of the covariates and the outcome. our results can also be extended to the model that includes both individual effects and pairwise interactions via the help of an auxiliary covariate.
cryptography	the domain name system (dns) is a core internet infrastructure that translates names to machine-readable information, such as ip addresses. security flaws in dns led to a major overhaul, with the introduction of the dns security (dnssec) extensions. dnssec adds integrity and authenticity to the dns using digital signatures. dnssec, however, has its own concerns. it suffers from availability problems due to packet fragmentation and is a potent source of distributed denial-of-service attacks. in earlier work, we argued that many issues with dnssec stem from the choice of rsa as default signature algorithm. a switch to alternatives based on elliptic curve cryptography (ecc) can resolve these issues. yet switching to ecc introduces a new problem: ecc signature validation is much slower than rsa validation. thus, switching dnssec to ecc imposes a significant additional burden on dns resolvers, pushing load toward the edges of the network. therefore, in this paper, we study the question: will switching dnssec to ecc lead to problems for dns resolvers, or can they handle the extra load? to answer this question, we developed a model that accurately predicts how many signature validations dns resolvers have to perform. this allows us to calculate the additional cpu load ecc imposes on a resolver. using real-world measurements from four dns resolvers and with two open-source dns implementations, we evaluate future scenarios where dnssec is universally deployed. our results conclusively show that switching dnssec to ecc signature schemes does not impose an insurmountable load on dns resolvers, even in worst case scenarios.
bioinformatics	histih3d gene encodes histone h3.1 and is involved in gene-silencing and heterochromatin formation. hist1h3d expression is upregulated in primary gastric cancer tissue. in this study, we explored the effects of hist1h3d expression on lung cancer, and its mechanisms. hist1h3d expression was measured by immunohistochemistry and rt-pcr in lung cancer tissues and human lung cancer cell lines. cell proliferation was assessed by mtt assay. flow cytometric analysis was used to determine cell cycle distribution and apoptosis. levels of related proteins were detected by western blotting. bioinformatics analysis was performed to investigate related signaling pathways. cdna microarray analysis was performed to identify differentially expressed genes following hist1h3d knockdown. hist1h3d expression was upregulated in lung cancer tissue samples and the h1299 human lung cancer cell line (p<0.01). regulation of hist1h3d expression in nucleus of cells in lung cancer tissues was significant associated with tumor stage (p=0.02) and lymph node metastases (p=0.04). downregulation of hist1h3d expression led to suppression of proliferation and colony forming ability, cell cycle arrest at the go/g, phase, and promotion of cell apoptosis. the microarray data revealed 522 genes that were differentially expressed after hist1h3d knockdown in h1299 cells. these genes were shown to be linked to numerous pathways, including the cell cycle, p53 signaling, and mcm. western blot analysis confirmed upregulated expression of the thbs1 and tp53i3 genes, and downregulated expression of the cdk6, cdkn1 and ccne2 genes. in conclusion, our results suggest that hist1h3d is highly expressed in lung cancer cell lines and tissues. furthermore, hist1h3d may be important in cell proliferation, apoptosis and cell cycle progression, and is implicated as a potential therapeutic target for lung cancer.
relational_databases	despite the huge amount of work devoted to the treatment of time within the relational context, few relevant temporal phenomena still remain to be addressed. one of them is the treatment of ""nearly periodic events"", i.e., events/facts that occur in intervals of time which repeat periodically (e.g., a meeting occurring twice each monday, possibly not at regular times). nearly periodic events are quite frequent in everyday life, and thus in many applicative contexts. their treatment within the relational model is quite challenging, since it involves the integrated treatment of three aspects: (i) the number of repetitions, (ii) their periodicity, and (iii) temporal indeterminacy. coping with this problem requires an in-depth extension of current temporal relational database techniques. in this paper, we introduce a new data model, and new definitions of relational algebraic operators coping with the above issues. we ascertain the properties of the new model and algebra, with emphasis on the expressiveness of our representation model, on the reducibility property, and on the correctness of the algebraic operators.
parallel_computing	earthquake-induced landslides are serious natural hazards that shocked us with tremendous casualties and great economic losses in many mountainous areas around the world. however, predicting and preventing the earthquake-induced landslides is very difficult due to the complicated relationship between seismic dynamics and coseismic landsliding. comprehensive understanding of earthquake-induced landslides from the perspective of seismic dynamic mechanism remains inadequate at present. this study employs an elastoplastic spectral element method incorporating parallel computing and represents a realistic three-dimensional slope model via a semi-structured hexahedral mesh to investigate the dynamic failure characteristics of earthquake-induced landslides. dynamic behaviours of slopes are simulated using a continuumbased approach with a mohr-coulomb yield criterion. displacement fields are calculated using the shear strength reduction technique. pseudo-static seismic loading is performed to assess the slope stability quantitatively and complex topography is taken into consideration. the xinzhong landslide that occurred in beichuan country is one of destructively collapsing landslides triggered by the wenchuan earthquake and is therefore selected as a case study for discussion. three-dimensional visualization of the calculated results quantitatively demonstrated that the three-dimensional numerical model well reproduced the coseismic landsliding response and its essential dynamic failure pattern, which could not be purely captured by geographic information systems (gis) and remote sensing (rs) technologies and calculated using simply two-dimensional numerical model. the numerical results also showed that tensile and shear fractures had significant influences on the nature of the failed surface development. in addition, the presence of seismic loading in the slope could cause obvious disturbances for the slope stability. comparative analysis indicated that the shear surface of the earthquake-induced slope was shorter and the tension crack surface was deeper than that of the normal gravity condition. moreover, the landslide mainly occurred in the transition from the upper to lower part of the slope, indicating that the slope topography was one of the crucial factors resulting in slope failure. although the model was constructed without the presence of a pre-existing failure surface, comparative analysis addressed that the failure surface obtained by the numerical simulation was in close agreement with that by the post-failure investigations. the results could provide insight into better understanding of the relationship between landslide and seismic dynamic mechanism. the study has practical significance for the effective prevention and mitigation of earthquake-induced landslide hazards.
relational_databases	nosql databases are designed to address performance and scalability requirements of web based application which cannot be addressed by traditional relational databases. due to their contrast in priorities and architecture to conventional relational databases using sql, these databases are referred as ""nosql"" databases since they incorporate lots of additional features in addition to the features of conventional databases. the relational databases strongly follow the acid (atomicity, consistency, isolation, and durability) properties while the nosql databases follow base (basically available, soft state, eventual consistency) principles. this survey paper is an analytical study on base features of some of nosql databases. (c) 2015 elsevier b.v. all rights reserved.
computer_programming	the present work addresses the development of a finite element formulation for handling bending, buckling, and post buckling analysis of composite laminated structures with damage. the inverse hyperbolic shear deformation theory (ihsdt) was applied in the finite element formulation. the effect of damage is analyzed for thin composite plates. an anisotropic damage formulation was used to simulate the damage, which is based on the concept of stiffness reduction. computer programming is developed in the matlab environment. the excellent agreement of the results obtained in the present method with those from references shows that the technique is effective and precise. parametric studies in the buckling behavior of a damaged composite plate are presented. critical buckling temperatures are computed for a damaged plate using the present model. thermal post buckling equilibrium paths are traced for various parametric variations for composite plates with mild damage and compared the results with that of undamaged cases. the validation of ihsdt has been demonstrated for buckling analysis in thermal environment for composite plates with an internal flaw. the present work is worthwhile compared with previous works due to the choice of finite element method and inverse hyperbolic shear deformation theory for analyzing the influence of damage on buckling and post buckling behavior of laminated plates. (c) 2016 the institution of structural engineers. published by elsevier ltd. all rights reserved.
data_structures	partial match queries constitute the most basic type of associative queries in multidimensional data structures such as -d trees or quadtrees. given a query where s of the coordinates are specified and are left unspecified (), a partial match search returns the subset of data points in the data structure that match the given query, that is, the data points such that whenever . there exists a wealth of results about the cost of partial match searches in many different multidimensional data structures, but most of these results deal with random queries. only recently a few papers have begun to investigate the cost of partial match queries with a fixed query . this paper represents a new contribution in this direction, giving a detailed asymptotic estimate of the expected cost for a given fixed query . from previous results on the cost of partial matches with a fixed query and the ones presented here, a deeper understanding is emerging, uncovering the following functional shape for <(l.o.t. lower order terms, throughout this work) in many multidimensional data structures, which differ only in the exponent and the constant , both dependent on s and k, and, for some data structures, on the whole pattern of specified and unspecified coordinates in as well. although it is tempting to conjecture that this functional shape is ""universal"", we have shown experimentally that it seems not to be true for a variant of -d trees called squarish -d trees.
symbolic_computation	in this paper, multiple exp-function method is employed to investigate exact multiple wave solutions for (2 + 1)-dimensional potential kadomtsev-petviashvili equation and (3 + 1)dimensional jimbo-miwa equation. not only already known multiple wave solutions are recovered, but also several new or more general multiple wave solutions are obtained. (c) 2014 elsevier ltd. all rights reserved.
image_processing	in this letter, a highly reconfigurable crossbar transmission line switch matrix for magnetic resonance imaging (mri) system is proposed. unlike the conventional m x n crossbar switch configuration, the proposed structure can manipulate 2m x n matrix without doubling the area occupancy. also, the proposed structure includes the signal loss-compensation circuitry based on inductor banks to enhance signal-to-noise ratio (snr). thus, for any required numbers of input and output channels, the proposed structure can reduce the overall size, the number of required component, and even increase the quality of mr images by snr enhancement. the proposed structure was implemented and verified at 4-tesla (170 mhz) mr system through the comparison of rf path loss, snr and phantom test image qualities.
software_engineering	software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. in this paper, we perform a systematic review of 78 primary studies from january 1992 to december 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. the review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations.
machine_learning	background and objective: to safely select the proper therapy for ventricullar fibrillation (vf) is essential to distinct it correctly from ventricular tachycardia (vt) and other rhythms. provided that the required therapy would not be the same, an erroneous detection might lead to serious injuries to the patient or even cause ventricular fibrillation (vf). the main novelty of this paper is the use of time-frequency (t-f) representation images as the direct input to the classifier. we hypothesize that this method allow to improve classification results as it allows to eliminate the typical feature selection and extraction stage, and its corresponding loss of information. methods: the standard aha and mit-bih databases were used for evaluation and comparison with other authors. previous to t-f pseudo wigner-ville (pwv) calculation, only a basic preprocessing for denoising and signal alignment is necessary. in order to check the validity of the method independently of the classifier, four different classifiers are used: logistic regression with l2 regularization (l2 rlr), adaptive neural network classifier (annc), support vector machine (ssvm), and bagging classifier (bagg). results: the main classification results for vf detection (including flutter episodes) are 95.56% sensitivity and 98.8% specificity, 88.80% sensitivity and 99.5% specificity for ventricular tachycardia (vt), 98.98% sensitivity and 97.7% specificity for normal sinus, and 96.87% sensitivity and 99.55% specificity for other rhythms. conclusion: results shows that using t-f data representations to feed classifiers provide superior performance values than the feature selection strategies used in previous works. it opens the door to be used in any other detection applications. (c) 2017 elsevier b.v. all rights reserved.
relational_databases	nowadays many applications must process events at a very high rate. these events are processed on the fly, without being stored. complex event processing technology (cep) is used to implement such applications. some of the cep systems, like apache storm the most popular ceps, lack a query language and operators to program queries as done in traditional relational databases. this paper presents paas-cep, a cep language that provides a sql-like language to program queries for cep and its integration with data stores (database or key-value store). our current implementation is done on top of apache storm however, the cep language can be used with any cep. the paper describes the architecture of the paas-cep, its query language and the algebraic operators. the paper also details the integration of the cep with traditional data stores that allows the correlation of live streaming data with the stored data.
data_structures	we propose a differential versioning based data storage (divers) architecture for distributed storage systems, which relies on a novel erasure coding technique that exploits sparsity across versions. the emphasis of this work is to demonstrate how sparsity exploiting codes (sec), originally designed for i/o optimization, can be extended to significantly reduce storage overhead in a repository of versioned data. in addition to facilitating reduced storage, we address some key reliability aspects for divers such as (i) mechanisms to deploy the coding technique with arbitrarily varying size of data across versions, and (ii) investigating the right allocation strategy for the encoded blocks over a network of distributed nodes across different versions so as to achieve the best fault tolerance. we also discuss system issues related to the management of data structures for accessing and manipulating the files over the differential versions. (c) 2016 elsevier b.v. all rights reserved.
computer_vision	dense motion field estimation is a key computer vision problem. many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, or non-rigid surface registration, but a unified methodology is still lacking. the authors introduce a general framework that robustly combines direct and feature-based matching. the feature-based cost is built around a novel robust distance function that handles keypoints and weak features such as segments. it allows us to use putative feature matches to guide dense motion estimation out of local minima. the authors' framework uses a robust direct data term. it is implemented with a powerful second-order regularisation with external and self-occlusion reasoning. their framework achieves state-of-the-art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). their framework has a modular design that customises to specific application needs.
software_engineering	maze is an extension of the object-z specification language supporting the specification and development of multi-agent systems (mas). following recommendations from the agent-oriented software engineering community, it supports three distinct levels of abstraction: (i) the macro level which focusses on the system 's overall, global behaviour, independently of how the agents of the system operate and interact, (ii) the meso level which focusses on agent interactions, and (iii) the micro level which focusses on the operation of individual agents. object-z 's high-level support for component based specification, which is well suited to modelling mas, is complemented in maze with support for action refinement to facilitate the top-down development process from the macro to micro level, and with a number of syntactic conventions aimed at abstractly specifying the low-level mechanisms required for dealing with asynchronous communication and timing constraints at the micro level. the latter are shorthands for existing object-z notation and so require no redefinition of object-z 's semantics. in this paper, we provide an overview of maze and illustrate its use on a non-trivial case study: a swarm robotic algorithm for self-assembly. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	presbyacusis, or age-related hearing loss, can be characterized in humans as metabolic and sensory phenotypes, based on patterns of audiometric thresholds that were established in animal models. the metabolic phenotype is thought to result from deterioration of the cochlear lateral wall and reduced endocochlear potential that decreases cochlear amplification and produces a mild, flat hearing loss at lower frequencies coupled with a gradually sloping hearing loss at higher frequencies. the sensory phenotype, resulting from environmental exposures such as excessive noise or ototoxic drugs, involves damage to sensory and non-sensory cells and loss of the cochlear amplifier, which produces a 50-70 db threshold shift at higher frequencies. the mixed metabolic + sensory phenotype exhibits a mix of lower frequency, sloping hearing loss similar to the metabolic phenotype, and steep, higher frequency hearing loss similar to the sensory phenotype. the current study examined audiograms collected longitudinally from 343 adults 50-93 years old (n = 686 ears) to test the hypothesis that metabolic phenotypes increase with increasing age, in contrast with the sensory phenotype. a quadratic discriminant analysis (qda) was used to classify audiograms from each of these ears as (1) older-normal, (2) metabolic, (3) sensory, or (4) metabolic + sensory phenotypes. although hearing loss increased systematically with increasing age, audiometric phenotypes remained stable for the majority of ears (61.5 %) over an average of 5.5 years. most of the participants with stable phenotypes demonstrated matching phenotypes for the left and right ears. audiograms were collected over an average period of 8.2 years for ears with changing audiometric phenotypes, and the majority of those ears transitioned to a metabolic or metabolic + sensory phenotype. these results are consistent with the conclusion that the likelihood of metabolic presbyacusis increases with increasing age in middle to older adulthood.
relational_databases	now-related temporal data play an important role in many applications. clifford et al. 's approach is a milestone to model the semantics of 'now' in temporal relational databases. several relational representation models for now-related data have been presented; however, the semantics of such representations has not been explicitly studied. additionally, the definition of a relational algebra to query now-related data is an open problem. we propose the first integrated approach that provides both a neat semantics for now-related data and a compact 1nf representation (data model and relational algebra) for them. additionally, our approach also extends current approaches to consider (i) domains where it is not always possible to know when changes in the world are recorded in the database and (ii) now-related data with a bound on their persistency in the future. to do so, we explicitly model the notion of temporal indeterminacy in the future for now-related data. the properties of our approach are also analyzed both from a theoretical (semantic correctness and reducibility of the algebra) and from an experimental point of view. experiments show that, despite the fact that our approach is a major extension to current temporal relational approaches, no significant overhead is added to deal with 'now'.
algorithm_design	a multistage graph is center problem of computer science, many coordination and consistency problems can be convert into multistage graph problem. we obtained the fitness function by coding the vertex of multistage graph, and designed the genetic algorithm for solving multistage graph problem. experiment results show that this algorithm is very effective and feasible.
parallel_computing	the elastodynamic boundary integral equation method (biem) in real space and in the temporal domain is an accurate semi-analytical tool to investigate the earthquake rupture dynamics on non-planar faults. however, its heavy computational demand for a historic integral generally increases with a time complexity of o(mn3)for the number of time steps n and elements m due to volume integration in the causality cone. in this study, we introduce an efficient biem, termed the 'fast domain partitioning method' (fdpm), which enables us to reduce the computation time to the order of the surface integral, o(mn2), without degrading the accuracy. the memory requirement is also reduced to o(m-2) from o((mn)-n-2). fdpm uses the physical nature of green 's function for stress to partition the causality cone into the domains of the p and s wave fronts, the domain in-between the p and s wave fronts, and the domain of the static equilibrium, where the latter two domains exhibit simpler dependences on time and/or space. the scalability of this method is demonstrated on the large-scale parallel computing environments of distributed memory systems. it is also shown that fdpm enables an efficient use of memory storage, which makes it possible to reduce computation times to a previously unprecedented level. we thus present fdpm as a powerful tool to break through the current fundamental difficulties in running dynamic simulations of coseismic ruptures and earthquake cycles under realistic conditions of fault geometries.
computer_vision	many computer vision problems involve exploring the synthesis and classification models that map images from the observed source space to a target space. recently, one popular and effective method is to transform images from both source and target space into a shared single sparse domain, in which a synthesis model is established. motivated by such a technique, this research attempts to explore an effective and robust linear function that maps the sparse representatio ns of images from the source space to the target space, and simultaneously develop a linear classifier on such a coupled space with both supervised and semi-supervised learning. in order to capture the sparse structure shared by each class, we represent this mapping using a linear transformation with the constraint of sparsity. the performance of our proposed method is evaluated on several benchmark image datasets for low-resolution faces/digits classification and super-resolution, and the experimental results verify the effectiveness of the proposed method.
software_engineering	problem-based learning (pbl) has often been seen as an all-or-nothing approach, difficult to apply in traditional curricula based on traditional lectured courses with exercise and lab sessions. aalborg university has since its creation in 1974 practiced pbl in all subjects, including computer science and software engineering, following a model that has become known as the aalborg model. following a strategic decision in 2009, the aalborg model has been reshaped. we first report on the software engineering program as it was in the old aalborg model. we analyze the programme wrt competence levels according to bloom 's taxonomy and compare it with the expected skills and competencies for an engineer passing a general software engineering 4-year program with an additional 4 years of experience as defined in the ieee software engineering body of knowledge (swebok) [abran et al. 2004]. we also compare with the graduate software engineering 2009 curriculum guidelines for graduate degree programmes in software engineering (gswe2009) [pyster 2009]. we then describe the new curriculum and draw some preliminary conclusions based on analyzing the curriculum according to bloom 's taxonomy and the results of running the program for 2 years. as the new program is structured to be compliant with the bologna process and thus presents all activities in multipla of 5 european credit transfer system points, we envision that elements of the program could be used in more traditional curricula. this should be especially easy for programs also complying with the bologna process.
cryptography	we propose a quantum public-key encryption (qpke) protocol for an unknown multi-qubit state based on qubit-wise teleportation. the private-key is a computational boolean function, whereas the public-key is a pair of a random bit string and a quantum state. a private-key corresponds to an exponential number of public-keys. security analysis showed that the proposed protocol has information-theoretic security from attacks for the private-key and the encryption. a multi-partite quantum secret state sharing protocol is presented based on the proposed multi-qubit-oriented qpke protocol. such secret state sharing protocol is information-theoretically secure.
algorithm_design	the exemplar breakpoint distance problem is motivated by finding conserved sets of genes between two genomes. it asks to find respective exemplars in two genomes to minimize the breakpoint distance between them. if one genome has no repeated gene (called trivial genome) and the other has genes repeating at most twice, it is referred to as the (1, 2)-exemplar breakpoint distance problem, ebd(1, 2) for short. little has been done on algorithm design for this problem by now. in this article, we propose a parameter to describe the maximum physical span between two copies of a gene in a genome, and based on it, design a fixed-parameter algorithm for ebd(1, 2). using a dynamic programming approach, our algorithm can take o(4(s)n(2)) time and o(4(s)n) space to solve an ebd(1, 2) instance that has two genomes of n genes where the second genome has each two copies of a gene spanning at most s copies of the genes. our algorithm can also be used to compute the maximum adjacencies between two genomes. the algorithm has been implemented in c++. simulations on randomly generated data have verified the effectiveness of our algorithm. the software package is available from the authors.
network_security	gpu is widely used in various applications that require huge computational power. in this paper, we contribute to the cryptography and high performance computing research community by presenting techniques to accelerate symmetric block ciphers (aes-128, cast-128, camellia, seed, idea, blowfish and threefish) in nvidia gtx 980 with maxwell architecture. the proposed techniques consider various aspects of block cipher implementation in gpu, including the placement of encryption keys and t-box in memory, thread block size, cipher operating mode, parallel granularity and data copy between cpu and gpu. we proposed a new method to store the encryption keys in registers with high access speed and exchange it with other threads by using the warp shuffle operation in gpu. the block ciphers implemented in this paper operate in ctr mode, and able to achieve high encryption speed with 149 gbps (aes-128), 143 gbps (cast-128), 124 gbps (camelia), 112 gbps (seed), 149 gbps (idea), 111 gbps (blowfish) and 197 gbps (threefish). to the best of our knowledge, this is the first implementation of block ciphers that exploits warp shuffle, an advanced feature in nvidia gpu. on the other hand, block ciphers can be used as pseudorandom number generator (prng) when it is operating under counter mode (ctr), but the speed is usually slower compare to other prng using lighter operations. hence, we attempt to modify idea and blowfish in order to achieve faster prng generation. the modified idea and blowfish manage to pass all nist statistical test and testu01 smallcrush except the more stringent tests in testu01 (crush and bigcrush).
algorithm_design	this paper presents the software framework established to facilitate cloud-hosted robot simulation. the framework addresses the challenges associated with conducting a task-oriented and real-time robot competition, the defense advanced research projects agency (darpa) virtual robotics challenge (vrc), designed to mimic reality. the core of the framework is the gazebo simulator, a platform to simulate robots, objects, and environments, as well as the enhancements made for the vrc to maintain a high fidelity simulation using a high degree of freedom and multisensor robot. the other major component used is the cloudsim tool, designed to enhance the automation of robotics simulation using existing cloud technologies. the results from the vrc and a discussion are also detailed in this work. note to practitioners-advances in robot simulation, cloud hosted infrastructure, and web technology have made it possible to accurately and efficiently simulate complex robots and environments on remote servers while providing realistic data streams for human-in-the-loop robot control. this paper presents the software and hardware frameworks established to facilitate cloud-hosted robot simulation, and addresses the challenges associated with conducting a task-oriented robot competition designed to mimic reality. the competition that spurred this innovation was the vrc, a precursor to the darpa robotics challenge, in which teams from around the world utilized custom human-robot interfaces and control code to solve disaster response-related tasks in simulation. winners of the vrc received both funding and access to atlas, a humanoid robot developed by boston dynamics. the gazebo simulator, an open source and high fidelity robot simulator, was improved upon to met the needs of the vrc competition. additionally, cloudsim was created to act as an interface between users and the cloud-hosted simulations. as a result of this work, we have achieved automated deployment of cloud resources for robotic simulations, near real-time simulation performance, and simulation accuracy that closely mimics real hardware. these tools have been released under open source licenses and are freely available, and can be used to help reduce robot and algorithm design and development time, and increase robot software robustness.
software_engineering	developing distributed applications, particularly those for distributed, real-time and embedded (dre) systems, is a difficult and complex undertaking due to the need to address four major challenges: the complexity of programming interprocess communication, the need to support a wide range of services across heterogeneous platforms and promote reuse, the need to efficiently utilize resources, and the need to adapt to changing conditions. the first two challenges are addressed to a large extent by standardized, general-purpose middleware (e.g. corba, dcom and java rmi) through the use of a ""black-box"" approach, such as the object-oriented paradigm (frameworks and design patterns). the need to support a large variety and range of applications and application domains has resulted in very feature-rich implementations of these standardized middleware. however, such a feature-richness acts counteractive to resolving the remaining two challenges; instead it incurs excessive memory footprint and performance overhead, as well as increased cost of testing and maintenance. to address the four challenges all at once while leveraging the benefits of general-purpose middleware requires a scientific approach to specializing the middleware. software engineering techniques, such as aspect-oriented programming (aop), feature-oriented programming (fop), and reflection make the specialization task simpler, albeit still requiring the dre system developer to manually identify the system invariants, and sources of performance and memory footprint bottlenecks that drive the specialization techniques. specialization reuse is also hampered due to a lack of common taxonomy to document the recurring specializations, and assess the strengths and weaknesses of these techniques. to address these requirements, this paper presents a case for an automated, multi-stage, feature-oriented middleware specialization process that improves both middleware developer productivity and middleware performance. three specific contributions are made in this paper. first, contemporary middleware specialization research is framed in terms of a three-dimensional taxonomy. second, the principles of separation of concerns are used in the context of this taxonomy to define six stages of a middleware specialization process lifecycle. finally, a concrete implementation of the six stage, automated middleware specialization process is presented along with empirical data illustrating the benefits accrued using the framework.
parallel_computing	a wide variety of large-scale data have been produced in bioinformatics. in response, the need for efficient handling of biomedical big data has been partly met by parallel computing. however, the time demand of many bioinformatics programs still remains high for large-scale practical uses because of factors that hinder acceleration by parallelization. recently, new generations of storage devices have emerged, such as nand flash-based solid-state drives (ssds), and with the renewed interest in near-data processing, they are increasingly becoming acceleration methods that can accompany parallel processing. in certain cases, a simple drop-in replacement of hard disk drives by ssds results in dramatic speedup. despite the various advantages and continuous cost reduction of ssds, there has been little review of ssd-based profiling and performance exploration of important but time-consuming bioinformatics programs. for an informative review, we perform in-depth profiling and analysis of 23 key bioinformatics programs using multiple types of devices. based on the insight we obtain from this research, we further discuss issues related to design and optimize bioinformatics algorithms and pipelines to fully exploit ssds. the programs we profile cover traditional and emerging areas of importance, such as alignment, assembly, mapping, expression analysis, variant calling and metagenomics. we explain how acceleration by parallelization can be combined with ssds for improved performance and also how using ssds can expedite important bioinformatics pipelines, such as variant calling by the genome analysis toolkit and transcriptome analysis using rna sequencing. we hope that this review can provide useful directions and tips to accompany future bioinformatics algorithm design procedures that properly consider new generations of powerful storage devices.
image_processing	an enhanced version of a segmentation algorithm applied in x-ray images using a prior shape and a straightened boundary image (sbi) is proposed. in the sbi method, the boundary of the target object is extracted with a constant width along the prior shape and transformed to a rectangular image in which the edges are straightened. a new minimal path algorithm is proposed and applied to sbi minimising a cost function to select the best path corresponding to the edges of the target object. the cost function is calculated based on all possible paths from each pixel to the beginning of the image while lowering the computational complexity. comparing with previous methods, the proposed method removes artefacts and provides clearer and smoother edges even when the prior shape is far from the target object. the method is also less sensitive to the initial positioning of the prior shape model.
algorithm_design	this study designed a path planning method based on fuzzy algorithm and genetic fuzzy algorithm for the security patrol robot. firstly, the fuzzy algorithm design of path planning was introduced, it included making the obstacle avoidance control strategy, establishing fuzzy language and its membership function of input and output, establishing fuzzy control rules for path planning, and carrying on fuzzy reasoning and defuzzification for path planning. although the fuzzy logic algorithm can avoid obstacle well for path planning, it need the membership function and control rules accurately, and affecting the application of fuzzy reasoning. therefore, genetic algorithm was used to optimize the fuzzy algorithm, it included description of model, determining the range of variation of each parameter, encoding scheme, algorithm flow, and designing of fuzzy logic controller based on genetic algorithm optimization. finally, in order to verify the effectiveness of the designed genetic fuzzy algorithm, the simulation was carried out.
algorithm_design	most of the recent mobile robot researchers focus on obstacle avoidance and path tracking in unknown environment. this paper presents a new algorithm using straight-line equation adaptation mechanism that makes robotino reaching its destination accurately, also to enable it to detect and to avoid static or dynamic obstacles using nine infrared sensors. a brief robotino dynamic description is discussed to help in understanding the proposed control algorithm. a detailed algorithm design procedure is evaluated. the simulation results showed the effectiveness of the proposed algorithm in the sense of avoiding obstacles without collision through robotino predefined path.
computer_vision	saliency modeling has played an important part in computer vision studies over the past 30 years. many state-of-the-art models adopted complex mathematical and machine learning theories. in this paper, a simple and effective visual attention model is proposed. we find that a single fixed template is enough for saliency map generation; this idea is inspired by the receptive field of the human visual system. all that is needed is to convolve the input image with this template with additional post-processing. experiments show that our model is extremely fast and performs better than state-of-the-art models in human eye fixation prediction.
cryptography	two new protocols for quantum binary voting are proposed. one of the proposed protocols is designed using a standard scheme for controlled deterministic secure quantum communication (cdsqc), and the other one is designed using the idea of quantum cryptographic switch, which uses a technique known as permutation of particles. a few possible alternative approaches to accomplish the same task (quantum binary voting) have also been discussed. security of the proposed protocols is analyzed. further, the effciencies of the proposed protocols are computed, and are compared with that of the existing protocols. the comparison has established that the proposed protocols are more efficient than the existing protocols.
machine_learning	psychiatry research has long experienced a stagnation stemming from a lack of understanding of the neurobiological underpinnings of phenomenologically defined mental disorders. recently, the application of computational neuroscience to psychiatry research has shown great promise in establishing a link between phenomenological and pathophysiological aspects of mental disorders, thereby recasting current nosology in more biologically meaningful dimensions. in this review, we highlight recent investigations into computational neuroscience that have undertaken either theory- or data-driven approaches to quantitatively delineate the mechanisms of mental disorders. the theory-driven approach, including reinforcement learning models, plays an integrative role in this process by enabling correspondence between behavior and disorder-specific alterations at multiple levels of brain organization, ranging from molecules to cells to circuits. previous studies have explicated a plethora of defining symptoms of mental disorders, including anhedonia, inattention, and poor executive function. the data-driven approach, on the other hand, is an emerging field in computational neuroscience seeking to identify disorder-specific features among high-dimensional big data. remarkably, various machine-learning techniques have been applied to neuroimaging data, and the extracted disorder-specific features have been used for automatic case-control classification. for many disorders, the reported accuracies have reached 90% or more. however, we note that rigorous tests on independent cohorts are critically required to translate this research into clinical applications. finally, we discuss the utility of the disorder-specific features found by the data-driven approach to psychiatric therapies, including neurofeedback. such developments will allow simultaneous diagnosis and treatment of mental disorders using neuroimaging, thereby establishing theranostics' for the first time in clinical psychiatry.
image_processing	rapid transport of water and solutes through desiccation soil cracks can lead to crop water and nutrient stress. the challenge of irrigation management of cracking soils is to take advantage of the rapid water intake rate of a dry, cracked soil, while keeping plant water stress at a minimum. therefore, mitigation not suppression of desiccation cracks is imperative and considered our objective of this study. a laboratory experiment was carried out to investigate the effects of sugarcane pith additive on mitigating desiccation cracks, the volumetric shrinkage strain, the total porosity, and water retention at field capacity of clay soils. the clay soil was treated with the sugarcane pith at dosages of 1, 2, 3, 4 and 5% on dry weight basis. various experimental methods were used to determine the variations in volumetric shrinkage, total porosity and water retention at field capacity. the characteristics of crack patterns were studied using an image processing technique. compared with the untreated soil, the results showed that the sugarcane pith can increase the total porosity and water content at field capacity, while reducing volumetric shrinkage strain, and consequently, mitigating the development of desiccation cracks. therefore, results suggested that the modification of clayey soil by the addition of the sugarcane pith by rates up to 2% on dry weight basis can be a viable and innovative method to mitigate the development of desiccation cracks. in addition, this application will enhance recycling efforts by converting sugarcane pith waste into usable amendment to mitigate cracks in clayey soil. (c) 2017 elsevier b.v. all rights reserved.
operating_systems	background: next-generation sequencing (ngs) has revolutionized how research is carried out in many areas of biology and medicine. however, the analysis of ngs data remains a major obstacle to the efficient utilization of the technology, as it requires complex multi-step processing of big data demanding considerable computational expertise from users. while substantial effort has been invested on the development of software dedicated to the individual analysis steps of ngs experiments, insufficient resources are currently available for integrating the individual software components within the widely used r/bioconductor environment into automated workflows capable of running the analysis of most types of ngs applications from start-to-finish in a time-efficient and reproducible manner. results: to address this need, we have developed the r/bioconductor package systempiper. it is an extensible environment for both building and running end-to-end analysis workflows with automated report generation for a wide range of ngs applications. its unique features include a uniform workflow interface across different ngs applications, automated report generation, and support for running both r and command-line software on local computers and computer clusters. a flexible sample annotation infrastructure efficiently handles complex sample sets and experimental designs. to simplify the analysis of widely used ngs applications, the package provides pre-configured workflows and reporting templates for rna-seq, chip-seq, var-seq and ribo-seq. additional workflow templates will be provided in the future. conclusions: systempiper accelerates the extraction of reproducible analysis results from ngs experiments. by combining the capabilities of many r/bioconductor and command- line tools, it makes efficient use of existing software resources without limiting the user to a set of predefined methods or environments. systempiper is freely available for all common operating systems from bioconductor (http://bioconductor.org/packages/devel/systempiper).
machine_learning	the quantitative simulation of forest fire spreading plays an essential role in designing quick risk management and implementing effective suppression policies. as a preferable modelling approach, the cellular automaton (ca) has been used to simulate the complex mechanisms of fire spreading. however, in traditional ca models, comprehensive studies on the physical principles of forest fires are needed to define the local transition rules. instead of defining transition rules, the extreme learning machine (elm) was applied in this study. by integrating the elm with the traditional forest fire ca framework, a new cellular automaton modelling approach was proposed. after that, its performance was validated using data collected from five fires in the west of united states. results show that the elm performed well in predicting each cell 's igniting probability. the impact of wind velocity on fire spreading pattern can be effectively described by the proposed modelling approach. furthermore, the validation against actual fire behavior observations shows that its simulation performance is acceptable and in most cases is better than that of the previously reported studies.(c) 2017 elsevier b.v. all rights reserved.
computer_programming	computer programming is regarded as a difficult skill to learn both by researchers and often by learners themselves. metacognition has been identified as an important factor to be a successful learner in learning computer programming. metacognitive in educational psychology is generally described as monitoring and controlling activities of one 's cognition. the researchers have examined the metacognitive awareness inventory om to identify how it relates to student academic achievement at school and universities. in this research work, an empirical research is conducted using the mai inventory with the objective to examine the correlation between the metacognitive awareness with the grade point average (gpa) performance of the introductory programming course at universities in malaysia. the experiment result indicates a positive relationship between metacognitive awareness with the learning success of introductory programming course at universities.
symbolic_computation	in this paper, the generalized unified method is used to construct multi-rational wave solutions of the ()-dimensional kadomtsev-petviashvili equation with variable coefficients. this is an extension of the previous work that was given by the same author in osman and abdel-gawad (epj plus 130(10):1-11, 2015). the (2 1)-dimensional kadomtsev-petviashvili equation with variable coefficients can be used to characterize many nonlinear phenomena in fluid dynamics, plasma physics and some other nonlinear science when the inhomogeneities of media and non-uniformities of boundaries are taken into consideration. to give more physical insight into the obtained solutions, we present graphically their representative structures by setting the arbitrary functions in the solutions as specific functions. moreover, the influences of the variable coefficient functions and interaction properties of solitary waves are discussed for physical interests and possible applications.
operating_systems	computer-based control systems, especially if they run under general-purpose operating systems, often exhibit variance of the scan period of processing inputs and outputs. although this fact is usually not taken into account when discrete control algorithms are used, it can cause worse performance of the control loop in comparison to the theoretical case. in this paper we describe a modified discrete lq control algorithm that takes disturbances of the scan period into account and partially compensates their influence. we also show that such a controller can be implemented even on low-performance hardware platforms, if they are equipped with a sufficient amount of memory.
machine_learning	in the field of weapon system of systems (wsos) simulation, various indicators are widely used to describe the capability of wsos, but it is always difficult to describe the comprehensive capability of wsos quickly and intuitively by visualization of multi-dimensional indicators. a method of machine learning and visualization is proposed, which can display and analyze the capabilities of different wsos in a two-dimensional plane. the analysis and comparison of the comprehensive capability of different components of wsos is realized by the method, which consists of six parts: multiple simulations, key indicators mining, three spatial distance calculation, fusion project calculation, calculation of individual capability density, and calculation of multiple capability ranges overlay. binding a simulation experiment, the collaborative analysis of six indicators and 100 possible kinds of red wsos are achieved. the experimental results show that this method can effectively improve the quality and speed of capabilities analysis, reveal a large number of potential information, and provide a visual support for the qualitative and quantitative analysis model.
image_processing	an image processing technique using the proper orthogonal decomposition (pod) of infrared thermal data was developed to improve the speed of assessment of 2d heat source fields accompanying mechanical transformation. this method involved the generation of a reduced orthonormal basis to approximate thermal fields prior to heat source estimation. the robustness of the method was first assessed using a penalising benchmark test. this test involved artificially setting several tricky situations that arise in practice (high diffusivity, low signal-to-noise ratio, complex heat source distribution, etc.). application of the method to several experimental temperature fields obtained by an infrared focal plane array camera is then presented. the error between the pod approximated solution in terms of heat sources and a reference solution, computed via a local least squares fitting method, was found to be negligible, thus confirming the efficiency and advantages of the pod preprocessing technique - the method enabled us to obtain a reliable estimate of heat sources while drastically reducing the computation cost in terms of cpu time.
computer_graphics	natural phenomena simulation attracts a lot of research attention and interest in virtual reality. the simulation for liquid has become a research focus in both computer graphics and computational physics, because of the difficulties in dynamic modelling and high computational complexity. we introduce the main research achievements in recent years with regard to liquids, which is a common natural phenomenon. a hybrid modelling approach for dynamic liquid simulation is proposed, followed by a surface reconstruction method using simulated results. in particle-based fluid simulation, surface construction is one procedure to contour the implicit function which determines an isosurface in a scalar field. we describe an adaptive polygonisation approach, by adaptively constructing an unconstrained octree structure, to reduce excessive subdivision which is required by traditional methods like marching cubes. dual marching cubes is then applied to perform surface extraction on a new grid which is topologically dual to the octree structure. experiment shows the advantages of our approach in generating satisfactory effects without excessively fine-grained grid structure. satisfactory visual effect is achieved and this application can be used in computer games, movie making and virtual simulation in medical areas.
parallel_computing	the bootstrap is a popular and powerful method for assessing precision of estimators and inferential methods. however, for massive datasets that are increasingly prevalent, the bootstrap becomes prohibitively costly in computation and its feasibility is questionable even with modern parallel computing platforms. recently, kleiner and co-authors, proposed a method called blb (bag of little bootstraps) for massive data, which is more computationally scalable with little sacrifice of statistical accuracy. building on blb and the idea of fast double bootstrap, we propose a new resampling method, the subsampled double bootstrap, for both independent data and time series data. we establish consistency of the subsampled double bootstrap under mild conditions for both independent and dependent cases. methodologically, the subsampled double bootstrap is superior to blb in terms of running time, more sample coverage, and automatic implementation with less tuning parameters for a given time budget. its advantage relative to blb and bootstrap is also demonstrated in numerical simulations and a data illustration. supplementary materials for this article are available online.
computer_vision	the paper develops a general regression framework for the analysis of manifold-valued response in a riemannian symmetric space (rss) and its association with multiple covariates of interest, such as age or gender, in euclidean space. such rss-valued data arise frequently in medical imaging, surface modelling and computer vision, among many other fields. we develop an intrinsic regression model solely based on an intrinsic conditional moment assumption, avoiding specifying any parametric distribution in rss. we propose various link functions to map from the euclidean space of multiple covariates to the rss of responses. we develop a two-stage procedure to calculate the parameter estimates and determine their asymptotic distributions. we construct the wald and geodesic test statistics to test hypotheses of unknown parameters. we systematically investigate the geometric invariant property of these estimates and test statistics. simulation studies and a real data analysis are used to evaluate the finite sample properties of our methods.
software_engineering	bayesian networks (bn) have been used for decision making in software engineering for many years. in other fields such as bioinformatics, bns are rigorously evaluated in terms of the techniques that are used to build the network structure and to learn the parameters. we extend our prior mapping study to investigate the extent to which contextual and methodological details regarding bn construction are reported in the studies. we conduct a systematic literature review on the applications of bns to predict software quality. we focus on more detailed questions regarding (1) dataset characteristics, (2) techniques used for parameter learning, (3) techniques used for structure learning, (4) use of tools, and (5) model validation techniques. results on ten primary studies show that bns are mostly built based on expert knowledge, i.e. structure and prior distributions are defined by experts, whereas authors benefit from bn tools and quantitative data to validate their models. in most of the papers, authors do not clearly explain their justification for choosing a specific technique, and they do not compare their proposed bns with other machine learning approaches. there is also a lack of consensus on the performance measures to validate the proposed bns. compared to other domains, the use of bns is still very limited and current publications do not report enough details to replicate the studies. we propose a framework that provides a set of guidelines for reporting the essential contextual and methodological details of bns. we believe such a framework would be useful to replicate and extend the work on bns.
machine_learning	machine learning algorithms applied to text categorization mostly employ the bag of words (bow) representation to describe the content of the documents. this method has been successfully used in many applications, but it is known to have several limitations. one way of improving text representation is usage of wikipedia as the lexical knowledge base - an approach that has already shown promising results in many research studies. in this paper we propose three path-based measures for computing document relatedness in the conceptual space formed by the hierarchical organization of a wikipedia category graph (wcg). we compare the proposed approaches with the standard path length method to establish the best relatedness measure for the wcg representation. to test overall wcg efficiency, we compare the proposed representations with the bow method. the evaluation was performed with two different types of clustering algorithms (optics and k-means), used for categorization of keyword-based search results. the experiments have shown that our approach outperforms the standard path length approach, and the wcg representation achieves better results than bow.
network_security	the economic situation highlights the significant role of the application of computer in human society since the 21st century, especially since 2015. this thesis probes into the security risk propagation of immune mechanism under complex network background. focusing on the issue of network security under network background, the thesis deploys the analysis by adopting the mechanism model of immune mechanism, immunizing parts of the nodes, establishing stochastic immune mechanism of security risk and introducing stochastic immune mechanism propagation model of security risk. to further study the security risk propagation of immune mechanism under complex network background, the thesis presents the model rules, introduces the security risk acquaintance immune mechanism and establishes models of acquaintances immune mechanism. finally, attributing to the simulation experiments, the simulation results, the characteristics of security risk propagation model, and the study of stochastic immune mechanism propagation model and acquaintance immune mechanism stochastic model, the thesis reaches the conclusion that at the initial stage of propagation, the die-out rate of the security risk is inversely proportional to the propagation rate.
algorithm_design	although the problem of k-area coverage has been intensively investigated for dense wireless sensor networks (wsns), how to arrive at a k-coverage sensor deployment that optimizes certain objectives in relatively sparse wsns still faces both theoretical and practical difficulties. moreover, only a handful of centralized algorithms have been proposed to elevate 2-d area coverage to 3-d surface coverage. in this paper, we present a practical algorithm, i.e., the autonomous deployment for load balancing k-surface coverage (apollo), to move sensor nodes toward k-surface coverage, aiming at minimizing the maximum sensing range required by the nodes. apollo enables purely autonomous node deployment as it only entails localized computations. we prove the termination of the algorithm and the (local) optimality of the output. we also show that our optimization objective is closely related to other frequently considered objectives for 2-d area coverage. therefore, our practical algorithm design also contributes to the theoretical understanding of the 2-d k-area coverage problem. finally, we use extensive simulation results to both confirm our theoretical claims and demonstrate the efficacy of apollo.
operating_systems	virtual desktop infrastructure (vdi) solutions seek to provide a satisfactory user experience at the client side when accessing remote desktop applications, even from mobile devices with limited capabilities. this paper presents a new approach, improving on previous work by the authors, in which a combination of virtual network computing (vnc) and streaming protocols allowed efficient remote web access to virtualized applications within a cloud architecture. the new approach simplifies virtual machine templates, from which virtual machine instances are deployed, by centralizing software modules, greatly simplifying their management. our new contribution consists of an integrated solution with specific webm video encoding modules in charge of application visual output processing, an hypertext transfer protocol (http) streaming server, and a vnc server. the solution can be installed in the hypervisor of the host machines instead of replicating the servers and modules throughout the guest (virtual) machines that run the virtualized applications. consequently, their implementations are unique and independent of the operating system of the virtual machines. in short, it is not necessary to provide different implementations for different operating systems, which reduces the complexity of virtual machine templates and greatly simplies platform management. to demonstrate our solution, we have modified the quick emulator (qemu)-kernel-based virtual machine (kvm) hypervisor source code accordingly. we also present qualitative and quantitative analyses that demonstrate that the new approach is advantageous in terms of software management and quality of experience, compared with our previous work and other well-known thin clients, contributing to the enhancement of vdi systems. copyright (c) 2015john wiley & sons, ltd.
distributed_computing	distributed denial of service attacks produce large volumes of spoofed network data. manual analysis of gigabytes of network logs to determine source of the attacks, victim ips and vulnerability exploitation is time- consuming and error prone. cloud computing has recently emerged as a promising technology which allows everyday users to harness the massively parallel processing capabilities of commodity machines as a payas- you- go utility service. the contribution of this work is the conceptualization, design and implementation of a distributed ddos analysis framework that uses the power of the cloud via the mapreduce paradigm to perform an entropy based clustering and security analysis of the key features of attack traffic. we have evaluated our framework on two large and publicly available ddos attack datasets. moreover, we achieve 86% speedup in analysis with a modestly sized cluster of ten nodes.
data_structures	the study of tumor growth biology with computer-based models is currently an area of active research. different simulation techniques can be used to describe the complexity of any real tumor behavior, among these, ""cellular automata""-based simulations provide an accurate tumor growth graphical representation while, at the same time, keep simpler the implementation of the automata as computer programs. several authors have recently published relevant proposals, based on the latter approach, to solve tumor growth representation problem through the development of some strategies for accelerating the simulation model. these strategies achieve computational performance of cellular-models representation by the appropriate selection of data types, and the clever use of supporting data structures. however, as of today, multithreaded processing techniques and multicore processors have not been used to program cellular growth models with generality. this paper presents a new model that incorporates parallel programming for multi and manycore processors, and implements any synchronization requirement necessary to implement the solution. the proposed parallel model has been proved using java and c++ program implementations on two different platforms: chipset intel i5-4440 and one node of 16-processors cluster of our university. the improvement resulting from the introduction of parallelism into the model is analyzed in this paper, comparing it with the standard sequential simulation model currently used by researchers in mathematical oncology.
machine_learning	generating synthetically mixed data from library spectra provides a direct means to train empirical regression models for subpixel mapping. in order to best represent the subpixel composition of image data, the generation of synthetic mixtures must incorporate a multitude of mixing possibilities. this can lead to an excessive amount of training samples. we show that increasing mixing complexity in the training set improves model performance when quantifying urban land cover with support vector regression (svr). to cope with the challenging increase in the number of training samples, we propose the use of ensemble learning based on bootstrap aggregation from synthetically mixed training data. the workflow is tested on simulated spaceborne imaging spectrometer data acquired over berlin, germany. comparisons to svr without bagging and multiple endmember spectral mixture analysis reveal the usefulness of the methodology for quantitative urban mapping.
data_structures	the finnish patient data repository is a nationwide electronic health record (ehr) system collecting patient data from all healthcare providers. the usefulness of the large amount of data stored in the system depends on the underlying data structures, and thus a solid understanding of these structures is in focus in further development of the data repository. this study seeks to improve that understanding by a systematic literature review. the review takes the physician 's perspective to the use and usefulness of the data structures. the articles included in this review study data structures intended to be used in the actual care process. secondary use and nursing aspects have been covered in separate reviews. after applying the predefined inclusion and exclusion criteria only 40 articles were included in the review. the research on widespread systems in everyday use was especially scarce, most studies concentrated on narrow fields. majority of these studies were primarily developed for specialist use in secondary care units. most structures or applications studied were at an early stage of development. in many applications the use of structured data was found to improve the completeness of the documented data and facilitate its automated use. however, there seem to be some applications where narrative text cannot be easily replaced by structured data. usability results regarding structured representation were conflicting. the scattered nature and paucity of research hinders the generalizability of the findings, and from the system design or implementation point of view the practical value of the scientific literature reviewed is limited.
bioinformatics	the hla-g molecule presents immunomodulatory properties that might inhibit immune responses when interacting with specific natural killer and t cell receptors, such as kir2dl4, ilt2 and ilt4. thus, hla-g might influence the outcome of situations in which fine immune system modulation is required, such as autoimmune diseases, transplants, cancer and pregnancy. the majority of the studies regarding the hla-g gene variability so far was restricted to a specific gene segment (i.e., promoter, coding or 3' untranslated region), and was performed by using sanger sequencing and probabilistic models to infer haplotypes. here we propose a massively parallel sequencing (ngs) with a bioinformatics strategy to evaluate the entire hla-g regulatory and coding segments, with haplotypes inferred relying more on the straightforward haplotyping capabilities of ngs, and less on probabilistic models. then, hla-g variability was surveyed in two admixed population samples of distinct geographical regions and demographic backgrounds, cyprus and brazil. most haplotypes (promoters, coding, 3'utr and extended ones) were detected both in brazil and cyprus and were identical to the ones already described by probabilistic models, indicating that these haplotypes are quite old and may be present worldwide. (c) 2017 elsevier ltd. all rights reserved.
cryptography	with reference to a distributed architecture consisting of sensor nodes connected by wireless links in an arbitrary network topology, we consider a segment-oriented implementation of the single address space paradigm of memory reference. in our approach, applications consist of active entities called components, which are distributed in the network nodes. a component accesses a given segment by presenting a handle for this segment. a handle is a form of pointer protected cryptographically. handles allow an effective implementation of communications between components, and key replacement. the number of messages generated by the execution of the communication primitives is independent of the network size. the key replacement mechanism is well suited to reliable application rekeying over an unreliable network.
machine_learning	heterogeneous computing, combining devices with different architectures such as cpus and gpus, is rising in popularity and promises increased performance combined with reduced energy consumption. opencl has been proposed as a standard for programming such systems and offers functional portability. however, it suffers from poor performance portability, because applications must be retuned for every new device. in this paper, we use machine learning-based auto-tuning to address this problem. benchmarks are run on a random subset of the tuning parameter spaces, and the results are used to build a machine learning-based performance model. the model can then be used to find interesting subspaces for further search. we evaluate our method using five image processing benchmarks, with tuning parameter space sizes up to 2.3 m, using different input sizes, on several devices, including an intel i7 4771 (haswell) cpu, an nvidia tesla k40 gpu, and an amd radeon hd 7970 gpu. we compare different machine learning algorithms for the performance model. our model achieves a mean relative error as low as 3.8% and is able to find solutions on average only 0.29% slower than the best configuration in some cases, evaluating less than 1.1% of the search space. the source code of our framework is available at https://github.com/acelster/ml-autotuning.
algorithm_design	this paper investigates the physical-layer security of a multiuser peer-to-peer (mup2p) relay network for amplify-and-forward (af) protocol, where a secure user and other unclassified users coexist with a multi-antenna eavesdropper and the eavesdropper can wiretap the confidential information in both two cooperative phases. our goal is to optimize the transmit power of the source and the beamforming weights of the relays jointly for secrecy rate maximization subject to the minimum signal-to-interference-noise-ratio (sinr) constraint at each user, and the individual and total power constraints. mathematically, the optimization problem is non-linear and non-convex, which does not facilitate an efficient resource allocation algorithm design. as an alternative, a null space beamforming scheme is adopted at the relays for simplifying the joint optimization and eliminating the confidential information leakage in the second cooperative phase, where the relay beamforming vector lies in the null space of the equivalent channel of the relay to eavesdropper links. although the null space beamforming scheme simplifies the design of resource allocation algorithm, the considered problem is still non-convex and obtaining the global optimum is very difficult, if not impossible. employing a sequential parametric convex approximation (spca) method, we propose an iterative algorithm to obtain an efficient solution of the non-convex problem. besides, the proposed joint design algorithm requires a feasible starting point, we also propose a low complexity feasible initial points searching algorithm. simulations demonstrate the validity of the proposed strategy.
distributed_computing	this paper presents a novel frequency-domain approach for distributed harmonic analysis (dha) of a multi-area interconnected electric power system within restructured environment. the proposed approach is based on a decentralized structure in which harmonic analysis of an area is independently conducted, even with limited available data, via local computing resources. the large-change sensitivity (lcs) concept is then applied in a secure platform to account for the effects of whole network to each single area of the interconnected power system. frequency-dependent models of system elements accompanied by any existent harmonic assessment method can be utilized for harmonic penetration calculation. the proposed dha approach is capable of finding exact values as those of the interconnected system using tcp/ip communication facility. moreover, it allows operator of a utility to independently conduct dha within a restructured power network. the developed method is implemented in an existing software package and applied to several networks including the ieee 118-bus test system. (c) 2016 elsevier b.v. all rights reserved.
network_security	recent variants of distributed denial-of-service (ddos) attacks leverage the flexibility of application-layer protocols to disguise malicious activities as normal traffic patterns, while concurrently overwhelming the target destination with a large request rate. new countermeasures are necessary, aimed at guaranteeing an early and reliable identification of the compromised network nodes (the botnet). in this work we introduce a formal model for the aforementioned class of attacks, and we devise an inference algorithm that estimates the botnet hidden in the network, converging to the true solution as time progresses. notably, the analysis is validated over real network traces.
operating_systems	as one of the most developed intelligent operating systems on mobile devices, android has taken the most part of the cell phone market. a rapid increase in the number of mobile applications make them more and more relevant to people 's daily lives than ever before. due to android 's security mechanism and the validation lack of publishing android apps, android malware detection still remains to be a critical issue. to solve this problem, this paper found that the statistical information of android components (mainly activity) from the manifest file cannot be ignored, based on the traditional method of android permission detection. in this paper, a new feature vector is extracted from the androidmanifest file, which combines the permission information and the component information of the android application. we combine the naive bias classification algorithm, and propose a malicious application detection method based on androidmanifest file information. the experimental results show that the new method performance better than that of the traditional permission detection.
operating_systems	the aim of this work is to propose a methodology that seeks to discover how, when and as the increased performance of the algorithms and how the configuration parameters can influence each other, and finally, discover using statistical methods which settings of virtual environment achieve the best results on average. the experimental design is a preestablished set of tests using scientific and statistical criteria mainly, in order to determine the influence of various factors on the results (metric) of a system or process, identifying and observing the reasons that led to change in the expected value. the planning that was used is factorial planning 3(4), where each factor (core, memory, operating system and virtual machine) were varied in three levels. tested operating systems were ubuntu 14.04 64bit, centos 7.0 64bit and windows 8.0 64bit; and virtual machines were tested kvm, xen and vmware. data were collected and analyzed using analysis of variance. the results show that the major analyzed factors changes the algorithm performance, but they cannot be analyzed separately because there are also significant interactions belonging to these factors. at a 5% significance level, analysis of variance showed that the core interactions: memory, memory with os, memory with vm and os with vm, all these factors impact the runtime of the analyzed algorithm.
image_processing	although recent laboratory tests are showing promising progresses in the materials and production technologies of photovoltaic (pv) devices, the commercial pv modules do not show analogous impressive improvements. therefore, a diagnostic approach, able to check the current state of health of already installed pv systems, as well as their trend of ageing, assumes a strategic importance. in this scenario, we introduce a thermography-based diagnostics able to provide a detailed, clear, and unambiguous information, thanks to a computer-aided investigation that is much deeper than the today available infrared analysis. the proposed approach allows a numerical and qualitative evaluation of each cell of the pv device. this part i-framework introduces the methodology, based on two main analyses. the first one (cell analysis) studies each single cell, while the second one (cluster analysis) focuses the attention on groups of pv cells. the framework is also characterized by a preprocessing in which the region of interest is extracted from the infrared image in order to focus the successive processing and analyses only on this area. the part ii-platform and results shows the cloud platform implementing the workflow (it automatically generates a comprehensive and detailed report), and discusses also several significant cases of study.
operating_systems	positron emission tomography (pet) images are degraded by a phenomenon known as the partial volume effect (pve). approaches have been developed to reduce pves, typically through the utilisation of structural information provided by other imaging modalities such as mri or ct. these methods, known as partial volume correction (pvc) techniques, reduce pves by compensating for the effects of the scanner resolution, thereby improving the quantitative accuracy. the petpvc toolbox described in this paper comprises a suite of methods, both classic and more recent approaches, for the purposes of applying pvc to pet data. eight core pvc techniques are available. these core methods can be combined to create a total of 22 different pvc techniques. simulated brain pet data are used to demonstrate the utility of toolbox in idealised conditions, the effects of applying pvc with mismatched point-spread function (psf) estimates and the potential of novel hybrid pvc methods to improve the quantification of lesions. all anatomy-based pvc techniques achieve complete recovery of the pet signal in cortical grey matter (gm) when performed in idealised conditions. applying deconvolution-based approaches results in incomplete recovery due to premature termination of the iterative process. pvc techniques are sensitive to psf mismatch, causing a bias of up to 16.7% in gm recovery when overestimating the psf by 3 mm. the recovery of both gm and a simulated lesion was improved by combining two pvc techniques together. the petpvc toolbox has been written in c++, supports windows, mac and linux operating systems, is open-source and publicly available.
computer_vision	the ability to present effectively is essential for professionals; therefore, oral communication courses have become part of the curricula for higher education studies. however, speaking in public is still a challenge for many graduates. to tackle this problem, driven by the recent advances in computer vision techniques and prosody analysis, multimodal tools have been designed to support the development of public speaking skills. one of these tools is the presentation trainer, a research prototype able to provide learners with real-time feedback on a set of nonverbal communication aspects. despite initial positive evaluations, the application still lacks grounding in a valid assessment model for nonverbal communication aspects in the context of presentations. to come up with such a model, we conducted semi-structured interviews with experts in the public speaking domain. furthermore, the objective of these interviews was also to have a formative evaluation of the presentation trainer, analysing how it suits with common practices for teaching and learning public speaking skills. the results of this study identify 131 nonverbal communication practices that affect the quality of a presentation and summarize experts' points of view regarding multimodal public speaker instructors.
data_structures	a gradient-statistic-based diagnostic measure is developed in the context of the generalized linear mixed models. its performance is assessed by some real examples and simulation studies, in terms of ability in detecting influential data structures and of concordance with the most used influence measures.
symbolic_computation	in order to monitor carbon monoxide in industrial production, we developed a passive gas radiation measurement system based on fourier transform infrared spectroscopy and carried out infrared radiation measurement experiment of carbon monoxide detection in simulated industrial production environment by this system. the principle, condition, device and data processing method of the experiment are introduced in this paper. in order to solve the problem of light path jitter in the actual industrial field, we simulated the noise in the industrial environment. we combine the advantages of mathematica software in the aspects of graph processing and symbolic computation to data processing to improve the signal noise ratio and noise suppression. based on the hitran database, the nonlinear least square fitting method was used to calculate the concentration of the co spectra before and after the data processing. by comparing the calculated concentration, the data processed by mathematica is reliable and necessary in the industrial production environment.
relational_databases	the physical design of data storage is a critical administrative factor for optimizing system performance. improved system performance can be achieved by building indices. it must be noted that, although indices can improve system performance, creating many random indices may have a negative impact on system performance, and result in wasted space. selecting indices properly is a fundamental aspect of system design optimization, but it is often a complex task. index-selection optimization techniques have been widely studied in database management system (dbmss). however, they have not been get the same study in mapreduce relational-databases. this paper focuses on the index-selection process in hadoop-database hybrid systems. the main contribution is the utilization of data mining techniques to develop a tool for determining optimal index-set configurations. an overall evaluation shows that the index configurations recommended by the developed tool achieved an average performance gain of up to 48% in total analytical tasks performed.
relational_databases	this paper proposes a novel approach for building a natural language interface to a relational database (nli-rdb) using conversational agent (ca), information extraction (ie) and object relational mapping (orm) framework. the ca will help in disambiguating the user 's queries and guiding the user interaction. ie will play an important role in named entities extraction in order to map natural language queries into database queries. the orm framework i.e. the hibernate framework resolves the impedance mismatch between the object oriented paradigms (oop) and relational databases (rdbs) i.e. oop concepts differ from rdb concepts, thus it reduces the complexity in generating sql statements. also, by utilizing orm framework, the rdbs entities are mapped into real world objects, which bring the rdbs a step closer to the user. in addition, the orm framework simplify the interaction between oop and rdbs. the developed nli-rdb system allows the user to interact with objects directly in natural language and through navigation, rather than by using sql statements. this direct interaction tends to be easier and more acceptable for humans whom are nor technically orientated and have no sql knowledge. the nli-rdb system also offers friendly and interactive user interface in order to refine the query generated automatically. the nli-rdb system has been evaluated by a group of participants through a combination of qualitative and quantitative measures. the experimental results show good performance of the prototype and excellent user 's satisfaction.
operating_systems	this paper develops an internet-of-things data highway embracing end sensors, sensor nodes, databases, big data processors, web connections, and high-end statistics engines. it is aiming at automatic, pseudo real-time, and integrative sensor stream processing, fully benefitting from the capability of sophisticated statistics packages supporting a variety of artificial intelligence and data mining libraries. specifically, raspberry pi nodes capture signals from attached sensors via gpio interfaces and insert into a remote mysql database table using its connector utility. in the linux machine, the table entry is purged at each fixed time and dumped to a text file for a later batch analysis using hadoop. the r package running in a windows pc periodically downloads the sensor stream from the database table via the implementation of a library extension invoking relevant operating systems calls. in the r space, even a spatial analysis and visualization can be provided comprehensively.
symbolic_computation	the main aim is to present recent developments in applications of symbolic computing in probabilistic and stochastic analysis, and this is done using the example of the well-known maple system. the key theoretical methods discussed are (i) analytical derivations, (ii) the classical monte-carlo simulation approach, (iii) the stochastic perturbation technique, as well as (iv) some semi-analytical approaches. it is demonstrated in particular how to engage the basic symbolic tools implemented in any system to derive the basic equations for the stochastic perturbation technique and how to make an efficient implementation of the semi-analytical methods using an automatic differentiation and integration provided by the computer algebra program itself. the second important illustration is probabilistic extension of the finite element and finite difference methods coded in maple, showing how to solve boundary value problems with random parameters in the environment of symbolic computing. the response function method belongs to the third group, where interference of classical deterministic software with the non-linear fitting numerical techniques available in various symbolic environments is displayed. we recover in this context the probabilistic structural response in engineering systems and show how to solve partial differential equations including gaussian randomness in their coefficients.
relational_databases	this paper describes the design and implementation of a tool to extract the imdb dataset files and import them into a database. this approach differs from other published tools or research in that the previous work used relational databases. this tool uses document oriented data structures, and allows others to augment the code to change structures based on their needs. the project development required the use of technologies currently in demand for web developers and software engineers, which allows other developers to fork a copy of the work and utilize in their own work. in addition, it provided the project team an opportunity to develop additional marketable skills. finally, a web interface to perform queries against the import data to validate the import process was also developed. these queries include searching by people 's names, searching by movie/tv titles, or viewing specific data on an individual person or movie/tv title..
cryptography	dramatic technology progress in data manipulation induced several attempts of baleful and illegal processing. in this regard, several protection techniques, including cryptography, steganography, and watermarking have been used to avoid the illegal production and distribution of data online. on the other side, the attacks are still a major problem that limits the effectiveness of watermarking techniques. the geometric attacks involve displacement of pixels. therefore, they induce synchronization errors between the original and attacked image that complicates the watermark recovery process. so in order to solve the de-synchronization problem, we propose to use a robust meshing technique between different geometrical data of the image to correct the geometric distortion. in this work, we propose a novel geometrically invariant digital image watermarking approach based on the geometrical feature of the cover image and the defragmented delaunay triangulation on respecting the three constraints of watermarking approaches (imperceptibility of embedded watermark, embedding capacity, and robustness). the aim idea of this work is to propose a blind, imperceptible, and robust digital image watermarking scheme based on defragmented delaunay tessellation and weber 's law. the defragmented triangulation provides a best synchronization of the embedded data after attacks application. the weber 's law is used to propose an auto-thresholding algorithm to compute the optimal embedding gain factor for each watermark 's bit in order to ensure the imperceptibility of the hidden data. firstly, the invariant features in the host gray scale image are extracted by using the canny edge detector. then, the delaunay tessellation of the saved keys points set is generated and defragmented to select the optimal robust triangles for watermark embedding. simulation results illustrate the imperceptibility of the embedded data and the robustness of the proposed approach against intentional and unintentional geometrical attacks are presented in the finally section. copyright (c) 2017 john wiley & sons, ltd.
image_processing	a suitable piece of software is presented to connect abaqus, a sophisticated finite element package, with matlab, the most comprehensive program for mathematical analysis. this interface between these well-known codes not only benefits from the image processing and the integrated graph-plotting features of matlab but also opens up new opportunities in results post-processing, statistical analysis and mathematical optimization, among many other possibilities. the software architecture and usage are appropriately described and two problems of particular engineering significance are addressed to demonstrate its capabilities. firstly, the software is employed to assess cleavage fracture through a novel 3-paranieter weibull probabilistic framework. then, its potential to create and train neural networks is used to identify damage parameters through a hybrid experimental-numerical scheme, and model crack propagation in structural materials by means of a cohesive zone approach. the source code, detailed documentation and a large number of tutorials can be freely downloaded from www.abaqus2matlab.com. (c) 2017 elsevier ltd. all rights reserved.
bioinformatics	accumulating evidence suggests that ribosomal proteins may have extraribosomal functions in various physiological and pathological processes, including cancer. we analyzed the expression of the cirh1a ribosomal protein in colorectal carcinoma and para-carcinoma samples by bioinformatics analyses of data extracted from the cancer genome atlas and in colorectal cancer cell lines in vitro by qpcr. cirh1a was highly expressed in carcinoma samples and colorectal cancer cells. we also transduced the rko colorectal cancer (crc) cell line with lentivirus-mediated small interfering rnas (sirnas) and studied the impact that this knockdown of cirh1a expression had on cell growth. rna interference (rnai)-mediated inhibition of cirh1a expression significantly suppressed proliferation and increased apoptosis of transduced cells, and tended to arrest them in g(1) phase. our data suggest that cirh1a plays a critical role in the proliferation, cell cycle distribution, and apoptosis of human malignant colorectal cells, and might therefore be a potential target for therapeutic strategies.
algorithm_design	the rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. recently, deep learning (a surrogate of machine learning) have won several contests in pattern recognition and machine learning. this review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. this paper also discusses the motivations and principles regarding learning algorithms for deep architectures.
operating_systems	in this paper, we introduce software asset analyzer (saa), a system that monitors and detects potentially vulnerable software asset modifications in end devices, and can be used to guide patch management. software patching is a complex and failure-prone process that, on enterprise networks, requires triage. accurate inventories of software (applications and operating systems) improve patching efficiency, which is a significant concern for security analysts. by generating asset baselines, the saa identifies and reports abnormal deviations in individual end-devices, which allows security analysts to identify vulnerable devices and further enforce security patching. this system is also suited for detecting vulnerable software installs and remediation process verification. saa is a low-cost and efficient method that yields accurate and complete inventories of assets on end-devices, reducing the potential loss from new vulnerabilities.
data_structures	we present a technique for representing bounded-degree planar graphs in a succinct fashion while permitting i/o-efficient traversal of paths. using our representation, a graph with n vertices, (in this paper denotes ) each with an associated key of bits, can be stored in bits and traversing a path of length k takes i/os, where b denotes the disk block size. by applying our construction to the dual of a terrain represented as a triangular irregular network, we can represent the terrain in the above space bounds and support path traversals on the terrain using i/os, where k is the number of triangles visited by the path. this is useful for answering a number of queries on the terrain, such as reporting terrain profiles, trickle paths, and connected components.
cryptography	storing and retrieving data in cloud are important in today 's environment. it also adds insecurity as data sharing in cloud would be affected by hacking or modifying the original content of the data. for secure data transmission, encryption and decryption are the most followed methods. this existing method demands the authentication, security keys from a third party cannot be considered safe because the entire network becomes questionable when such party is not trustworthy. this paper proposes a method, where encrypting the public keys and decrypting the private keys would be generated from cloud node itself, not by a separate trusted authority. this paper also uses the attribute real-time parameters taken for building security keys for each requesting nodes in time dependent manner. since attributes are real-time parameters and changes on random deployment add to security to the contributing nodes. furthermore, some parameters like busy state help to route data and ensure good packet delivery and client data storage.
computer_programming	in this note, we present a novel approach in using visual programming languages and in treating specific language impairments (sli). starting from the characteristics of sli, in particular of syntactic impairment, and from the need to develop metalinguistic awareness, assuming that the latest researches in computer programming assessing it as a language are right, we show the theoretical possibility to treat linguistic impairments, for what concerns linguistic and social difficulties, through the use of visual programming, in particular of scratch. this study does not have an application in the field yet, but we believe in the importance of sharing these ideas.
data_structures	conventional multilevel modeling works well with purely hierarchical data; however, pure hierarchies rarely exist in real datasets. applied researchers employ ad hoc procedures to create purely hierarchical data. for example, applied educational researchers either delete mobile participants' data from the analysis or identify the student only with the last school attended while including an explanatory variable indicating whether a student is mobile. this simulation study compared the parameter and standard error estimates of these two ad hoc procedures for handling and assessing the influence of mobility on outcomes with results based on use of the multiple membership random effects model. substantial bias was found for some parameters when multiple membership data structures were ignored.
data_structures	multiversion databases store both current and historical data. rows are typically annotated with timestamps representing the period when the row is/was valid. we develop novel techniques to reduce index maintenance in multiversion databases, so that indexes can be used effectively for analytical queries over current data without being a heavy burden on transaction throughput. to achieve this end, we re-design persistent index data structures in the storage hierarchy to employ an extra level of indirection. the indirection level is stored on solid-state disks that can support very fast random i/os, so that traversing the extra level of indirection incurs a relatively small overhead. the extra level of indirection dramatically reduces the number of magnetic disk i/os that are needed for index updates and localizes maintenance to indexes on updated attributes. additionally, we batch insertions within the indirection layer in order to reduce physical disk i/os for indexing new records. in this work, we further exploit ssds by introducing novel deltablock techniques for storing the recent changes to data on ssds. using our deltablock, we propose an efficient method to periodically flush the recently changed data from ssds to hdds such that, on the one hand, we keep track of every change (or delta) for every record, and, on the other hand, we avoid redundantly storing the unchanged portion of updated records. by reducing the index maintenance overhead on transactions, we enable operational data stores to create more indexes to support queries. we have developed a prototype of our indirection proposal by extending the widely used generalized search tree open-source project, which is also employed in postgresql. our working implementation demonstrates that we can significantly reduce index maintenance and/or query processing cost by a factor of 3. for the insertion of new records, our novel batching technique can save up to 90 % of the insertion time. for updates, our prototype demonstrates that we can significantly reduce the database size by up to 80 % even with a modest space allocated for deltablocks on ssds.
image_processing	generalized linear image processing systems have been developed from physical image formation models, human visual perception models, and mathematical models. although there have been many papers on the extension, parameterization, and symmetrization of some of these systems, what is lacking is a unified framework such that the development and study of such systems can be performed based on a common ground. in this paper, we propose a conceptual image sensor which models how the light energy is converted into the sensor data. in the proposed sensor model, the input energy is regarded as a random variable and the conversion is through the cumulative distribution function. based on the sensor model, we suggest a statistical framework by which new systems can be derived, and existing and seemingly unrelated systems can be studied from a unified perspective. the proposed statistical framework not only provides a principled way to symmetrizing systems through the even extension of the probability distribution function (pdf) and a natural way for the parameterization of systems through parameters of pdf. in this paper, we demonstrate new applications of the statistical framework through a numerical approximation of the lower incomplete gamma function, through the enhancement of the dynamic range and manipulation of the sharpness of images by using the scalar multiplication operation of the parametric system, through an application of a new system in fusion of multi-exposure images, and through an application of the three new systems for the correction of incorrect exposure.
computer_graphics	despite the fact that numerous online 3d virtual worlds (3dvws) are implemented as elearning and e-training platforms, mainly in academic milieus, these environments are still employed for research purposes and not as a current practice. the present paper presents a synthetic evaluation of an experimental online multi-user 3dvw for teaching and learning computer graphics classes and discusses findings against results from an evaluation of similar activities performed on a moodle lms platform. the paper concludes on different aspects, such as the usability, pedagogical efficacy and technology acceptance, and gives recommendations for good practices in designing educational 3dvws.
relational_databases	with the ever-increasing usage of internet, the availability of digital data is in tremendous demand. in this context, it is essential to protect the ownership of the data and to be able to find the guilty user. in this paper, a fingerprinting scheme is proposed to provide protection for numeric relational database (rdb), which focuses on challenges like: 1. minimum distortion in numeric database, 2. usability preservation, 3. non-violation of the requirement of blind decoding. when the digital data in concern is numeric in nature the usability of data needs to be keenly preserved, this is made possible by achieving minimum distortion.
computer_graphics	one of the methods for obtaining the curve and the surface of complex shape in the engineering geometry, computer graphics, including area of computer vision multi-view geometry, multi-view stereo reconstruction is chaikin algorithm, suggested in 1974. unfortunately, the algorithm works with broken lines so at each step we get a continuous, but not a smooth curve or surface. the current article (paper, work) aims to generalize known in engineering geometry chaikin algorithm for modeling of curves and surfaces of arbitrary smoothness class. we have proved that chaikin 's algorithm is a special case of the wavelet recovery of b-spline curve, or b-spline surface smoothness of class c-1. using a filterbank constructed on the basis of b-splines of arbitrary order for spline wavelets, we suggested a generalization of the chaikin algorithm for modeling of curves and surfaces of arbitrary smoothness class. the generalized chaikin algorithm proposed is a set of theoretical tools for computer-aided design, computer vision systems.
computer_programming	the courses of computer programming language are important basic specialty courses for majors of science and technology in universities. these courses are often both highly abstract and practical, and many works involved in the teaching process are worth exploring and researching. focusing on teaching methods of "" c++ programming language"" course, this paper presents our ideas and practice in performing teaching work in the course. in the paper, we discusses some important works in different teaching links, including textbook selection, courseware preparation, classroom teaching, practical ability training. our practice has proved that, by adopting appropriate teaching methods to adapt to the actual situation in the teaching process, the teachers can control teaching process flexibly and improve teaching effectiveness and quality.
operating_systems	in this paper, we introduce the analysis and the design of a new web-based interactive software tool, called webubu, which has been developed to serve the undergraduate students' needs related to linux operating system (os) issues. the aim of this software development was twofold: on the one hand to familiarize students with ubuntu os environment and, on the other hand to promote the self-learning related to this linux distribution, in the context of everyday os classrooms. wbubu software can be used only for interactive demonstration of features of linux during lectures and to support instructors for evaluation of students' skills in the lab. for other tasks such as long term studying of linux-based os 's physical' linux must be used. our software is essentially a website that simulates ubuntu operating system inside a web browser. undergraduate students can easily explore both graphical user interface (gui) and command line of ubuntu 's environment. additionally, students can ascertain the acquired knowledge through an automated examination process and learn from their mistakes as they shown automatically by the software in real time. educator can manage students' performances that are stored in a database system and assess the individual 's cognitive progress resulting from the software contribution. however, webubu will give students the chance for practice and self assessment when they not involved in the university 's educational process or physical' linux is unavailable. that software intends to complement the existing teaching and learning methods concerning operating systems. (c) 2015 wiley periodicals, inc.
computer_vision	object detection is one of the most important tasks of computer vision. it is usually performed by evaluating a subset of the possible locations of an image, that are more likely to contain the object of interest. exhaustive approaches have now been superseded by object proposal methods. the interplay of detectors and proposal algorithms has not been fully analyzed and exploited up to now, although this is a very relevant problem for object detection in video sequences. we propose to connect, in a closed-loop, detectors and object proposal generator functions exploiting the ordered and continuous nature of video sequences. different from tracking we only require a previous frame to improve both proposal and detection: no prediction based on local motion is performed, thus avoiding tracking errors. we obtain three to four points of improvement in map and a detection time that is lower than faster regions with cnn features (r-cnn), which is the fastest convolutional neural network (cnn) based generic object detector known at the moment.
cryptography	non-malleable codes, defined by dziembowski, pietrzak, and wichs (ics '10), provide roughly the following guarantee: if a codeword c encoding some message x is tampered to c' = f (c) such that c' not equal c, then the tampered message x' contained in c' reveals no information about x. the nonmalleable codes have applications to immunizing cryptosystems against tampering attacks and related -key attacks. one cannot have an efficient non-malleable code that protects against all efficient tampering functions f. however, in this paper we show ""the next best thing"": for any polynomial bound s given a -priori, there is an efficient non-malleable code that protects against all tampering functions f computable by a circuit of size s. more generally, for any family of tampering functions of size vertical bar f vertical bar <= 2(s), there is an efficient non-malleable code that protects against all f is an element of f. the rate of our codes, defined as the ratio of message to codeword size, approaches 1. our results are information -theoretic and our main proof technique relies on a careful probabilistic method argument using limited independence. as a result, we get an efficiently samplable family of efficient codes, such that a random member of the family is non-malleable with overwhelming probability. alternatively, we can view the result as providing an efficient non-malleable code in the ""common reference string"" model. we also introduce a new notion of non-malleable key derivation, which uses randomness x to derive a secret key y = h(x) in such a way that, even if x is tampered to a different value x' = f (x), the derived key y' = h(x') does not reveal any information about y. our results for non-malleable key derivation are analogous to those for non-malleable codes. as a useful tool in our analysis, we rely on the notion of ""leakage -resilient storage"" of davi, dziembowski, and venturi (scn '10), and, as a result of independent interest, we also significantly improve on the parameters of such schemes.
parallel_computing	this work focuses on the development of a multiscale computational fluid dynamics (cfd) simulation framework with application to plasma-enhanced chemical vapor deposition of thin film solar cells. a macroscopic, cfd model is proposed which is capable of accurately reproducing plasma chemistry and transport phenomena within a 2d axisymmetric reactor geometry. additionally, the complex interactions that take place on the surface of a-si: h thin films are coupled with the cfd simulation using a novel kinetic monte carlo scheme which describes the thin film growth, leading to a multiscale cfd model. due to the significant computational challenges imposed by this multiscale cfd model, a parallel computation strategy is presented which allows for reduced processing time via the discretization of both the gas-phase mesh and microscopic thin film growth processes. finally, the multiscale cfd model has been applied to the pecvd process at industrially relevant operating conditions revealing non-uniformities greater than 20% in the growth rate of amorphous silicon films across the radius of the wafer.
computer_vision	this article addresses the problem of creating interactive mixed reality applications where virtual objects interact in images of real world scenarios. this is relevant to create games and architectural or space planning applications that interact with visual elements in the images such as walls, floors and empty spaces. these scenarios are intended to be captured by the users with regular cameras or using previously taken photographs. introducing virtual objects in photographs presents several challenges, such as pose estimation and the creation of a visually correct interaction between virtual objects and the boundaries of the scene. the two main research questions addressed in this article include, the study of the feasibility of creating interactive augmented reality (ar) applications where virtual objects interact in a real world scenario using the image detected high-level features and, also, verifying if untrained users are capable and motivated enough to perform ar initialization steps. the proposed system detects the scene automatically from an image with additional features obtained using basic annotations from the user. this operation is significantly simple to accommodate the needs of non-expert users. the system analyzes one or more photos captured by the user and detects high-level features such as vanishing points, floor and scene orientation. using these features it will be possible to create mixed and augmented reality applications where the user interactively introduces virtual objects that blend with the picture in real time and respond to the physical environment. to validate the solution several system tests are described and compared using available external image datasets.
relational_databases	in the article the detection of ""missing"" or ""hidden"" data during the processing of the well-defined queries to database is discussed and on terms of quality the importance is justified, the article also covers organization, processing, and program implementation issues of fuzzy queries to database of information systems. (c) 2016 the authors. published by elsevier b.v.
software_engineering	purpose - software product management (spm) unites disciplines related to product strategy, planning, development, and release. there are many organizational activities addressing technical, social, and market issues when releasing a software product. owing to the high number of activities involved, spm remains a complex discipline to adopt. the purpose of this paper is to understand what are the core and supporting spm activities. design/methodology/approach - the authors adopted the research method of meta-ethnography to present a set of techniques for synthesizing individual qualitative studies to increase the degree of conceptualization. the results obtained from three empirical studies were synthesized using the meta-ethnography approach to enhance, rethink, and create a higher level abstraction of the findings. findings - the results show that the study has both theoretical and practical contribution. as the meta-ethnography synthesis has not been widely applied in software engineering, the authors illustrate how to use this research method in the practice of software engineering research. the practical contribution of the study is in the identification of five core and six supporting spm activities. originality/value - the practical value of this paper is in the identification of core spm activities that should be present in any company practicing spm. the list of supporting spm consists of activities that are not reported to product manager but affect the product success.
machine_learning	a smart camera is a vision system capable of extracting application-specific information from the captured images. the paper proposes a decentralized and efficient solution for visual parking lot occupancy detection based on a deep convolutional neural network (cnn) specifically designed for smart cameras. this solution is compared with state-of-the-art approaches using two visual datasets: pklot, already existing in literature, and cnrparlc-ext. the former is an existing dataset, that allowed us to exhaustively compare with previous works. the latter dataset has been created in the context of this research, accumulating data across various seasons of the year, to test our approach in particularly challenging situations, exhibiting occlusions, and diverse and difficult viewpoints. this dataset is public available to the scientific community and is another contribution of our research. our experiments show that our solution outperforms and generalizes the best performing approaches on both datasets. the performance of our proposed cnn architecture on the parking lot occupancy detection task, is comparable to the well-known alexnet, which is three orders of magnitude larger. (c) 2016 elsevier ltd. all rights reserved.
software_engineering	software refactoring has been recognized as a valuable process during software development and is often aimed at repaying technical debt. technical debt arises when a software product has been built or amended without full care for structure and extensibility. refactoring is useful to keep technical debt low and if it can be automated there are obvious efficiency benefits. using a combination of automated refactoring techniques, software metrics and metaheuristic searches, an automated refactoring tool can improve the structure of a software system without affecting its functionality. in this paper, four different refactoring approaches are compared using an automated software refactoring tool. weighted sums of metrics are used to form different fitness functions that drive the search process towards certain aspects of software quality. metrics are combined to measure coupling, abstraction and inheritance and a fourth fitness function is proposed to measure reduction in technical debt. the 4 functions are compared against each other using 3 different searches on 6 different open source programs. four out of the 6 programs show a larger improvement in the technical debt function after the search based refactoring process. the results show that the technical debt function is useful for assessing improvement in quality. (c) 2016 elsevier inc. all rights reserved.
computer_graphics	recent years, the stylized draw technology is attracted eyes in non-photorealistic rendering. in this paper, a generation method of an image with chinese ink-wash effect based on blur and edge detection was provided, and implemented in programming on matlab sdk platform. using this method on the platform, a given color image could converse to the picture stylized with chinese ink-wash effect automatically for the user even who has no painting background. the experimental results proved that these methods are effective and satisfied in use, simple, and easy to operate or learn.
distributed_computing	an efficient exploitation of distributed computing infrastructures (dcis) is needed to deal with the data deluge that the scientific community is facing, in particular the astrophysics one due to the emerging square kilometre array (ska) telescope that will reach data rates in the exascale domain. hence, science gateways are being enriched with advanced tools that not only enable the scientists to build their experiments but also to optimize their adaptation to different infrastructures. in this work we present a method, called ""two-level workflow system"", to build this kind of tools and we apply it to a set of analysis tasks of interest for some use applications to the ska. this method uses the software-as-a-service model to keep the scientists insulated from technical complexity of dcis, and the compss programming model to achieve an efficient use of the computing resources.
algorithm_design	a continuous monitoring system (cms) model is presented, having new improved capabilities. the system is based on the actual real-time configuration of the system. existing risk scoring models assume damage potential is estimated by systems' owner, thus rejecting the information relying in the technological configuration. the assumption underlying this research is based on users' ability to estimate business impacts relating to systems' external interfaces which they use regularly in their business activities, but are unable to assess business impacts relating to internal technological components. according to the proposed model systems' damage potential is calculated using technical information on systems' components using a directed graph. the graph is incorporated into the common vulnerability scoring systems' (cvss) algorithm to produce risk scoring measures. framework presentation includes system design, damage potential scoring algorithm design and an illustration of scoring computations.
relational_databases	order dependencies (ods) describe a relationship of order between lists of attributes in a relational table. ods can help to understand the semantics of datasets and the applications producing them. they have applications in the field of query optimization by suggesting query rewrites. also, the existence of an od in a table can provide hints on which integrity constraints are valid for the domain of the data at hand. this work is the first to describe the discovery problem for order dependencies in a principled manner by characterizing the search space, developing and proving pruning rules, and presenting the algorithm order, which finds all order dependencies in a given table. order traverses the lattice of permutations of attributes in a level-wise bottom-up manner. in a comprehensive evaluation, we show that it is efficient even for various large datasets.
symbolic_computation	the korteweg-de vries (kdv)-type equations have been seen in fluid mechanics, plasma physics and lattice dynamics, etc. this paper will address the bilinearization problem for some higher-order kdv equations. based on the relationship between the bilinear method and bell-polynomial scheme, with introducing an auxiliary independent variable, we will present the general bilinear forms. by virtue of the symbolic computation, one- and two-soliton solutions are derived.
parallel_computing	increasing availability of multicore systems has led to greater focus on the design and implementation of languages for writing parallel programs. such languages support various abstractions for parallelism, such as fork-join, async-finish, futures. while they may seem similar, these abstractions lead to different semantics, language design and implementation decisions, and can significantly impact the performance of end-user applications. in this paper, we consider the question of whether it would be possible to unify various paradigms of parallel computing. to this end, we propose a calculus, called dag calculus, that can encode fork-join, async-finish, and futures, and possibly others. we describe dag calculus and its semantics, establish translations from the aforementioned paradigms into dag calculus. these translations establish that dag calculus is sufficiently powerful for encoding programs written in prevailing paradigms of parallelism. we present concurrent algorithms and data structures for realizing dag calculus on multicore hardware and prove that the proposed techniques are consistent with the semantics. finally, we present an implementation of the calculus and evaluate it empirically by comparing its performance to highly optimized code from prior work. the results show that the calculus is expressive and that it competes well with, and sometimes outperforms, the state of the art.
network_security	this paper acquaints with a created application for generating rainbow tables and the results of testing rainbow tables, according to the length of the chosen chain. the paper presents a specialized application containing its own algorithms for reduction functions, changing the length of chain, generating rainbow tables and measuring the effectivity of the password search in detail. within the executed tests, the dependence of rainbow tables size on the password length, the affection of the hash search by the size of the chosen chain and their links to collisions, which arise from the principle of using the reduction function, were observed. the results objectively describe the pros and cons of using rainbow tables and show the possibilities and restrictions for their effective usage.
machine_learning	despite marked progress over the past several decades, convective storm nowcasting remains a challenge because most nowcasting systems are based on linear extrapolation of radar reflectivity without much consideration for other meteorological fields. the variational doppler radar analysis system (vdras) is an advanced convective-scale analysis system capable of providing analysis of 3-d wind, temperature, and humidity by assimilating doppler radar observations. although potentially useful, it is still an open question as to how to use these fields to improve nowcasting. in this study, we present results from our first attempt at developing a support vector machine (svm) box-based nowcasting (sbow) method under the machine learning framework using vdras analysis data. the key design points of sbow are as follows: (1) the study domain is divided into many position-fixed small boxes, and the nowcasting problem is transformed into one question, i.e., will a radar echo >35dbz appear in a box in 30min? (2) box-based temporal and spatial features, which include time trends and surrounding environmental information, are constructed. (3) and the box-based constructed features are used to first train the svm classifier, and then the trained classifier is used to make predictions. compared with complicated and expensive expert systems, the above design of sbow allows the system to be small, compact, straightforward, and easy to maintain and expand at low cost. the experimental results show that although no complicated tracking algorithm is used, sbow can predict the storm movement trend and storm growth with reasonable skill.
computer_vision	non-negative matrix factorization (nmf) is a very effective method for high dimensional data analysis, which has been widely used in computer vision. it can capture the underlying structure of image in the low dimensional space using its parts-based representations. however, nonnegative entries are usually required for the data matrix in nmf, which limits its application. besides, it is actually an unsupervised method without making use of prior information of data. in this paper, we propose a novel method called pairwise constrained graph regularized convex nonnegative matrix factorization (pgcnmf), which not only allows the processing of mixed-sign data matrix but also incorporates pairwise constraints generated among all labeled data into convex nmf framework. we expect that images which have the same class label will have very similar representations in the low dimensional space as much as possible, while images with different class labels will have dissimilar representations as much as possible. clustering experiments on nonnegative and mixed-sign real-world image datasets are conducted to demonstrate the effectiveness of the proposed method. (c) 2016 elsevier b.v. all rights reserved.
bioinformatics	male infertility is a multifactorial disorder with impressively genetic basis; besides, sperm abnormalities are the cause of numerous cases of male infertility. in this study, we evaluated the genetic variants in exons 4 and 5 and their intron-exon boundaries in rabl2b gene in infertile men with oligoasthenoteratozoospermia (oat) and immotile short tail sperm (ists) defects to define if there is any association between these variants and human male infertility. to this purpose, dna was extracted from peripheral blood and after pcr reaction and sequencing, the results of sequenced segments were analyzed. in the present study, 30 infertile men with ists defect and 30 oligoasthenoteratozoospermic infertile men were recruited. all men were of iranian origin and it took 3 years to collect patient 's samples with ists defect. as a result, the 50776482 delc intronic variant (rs144944885) was identified in five patients with oligoasthenoteratozoospermia defect and one patient with ists defect in heterozygote form. this variant was not identified in controls. the allelic frequency of the 50776482 delc variant was significantly statistically higher in oligoasthenoteratozoospermic infertile men (p < 0.05). bioinformatics studies suggested that the 50776482 delc allele would modify the splicing of rabl2b pre-mrna. in addition, we identified a new genetic variant in rabl2b gene. according to the present study, 50776482 delc allele in the rabl2b gene could be a risk factor in iranian infertile men with oligoasthenoteratozoospermia defect, but more genetic studies are required to understand the accurate role of this variant in pathogenesis of human male infertility.
machine_learning	data clustering is a popular analysis tool for data statistics in many fields such as pattern recognition, data mining, machine learning, image analysis, and bioinformatics. the aim of data clustering is to represent large datasets by a fewer number of prototypes or clusters, which brings simplicity in modeling data and thus plays a central role in the process of knowledge discovery and data mining. in this paper, a novel data clustering algorithm based on modified gravitational search algorithm is proposed, which is called bird flock gravitational search algorithm (bfgsa). the bfgsa introduces a new mechanism into gsa to add diversity, a mechanism which is inspired by the collective response behavior of birds. this mechanism performs its diversity enhancement through three main steps including initialization, identification of the nearest neighbors, and orientation change. the initialization is to generate candidate populations for the second steps and the orientation change updates the position of objects based on the nearest neighbors. due to the collective response mechanism, the bfgsa explores a wider range of the search space and thus escapes suboptimal solutions. the performance of the proposed algorithm is evaluated through 13 real benchmark datasets from the well-known uci machine learning repository. its performance is compared with the standard gsa, the artificial bee colony (abc), the particle swarm optimization (pso), the firefly algorithm (fa), k-means, and other four clustering algorithms from the literature. the simulation results indicate that the bfgsa can effectively be used for data clustering.
symbolic_computation	on the hilbert space the singular integral operator with non-carleman shift and conjugation is considered, where are the cauchy projectors, , , , are continuous functions on the unit circle , u is the shift operator and c is the operator of complex conjugation. we show how the symbolic computation capabilities of the computer algebra system mathematica can be used to explore the dimension of the kernel of the operator k. the analytical algorithm [adimker-noncarleman] is presented; several nontrivial examples are given.
cryptography	berta et al 's uncertainty principle in the presence of quantum memory (berta et al 2010 nat. phys. 6 659) reveals uncertainties with quantum side information between the observables. in the recent important work of coles and piani (2014 phys. rev. a 89 022112), the entropic sum is controlled by the first and second maximum overlaps between the two projective measurements. we generalize the entropic uncertainty relation in the presence of quantum memory and find the exact dependence on all d largest overlaps between two measurements on any d-dimensional hilbert space. our bound is rigorously shown to be strictly tighter than previous entropic bounds in the presence of quantum memory, which have potential applications to quantum cryptography with entanglement witnesses and quantum key distributions.
operating_systems	automotive systems are widely used in industry and our daily life. as the reliability of automotive systems is becoming a greater challenge in our community, increasingly more automotive companies are interested in applying formal methods to improve the reliability of automotive systems. we focus on automotive operating systems conforming to the osek/vdx standard. such operating systems are considered as important components to ensure the reliability of the automotive systems. in previous work, we proposed a framework to verify the design models of reactive systems against their specifications. this framework allows us to check whether the design model conforms to the specification based on a simulation relation. this paper shows a case study in which the framework is applied to a real design of the osek/vdx operating system. as a result, we found that we were able to check several important properties of the design model. we show the effectiveness and practicality of the framework based on the results of the case study.
algorithm_design	in the last decade, biology and medicine have undergone a fundamental change: next generation sequencing (ngs) technologies have enabled to obtain genomic sequences very quickly and at small costs compared to the traditional sanger method. these ngs technologies have thus permitted to collect genomic sequences (genes, exomes or even full genomes) of individuals of the same species. these latter sequences are identical to more than 99%. there is thus a strong need for efficient algorithms for indexing and performing fast pattern matching in such specific sets of sequences. in this paper we propose a very efficient algorithm that solves the exact pattern matching problem in a set of highly similar dna sequences where only the pattern can be pre-processed. this new algorithm extends variants of the boyer-moore exact string matching algorithm. experimental results show that it exhibits the best performances in practice.
computer_graphics	with the increasing availability of high-resolution images, videos, and 3d models, the demand for scalable large data processing techniques increases. we introduce a method of sparse dictionary learning for edit propagation of large input data. previous approaches for edit propagation typically employ a global optimization over the whole set of pixels (or vertexes), incurring a prohibitively high memory and time-consumption for large input data. rather than propagating an edit pixel by pixel, we follow the principle of sparse representation to obtain a representative and compact dictionary and perform edit propagation on the dictionary instead. the sparse dictionary provides an intrinsic basis for input data, and the coding coefficients capture the linear relationship between all pixels and the dictionary atoms. the learned dictionary is then optimized by a novel scheme, which maximizes the kullback-leibler divergence between each atom pair to remove redundant atoms. to enable local edit propagation for images or videos with similar appearance, a dictionary learning strategy is proposed by considering range constraint to better account for the global distribution of pixels in their feature space. we show several applications of the sparsity-based edit propagation, including video recoloring, theme editing, and seamless cloning, operating on both color and texture features. our approach can also be applied to computer graphics tasks, such as 3d surface deformation. we demonstrate that with an atom-to-pixel ratio in the order of 0.01% signifying a significant reduction on memory consumption, our method still maintains a high degree of visual fidelity.
computer_programming	novice students (n=99) participated in a lab study in which they learned the fundamentals of computer programming in python using a self-paced computerized learning environment involving a 25-min scaffolded learning phase and a 10-min unscaffolded fadeout phase. students provided affect judgments at approximately 100 points (every 15 s) over the course of viewing videos of their faces and computer screens recorded during the learning session. the results indicated that engagement, confusion, frustration, boredom, and curiosity were the most frequent affective states, while anxiety, happiness, anger, surprise, disgust, sadness, and fear were rare. confusion + frustration and curiosity + engagement were identified as two frequently co-occurring pairs of affective states. an analysis of affect dynamics indicated that there were reciprocal transitions between engagement and confusion, confusion and frustration, and one way transitions between frustration and boredom and boredom and engagement. considering interaction events in tandem with affect revealed that constructing code was the central activity that preceded and followed each affective state. further, confusion and frustration followed errors and preceded hint usage, while curiosity and engagement followed reading or coding. an analysis of affect-learning relationships after partialling out control variables (e. g., scholastic aptitude, hint usage) indicated that boredom (r=-.149) and frustration (r=-.218) were negative correlated with learning while transitions between confusion. frustration (r=.103), frustration. confusion (r=.105), and boredom. engagement (r=.282) were positively correlated with learning. implications of the results to theory on affect incidence and dynamics and on the design of affect-aware learning environments are discussed.
computer_programming	game-based learning is considered as a very motivational tool to accelerate active learning of students. as such learning environments usually follow a computer-assisted instruction concept that offers no adaptability to each student, some idea from intelligent tutoring systems (its) are borrowed and applied in educational games to teach introductory programming. thus, we developed a game-based intelligent tutoring system (gits) in the form of a competitive board game. the board game revises the classic table game ""snakes and ladders"" to improve web-based problem solving skills and learning computer programming. moreover, a mini-game, tictac-toe quiz, is applied in gits to update the bayesian network used for the process of decision-making in our proposed system. our future work is to evaluate the gits by conducting an experimental study using novices.
symbolic_computation	in this article, the novel (g '/ g)-expansion method is successfully applied to construct the abundant travelling wave solutions to the kdv-mkdv equation with the aid of symbolic computation. this equation is one of the most popular equation in soliton physics and appear in many practical scenarios like thermal pulse, wave propagation of bound particle, etc. the method is reliable and useful, and gives more general exact travelling wave solutions than the existing methods. the solutions obtained are in the form of hyperbolic, trigonometric and rational functions including solitary, singular and periodic solutions which have many potential applications in physical science and engineering. many of these solutions are new and some have already been constructed. additionally, the constraint conditions, for the existence of the solutions are also listed.
data_structures	detection of data structures in spectral clustering approaches becomes a difficult task when dealing with complex distributions. moreover, there is a need of a real user prior knowledge about the influence of the free parameters when building the graph. here, we introduce a graph pruning approach, termed kernel alignment based graph pruning (kagp), within a spectral clustering framework that enhances both the local and global data consistencies for a given input similarity. the kagp allows revealing hidden data structures by finding relevant pair-wise relationships among samples. so, kagp estimates the loss of information during the pruning process in terms of a kernel alignment-based cost function. besides, we encode the sample similarities using a compactly supported kernel function that allows obtaining a sparse data representation to support spectral clustering techniques. attained results shows that kagp enhances the clustering performance in most of the cases. in addition, kagp avoids the need for a comprehensive user knowledge regarding the influence of its free parameters. (c) 2015 elsevier b.v. all rights reserved.
image_processing	this study presents the results from the rheological measurement of clay suspensions using vane geometry in a wide gap configuration. it focuses on how measurement of viscosity cannot be effective for two reasons: the limits of the vane geometry itself and the limits of the material depending on its content of solid particles. image analysis of the flow while shearing the material is carried out to relate the flow behavior. several approaches to compute the shear flow curve from torque-rotational velocity data are used. the results demonstrate that the applied setpoint while applying a logarithmic shear rate ramp can be very different from the calculated shear rate from existing theories. depending on the solid volume fraction of the particles in the mixture, we relate the macroscopic behavior using image analysis and the shear flow curves to the rheophysical regime of the flow of the suspensions. therefore, this paper has two simultaneous goals: the first one is to describe the physical phenomena which control macroscopic behavior and the second one is to highlight the limits of the vane geometry for viscosity measurement of mineral suspensions like kaolinite pastes.
relational_databases	over the past few years, reversible watermarking techniques for relational databases have been proposed to provide protection of ownership rights, data tempering, and data integrity. mainly, these techniques ensure original data recovery from watermarked data, whereas irreversible watermarking schemes only protect ownership rights. this characteristic of reversible watermarking has emerged as a candidate solution for the protection of ownership rights of data, intolerable to modifications such as medical data, genetic data, credit card, and bank account data. the main objective of this paper is to make an extensive survey of the state-of-the-art in reversible watermarking techniques for relational databases to reflect recent research progress and to point out the key issues for future research. in order to analyze these techniques, a classification has been performed on the basis of (i) the extent of modifications introduced by the watermarking scheme in the underlying data and (ii) the robustness of the embedded watermark against malicious attacks. copyright (c) 2015 john wiley & sons, ltd.
cryptography	we propose multi-party quantum summation protocols based on single particles, in which participants are allowed to compute the summation of their inputs without the help of a trusted third party and preserve the privacy of their inputs. only one participant who generates the source particles needs to perform unitary operations and only single particles are needed in the beginning of the protocols.
relational_databases	spreadsheets are among the most commonly used applications for data management and analysis. they combine data processing with very diverse supplementary features: statistics, visualization, reporting, linear programming solvers, web queries periodically downloading data from external sources, etc. however, the spreadsheet paradigm of computation still lacks sufficient analysis. in this article, we demonstrate that a spreadsheet can implement all data transformations definable in sql, merely by utilizing spreadsheet formulas. we provide a query compiler, which translates any given sql query into a worksheet of the same semantics, including null values. thereby, database operations become available to the users who do not want to migrate to a database. they can define their queries using a high-level language and then get their execution plans in a plain vanilla spreadsheet. the functions available in spreadsheets impose limitations on the algorithms one can implement. in this paper, we offer o(n log(2) n) sorting spreadsheet, using a non-constant number of rows, and, surprisingly, depth-first-search and breadth-first-search on graphs.
network_security	with the ever growing demand of location-independent access to autonomous decentralized systems (ads), anomaly detection scheme for industrial ethernet, which highly is satisfied with demanding real-time and reliable industrial applications, becomes one of the most pressing subjects in ads. in this paper, we present an innovative approach to build a traffic model based on structural time series model for a chemical industry system. a basic structural model that decomposes time series into four items is established by the stationary analysis of industrial traffic. parameters in the model are identified by the state space model, which is conducted from the training sequence using standard kalman filter recursions and the em algorithm. furthermore, the performance of state space model is evaluated by the experimental results that confirm significant improvement in detection accuracy and the validity of abnormal data localization. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	apache hadoop is a widely used distributed computing framework and its file system is hadoop distributed file system (hdfs), which assumes that datanodes in a system are homogeneous in nature. when a cloud system scales up, datanodes are very likely to become heterogeneous. thus, extensive research has been placed on improving performance for heterogeneous hadoop systems, but little attention has been placed on security improvements. this motivates us to investigate a data allocation scheme called the secure hdfs (sechdfs) by integrating the secret sharing technique to improve storage security in a heterogeneous hadoop system. datanodes in a hadoop system are classified into a variety of different types of groups based on their vulnerability characteristics. sechdfs addresses the increased risk issue caused by data replication in hdfs by allocating fragments of a file to as many different types of datanodes as possible and multiple replicas of the same fragment to datanodes of the same type. a storage assurance model is developed to evaluate the quality of security offered by sechdfs. analysis of the assurance model and performance evaluation experiments show that sechdfs does not impact the performance of the hadoop system that much in comparison with the default hdfs, while significantly improving data assurance in a heterogeneous environment.
computer_graphics	in this paper, the authors propose a ""multi modification design model"", as a new type of visual expression. this visual expression by a computer has been unified as so-called computer graphics (cg). today, in most circumstances, visual expression by a computer indicates cg; its main role is to generate a image on the virtual space of a computer and this is an image that is difficult to be drawn or recreated physically. in recent years, expressive abilities by cg have been enhanced. in other words, complicated images and drawings that require much calculation became possible and easy due to the speeding up of computer processing. however this alone cannot broaden the variety of expressivity of visual expression by a computer we need to extend its ability. usually the history of visual expression referred to its cg history and there has not been much progress or innovation in terms of creativity since 1980s when the socalled cg was determined. the author describes ""multi modification design model"", as a new visual expression by a computer which is demonstrated through the actual produced work.
operating_systems	light emitting diode (led) luminaires provide compelling benefits such as long-term cost savings, and safety advantages through longer life and durability. led luminaires have lower maintenance costs, and improved instant restrike capability when compared to legacy luminaire types. in industrial applications, the proven lower total cost of ownership and increased safety for leds are the driving forces for led luminaires to replace traditional light sources. as with any new technology, challenges exist in designing and maintaining an optimal system to withstand the intended application and environmental conditions. common challenges and considerations include luminaire placement, the intensity/dispersion/quality of light, thermal management, material selection, and redundant safety and operating systems. the authors will examine best practices and common misconceptions in the selection and application of led luminaires focusing on process and harsh/hazardous locations. the probability of success in maximizing reliability, safety & efficiency of led technology in such applications is enhanced by utilizing proven design principles & methods. the selection methods and design principles will be explained using actual application examples and case studies.
operating_systems	the object modeling technique, a generally used object-oriented software development technique, comprises the object, dynamic, and functional models to provide three corresponding views that graphically express diverse aspects of software systems. this paper adds security engineering into an object oriented model-driven software development for real life web applications. in this paper, we use mining patterns in web applications. this research paper proposes a unified modeling language based secure software maintenance procedure. the proposed method is applied for maintaining a large-scale software product and real-life product-line products. after modeling we can implement and run this web application, on spf based trusted operating systems. reverse engineering has become a main concern in software development industry because of their complete size and so much difficulty. this difficulty needs to be tackled since the software systems in question are of significant importance to their owners and maintainers. for secure designing of web applications, this paper propose system security performance model for trusted operating system. for re engineering and re-implementation process of web applications, this paper proposes the model driven roundtrip engineering approach.
symbolic_computation	this paper addresses the potential korteweg-de vries equation. the singular 1-soliton solution is obtained by the aid of ansatz method. subsequently, the -expansion method and the exp-function approach also gives a few more interesting solutions. finally, the lie symmetry analysis leads to another plethora of solution to the equation. these results are going to be extremely useful and applicable in applied mathematics and theoretical physics.
operating_systems	this article analyzes dynamic changes in mobile phone popularity based on phone features. the time period, 2004-2013, selected for the study is interesting because many technical innovations took place molding the mobile phone market dramatically. the study utilizes comprehensive phone model and sales data collected in finland, combined with temporally ordered probabilistic models to discover the time behavior of predictivity. more precisely, the tree augmented na ve bayes - classification method is adapted to detect those phone characteristics that best predict the annual phone popularity measured as phone model unit sales. linear regression and the chow test are used to discover potential trends and structural breaks. the strength of the predictivity is measured as kullback-leibler divergence. this kind of systematic longitudinal analysis highlights patterns, which are otherwise not possible to observe. the study discovered that the operating system is clearly the only feature with an increasing strength in predicting popularity over time. in contrast, sixteen features have structural breaks between 2004 and 2013. most such breaks are related to the technical evolution of phones: their display, communication, and camera capabilities. notably, the structural break in 2007-2008 related to the phone manufacturer brand is interpreted as the market turning from hardware to software driven mode, which contributed to nokia 's failure with symbian and windows operating systems, and to google 's success with a hardware independent operating system android. (c) 2016 elsevier ltd. all rights reserved.
computer_vision	breakthrough performances have been achieved in computer vision by utilizing deep neural networks. in this paper we propose to use random forest to classify image representations obtained by concatenating multiple layers of learned features of deep convolutional neural networks for scene classification. specifically, we first use deep convolutional neural networks pre-trained on the large-scale image database places to extract features from scene images. then, we concatenate multiple layers of features of the deep neural networks as image representations. after that, we use random forest as the classifier for scene classification. moreover, to reduce feature redundancy in image representations we derived a novel feature selection method for selecting features that are suitable for random forest classification. extensive experiments are conducted on two benchmark datasets, i.e. mit-indoor and uiuc-sports. obtained results demonstrated the effectiveness of the proposed method. the contributions of the paper are as follows. first, by extracting multiple layers of deep neural networks, we can explore more information of image contents for determining their categories. second, we proposed a novel feature selection method that can be used to reduce redundancy in features obtained by deep neural networks for classification based on random forest. in particular, since deep learning methods can be used to augment expert systems by having the systems essentially training themselves, and the proposed framework is general, which can be easily extended to other intelligent systems that utilize deep learning methods, the proposed method provide a potential way for improving performances of other expert and intelligent systems. (c) 2016 elsevier ltd. all rights reserved.
bioinformatics	amr h sawalha is professor of internal medicine and marvin and betty danto research professor of connective tissue research at the university of michigan, department of internal medicine, division of rheumatology. he also holds faculty appointments at the center for computational medicine and bioinformatics and the graduate program in immunology at the university of michigan. he was recently appointed as guest professor at central south university in changsha, china. he received his medical degree from jordan university of science and technology and completed his residency training in internal medicine at the university of oklahoma health sciences center, and his fellowship in rheumatology at the university of michigan. his research focus is the genetics and epigenetics of complex autoimmune and inflammatory diseases, including lupus and systemic vasculitis. he has authored over 100 peer-reviewed manuscripts, book chapters and review articles, and is on the editorial board of several journals in his field. he has been elected as a member of the american society for clinical investigation, and has received numerous awards, including the edmund l dubois, md, memorial lectureship award from the american college of rheumatology in recognition for his work in lupus. he is chair of the lupus foundation of america research subcommittee and is a member of the vasculitis foundation medical and scientific advisory board. he also provides clinical care and teaching in the rheumatology outpatient and inpatient services, and he is the director of the nih-funded rheumatology training grant at the university of michigan.
symbolic_computation	in order to develop a computationally efficient implementation of the maximally permissive deadlock avoidance policy (dap) for complex resource allocation systems (ras), a recent approach focuses on the identification of a set of critical states of the underlying ras state-space, referred to as minimal boundary unsafe states. the availability of this information enables an expedient one-step-lookahead scheme that prevents the ras from reaching outside its safe region. the work presented in this paper seeks to develop a symbolic approach, based on binary decision diagrams (bdds), for efficiently retrieving the (minimal) boundary unsafe states from the underlying ras state-space. the presented results clearly demonstrate that symbolic computation enables the deployment of the maximally permissive dap for complex ras with very large structure and state-spaces with limited time and memory requirements. furthermore, the involved computational costs are substantially reduced through the pertinent exploitation of the special structure that exists in the considered problem. note to practitioners-a key component of the real-time control of many flexibly automated operations is the management of the allocation of a finite set of reusable resources among a set of concurrently executing processes so that this allocation remains dead-lock-free. the corresponding problem is known as deadlock avoidance, and its resolution in a way that retains the sought operational flexibilities has been a challenging problem due to: (i) the inability to easily foresee the longer-term implications of an imminent allocation and (ii) the very large sizes of the relevant state spaces that prevent an online assessment of these implications through exhaustive enumeration. a recent methodology has sought to address these complications through the offline identification and storage of a set of critical states in the underlying state space that renders efficient the safety assessment of any given resource allocation. the results presented in this paper further extend and strengthen this methodology by complementing it with techniques borrowed from the area of symbolic computation; these techniques enable a more compressed representation of the underlying state spaces and of the various subsets and operations that are involved in the pursued computation.
machine_learning	a critical aspect of dimensionality reduction is to assess the quality of selected (or produced) feature subsets properly. feature subset assessment in machine learning refers to split a given feature subset into a training set, which is used to estimate the parameters of a classification model, and a test set used to estimate the predictive performance of the model. then, averaging the results of multiple splitting (i.e., cross-validation, cv) is commonly used to decrease the variance of the estimator. but in practice, cv scheme is very computationally expensive. in this paper, we propose a new statistics index method called lw-index for evaluation of feature subset and dimensionality reduction algorithms in general. the proposed method is a type of ""classical statistics"" approach that uses the feature subset to compute an empirical estimate of the quality of feature subset. a large number of performance comparisons with the machine learning approach conducted on fourteen benchmark collections show that the proposed lw index is highly correlated with the external indices (i.e., macrof(1), microf(1)) of svm and centroid-based classifier (cbc) trained by five-fold cv scheme. furthermore, the experimental results indicate that lw index has the same performance as the traditional cv scheme for evaluating the dimensionality reduction algorithms and it is more efficient than the traditional methodology. therefore, one contribution of this paper is to present an alternative methodology, based on an internal index typically used in the unsupervised learning context, that is computationally cheaper than the traditional cv methodology. another contribution is to propose a new internal index that behaves better than other similar indices widely used in clustering and shows high correlation with the results obtained by the traditional methodology. (c) 2017 the authors. published by elsevier b.v.
algorithm_design	the effectiveness of in situ remediation to treat contaminated aquifers is limited by the degree of contact between the injected treatment chemical and the groundwater contaminant. in this study, candidate designs that actively spread the treatment chemical into the contaminant are generated using a multi-objective evolutionary algorithm. design parameters pertaining to the amount of treatment chemical and the duration and rate of its injection are optimized according to objectives established for the remediation - maximizing contaminant degradation while minimizing energy and material requirements. because groundwater contaminants have different reaction and sorption properties that influence their ability to be degraded with in situ remediation, optimization was conducted for six different combinations of reaction rate coefficients and sorption rates constants to represent remediation of the common groundwater contaminants, trichloroethene, tetrachloroethene, and toluene, using the treatment chemical, permanganate. results indicate that active spreading for contaminants with low reaction rate coefficients should be conducted by using greater amounts of treatment chemical mass and longer injection durations relative to contaminants with high reaction rate coefficients. for contaminants with slow sorption or contaminants in heterogeneous aquifers, two different design strategies are acceptable - one that injects high concentrations of treatment chemical mass over a short duration or one that injects lower concentrations of treatment chemical mass over a long duration. thus, decision-makers can select a strategy according to their preference for material or energy use. finally, for scenarios with high ambient groundwater velocities, the injection rate used for active spreading should be high enough for the groundwater divide to encompass the entire contaminant plume. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	the algorithms of exposure of signals of unknown form are offered in multidimensional time series with subsequent presentation of separate time series from every mining hole as points multidimensional space of parameters of these mining holes. by means of facilities of cognitive machine graphic arts these mining holes form kappa cognitive characters in that evidently as aberrant behavior of parameters of these mining holes can be educed in case of multidimensional terabyte of time series.
parallel_computing	dynamic simulation for transient stability assessment is one of the most important, but intensive, computational tasks for power system planning and operation. several commercial software tools provide functionality for performing multiple dynamic simulations such as those in contingency analysis simultaneously on parallel computers. nevertheless, a single dynamic simulation is still a time consuming process performed sequentially on one single computing core as the tools were originally designed. modern high performance computing (hpc) holds the promise to accelerate a single dynamic simulation by parallelizing its kernel algorithms without compromising computational accuracy. parallelizing a single dynamic simulation is a much more challenging problem than the contingency-type parallel computing. it requires a good match between simulation algorithms and computing hardware. this paper provides guidance for such a match so as to design and implement parallel dynamic simulation to maximize the utilization of computing hardware and the performance of the simulation. the guidance is derived through comparative implementation of four parallel dynamic simulation schemes in two state-of-the-art hpc environments: 1) message passing interface and 2) open multi-processing. the scalability and speedup performance of parallelized dynamic simulation are thoroughly studied to determine the impact of simulation algorithms and computing hardware configurations. several testing cases are presented to illustrate the derived guidance.
network_security	a darknet monitoring system is developed to grasp malicious activities on the internet in an early stage and to cope with them. the darknet monitoring system consists of network sensors deployed widely on the internet. the sensors capture incoming unsolicited packets. a goal of this system analyzes captured malicious packets and provides effective information for protecting good internet users from malicious activities. to provide effective and reliable information, sensors must be deployed in secret and hidden from outside. on the other hand, attackers intend to detect sensors for evading them. this attempt is known as localization attacks to darknet monitoring systems. if actual location of sensors is revealed to attackers, it is almost impossible to grasp the latest tactics used by attackers. thus in our previous work, we proposed a packet sampling method, which samples incoming packets based on an attribute of packets sender, to increase a tolerance to a localization attack and to keep a high quality of information publicized by the system. as a result, we almost succeeded to counter from a localization attack, which generates spike on the publicized graph to detect a sensor. however in some cases, proposed sampling method works to attacker 's advantage and spikes appear clearly on the graph. therefore, we propose advanced sampling methods, which sample incoming packets based on multiple attributes of packets sender. in this paper, we present our improved methods and show a promising evaluation result obtained from the simulation.
data_structures	given an initial assignment of processes to machines, the machine reassignment problem is to find an assignment that improves the machine usage, subject to several resource and allocation constraints, and considering reassignment costs. we propose a heuristic based on simulated annealing for solving this problem. it uses two neighborhoods, one that moves a process from one machine to another, and a second one that swaps two processes on different machines. we present data structures that permit to validate and execute a move in time where is the number of resources and the number of dependencies of the service the process belongs to. the heuristic runs with two different sets of parameters in parallel until a convergence criterion is satisfied. the machine reassignment problem was subject of the roadef/euro challenge in 2012, and the proposed algorithm ranked fourth in the final round of the senior category of the competition.
symbolic_computation	we study the nonlinear dynamics of (2 + 1) dimensional ferromagnetic (fm) spin system with bilinear and biquadratic interactions in the semiclassical limit and the dynamics is found to be governed by a new integrable fourth order nonlinear schrodinger (nls) equation in (2 + 1) dimensions. the integrability is identified by using lax pair operators and soliton solutions are obtained using straightforward darboux transformation (dt) technique. the model hamiltonian representing (2 + 1) dimensional fm spin chain with varying bilinear and biquadratic interactions are also constructed and inhomogeneity effects are studied by performing a perturbation analysis. moreover, the modulational instability (mi) aspects are discussed through analytical solutions and graphical illustrations. (c) 2015 elsevier b.v. all rights reserved.
parallel_computing	we have witnessed the tremendous momentum of the second spring of parallel computing in recent years. but, we should remember the low points of the field more than 20 years ago and review the lesson that has led to the question at that point whether ""parallel computing will soon be relegated to the trash heap reserved for promising technologies that never quite make it"" in an article entitled ""the death of parallel computing"" written by the late ken kennedy a prominent leader of parallel computing in the world. facing the new era of parallel computing, we should learn from the robust history of sequential computation in the past 60 years. we should study the foundation established by the model of turing machine (1936) and its profound impact in this history. to this end, this paper examines the disappointing state of the work in parallel turing machine models in the past 50 years of parallel computing research. lacking a solid yet intuitive parallel turing machine model will continue to be a serious challenge in the future parallel computing. our paper presents an attempt to address this challenge by presenting a proposal of a parallel turing machine model. we also discuss why we start our work in this paper from a parallel turing machine model instead of other choices.
algorithm_design	responding to the difficulties of implementing meta-heuristics hybridization, this paper introduces a set of scripting language constructs for the rapid algorithm design and development focusing on the two well-known meta-heuristics, namely particle swarm optimization (pso) and genetic algorithm (ga). additionally, the pso-ga hybrids have been embedded with dynamic parameterization. in this paper, the compiler specification and codes for developing the scripting language constructs are described. then, based on the several algorithms of pso-ga hybrids that have been developed with the scripting language constructs, the line of codes (loc) are measured in order to test the easiness of the programming language. the results show that across all algorithms, the scripting language is anticipated to enable easy programming, which has been presented by the very less of loc compared to the java programming language.
computer_graphics	3d models of humans are commonly used within computer graphics and vision, and so the ability to distinguish between body shapes is an important shape retrieval problem. we extend our recent paper which provided a benchmark for testing non-rigid 3d shape retrieval algorithms on 3d human models. this benchmark provided a far stricter challenge than previous shape benchmarks. we have added 145 new models for use as a separate training set, in order to standardise the training data used and provide a fairer comparison. we have also included experiments with the faust dataset of human scans. all participants of the previous benchmark study have taken part in the new tests reported here, many providing updated results using the new data. in addition, further participants have also taken part, and we provide extra analysis of the retrieval results. a total of 25 different shape retrieval methods are compared.
data_structures	low-rank representation (lrr) is a useful tool for seeking the lowest rank representation among all the coefficient matrices that represent the images as linear combinations of the basis in the given dictionary. however, it is an unsupervised method and has poor applicability and performance in real scenarios because of the lack of image information. in this paper, based on lrr, we propose a novel semi-supervised approach, called label constrained sparse low-rank representation (lcslrr), which incorporates the label information as an additional hard constraint. specifically, this paper develops an optimization process in which the improvement of the discriminating power of the low-rank decomposition is presented explicitly by adding the label information constraint. we construct lcslrr-graph to represent data structures for semi-supervised learning and provide the weights of edges in the graph by seeking a low-rank and sparse matrix. we conduct extensive experiments on publicly available databases to verify the effectiveness of our novel algorithm in comparison to the state-of-the-art approaches through a set of evaluations.
network_security	in the current era of computer and communication rapid development, network security has become one of the most important factors to consider. security considerations in wireless sensor networks (wsns) have been an interesting point in research especially with the fast spread of wsns. in this paper, an efficient two-layer and three-layer intrusion detection models are introduced. the two-layer model represents the levels of the sensor and sink nodes. the three-layer model represents the levels of the sensor, sink and base station. the models are elaborated and examined through a set of experiments. a supervised learning algorithm is introduced to be used in the sensor node layer and an unsupervised learning algorithm is introduced to be used in the other layers. the learning algorithms used only 10% of the data for training and gave a high detection accuracy on the used dataset, using lesser number of features compared to other approaches.
symbolic_computation	based on the invariant subspace method, a symbolic computation scheme and its corresponding maple package are developed to construct exact solutions for nonlinear evolution equations. in the symbolic computation scheme, a crucial step is constructing the linear differential equations as invariant subspaces that systems of evolution equations admin and taking their solutions as subspaces to construct exact solutions. the maple package is proved to provide an easy way for constructing exact solutions of evolution equations automatically by only inputting several necessary parameters. three different types of examples are given to illustrate the scope and demonstrate the validity of our package, especially for wave equation. the results of the examples reveal that there are polynomial subspaces, trigonometric subspaces, exponential subspaces and other complex subspaces as invariant subspaces that evolutions equations admit. in addition, our maple software package provides a helpful and easy-to-use tool in science and engineering to deal with a wide variety of (1+1) dimensional nonlinear evolution equations.
symbolic_computation	in this paper, a -dimensional generalized shallow water wave equation is investigated through bilinear hirota method. interestingly, the breather-type and lump-type soliton solutions are obtained. furthermore, dynamic properties of the soliton waves are revealed by means of the asymptotic analysis. based on hirota bilinear method and riemann theta function, we succeed in constructing quasi-periodic wave solutions with a generalized form. we also display the asymptotic properties of these quasi-periodic wave solutions and point out the relation between the quasi-periodic wave solutions and the soliton solutions.
parallel_computing	as a global optimization algorithm, genetic algorithm is an advantageous tool due to its global convergence, great robustness, suitable for parallel computing and so on. selection of optimal parameters, e.g., maximum generations, population size, and genetic operators (crossover fraction and mutation fraction), is extremely crucial for particle size characterization by ultrasound attenuation spectroscopy with genetic algorithm. a series of particle system with different distribution functions were numerically investigated in this work it revealed that the simulated results were consistent with the given particle size distribution when the maximum generations, population size crossover fraction and mutation fraction were appropriate chosen. furthermore, the anti-noise performance of genetic algorithm and its three improved forms applied in particle system with bimodal distribution were also studied. two groups of samples (micron-sized glass beads-glycerol suspension and aqueous polystyrene suspension) were investigated experimentally by ultrasound attenuation spectroscopy, and it showed a good agreement between improved genetic algorithm 3 (iga3) and microscope image analysis (mia). (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	fpga-based accelerators have recently evolved as strong competitors to the traditional gpu-based accelerators in modern high-performance computing systems. they offer both high computational capabilities and considerably lower energy consumption. high-level synthesis (hls) can be used to overcome the main hurdle in the mainstream usage of the fpga-based accelerators, i.e., the complexity of their design flow. hls enables the designers to program an fpga directly by using high-level languages, e.g., c, c++, systemc, and opencl. this paper presents an hls-based fpga implementation of several algorithms from a variety of application domains. a performance comparison in terms of execution time, energy, and power consumption with some high-end gpus is performed as well. the algorithms have been modeled in opencl for both gpu and fpga implementation. we conclude that fpgas are much more energy-efficient than gpus in all the test cases that we considered. moreover, fpgas can sometimes be faster than gpus by using an fpga-specific opencl programming style and utilizing a variety of appropriate hls directives.
operating_systems	the pillars of computer science and engineering (cse) curriculum are data structures, database management systems, languages, operating systems and algorithms. this article explains the relationship and connectivity of these core courses. authors follow the pedagogy technique to teach the data structures (ds) by considering its evolution. the article focus on the method of providing connectivity between the building blocks of ds like data containers, container iterators, algorithms and functors. each data structure is explained by considering its property, iterations, problems and applications. the three fold method is followed to teach ds, which includes think, build and discuss phases. the pedagogy techniques practiced by authors are revealed many mysteries, which are not discussed in most of the ds text books.
parallel_computing	modern semiconductor detectors allow for charged particle tracking with ever increasing position resolution. due to the reduction of the spatial hit uncertainties, multiple coulomb scattering in the detector layers becomes the dominant source for tracking uncertainties. in this case long distance effects can be ignored for the momentum measurement, and the track fit can consequently be formulated as a sum of independent fits to hit triplets. in this paper we present an analytical solution for a three-dimensional triplet(s) fit in a homogeneous magnetic field based on a multiple scattering model. track fitting of hit triplets is performed using a linearization ansatz. the momentum resolution is discussed for a typical spectrometer setup. furthermore the track fit is compared with other track fits for two different pixel detector geometries, namely the mu3e experiment at psi and a typical high-energy collider experiment. for a large momentum range the triplets fit provides a significantly better performance than a single helix fit. the triplets fit is fast and can easily be parallelized, which makes it ideal for the implementation on parallel computing architectures.
computer_programming	image definition will be influenced by alignment errors of mirrors in an optical system consisting of hyperbolic, parabolic or ellipse mirrors. the major factors of alignment errors are gravity, wind loads and heat exchange for some optical systems like ground-based telescopes, while vibration and temperature gradient for systems like space telescopes. larger telescopes are more sensitive to these error sources, which becomes the concerns of researchers. so the alignment errors of mirrors must be corrected in time to keep systems working in best condition. in order to solve the problem, many methods are proposed based on the detection of wave-front errors using wave-front sensors like hartmann-shack. however, wave-front sensors may not be used or cause optical systems to be more complicated. for example, multi-fields must be tested when telescope is working. on the one hand, if a wave-front sensor is used, it must be moved around imaging plane, on the other hand, if more wave-front sensors are used, system must be more complicated. so a new method is discussed for alignment error correction by evaluating the quality of spot diagrams based on the using of stochastic parallel gradient descent (spgd) algorithm. the method considers the performance metric like spot diagram radius as a function of control parameters and then uses the spgd optimization algorithm to improve the performance metric. the control parameters include positions of mirrors. the iteration process must be used in the right way to control position parameters. if it is not considered, a problem may come up that positions of spot diagrams may be influenced by the iteration. furthermore, spot diagrams will probably disappear from detectors. then the radii of spot diagrams are not correct. so a better way is put forward by the combination of de-center and tilt of mirrors. the way ensures that the position error produced by de-center and tilt are compensated for. a formula is provided in this paper to give the relationship between them. based on the analysis, an optical system is designed to verify the conclusion. the spgd algorithm is achieved by computer programming and the position of the mirror is controlled by a hexapod. firstly, the problem is verified that the spot diagram will disappear from the detector with a normal iteration process. then the new way is implemented. in the iteration process, the spot diagram is always in the center of the detectors. in order to prove the feasibility of the method, three different alignment errors are tested and all of them each give an airy disk finally. the experiment can provide reference for engineering practice.
cryptography	modern cryptography increasingly employs random numbers generated from physical sources in lieu of conventional software-based pseudorandom numbers, primarily owing to the great demand of unpredictable, indecipherable cryptographic keys from true random numbers for information security. thus, far, the sole demonstration of true random numbers has been generated through thermal noise and/or quantum effects, which suffers from expensive and complex equipment. in this paper, we demonstrate a method for self-powered creation of true random numbers by using triboelectric technology to collect random signals from nature. this random number generator based on coupled triboelectric and electrostatic induction effects at the liquid dielectric interface includes an elaborately designed triboelectric generator (teng) with an irregular grating structure, an electronic optical device, and an optical electronic device. the random characteristics of raindrops are harvested through teng and consequently transformed and converted by electronic optical device and an optical electronic device with a nonlinear characteristic. the cooperation of the mechanical, electrical, and optical signals ensures that the generator possesses complex nonlinear input output behavior and contributes to increased randomness. the random number sequences are deduced from final electrical signals received by an optical electronic device using a familiar algorithm. these obtained random number sequences exhibit good statistical characteristics, unpredictability, and unrepeatability. our study supplies a simple, practical, and effective method to generate true random numbers, which can be widely used in cryptographic protocols, digital signatures, authentication, identification, and other information security fields.
image_processing	high spatial resolution images as well as image processing and object detection algorithms are recent technologies that aid the study of biodiversity and commercial plantations of forest species. this paper seeks to contribute knowledge regarding the use of these technologies by studying randomly dispersed native palm tree. here, we analyze the automatic detection of large circular crown (lcc) palm tree using a high spatial resolution panchromatic geoeye image (0.50 m) taken on the area of a community of small agricultural farms in the brazilian amazon. we also propose auxiliary methods to estimate the density of the lcc palm tree attalea speciosa (babassu) based on the detection results. we used the ""compt-palm"" algorithm based on the detection of palm tree shadows in open areas via mathematical morphology techniques and the spatial information was validated using field methods (i.e. structural census and georeferencing). the algorithm recognized individuals in life stages 5 and 6, and the extraction percentage, branching factor and quality percentage factors were used to evaluate its performance. a principal components analysis showed that the structure of the studied species differs from other species. approximately 96% of the babassu individuals in stage 6 were detected. these individuals had significantly smaller stipes than the undetected ones. in turn, 60% of the stage 5 babassu individuals were detected, showing significantly a different total height and a different number of leaves from the undetected ones. our calculations regarding resource availability indicate that 6870 ha contained 25,015 adult babassu palm tree, with an annual potential productivity of 27.4 t of almond oil. the detection of lcc palm tree and the implementation of auxiliary field methods to estimate babassu density is an important first step to monitor this industry resource that is extremely important to the brazilian economy and thousands of families over a large scale. (c) 2017 elsevier ltd. all rights reserved.
symbolic_computation	this research deals with a novel approach to classification. new classifiers are synthesized as a complex structure via evolutionary symbolic computation techniques. compared to previous research, this paper synthesizes multi-input-multi-output (mimo) classifiers with different cost function based on distance measurements. an inspiration for this work came from the field of artificial neural networks (ann). the proposed technique creates a relation between inputs and outputs as a whole structure together with numerical values which could be observed as weights in ann. distances used in cost functions were: manhattan (absolute distances of output vectors), euclidean, chebyshev (maximum distance value), canberra distance, bray - curtis. the analytic programming (ap) was utilized as the tool of synthesis by means of the evolutionary symbolic regression. for experimentation, differential evolution for the main procedure and also for meta-evolution version of analytic programming was used iris data (a known benchmark for classifiers) was used for testing of the proposed method.
image_processing	in the present research, we examined the effect of the starting and turning performances on the subsequent swimming parameters by (1) comparing the starting and turning velocities with the swimming parameters on the emersion and mid-pool segments and (2) by relating the individual behaviour of swimmers during the start and turns with subsequent behaviour on each swimming lap. one hundred and twelve 100m performances on the fina 2013 world swimming championships were analysed by an image-processing system (inthepool 2.0 (r)). at the point of the start emersion, the swimming parameters of the 100-m elite swimmers were substantially greater than the mid-pool parameters, except on the breaststroke races. on the other hand, no diminution in the swimming parameters was observed between the turn emersion and the mid-pool swimming, except on the butterfly and backstroke male races. changes on the surface swimming kinematics were not generally related to the starting or turning parameters, although male swimmers who develop faster starts seem to achieve faster velocities at emersion. race analysts should be aware of a transfer of momentum when swimmers emerge from underwater with implications on the subsequent swimming kinematics, especially for male swimmers who employ underwater undulatory techniques.
algorithm_design	it 's essential to establish a dynamic model of a wheeled tractor with suspended driver seat including air spring and magnetorheological (mr) damper to analyze its vibration characteristics and performance. the equivalent linearized model of the wheeled tractor always neglects the effect of nonlinear stiffness for scissors linkage driver seat and air spring with auxiliary chamber as well as dynamic characteristics of real tractor tyre and considers the driver seat as linear spring and damper elements, which causes the analysis to have a lower precision. we considered the effect of nonlinear stiffness for scissors linkage seat and air spring with auxiliary chamber as well as mr damper and dynamic characteristics of real tyre and developed a complete nonlinear dynamic model of wheeled tractor with suspended driver seat including air spring and mr damper. the experimental results demonstrate that the improved nonlinear dynamic model is more consistent with the actual state than the equivalent linearized model, and the validity of the proposed model is verified. furthermore, the effect of forward speed, ratio of volume for air spring with respect to volume of auxiliary chamber, and area of orifice on the acceleration of wheeled tractor was also investigated, which can provide a theoretical basis for control algorithm design of semiactive vibration isolation system.
algorithm_design	this paper presents a robust, distributed algorithm to solve general linear programs. the algorithm design builds on the characterization of the solutions of the linear program as saddle points of a modified lagrangian function. we show that the resulting continuous-time saddle-point algorithm is provably correct but, in general, not distributed because of a global parameter associated with the nonsmooth exact penalty function employed to encode the inequality constraints of the linear program. this motivates the design of a discontinuous saddle-point dynamics that, while enjoying the same convergence guarantees, is fully distributed and scalable with the dimension of the solution vector. we also characterize the robustness against disturbances and link failures of the proposed dynamics. specifically, we show that it is integral-input-to-state stable but not input-to-state stable. the latter fact is a consequence of a more general result, that we also establish, which states that no algorithmic solution for linear programming is input-to-state stable when uncertainty in the problem data affects the dynamics as a disturbance. our results allow us to establish the resilience of the proposed distributed dynamics to disturbances of finite variation and recurrently disconnected communication among the agents. simulations in an optimal control application illustrate the results.
machine_learning	objective: the goal of this study was to develop a practical framework for recognizing and disambiguating clinical abbreviations, thereby improving current clinical natural language processing (nlp) systems' capability to handle abbreviations in clinical narratives. methods: we developed an open-source framework for clinical abbreviation recognition and disambiguation (card) that leverages our previously developed methods, including: (1) machine learning based approaches to recognize abbreviations from a clinical corpus, (2) clustering-based semiautomated methods to generate possible senses of abbreviations, and (3) profile-based word sense disambiguation methods for clinical abbreviations. we applied card to clinical corpora from vanderbilt university medical center (vumc) and generated 2 comprehensive sense inventories for abbreviations in discharge summaries and clinic visit notes. furthermore, we developed a wrapper that integrates card with metamap, a widely used general clinical nlp system. results and conclusion: card detected 27 317 and 107 303 distinct abbreviations from discharge summaries and clinic visit notes, respectively. two sense inventories were constructed for the 1000 most frequent abbreviations in these 2 corpora. using the sense inventories created from discharge summaries, card achieved an f1 score of 0.755 for identifying and disambiguating all abbreviations in a corpus from the vumc discharge summaries, which is superior to metamap and apache 's clinical text analysis knowledge extraction system (ctakes). using additional external corpora, we also demonstrated that the metamap-card wrapper improved metamap 's performance in recognizing disorder entities in clinical notes. the card framework, 2 sense inventories, and the wrapper for metamap are publicly available at https://sbmi.uth.edu/ccb/resources/abbreviation. htm. we believe the card framework can be a valuable resource for improving abbreviation identification in clinical nlp systems.
network_security	with the development of computer network technology, more closely the relationship between people and network. the current network security problem has also been gradually into the public 's field of vision, actively carried out on the network intrusion detection becomes an important direction of the development of the network security technology. on the basis of the original bp neural network, this paper puts forward an improved algorithm, and applied to network intrusion detection. after the test, the method is better than traditional convergence, better performance.
machine_learning	avian scavengers are declining throughout the world, and are affected by a large number of threats such as poisoning, electrocution, collision with man-made structures, direct persecution, changes in agricultural practices, landscape composition, and sanitary regulations that can reduce food availability. to formulate effective conservation strategies, it is important to quantify which of these factors has the greatest influence on demographic parameters such as territory occupancy and breeding success, and whether quantitative models can be transferred across geographic regions and political boundaries. we collated territory and nest monitoring data of the endangered egyptian vulture neophron percnopterus in the balkans to understand the relative influence of various factors on population declines. we monitored occupancy in 87 different territories and breeding performance of 405 territory-monitoring years between 2003 and 2015, with an overall territory occupancy rate of 69% and a mean productivity of 0.80 fledglings per occupied territory. we examined which of 48 different environmental variables were most influential in explaining variation in territory occupancy and breeding success in bulgaria and greece, and tested whether these models were transferrable to the former yugoslav republic of macedonia. territory occupancy and breeding success were affected by a wide range of environmental variables, each of which had a small effect that may not be the same across political boundaries. both models had reasonably good discriminative ability [area under the receiver-operated characteristic curve (auc) for territory occupancy = 0.871, auc for breeding success = 0.744], but were unsuccessful in predicting occupancy or breeding success in the external validation data set from a different country, possibly because the most influential factors vary geographically. management focussing on a small number of environmental variables is unlikely to be effective in slowing the decline of egyptian vultures on the balkan peninsula. we recommend that in the short term the reduction of adult mortality through the enforcement of anti-poison laws, and in the long term the adoption of large-scale landscape conservation programs that retain or restore historical small-scale farming practices may benefit vultures and other biodiversity.
bioinformatics	micrornas (mirna) have been implicated in a variety of pathological conditions including infectious diseases. knowledge of the mirnas affected by poly(i: c), a synthetic analog of viral double-stranded rna, in porcine airway epithelial cells (paecs) contributes to understanding the mechanisms of swine viral respiratory diseases, which bring enormous economic loss worldwide every year. in this study, we used high throughput sequencing to profile mirna expression in paecs treated with poly(i: c) as compared to the untreated control. this approach revealed 23 differentially expressed mirnas (dems),five of which have not been implicated in viral infection before. nineteen of the 23 mirnas were downregulated including members of the mir-17-92 cluster, a well-known polycistronic oncomir and extensively involved in viral infection in humans. target genes of dems, predicted using bioinformatic methods and validated by luciferase reporter analysis on two representative dems, were significantly enriched in several pathways including transforming growth factor-b signaling. a large quantity of sequence variations (isomirs) were found including a substitution at position 5, which was verified to redirect mirnas to a new spectrum of targets by luciferase reporter assay together with bioinformatics analysis. twelve novel porcine mirnas conserved in other species were identified by homology analysis together with cloning verification. furthermore, the expression analysis revealed the potential importance of three novel mirnas in porcine immune response to viruses. overall, our data contribute to clarifying the mechanisms underlying the host immune response against respiratory viruses in pigs, and enriches the repertoire of porcine mirnas.
cryptography	wireless body area network (wban) provide a mechanism of transmitting a persons physiological data to application providers e.g. hospital. given the limited range of connectivity associated with wban, an intermediate portable device e.g. smartphone, placed within wban 's connectivity, forwards the data to a remote server. this data, if not protected from an unauthorized access and modification may be lead to poor diagnosis. in order to ensure security and privacy between wban and a server at the application provider, several authentication schemes have been proposed. recently, wang and zhang proposed an authentication scheme for wban using bilinear pairing. however, in their scheme, an application provider could easily impersonate a client. in order to overcome this weakness, we propose an efficient remote authentication scheme for wban. in terms of performance, our scheme can not only provide a malicious insider security, but also reduce running time of wban (client) by 51 % as compared to wang and zhang scheme.
symbolic_computation	in this paper, the stability and local bifurcation for the rotating blade under high-temperature supersonic gas flow are investigated using analytical and numerical methods. based on obtained four-dimensional averaged equation for the case of 1:1 internal resonance and primary resonance, two types of critical points for the bifurcation response equations are considered. the points are characterized by a double zero and two negative eigenvalues and two pairs of purely imaginary eigenvalues, respectively. for each type,the steady state solutions and the stability region is obtained with the aid of center manifold theory and normal form theory. we find the hopf bifurcation solution which indicates the blade will flutter. in summary, the numerical solutions, whose initial conditions are chosen in the stability region, agree with the analytic results. (c) 2015 elsevier inc. all rights reserved.
algorithm_design	digital tomosynthesis is a three-dimensional imaging technique with a lower radiation dose than computed tomography (ct). due to the missing data in tomosynthesis systems, out-ofplane structures in the depth direction cannot be completely removed by the reconstruction algorithms. in this work, we analyzed the impulse responses of common tomosynthesis systems on a plane-to-plane basis and proposed a fast and accurate convolution-based blur-and-add (baa) model to simulate the backprojected images. in addition, the analysis formalism describing the impulse response of out-of-plane structures can be generalized to both rotating and parallel gantries. we implemented a ray tracing forward projection and backprojection (ray-based model) algorithm and the convolution-based baa model to simulate the shift-and-add (backproject) tomosynthesis reconstructions. the convolution-based baa model with proper geometry distortion correction provides reasonably accurate estimates of the tomosynthesis reconstruction. a numerical comparison indicates that the simulated images using the two models differ by less than 6% in terms of the root-mean-squared error. this convolution-based baa model can be used in efficient system geometry analysis, reconstruction algorithm design, out-of-plane artifacts suppression, and ct-tomosynthesis registration.
relational_databases	in this digital world every one seeks for the information on the internet. as the time passed every one placed their digital content on the web. on the web the website developers support to place the data in the relational databases. the databases from different organizations and agencies are available online and are accessible through web query interfaces. the web query interfaces act as the front door to access the information from these relational databases. but these web forms or web query interfaces need human intervention to submit the form data. so in order to make search engines capable of accessing the relational databases i.e. data behind query interfaces, the query generation should be done automatically. so in this paper we discuss the different form submission techniques to fetch the data from the databases and propose a novel technique ""dynamic query processing for hidden web data extraction"".
network_security	with the development of trusted network, the research of trusted evaluation mechanism of user behavior is a hotpot in the network security. in order to solve the problems of subjectivity, limitations and static in traditional trusted network user behavior evaluation models, we have to find a real-time and dynamic evaluation method for user behavior. in this paper, the authors construct a real-time evaluation mechanism based on double evidence classification of user behavior (dec-ub). the evaluation mechanism includes the process classification and characteristic classification of user behavior evidence, which makes the user behavior evidence of any time can be directly involved in the trust evaluation, and the evaluation result is more comprehensive and accurate. simulation experiments have evaluated the three kinds of user behaviors based on the dec-ub, and compared them with the other two kinds of trust evaluation methods of user behavior, the results show that the proposed methods can evaluate the user 's behavior comprehensively, accurately and dynamically in complex network environments, and the results are more realistic.
computer_vision	establishing correspondences is a fundamental task in many image processing and computer vision applications. in particular, finding the correspondences between a non-linearly deformed image pair induced by different modality conditions is a challenging problem. this paper describes a simple but powerful image transform called local area transform (lat) for modality-robust correspondence estimation. specifically, tat transforms an image from the intensity domain to the local area domain, which is invariant under nonlinear intensity deformations, especially radiometric, photometric, and spectral deformations. experimental results show that latransformed images provide a consistency for nonlinearly deformed images, even under random intensity deformations. lat reduces the mean absolute difference by approximately 0.20 and the different pixel ratio by approximately 58% on average, as compared to conventional methods. furthermore, the reformulation of descriptors with lat shows superiority to conventional methods, which is a promising result for the tasks of cross-spectral and modality correspondence matching. lat gains an approximately 23% improvement in the correct detection ratio and a 10% improvement in the recognition rate for the tasks of rgb-nir cross-spectral template matching and cross-spectral feature matching, respectively. lat reduces the bad pixel percentage by approximately 15% and the root mean squared errors by 13.5 in the task of cross-radiation stereo matching. lat also improves the cross-modal dense flow estimation task in terms of warping error, providing 50% error reduction.
computer_graphics	this paper presents two within-subjects studies (n=23) exploring how different combinations of visual and auditory feedback influence perceived realism, virtual self-perception and the experience of safety during walks on a virtual platform suspended over a canyon. in the first study, the frequency factor of the footstep sounds was altered and the visual appearance was changed between a newly built wooden bridge and an old bridge with a weaker structure and broken planks. in the second study, the sounds of creaking wood were added to the footstep sounds in half of the trails and compared against footsteps without creaking sounds. moreover, the frequency factor of the frequency controls for footsteps was also manipulated between trails, but the visual appearance of the bridge was limited to the model of the old broken bridge.
parallel_computing	mining with big data or big data mining has become an active research area. it is very difficult using current methodologies and data mining software tools for a single personal computer to efficiently deal with very large datasets. the parallel and cloud computing platforms are considered a better solution for big data mining. the concept of parallel computing is based on dividing a large problem into smaller ones and each of them is carried out by one single processor individually. in addition, these processes are performed concurrently in a distributed and parallel manner. there are two common methodologies used to tackle the big data problem. the first one is the distributed procedure based on the data parallelism paradigm, where a given big dataset can be manually divided into n subsets, and n algorithms are respectively executed for the corresponding n subsets. the final result can be obtained from a combination of the outputs produced by the n algorithms. the second one is the mapreduce based procedure under the cloud computing platform. this procedure is composed of the map and reduce processes, in which the former performs filtering and sorting and the later performs a summary operation in order to produce the final result. in this paper, we aim to compare the performance differences between the distributed and mapreduce methodologies over large scale datasets in terms of mining accuracy and efficiency. the experiments are based on four large scale datasets, which are used for the data classification problems. the results show that the classification performances of the mapreduce based procedure are very stable no matter how many computer nodes are used, better than the baseline single machine and distributed procedures except for the class imbalance dataset. in addition, the mapreduce procedure requires the least computational cost to process these big datasets. (c) 2016 elsevier inc. all rights reserved.
algorithm_design	in this paper, a new approach has been proposed to design any arbitrary e-plane filters, including band-pass, and band-stop filters, with a desired frequency response. the proposed method is based on replacing all conventional resonators with a new form of a resonator, which is made of a patterned plane. the patterned plane is a metal plane with infinitesimal thickness that some of its parts, are removed. the introduced patterned plane is supported by a thin and low permittivity dielectric slab, and is located longitudinally in the middle of a rectangular waveguide, parallel to the e-plane. to design the proposed filters, the scattering parameters of the structure should be calculated. for this purpose, a coupled set of electric field integral equations have been derived, and solved by method of moments. then, a suitable cost function was defined, and optimized using genetic algorithm. matlab ga tool has been used to optimize the cost function. the proposed method facilitates and accelerates the optimization process in comparison to full wave simulator software. as examples, both band-pass, and band-stop e-plane filters have been designed. the results show that the proposed filter, in comparison to the conventional filters, has some advantages, such as frequency band selectivity, compactness, and the ability of adjustment to any desired frequency response. the results of designed structures are validated by hfss simulator software.
bioinformatics	a previous bioinformatics analysis identified the mycobacterium tuberculosis proteins rv2125 and rv2714 as orthologs of the eukaryotic proteasome assembly chaperone 2 (pac2). we set out to investigate whether rv2125 or rv2714 can function in proteasome assembly. we solved the crystal structure of rv2125 at a resolution of 3.0 angstrom, which showed an overall fold similar to that of the pac2 family proteins that include the archaeal pbab and the yeast pba1. however, rv2125 and rv2714 formed trimers, whereas pbab forms tetramers and pba1 dimerizes with pba2. we also found that purified rv2125 and rv2714 could not bind to m. tuberculosis 20s core particles. finally, proteomic analysis showed that the levels of known proteasome components and substrate proteins were not affected by disruption of rv2125 in m. tuberculosis. our work suggests that rv2125 does not participate in bacterial proteasome assembly or function. importance although many bacteria do not encode proteasomes, m. tuberculosis not only uses proteasomes but also has evolved a posttranslational modification system called pupylation to deliver proteins to the proteasome. proteasomes are essential for m. tuberculosis to cause lethal infections in animals; thus, determining how proteasomes are assembled may help identify new ways to combat tuberculosis. we solved the structure of a predicted proteasome assembly factor, rv2125, and isolated a genetic rv2125 mutant of m. tuberculosis. our structural, biochemical, and genetic studies indicate that rv2125 and rv2714 do not function as proteasome assembly chaperones and are unlikely to have roles in proteasome biology in mycobacteria.
computer_vision	the core aspect of this work is a review for player detection and tracing. coaches and players prepare widely by studying the opponents attacking and self-protecting formations, plays and metrics before every game. moving object detection is one of the serious problems in the field of surveillance, traffic monitoring, computer vision, player tracking in sports video and so forth. detecting the objects in the video and tracking its movement to recognise its qualities have been a demanding examination zone in the area of computer image and image processing. the core aspect of this work is a review for player detection and tracking. as coaches and players prepare widely by studying the opponents offensive and defensive formations, plays and metrics before every game, tracking of player becomes a major problem because of different problems in appearance, occlusion and so on. this work offers an analysis on detection of objects, segmentation of objects and tracking of objects. it also provides the study of comparison of diverse methods used for player tracking.
computer_vision	switched systems theory is used to analyze the stability of image-based observers for three-dimensional localization of objects in a scene in the presence of intermittent measurements due to occlusions, feature tracking losses, or a limited camera field of view, for example. generally, observers or filters that are exponentially stable under persistent measurement availability may have unbounded error growth under intermittent measurement loss, even while providing seemingly accurate state estimates. by constructing a framework that utilizes a state predictor during periods when measurements are not available, a class of image-based observers is shown to be exponentially convergent in the presence of intermittent measurements if an average dwell time, and a total unmeasurability time, condition is satisfied. the conditions are developed in a general form, applicable to any observer that is exponentially convergent assuming persistent visibility, and utilizes object motion knowledge to reduce the amount of time measurements must be available to maintain convergence guarantees. based on the stability results, simulations are provided to show improved performance compared to a zero-order hold approach, where state estimates are held constant when measurements are not available. experimental results are also included to verify the theoretical results, to demonstrate applicability of the developed observer and predictor design, and to compare against a typical approach using an extended kalman filter.
operating_systems	uncertainty quantification (uq) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. it is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. this paper describes a uq platform called uq-pyl (uncertainty quantification python laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. uq-pyl integrates different kinds of uq methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. it is written in python language and runs on all common operating systems. uq-pyl has a graphical user interface that allows users to enter commands via pull-down menus. it is equipped with a model driver generator that allows any computer model to be linked with the software. we illustrate the different functions of uq-pyl by applying it to the uncertainty analysis of the sacramento soil moisture accounting model. we will also demonstrate that uq-pyl can be applied to a wide range of applications. (c) 2015 the authors. published by elsevier ltd.
machine_learning	the problem of stereoscopic image quality assessment, which finds applications in 3d visual content delivery such as 3dtv, is investigated in this work. specifically, we propose a new paraboost (parallel boosting) stereoscopic image quality assessment (pbsiqa) system. the system consists of two stages. in the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. these scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. in the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. extensive experimental results are conducted to compare the performance of the proposed pbsiqa system with those of existing stereo image quality assessment (siqa) metrics. the developed quality metric can serve as an objective function to optimize the performance of a 3d content delivery system. (c) 2017 elsevier inc. all rights reserved.
software_engineering	along with the progress in computer hardware architecture and computational power, in order to overcome technological bottlenecks, software applications that make use of expert and intelligent systems must race against time where nanoseconds matter in the long-awaited future. this is possible with the integration of excellent solvers to software engineering methodologies that provide optimization-based decision support for planning. since the logistics market is growing rapidly, the optimization of routing systems is of primary concern that motivates the use of vehicle routing problem (vrp) solvers as software components integrated as an optimization engine. a critical success factor of routing optimization is quality vs. response time performance. less time-consuming and more efficient automated processes can be achieved by employing stronger solution algorithms. this study aims to solve the vehicle routing problem with simultaneous pickup and delivery (vrpspd) which is a popular extension of the basic vehicle routing problem arising in real world applications where pickup and delivery operations are simultaneously taken into account to satisfy the vehicle capacity constraint with the objective of total travelled distance minimization. since the problem is known to be np-hard, a hybrid metaheuristic algorithm based on an ant colony system (acs) and a variable neighborhood search (vns) is developed for its solution. vns is a powerful optimization algorithm that provides intensive local search. however, it lacks a memory structure. this weakness can be minimized by utilizing long term memory structure of acs and hence the overall performance of the algorithm can be boosted. in the proposed algorithm, instead of ants, vns releases pheromones on the edges while ants provide a perturbation mechanism for the integrated algorithm using the pheromone information in order to explore search space further and jump from local optima. the performance of the proposed acs empowered vns algorithm is studied on well-known benchmarks test problems taken from the open literature of vrpspd for comparison purposes. numerical results confirm that the developed approach is robust and very efficient in terms of both solution quality and cpu time since better results provided in a shorter time on benchmark data sets is a good performance indicator. (c) 2016 elsevier ltd. all rights reserved.
operating_systems	these days the number of processors in computer systems is increasing in a very fast speed, and different manufacturers are producing computers with manycore processors. in turn, these manycore computer systems require a new operating system that can solve the problems of commodity operating systems. commodity operating systems originally made for single, dual or multi-cores systems; they will have lower performance and scalability problem for higher number of cores that are hundreds or thousands in number. in this regard, scholars of different research institutions are trying to develop their own solutions for existing problems among commodity operating systems and the recently emerging the manycore processor. this paper intends to see what those manycore operating systems are. the main objective of this paper is to compare and contrast some of manycore operating systems, analyze their experimental results, and give some summarizing concepts. manycore operating systems are developing from either the scratch or the amendment on the existing commodity operating systems. operating systems projects that develop a manycore operating system from the scratch, need lots of effort, and generally new approaches are applied. they use completely new technical approaches for development of oses for increased number of cores. operating systems like fos and barrelfish are some examples of operating systems that developed from the scratch. on the other hand, some operating system projects focus on commodity oses problems for increased number of cores. these projects amend commodity operating systems and/or used them collectively. therefore, their main task is preparing systems that play an important role for this amendment. as compared with the above operating system development approach, these approaches are simple and more manycore oses used it. operating systems like fusedos, cerberus and others are such a type of operating systems. in this respect, this paper will see the reason behind the selection of either of the two approaches. also will provide highlights about the basic structural differences among the existing manycore operating systems, in order to catch the philosophy behind them.
operating_systems	this paper proposes a framework for the development of sensor node software for various operating systems in a sensor network environment. the proposed development framework consists of attributes, code templates and development support tool. sensor node software is developed, based on the framework through four steps - sensor network modeling, pim design, psm design and code generation. accordingly, this paper presents the methods for attributes design, code templates design, pim-to-psm mapping, and source code generation. through the proposed technique, reusability of sensor network software will be increased since models, attributes and code templates can be reused for various operating systems through the framework. productivity of software development will be increased, because software design is easily performed using attributes and software codes for all nodes in the sensor network can be generated at once from a model. also, expandability of sensor network software will be increased, since new functions of existing operating systems or new operating systems can be added through the framework and sensor network software can be rebuilt by applying the added functions or operating systems.
software_engineering	among many factors that influence the success of a software project, the software process model employed is an essential one. an improper process model will be time consuming, error-prone and cost expensive, and further lower the quality of software. therefore, how to choose an appropriate software process model is a very important problem for software development. current works focus on the selection criteria and often lead to subjective results. in this paper, we propose a software process model recommendation method, to help project managers choose the most appropriate software process model for a new project at an early stage of development process according to historical software engineering data. the proposed method casts the process model recommendation into a classification problem. it first evaluates the different combinations of the alternative classification and attribute selection algorithms, and the best one is used to build the recommendation model with historical software engineering data; then, the constructed recommendation model is used to predict process models for a new software project with only a few data. we also analyze the mutual impacts between process models and different types of project factors, to further help managers locate the most suitable process model. we found process models are also responsible for defect count, defect severity and software change. experiments on the data sets from 37 different development teams of different countries show that the average recommendation accuracy of our method reaches up to 82.5%, which makes it potentially useful in practice. (c) 2016 elsevier inc. all rights reserved.
image_processing	in this article, we present literature reviews on fire prevention methods, especially in mining industries, using thermal image processing techniques. fire protection systems are crucial because of the increased loss of human lives due to coal fires and fatal explosions in coal mines across the world in the past few decades. and with the growth in the demand for energy and the mining of coal expected up to the year 2050, determining conditions leading up to a breakout of fire is paramount. to detect uncertain fire breakout conditions, thermal imaging is considered the most significant among several early warning methods to recognize spontaneous combustion of coal piles (e.g., temperature recordings by sensors, compaction testing of ore seam, gas tests). the evolution of thermographic imaging applied in various industrial sectors (e.g., coal furnaces, oil tankers, building inspections, security) with numerous applications of mathematical models will be presented in the light of safety dimensions in the mining industry. the missing links or unattended areas of mathematics in the application of thermal image processing in mining, especially in the coal industry, will be evolved as the gap in knowledge suggested in our concluding statements.
relational_databases	the large-scale aggregation and analysis of user opinions is becoming increasingly relevant to a variety of applications, from detecting social mood on some political topics to tracking their sentiment changes related to events. the analysis of diverse sentiments is another important application, which becomes possible based on the ability of modern methods to capture sentiment polarity on various topics with high precision and on the ever-growing scale. therefore, there is a need for a scalable way of sentiment aggregation with respect to the time dimension, which stores enough information to preserve diversity, and which allows statistically accurate analysis of sentiment trends and opinion shifts. in this paper, we are focusing on the novel problem of aggregating diverse sentiments at a large scale, based on data sources that are continuously updated. first, we develop a theoretical framework that models sentiment diversity (contradiction) and defines two types of contradictions, depending on the distribution of sentiments over time. second, we introduce novel measures that capture sentiment diversity from aggregated sentiment statistics. third, we develop robust and scalable indexing and storage methods for diverse sentiments. finally, we propose an adaptive approach for identifying contradictions at different time scales. the experimental evaluation demonstrates the effectiveness of the proposed method of capturing contradictions and its superiority over relational databases in real-world scenarios.
software_engineering	software product line (spl) development is a new approach to software engineering which aims at the development of a whole range of products. however, as long as spl can be useful, there are many challenges regarding the use of that approach. one of the main problems which hinders the adoption of software product line (spl) is the complexity regarding product management. in that context, we can remark the scoping problem. one of the existent ways to deal with scoping is the product portfolio scoping (pps). pps aims to define the products that should be developed as well as their key features. in general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. defining a product portfolio by using the many different available aspects is a np-hard problem. this work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. the proposal is based in a hybrid approach not dependent on any particular algorithm/technology. we have evaluated the usefulness and scalability of our approach using one real spl (argouml-spl) and synthetic spls. as per the evaluation results, our approach is both useful from a practitioner 's perspective and scalable. (c) 2016 elsevier b.v. all rights reserved.
computer_programming	sparse matrix-vector multiplication (spmvm) is the most time-consuming kernel in many numerical algorithms and has been studied extensively on all modern processor and accelerator architectures. however, the optimal sparse matrix data storage format is highly hardware-specific, which could become an obstacle when using heterogeneous systems. also, it is as yet unclear how the wide single instruction multiple data (simd) units in current multi-and many-core processors should be used most efficiently if there is no structure in the sparsity pattern of the matrix. we suggest sellc-sigma, a variant of sliced ellpack, as a simd-friendly data format which combines long-standing ideas from general-purpose graphics processing units and vector computer programming. we discuss the advantages of sell-c-sigma compared to established formats like compressed row storage and ellpack and show its suitability on a variety of hardware platforms (intel sandy bridge, intel xeon phi, and nvidia tesla k20) for a wide range of test matrices from different application areas. using appropriate performance models we develop deep insight into the data transfer properties of the sell-c-sigma spmvm kernel. sell-c-sigma comes with two tuning parameters whose performance impact across the range of test matrices is studied and for which reasonable choices are proposed. this leads to a hardware-independent (""catch-all"") sparse matrix format, which achieves very high efficiency for all test matrices across all hardware platforms.
symbolic_computation	we have investigated the femtosecond soliton propagation in inhomogeneous fiber, which is described by the modified inhomogeneous hirota equation with variable coefficient (mih-vc). with the aid of akns method, corresponding lax pair is constructed. by virtue of the darboux transformation method and symbolic computation, the analytic one- and two-soliton solutions are explicitly obtained. using obtained solutions, we graphically discuss the features of femtosecond solitons in modified inhomogeneous hirota system by changing the profile of variable coefficients. we analyze various form of group velocity dispersion, third order dispersion and nonlinearity parameter for periodic amplification system, exponentially distributed system, parabolic solitons, periodic exponentially modulated system, which will be observable in the future experiments. these results are potentially useful in future experiments and soliton control for long-distance optical communication. finally, the soliton solutions of the mih-vc equation in double wronskian form is constructed and further verified using the wronskian technique by substitute in bilinear equations.
network_security	complex traffic networks include a number of controlled intersections, and, commonly, multiple districts or municipalities. the result is that the overall traffic control problem is extremely complex computationally. moreover, given that different municipalities may have distinct, non-aligned, interests, traffic light controller design is inherently decentralized, a consideration that is almost entirely absent from related literature. both complexity and decentralization have great bearing both on the quality of the traffic network overall, as well as on its security. we consider both of these issues in a dynamic traffic network. first, we propose an effective local search algorithm to efficiently design system-wide control logic for a collection of intersections. second, we propose a game theoretic (stackelberg game) model of traffic network security in which an attacker can deploy denial-of-service attacks on sensors, and develop a resilient control algorithm to mitigate such threats. finally, we propose a game theoretic model of decentralization, and investigate this model both in the context of baseline traffic network design, as well as resilient design accounting for attacks. our methods are implemented and evaluated using a simple traffic network scenario in sumo.
cryptography	field inversion in dominates the cost of modern software implementations of certain elliptic curve cryptographic operations, such as point encoding/hashing into elliptic curves (brown et al. in: submission to nist, 2008; brown in: iacr cryptology eprint archive 2008:12, 2008; aranha et al. in: cryptology eprint archive, report 2014/486, 2014) itoh-tsujii inversion using a polynomial basis and precomputed table-based multi-squaring has been demonstrated to be highly effective for software implementations (taverne et al. in: ches 2011, 2011; oliveira et al. in: j cryptogr eng 4(1):3-17, 2014; aranha et al. in: cryptology eprint archive, report 2014/486, 2014), but the performance and memory use depend critically on the choice of addition chain and multi-squaring tables, which in prior work have been determined only by suboptimal ad-hoc methods and manual selection. we thoroughly investigated the performance/memory tradeoff for table-based linear transforms used for efficient multi-squaring. based upon the results of that investigation, we devised a comprehensive cost model for itoh-tsujii inversion and a corresponding optimization procedure that is empirically fast and provably finds globally-optimal solutions. we tested this method on eight binary fields commonly used for elliptic curve cryptography; our method found lower-cost solutions than the ad-hoc methods used previously, and for the first time enables a principled exploration of the time/memory tradeoff of inversion implementations.
data_structures	full-spectrum dependent types promise to enable the development of correct-by-construction software. however, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simplytyped values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. in this paper, we give a semantic account of dependent interoperability. our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. our framework is developed in coq; it is thus constructive and verified in the strictest sense of the terms. using our library, users can specify domain-specific partial equivalences between data structures. our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. it thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable ocaml programs within coq itself.
bioinformatics	strawberry is one of the most economically important fruit crops in the world. cytokinins (cks) play a critical role in plant growth and development, as well as the stress response, and the level of cks in plants is regulated by synthesis and degradation pathways. the key synthetic enzymes of cks are isopentenyl transferases (ipts) and lonely guys (logs). we surveyed the strawberry genome and identified seven fvipt genes and nine fvlog genes. we analyzed gene structures, conserved domains, and their phylogenetic relationships with rice and arabidopsis. the isoelectric points and glycosylation sites of the proteins were predicted. we also analyzed tissue- or organ-specific expression patterns of the fvipt and fvlog genes. the fvipt and fvlog genes showed different expression profiles in different organs. most fvipt and fvlog genes were down-regulated in response to osmotic stress, high-temperature treatment, and exogenous abscisic acid (aba) application, suggesting possible roles of these genes in the plants' resistance to abiotic stresses. in addition, we found that the results of bioinformatics analyses to identify cis-regulatory elements may not be consistent with experimental expression data; thus, computer-predicted putative cis-elements need to be confirmed by experiments. our systematic analyses of the fvipt and fvlog families provide a foundation for characterizing the function of these genes in the regulation of growth, development, and stress tolerance in fragaria vesca, as well as a reference for improving stress tolerance by manipulating ck content.
relational_databases	the eminent web-applications of today are data-intensive. the data generated is of the order of petabytes and zetabytes. using relational databases for storing them only complicates the storage and retrieval in the db and degradation of its performance. the big data explosion demanded the need for a more flexible, high-performance storage concept the nosql movement. the nosql databases were designed to overcome the flaws of the relational databases including the security aspects. the effective performance and efficient storage criteria were satisfied by the non-relational databases. the attackers, as usual found their way into the nosql databases that were considered to be secure. the injection attacks, one of the top-listed attack type of the relational databases poses threat to the non-relational databases as well. mongodb is one of the prominent nosql databases to which the application development trends are shifting. in this paper, we present the different injection attacks on the leading nosql database and an automata based detection and prevention technique for this attack. we also evaluate the effectiveness on different subjects with a number of legitimate as well as illegitimate inputs. our results show that our approach was able to detect all the attacks.
bioinformatics	the multidrug and toxin extrusion (mate) transporter family comprises 70 members in the medicago truncatula genome, and they play seemingly important, yet mostly uncharacterized, physiological functions. here, we employed bioinformatics and molecular genetics to identify and characterize mate transporters involved in citric acid export, al3+ tolerance and fe translocation. mtmate69 is a citric acid transporter induced by fe-deficiency. overexpression of mtmate69 in hairy roots altered fe homeostasis and hormone levels under fe-deficient or fe-oversupplied conditions. mtmate66 is a plasma membrane citric acid transporter primarily expressed in root epidermal cells. the mtmate66 mutant had less root growth than the wild type under al3+ stress, and seedlings were chlorotic under fe-deficient conditions. overexpression of mtmate66 rendered hairy roots more tolerant to al3+ toxicity. mtmate55 is involved in seedling development and iron homeostasis, as well as hormone signaling. the mtmate55 mutant had delayed development and chlorotic leaves in mature plants. both knock-out and overexpression mutants of mtmate55 showed altered fe accumulation and abnormal hormone levels compared with the wild type. we demonstrate that the zinc-finger transcription factor mtstop is essentially required for mtmate66 expression and plant resistance to h+ and al3+ toxicity. the proper expression of two previously characterized mate flavonoid transporters mtmate1 and mtmate2 also depends on several transcription factors. this study reveals not only functional diversity of mate transporters and regulatory mechanisms in legumes against h+ and al3+ stresses, but also casts light on their role in metal nutrition and hormone signaling under various stresses. significance statement as there are many mate transporters in legumes, they potentially have diversified functions. here we analyzed mutants and overexpression lines for two mate transporters involved in citric acid export and aluminum tolerance and iron translocation, and one mate transporter affecting growth and development and iron homeostasis. manipulating the expression of these mate transporters might prove useful in improving tolerance to acidic soils and aluminum stress.
distributed_computing	by the reason of the variability of light and pedestrians' appearance, it is hard for a camera to obtain a clear human figure. person re-identification with different cameras is a difficult visual recognition task. in this paper, a novel approach called attribute learning based on distributed deep convolutional neural network model is proposed to address person re-identification task. it shows how attributes, namely the mid-level medium between classes and features, are obtained automatically and how they are employed to re-identify person with semantics when an author-topic model is used to mapping category. besides, considering the ability to operate on raw pixel input without the need to design special features, deep convolutional neural network is employed to generate features without supervision for attributes learning model. to overcome the model 's weakness in computing speed, parallelized implementations such as distributed parameter manipulation and attributes learning are employed in attribute learning based on distributed deep convolutional neural network model. experiments show that the proposed approach achieves state-of-the-art recognition performance in the viper data set and is with a good semantic explanation. copyright (c) 2016 john wiley & sons, ltd.
cryptography	first-order and high-order correlation-power-analysis attacks have been shown to be a severe threat to cryptographic devices. as such, they serve as a security measure for evaluation and comparison of security-oriented implementations. when properly designed, data-dependent delays can be used as a barrier to these attacks. this paper introduces a security-oriented delay assignment algorithm for mitigating single and multibit attacks. the algorithm enables a reduction of the correlation between the processed data and the consumed current by utilizing the data-dependent delays as a source of correlated noise. this is done while minimizing the area overhead, propagation time, and power. we show that for the same security level this new algorithm provides x2 and x6 more area efficiency, and x1.5 and x2.25 higher frequencies than a permuted path delay assignment and random embedding of delay elements.
cryptography	revocation and key evolving paradigms are central issues in cryptography, and in pki in particular. a novel concern related to these areas was raised in the recent work of sahai, seyalioglu, and waters (crypto 2012) who noticed that revoking past keys should at times (e.g., the scenario of cloud storage) be accompanied by revocation of past ciphertexts (to prevent unread ciphertexts from being read by revoked users). they introduced revocable-storage attribute-based encryption (rs-abe) as a good access control mechanism for cloud storage. rs-abe protects against the revoked users not only the future data by supporting key-revocation but also the past data by supporting ciphertext-update, through which a ciphertext at time t can be updated to a new ciphertext at time t 1 using only the public key. motivated by this pioneering work, we ask whether it is possible to have a modular approach, which includes a primitive for time managed ciphertext update as a primitive. we call encryption which supports this primitive a ""self-updatable encryption"" (sue). we then suggest a modular cryptosystems design methodology based on three sub-components: a primary encryption scheme, a key-revocation mechanism, and a time-evolution mechanism which controls the ciphertext self-updating via an sue method, coordinated with the revocation (when needed). our goal in this is to allow the self-updating ciphertext component to take part in the design of new and improved cryptosystems and protocols in a flexible fashion. specifically, we achieve the following results: we first introduce a new cryptographic primitive called self-updatable encryption (sue), realizing a time-evolution mechanism. in sue, a ciphertext and a private key are associated with time. a user can decrypt a ciphertext if its time is earlier than that of his private key. additionally, anyone (e.g., a cloud server) can update the ciphertext to a ciphertext with a newer time. we also construct an sue scheme and prove its full security under static assumptions. following our modular approach, we present a new rs-abe scheme with shorter ciphertexts than that of sahai et al. and prove its security. the length efficiency is mainly due to our sue scheme and the underlying modularity. we apply our approach to predicate encryption (pe) supporting attribute-hiding property, and obtain a revocable storage pe (rs-pe) scheme that is selectively-secure. we further demonstrate that sue is of independent interest, by showing it can be used for timed-release encryption (and its applications), and for augmenting key-insulated encryption with forward-secure storage. (c) 2017 elsevier b.v. all rights reserved.
operating_systems	geoweb 2.0, laying the foundations of volunteered geographic information (vgi) systems, has led to platforms where users can contribute to the geographic knowledge that is open to access. moreover, as a result of the advancements in 3d visualization, virtual globes able to visualize geographic data even on browsers emerged. however the integration of vgi systems and virtual globes has not been fully realized. the study presented aims to visualize volunteered data in 3d, considering also the ease of use aspects for general public, using free and open source software (foss). the new application programming interface (api) of nasa, web world wind, written in javascript and based on web graphics library (webgl) is cross-platform and cross-browser, so that the virtual globe created using this api can be accessible through any webgl supported browser on different operating systems and devices, as a result not requiring any installation or configuration on the client-side, making the collected data more usable to users, which is not the case with the world wind for java as installation and configuration of the java virtual machine (jvm) is required. furthermore, the data collected through various vgi platforms might be in different formats, stored in a traditional relational database or in a nosql database. the project developed aims to visualize and query data collected through open data kit (odk) platform and a cross-platform application, where data is stored in a relational postgresql and nosql couchdb databases respectively.
computer_programming	this work presents a new method of examining the structure of public-transport networks (ptns) and analyzes their topological properties through a combination of computer programming, statistical data and large-network analyses. in order to automate the extraction, processing and exporting of data, a software program was developed allowing to extract the needed data from general transit feed specification, thus overcoming difficulties occurring in accessing and collecting data. the proposed method was applied to a real-life ptnin auckland, new zealand, with the purpose of examining whether it showed characteristics of scale-free networks and exhibited features of ""small-world"" networks. as a result, new regression equations were derived analytically describing observed, strong, non-linear relationships among the probabilities of randomly chosen stops in the ptn to be serviced by a given number of routes. the established dependence is best fitted by an exponential rather than a power-law function, showing that the ptn examined is neither random nor scale-free, but a mixture of the two. this finding explains the presence of hubs that are not typical of exponential networks and simultaneously not highly connected to the other nodes as is the case with scale-free networks. on the other hand, the observed values of the topological properties of the network show that although it is highly clustered, owing to its representation as a directed graph, it differs slightly from ""small-world"" networks, which are characterized by strong clustering and a short average path length. (c) 2016 elsevier b.v. all rights reserved.
image_processing	the montage image mosaic engine was designed as a scalable toolkit, written in c for performance and portability across *nix platforms, that assembles fits images into mosaics. this code is freely available and has been widely used in the astronomy and it communities for research, product generation, and for developing next-generation cyber-infrastructure. recently, it has begun finding applicability in the field of visualization. this development has come about because the toolkit design allows easy integration into scalable systems that process data for subsequent visualization in a browser or client. the toolkit it includes a visualization tool suitable for automation and for integration into python: mviewer creates, with a single command, complex multi-color images overlaid with coordinate displays, labels, and observation footprints, and includes an adaptive image histogram equalization method that preserves the structure of a stretched image over its dynamic range. the montage toolkit contains functionality originally developed to support the creation and management of mosaics, but which also offers value to visualization: a background rectification algorithm that reveals the faint structure in an image; and tools for creating cutout and downsampled versions of large images. version 5 of montage offers support for visualizing data written in healpix sky-tessellation scheme, and functionality for processing and organizing images to comply with the toast sky-tessellation scheme required for consumption by the world wide telescope (wwt). four online tutorials allow readers to reproduce and extend all the visualizations presented in this paper.
computer_graphics	in this paper we survey all known (including own recent results) properties of the longest-edge n-section algorithms. these algorithms (in classical and recently designed conforming form) are nowadays used in many applications, including finite element simulations, computer graphics, etc. as a reliable tool for controllable mesh generation. in addition, we present a list of open problems arising in and around this topic. (c) 2015 elsevier b.v. all rights reserved.
bioinformatics	background: female moths synthesize species-specific sex pheromone components and release them to attract male moths, which depend on precise sex pheromone chemosensory system to locate females. two types of genes involved in the sex pheromone biosynthesis and degradation pathways play essential roles in this important moth behavior. to understand the function of genes in the sex pheromone pathway, this study investigated the genome-wide and digital gene expression of sex pheromone biosynthesis and degradation genes in various adult tissues in the diamondback moth (dbm), plutella xylostella, which is a notorious vegetable pest worldwide. results: a massive transcriptome data (at least 39.04 gb) was generated by sequencing 6 adult tissues including male antennae, female antennae, heads, legs, abdomen and female pheromone glands from dbm by using illumina 4000 next-generation sequencing and mapping to a published dbm genome. bioinformatics analysis yielded a total of 89,332 unigenes among which 87 transcripts were putatively related to seven gene families in the sex pheromone biosynthesis pathway. among these, seven [ two desaturases (des), three fatty acyl-coa reductases (far) one acetyltransferase (act) and one alcohol dehydrogenase (ad)] were mainly expressed in the pheromone glands with likely function in the three essential sex pheromone biosynthesis steps: desaturation, reduction, and esterification. we also identified 210 odorantdegradation related genes (including sex pheromone-degradation related genes) from seven major enzyme groups. among these genes, 100 genes are new identified and two aldehyde oxidases (aoxs), one aldehyde dehydrogenase (aldh), five carboxyl/cholinesterases (cces), five udp-glycosyltransferases (ugts), eight cytochrome p450 (cyp) and three glutathione s-transferases (gsts) displayed more robust expression in the antennae, and thus are proposed to participate in the degradation of sex pheromone components and plant volatiles. conclusions: to date, this is the most comprehensive gene data set of sex pheromone biosynthesis and degradation enzyme related genes in dbm created by genome-and transcriptome-wide identification, characterization and expression profiling. our findings provide a basis to better understand the function of genes with tissue enriched expression. the results also provide information on the genes involved in sex pheromone biosynthesis and degradation, and may be useful to identify potential gene targets for pest control strategies by disrupting the insect-insect communication using pheromone-based behavioral antagonists.
machine_learning	we present a set of matlab/octave functions to compute measures of emergence, self-organization, and complexity applied to discrete and continuous data. these measures are based on shannon 's information and differential entropy. examples from different datasets and probability distributions are provided to show how to use our proposed code.
operating_systems	as recent heterogeneous system designs integrate general purpose processors, gpus, and other specialized accelerator devices into a single platform to provide both power and performance benefits, it is important to support efficient dispatching mechanisms in terms of performance and programmability. this work proposes models for integrating hardware accelerators with applications executing under standard operating systems on an embedded processor. instead of using direct mapping of accelerator units to user applications, or using legacy drivers that incur communication overheads and large programming effort, we develop an abstraction layer in kernel driver which driver communicates with a custom dispatcher to interface a number of hardware accelerators. at the same time we remove the need to include iommus for virtual-to-physical translation from the device side, and the need to perform copies from user to kernel space when offtoading computational instensive tasks to the accelerators. we demonstrate the effectiveness of our solutions running real applications on a prototype hybrid heterogeneous system-on-chip platform.
parallel_computing	using gpus as general-purpose processors has revolutionized parallel computing by providing, for a large and growing set of algorithms, massive data-parallelization on desktop machines. an obstacle to their widespread adoption, however, is the difficulty of programming them and the low-level control of the hardware required to achieve good performance. this paper proposes a programming approach, safegpu, that aims to make gpu data-parallel operations accessible through high-level libraries for object-oriented languages, while maintaining the performance benefits of lower-level code. the approach provides data-parallel operations for collections that can be chained and combined to express compound computations, with data synchronization and device management all handled automatically. it also integrates the design-by-contract methodology, which increases confidence in functional program correctness by embedding executable specifications into the program text. we present a prototype of safegpu for eiffel, and show that it leads to modular and concise code that is accessible for gpgpu non-experts, while still providing performance comparable with that of hand-written cuda code. we also describe our first steps towards porting it to c#, highlighting some challenges, solutions, and insights for implementing the approach in different managed languages. finally, we show that runtime contract-checking becomes feasible in safegpu, as the contracts can be executed on the gpu. (c) 2016 elsevier ltd. all rights reserved.
data_structures	this paper surveys value systems for developmental cognitive robotics. a value system permits a biological brain to increase the likelihood of neural responses to selected external phenomena. many machine learning algorithms capture the essence of this learning process. however, computational value systems aim not only to support learning, but also autonomous attention focus to direct learning. this combination of unsupervised attention focus and learning aims to address the grand challenge of autonomous mental development for machines. this survey examines existing value systems for developmental cognitive robotics in this context. we examine the definitions of value used-including recent pioneering work in intrinsic motivation as value-as well as initialisation strategies for innate values, update strategies for acquired value and the data structures used for storing value. we examine the extent to which existing value systems support attention focus, learning and prediction in an unsupervised setting. the types of robots and applications in which these value systems are used are also examined, as well as the ways that these applications are evaluated. finally, we study the strengths and limitations of current value systems for developmental cognitive robots and conclude with a set of research challenges for this field. (c) 2016 elsevier b.v. all rights reserved.
data_structures	v-order is a global order on strings related to unique maximal factorization families (umffs), themselves generalizations of lyndon words. v-order has recently been proposed as an alternative to lexicographic order in the computation of suffix arrays and in the suffix-sorting induced by the burrows-wheeler transform. efficient v-ordering of strings thus becomes a matter of considerable interest. in this paper we discover several new combinatorial properties of v-order, then explore the computational consequences; in particular, a fast, simple on-line v-order comparison algorithm that requires no auxiliary data structures. (c) 2016 elsevier b.v. all rights reserved.
image_processing	when an image is given with only some measurable data, e.g., projections, the most important task is to reconstruct it, i.e., to find an image that provides the measured data. these tomographic problems are frequently used in the theory and applications of image processing. in this paper, memetic algorithms are investigated on triangular grids for the reconstruction of binary images using their three and six direction projections. the algorithm generates an initial population using the network flow algorithm for two of the input projections. the reconstructed images evolve towards an optimal solution or close to the optimal solution, by using crossover operators and guided mutation operators. the quality of the images is improved by using switching components and compactness operator. (c) 2016 elsevier b.v. all rights reserved.
parallel_computing	this paper proposes a parallelized online optimization of low voltage distribution network (lvdn) operation. it is performed on a graphics processing unit (gpu) by combining the optimization procedure with the load flow method. in the case study, performed for the test lvdn with distributed generators (dgs) and controllable loads, differential evolution optimization based on a backward-forward sweep load flow method was parallelized on gpu. the goal of online optimization is to keep the lvdn voltage profile within the prescribed limits, to minimize lvdn losses, and to enable demand response functionality. this is achieved by the optimization determined reference values for the controllable load 's operation, and the reactive power generation, and active power curtailment of dgs. the results show that the parallelized gpu implemented optimization can be significantly faster than similar implementation on a central processing unit, and is, therefore, suitable for the online optimization of the presented lvdn.
data_structures	we consider the problems of computing the maximal and the minimal non-empty suffixes of substrings of a longer text of length n. for the minimal suffix problem we show that for every tau, 1 <= tau <= logn, there exists a linear-space data structure with o(tau) query time and o(n logn/tau) preprocessing time. as a sample application, we show that this data structure can be used to compute the lyndon decomposition of any substring of the text in o(k tau) time, where k is the number of distinct factors in the decomposition. for the maximal suffix problem, we give a linear-space structure with o(1) query time and o(n) preprocessing time. in other words, we simultaneously achieve both the optimal query time and the optimal construction time. (c) 2015 elsevier b.v. all rights reserved.
machine_learning	this paper addresses the problem of finding credible sources among twitter social network users to detect and prevent various malicious activities, such as spreading false information on a potentially inflammatory topic, forging accounts for false identities, etc. existing research works related to source credibility are graph-based, considering the relationships among users to predict the spread information; human-based, using human perspectives to determine reliable sources; or machine learning-based, relying on training classifiers to predict users' credibility. very few of these approaches consider a user 's sentimentality when analyzing his/her credibility as a source. in this paper, we propose a novel approach that combines analysis of the user 's reputation on a given topic within the social network, as well as a measure of the user 's sentiment to identify topically relevant and credible sources of information. in particular, we propose a new reputation metric that introduces several new features into the existing models. we evaluated the performance of the proposed metric in comparison with two machine learning techniques, determining that the accuracy of the proposed approach satisfies the stated purpose of identifying credible twitter users. copyright (c) 2016 john wiley & sons, ltd.
operating_systems	as the deceleration of processor scaling due to moore 's law accelerates research in new types of computing structures, the need arises for rethinking operating systems paradigms. traditionally, an operating system is a layer between hardware and applications and its primary function is in managing hardware resources and providing a common abstraction to applications. how does this function apply, however, to new types of computing paradigms? are operating systems even needed for these new structures? this paper revisits operating system functionality for new computing paradigms. the structure of these new computers is uncertain as there are many possibilities such as neuromorphic, bio-inspired, adiabatic, reversible, approximate, quantum, combinations of these and others unforeseen [1]. we do know, however, that whatever these new computers will be, there will be some need to manage their resources, to provide programming support, to partition, scale, and connect them and to deal with (partial) failure, along with other traditional operating system 's functionality. there might also be some new functionality, such as creating abstract control loops, reasoning about precision, new ways of reconfiguring, and more. we strongly believe that even if traditional operating systems functionality evolves, that the need for operating systems will remain in the new era of computing.
computer_vision	the computer graphics and computer vision communities have been working closely together in recent years, and a variety of algorithms and applications have been developed to analyze and manipulate the visual media around us. there are three major driving forces behind this phenomenon: 1) the availability of big data from the internet has created a demand for dealing with the ever-increasing, vast amount of resources; 2) powerful processing tools, such as deep neural networks, provide effective ways for learning how to deal with heterogeneous visual data; 3) new data capture devices, such as the kinect, the bridge between algorithms for 2d image understanding and 3d model analysis. these driving forces have emerged only recently, and we believe that the computer graphics and computer vision communities are still in the beginning of their honeymoon phase. in this work we survey recent research on how computer vision techniques benefit computer graphics techniques and vice versa, and cover research on analysis, manipulation, synthesis, and interaction. we also discuss existing problems and suggest possible further research directions.
software_engineering	software developers use many different communication tools and channels in their work. the diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. the availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. however, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. in this paper, we report on a large-scale survey conducted with 1,449 github users. we discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers.
software_engineering	context: reuse can improve productivity and maintainability in software development. research has proposed a wide range of methods and techniques. are these successfully adopted in practice? objective: we propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. method: we compare and interpret the study results with a focus on reuse practices, effects, and context. results: both companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. reusable entities are retrieved from a central repository, if present. otherwise, direct communication with trusted colleagues is crucial for access. reuse processes remain implicit and reflect the development style. in a homogeneous infrastructure supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. in a heterogeneous context with fragmented infrastructure, these benefits did not materialize. neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. in both cases, a lack of reuse led to duplicate implementations. conclusion: technological advances have improved the way reuse concepts can be applied in practice. homogeneity in development process and tool support seem necessary preconditions. developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging. (c) 2016 elsevier inc. all rights reserved.
bioinformatics	various model selection methods can be applied to seek sparse subsets of the covariates to explain the response of interest in bioinformatics. while such methods often offer very helpful predictive performances, their selections of the covariates may be much less trustworthy. indeed, when the number of covariates is large, the selections can be highly unstable, even under a slight change of the data. this casts a serious doubt on reproducibility of the identified variables. for a sound scientific understanding of the regression relationship, methods need to be developed to find the most important covariates that have higher chance to be confirmed in future studies. such a method based on variable selection deviation is proposed and evaluated in this work.
relational_databases	this article argues that the study of literary representations of landscapes can be aided and enriched by the application of digital geographic technologies. as an example, the article focuses on the methods and preliminary findings of litescape. pt-atlas of literary landscapes of mainland portugal, an on-going project that aims to study literary representations of mainland portugal and to explore their connections with social and environmental realities both in the past and in the present. litescape. pt integrates traditional reading practices and 'distant reading' approaches, along with collaborative work, relational databases, and geographic information systems (gis) in order to classify and analyse excerpts from 350 works of portuguese literature according to a set of ecological, socioeconomic, temporal and cultural themes. as we argue herein this combination of qualitative and quantitative methods-itself a response to the difficulty of obtaining external funding-can lead to (a) increased productivity, (b) the pursuit of new research goals, and (c) the creation of new knowledge about natural and cultural history. as proof of concept, the article presents two initial outcomes of the litescape. pt project: a case study documenting the evolving literary geography of lisbon and a case study exploring the representation of wolves in portuguese literature.
network_security	in this paper, we propose a secured openflowbased switch architecture. the architecture is a combination of openflow processing that routes packets according to the openflow protocol and security processing that defends against network attacks. therefore, the proposed switch can work not only as a openflow-based forwarding device but also as a network protection system. we implement our prototype switch on a xilinx virtex 5 xc5vtx240t fpga device. in this prototype version, we integrate two different ddos countermeasure techniques, the hop-count filtering and port ingress/egress filtering. the experimental results show that the switch achieves packet processing throughput by up to 19.7 gbps while a 100% ddos detection rate with up to a 2.9% false positive rate and a 0% false negative rate is obtained. our prototype system uses up to 36% look-up tables, 38% registers, and 62% block ram of the fpga device.
parallel_computing	a significant challenge in fitting metamodels of large-scale simulations with sufficient accuracy is in the computational time required for rigorous statistical validation. this paper addresses the statistical computation issues associated with the bootstrap and modified press statistic, which yield key metrics for error measurements in metamodelling validation. experimentation is performed on different programming languages, namely, matlab, r, and python, and implemented on different computing architectures including traditional multicore personal computers and high-power clusters with parallel computing capabilities. this study yields insight into the effect that programming languages and computing architecture have on the computational time for simulation metamodel validation. the experimentation is performed across two scenarios with varying complexity.
cryptography	in wireless communication systems, the conventional secret key exchange is based on the public key cryptography, which requires complex computations to retain the secrecy level of these key bits. the proposed physical layer-based algorithms have shown promising performance to extract secret keys from the privately shared randomness relying on the reciprocal channel state between both communicated nodes. in this study, the authors propose a physical layer key exchange method which transmits the key bits by encoding them within some phase randomisation (pr) sequences that are privately indexed to a specific channel criterion. the pr sequences only randomise the data phases and thus no efficiency reduction will be incurred. in fact, by choosing a pool of randomisation sequences with certain statistical properties, they could also be used to condition the signal to meet physical layer transmission requirements such as bandwidth, envelope and so on. they quantify the potential of the proposed method by demonstrating it within the context of a multiple-input multiple-output orthogonal frequency division multiplexing system. the results reveal that, relative to existing techniques, the proposed method offers superior key error rate performance at lower computational complexity with better secrecy level.
data_structures	many applications require specialized data structures not found in the standard libraries, but implementing new data structures by hand is tedious and error-prone. this paper presents a novel approach for synthesizing efficient implementations of complex collection data structures from high-level specifications that describe the desired retrieval operations. our approach handles a wider range of data structures than previous work, including structures that maintain an order among their elements or have complex retrieval methods. we have prototyped our approach in a data structure synthesizer called cozy. four large, real-world case studies compare structures generated by cozy against handwritten implementations in terms of correctness and performance. structures synthesized by cozy match the performance of handwritten data structures while avoiding human error.
image_processing	in recent decades, compressive sensing (cs) is a popular theory for studying the inverse problem, and has been widely used in synthetic aperture radar (sar) image processing. however, the computation complexity of cs-based methods limits its wide applications in sar imaging. in this paper, we propose a novel sparse sar imaging method using the multiple measurement vectors model to reduce the computation cost and enhance the imaging result. based on using the structure information and the matched filter processing, the new cs-sar imaging method can be applied to high-quality and high-resolution imaging under sub-nyquist rate sampling with the advantages of saving the computational cost substantially both in time and memory. the results of simulations and real sar data experiments suggest that the proposed method can realize sar imaging effectively and efficiently.
data_structures	the growth in large-scale data management capabilities and the successful care of patients with congenital heart defects have coincidentally paralleled each other for the last three decades, and participation in multicenter congenital heart disease databases and registries is now a fundamental component of cardiac care. this manuscript attempts for the first time to consolidate in one location all of the relevant databases worldwide, including target populations, specialties, web sites, and participation information. since at least 1,992 cardiac surgeons and cardiologists began leveraging this burgeoning technology to create multi-institutional data collections addressing a variety of specialties within this field. pediatric heart diseases are particularly well suited to this methodology because each individual care location has access to only a relatively limited number of diagnoses and procedures in any given calendar year. combining multiple institutions data therefore allows for a far more accurate contemporaneous assessment of treatment modalities and adverse outcomes. additionally, the data can be used to develop outcome benchmarks by which individual institutions can measure their progress against the field as a whole and focus quality improvement efforts in a more directed fashion, and there is increasing utilization combining clinical research efforts within existing data structures. efforts are ongoing to support better collaboration and integration across data sets, to improve efficiency, further the utility of the data collection infrastructure and information collected, and to enhance return on investment for participating institutions.
algorithm_design	intentional or unintentional leakage of confidential data is undoubtedly one of the most severe security threats that organizations face in the digital era. the threat now extends to our personal lives: a plethora of personal information is available to social networks and smartphone providers and is indirectly transferred to untrustworthy third party and fourth party applications. in this work, we present a generic data lineage framework lime for data flow across multiple entities that take two characteristic, principal roles (i. e., owner and consumer). we define the exact security guarantees required by such a data lineage mechanism toward identification of a guilty entity, and identify the simplifying non-repudiation and honesty assumptions. we then develop and analyze a novel accountable data transfer protocol between two entities within a malicious environment by building upon oblivious transfer, robust watermarking, and signature primitives. finally, we perform an experimental evaluation to demonstrate the practicality of our protocol and apply our framework to the important data leakage scenarios of data outsourcing and social networks. in general, we consider lime, our lineage framework for data transfer, to be an key step towards achieving accountability by design.
machine_learning	parentage data from beef calves has shown that in multiple-sire pastures a disproportionate number of calves are born from a single bull. investigating and accurately quantifying bull behavior within multiple sire pastures will begin to determine reason(s) for the variability in the number of calves sired. the study objective was to assess accelerometer data and various classification algorithms to accurately predict bull behavior events in a multiple-sire pasture. behavior events of interest in this study included lying, standing, walking, and mounting. two bulls and ten estrous synchronized cows were used. true behavior events were determined during daylight hours with video analysis, and matched with accelerometer data. accelerometers were attached to both ears, withers, and neck of both bulls. accelerometer data were recorded for every second over 3 days. accelerometer data were used to generate algorithms and accuracy was evaluated compared to known video behavioral data. the prevalence based on the raw video data for lying was 32.6%, standing was 59.4%, walking was 7.4%, and mounting was 0.6%. the random forest classifier had the highest accuracy compared to other classifiers (random tree and decision tree) for each tag location and behavior of interest. the accuracies from the random forest algorithms ranged from 92 to 99% for lying, 85 to 90% for standing, 73 to 77% for walking, and 74% to 80% for mounting. the classification algorithm was able to accurately predict a lying and standing event, and predict a walking and mounting event with a lower accuracy. further research is needed to determine how behaviors between bulls affects overall parentage data. (c) 2017 elsevier b.v. all rights reserved.
bioinformatics	background: a basic task in bioinformatics is the counting of k-mers in genome sequences. existing k-mer counting tools are most often optimized for small k= 32. our software is the result of an intensive process of algorithm engineering. it implements a two-step approach. in the first step, genome reads are loaded from disk and redistributed to temporary files. in a second step, the k-mers of each temporary file are counted via a hash table approach. in addition to its basic functionality, gerbil can optionally use gpus to accelerate the counting step. in a set of experiments with real-world genome data sets, we show that gerbil is able to efficiently support both small and large k. conclusions: while gerbil 's performance is comparable to existing state-of-the-art open source k-mer counting tools for small k < 32, it vastly outperforms its competitors for large k, thereby enabling new applications which require large values of k.
parallel_computing	the computational demand of exact-search procedures has pressed the exploitation of parallel processing accelerators to reduce the execution time of many applications. however, this often imposes strict restrictions in terms of the problem size and implementation efforts, mainly due to their possibly distinct architectures. to circumvent this limitation, a new exact-search alignment tool (bowmapcl) based on the burrows-wheeler transform and fm-index is presented. contrasting to other alternatives, bowmapcl is based on a unified implementation using opencl, allowing the exploitation of multiple and possibly different devices (e.g., nvidia, amd/ati, and intel gpus/apus). furthermore, to efficiently exploit such heterogeneous architectures, bowmapcl incorporates several techniques to promote its performance and scalability, including multiple buffering, work-queue task-distribution, and dynamic load-balancing, together with index partitioning, bit-encoding, and sampling. when compared with state-of-the-art tools, the attained results showed that bowmapcl (using a single gpu) is 2 x to 7.5x faster than mainstream multi-threaded cpu bwt-based aligners, like bowtie, bwa, and soap2; and up to 4 x faster than the best performing state-of-the-art gpu implementations (namely, soap3 and hpg-bwt). when multiple and completely distinct devices are considered, bowmapcl efficiently scales the offered throughput, ensuring a convenient load-balance of the involved processing in the several distinct devices.
cryptography	key encapsulation mechanism (kem) is an important key distribution mechanism that not only allows both sender and receiver to safely share a random session key, but also can be mainly applied to construct a hybrid public key encryption scheme. in this paper, we give an positive answer to the question of if it is possible to build an efficient kem over lattices. more precisely, wedesign an efficientkemscheme in standard model based on ideal lattices. we prove that the proposed scheme captures indistinguishability against active chosen ciphertext attacks (ind-cca) under the ring learning with errors problem, or more formally, ind-cca security. compared with the current cca secure kem schemes based on lattices in the standard model, our scheme has shorter public key, secret key and encapsulation ciphertext. in addition, our kem scheme realizes ind-cca security in the standard model.
computer_vision	recent studies experienced the use of advanced tools for smart aircraft maintenance and inspection. these tools often require the use of computer-vision based technologies to recognize and track a given aircraft mechanical part in order to make it possible to show additional information to a technician on a suitable display. in this paper we propose a visual recognition module of aircraft mechanical parts that has been included in a prototype system designed for the smart maintenance of the alenia-aermacchi m346. the evaluation, carried out on real aircrafts, considers different kind of maintenance operations that require the recognition of 20 different mechanical parts. the visual recognition module has been tested under different imaging conditions and varying the scale and the orientation of the parts of interest. the results confirm the feasibility of our proposal also in such a very challenging and realistic condition. (c) 2017 elsevier b.v. all rights reserved.
computer_programming	online question and answer (q&a) forums are emerging as excellent learning platforms for learners with varied interests. in this paper, we present our results on the clustering of stack overflow users into four clusters, namely naive users, surpassing users, experts, and outshiners. this clustering is based on various metrics available on the forum. we use the x-means and expectation maximization clustering algorithms and compare the results. the results have been validated using internal, external, and relative validation techniques. the objective of this clustering is to be able to trace and predict the activity of a user on this forum. according to our results, majority of users (71 % of 40,000 users under consideration) fall in the 'experts' category. this indicates that the users in stack overflow are of high quality thereby making the forum an excellent platform for users to learn about computer programming.
operating_systems	future low-end embedded systems will make an increased use of heterogeneous mpsocs. to utilize these systems efficiently, methods and tools are required that support the extraction and implementation of parallelism typically found in embedded applications. ideally, large amounts of existing legacy code should be reused and ported to these new systems. existing parallelization infrastructures, however, mostly support parallelization according to the requirements of hpec systems. for resource-restricted embedded systems, different parallelization strategies are necessary to achieve additional non-functional objectives such as the reduction of energy consumption. hpc-focused parallelization also assumes processor, memory and communication structures different from low-end embedded systems and therefore wastes optimization opportunities essential for improving the performance of resource-constrained embedded systems. this paper describes a new approach and infrastructure inspired by the openmp api to support the extraction and implementation of pipeline parallelism, which is commonly found in complex embedded applications. in addition, advanced techniques to extract parallelism from legacy applications requiring only minimal code modifications are presented. further, the resulting toolflow combines advanced parallelization, mapping and communication optimization tools leading to a more efficient approach to exploit parallelism for typical embedded applications on heterogeneous mpsocs running distributed real-time operating systems. (c) 2016 elsevier inc. all rights reserved.
software_engineering	software cybernetics research is to apply a variety of techniques from cybernetics research to software engineering research. for more than fifteen years since 2001, there has been a dramatic increase in work relating to software cybernetics. from cybernetics viewpoint, the work is mainly on the first-order level, namely, the software under observation and control. beyond the first-order cybernetics, the software, developers/users, and running environments influence each other and thus create feedback to form more complicated systems. we classify software cybernetics as software cybernetics i based on the first-order cybernetics, and as software cybernetics ii based on the higher order cybernetics. this paper provides a review of the literature on software cybernetics, particularly focusing on the transition from software cybernetics i to software cybernetics ii. the results of the survey indicate that some new research areas such as internet of things, big data, cloud computing, cyber-physical systems, and even creative computing are related to software cybernetics ii. the paper identifies the relationships between the techniques of software cybernetics ii applied and the new research areas to which they have been applied, formulates research problems and challenges of software cybernetics with the application of principles of phase ii of software cybernetics; identifies and highlights new research trends of software cybernetic for further research. (c) 2016 elsevier inc. all rights reserved.
machine_learning	commercial anti-virus software traditionally memorizes specific byte sequences (known as ""signatures"") in the file contents of previously encountered malware. however, malware authors can evade signature based detection in many ways; for instance, by using obfuscation techniques such as ""packing"" (encryption or compression) to hide snippets of malicious code; by writing metamorphic malware; or by tampering with existing malware. we hypothesize that certain evasion techniques can leave traces in the file 's entropy signal, revealing either similarities to known malware or the presence of tampering per se. to this end, we present suspend (suspicious entropy signal detector), an expert system which evaluates the suspiciousness of an executable file 's entropy signal in order to subserve malware classification. whereas traditionally, entropy analysis has been used for the goal of packer detection (and therefore entropy-based features often merely comprise mean entropy or the entropy of a few file subcomponents), suspend applies non-stationary time series modeling to aid in malware detection. in particular, suspend (a) quantifies the ""amount of structure"" in the entropy signal (through detrended fluctuation analysis), (b) finds the location and size of sudden jumps in entropy (through mean change point modeling), and (c) computes the distribution of entropic variation across multiple spatial scales (through wavelet decomposition). in addition, suspend (d) summarizes the entropy signal 's empirical probability distribution. because suspend 's run time can be made to scale linearly in file size, it is well-suited for large-scale malware analysis. we apply suspend to a large-scale malware detection task with 500,000 heterogeneous real-world samples and over 1 million features. we find that suspend boosts the predictive performance of traditional entropy analysis (as found in packer detectors) from 77.02% to 96.62%. moreover, suspend 's focus on entropy signals makes it a natural candidate for combining with other types of features; for instance, combining suspend with a strings-based feature set boosts predictive accuracy from 97.18% to 98.62%. thus, whereas traditionally, entropy analysis has focused on detecting that a file is packed, suspend 's more comprehensive representation of the entropy signal helps to determine that a file is malicious. we illustrate the application of suspend by studying 18 pieces of virransom, a family of viral ransomware which could cost millions to large organizations. suspend is able to detect 100% of the studied files with over 99% confidence, whereas a more traditional strings-based model was very close to undecided and represents the entire family with a single string. (c) 2016 elsevier ltd. all rights reserved.
symbolic_computation	in this paper, the hopf bifurcation in a new hyperchaotic system is studied. based on the first lyapunov coefficient theory and symbolic computation, the conditions of supercritical and subcritical bifurcation in the new hyperchaotic system are obtained. numerical simulations are used to illustrate some main results.
parallel_computing	the ex-core detector response calculation is an important part in reactor design. however, the response function cannot be measured by experiments quantitatively. ex-core detector response simulation is therefore required. for decades, the s-n code has been used as the dedicated tool. nowadays, more and more engineers are expressing an interest in using the monte-carlo method instead of the s-n method in simulations, as it is expected that the monte-carlo method will give higher accuracy. in this paper, the modeling and simulation of ex-core detector responses is briefly reviewed based on the korean kori unit 1 reactor. then, the differences between the s-n simulation and monte-carlo simulation are compared. the sensitivity of computational conditions is also discussed. it is shown that the problem dependence of cross sections and meshing dependence of spatial discretization in the ex-core detector response calculations are not as strong as expected. however, the ray effect is the main shortcoming for the s-n calculation. based on the analysis, two benefits are shown by using mcnp for the direct 3d calculation. firstly, the impact of ray effect is eliminated without using the s-n angular discretization. secondly, the direct 3d calculation is easier to perform based on the powerful ability of 3d modeling and parallel computing of the monte-carlo code. the new drf values are adopted in the dynamic control rod reactivity measurement of kori unit 1 reactor. the results show that the new drf values improve the error of measured control rod worth by a percentage of 3. (c) 2016 elsevier ltd. all rights reserved.
software_engineering	teams working on the development of software systems use certain tools and workflows of product and knowledge management. these tools and workflows help them plan, monitor and control the product at all stages of the product lifecycle as well as capture, develop and share project-related knowledge. this paper discusses the tools and workflows development teams of the german national library of science and technology (tib) use when developing the web portals. the use case illustrated here is a project, in which the interaction concept and screen design of the video portal of tib (tib vertical bar av-portal) had to be adapted to the new look and feel of the main portal (tib-portal). in the course of the project, the team gathered and structured preliminary requirements in the enterprise wiki, created an axure user interface prototype, evaluated the prototype and specified the requirements using sprints from the scrum framework, commissioned external screen designers and software developers to create a new screen design based on the prototype and implement the requirements, tested the implementations in a collaborative workflow and approved their deployment after debugging.
distributed_computing	the paper presents design and implementation of the cellular automata (ca) model, which predicts damage of forging tools due to fatigue. the transition rules for the model were developed on the basis of known information regarding crack initiation and propagation. the coefficients in the model were determined by the inverse analysis of the thermal fatigue tests performed on the gleeble 3800 simulator and in the special device with a rotating disc. the ca model was coupled with the conventional abrasive wear model. the layers of cell in the ca space, which are in contact with the workpiece, were removed successively following the abrasive wear of the tool. the ca model was connected with the finite element (fe) programme, which simulates stresses in tools during forging. since this multiscale approach appeared to be extremely demanding as far as computing times are considered, an efficient implementation of the model on heterogeneous hardware architectures was prepared. results of simulations were compared with the industrial data and good predictive capabilities of the model were confirmed. published by elsevier sp. z o.o. on behalf of politechnika wroclawska.
computer_vision	the flickr30k dataset has become a standard benchmark for sentence-based image description. this paper presents flickr30k entities, which augments the 158k captions from flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. such annotations are essential for continued progress in automatic image description and grounded language understanding. they enable us to define a new benchmark for localization of textual entity mentions in an image. we present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. while our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.
cryptography	for multi-output boolean functions (also called s-boxes), various measures of nonlinearity have been widely discussed in the literature but many problems are left open in this topic. the purpose of this paper is to present a new approach to estimating the nonlinearity of s-boxes. a more fine-grained view on the notion of nonlinearity of s-boxes is presented and new connections to some linear codes are established. more precisely, we mainly study the nonlinearity indicator (denoted by n-v) for s-boxes from a coding theory point of view. such a cryptographic parameter n-v is more related to best affine approximation attacks on stream ciphers. we establish a direct link between nv and the minimum distance of the corresponding linear code. we exploit that connection to derive the first general lower bounds on n-v of non-affine functions from f(2)n to f(2)m for m dividing n. furthermore, we show that n-v can be determined directly by the weight distribution of the corresponding linear code.
operating_systems	a design of an advanced-reliability piezoelectric transducer of acoustic emission (ae) is suggested. the advanced reliability is achieved by eliminating the main reasons that lead to failures; duplicating/backing the main operating systems; and imparting redundant or extended technical capabilities to the transducer design that make it possible to compensate for partial or complete performance loss of the antenna group. it is shown that given such a design approach, it is feasible to achieve technical characteristics that are stipulated for industrial acoustic-emission transducers. theoretical estimates of transducer reliability are presented that show that a combination of backing of ae registration channels with different versions of their connection gives ample opportunities for governing the reliability of sensors during their usage/service life.
computer_programming	in this paper, we describe the design, development, deployment and evaluation of nooblab, our novel on-line environment for the teaching and learning of computer programming. although originally devised for the teaching of elementary javascript, the system now also supports java, php and a prescriptive form of pseudocode designed to assist in the teaching of elementary programming concepts. the system incorporates a number of innovative features, such as automated checking of program code, detection of plagiarism, ""gamification"", and automatically logs all of each user 's interactions with it, facilitating performing learning analytics relating to student engagement and performance. the system has already proved to be of value in teaching programming, and helping people to learn to program, not just to specialist computer science students, but also people studying engineering or mathematics as their main subject discipline.
algorithm_design	batch processes are of great importance in process industry. however, the control algorithm design is difficult for those with constraints. this is because stability and recursive feasibility along directions of time and batch should be guaranteed simultaneously. in this paper, a stable model predictive control strategy with zero terminal state constraints is proposed. stability and recursive feasibility along two directions are guaranteed and proved. simulation results are given to show the effectiveness of the algorithm. (c) 2015, ifac (international federation of automatic control) hosting by elsevier ltd. all rights reserved.
parallel_computing	the fast sweeping method is a popular algorithm for solving a variety of static hamilton-jacobi equations. fast sweeping algorithms for parallel computing have been developed, but are severely limited. in this work, we present a multilevel, hybrid parallel algorithm that combines the desirable traits of two distinct parallel methods. the fine and coarse grained components of the algorithm take advantage of heterogeneous computer architecture common in high performance computing facilities. we present the algorithm and demonstrate its effectiveness on a set of example problems including optimal control, dynamic games, and seismic wave propagation. we give results for convergence, parallel scaling, and show state-of-the-art speedup values for the fast sweeping method. (c) 2016 elsevier inc. all rights reserved.
computer_programming	this section based on the gis technology, spatial database technology, computer programming technology, based on the transformation of villages in the city management system of gis, developed a set of data management, query (query attribute query, spatial query, condition), thematic map making, drawing generation and other functions in one of the gis system, provide data support and technical services for the reconstruction planning of city villages.
parallel_computing	this paper presents a new parallel domain decomposition algorithm based on integer linear programming (ilp), a mathematical optimization method. to minimize the computation time of coastal ocean circulation models, the ilp decomposition algorithm divides the global domain in local domains with balanced work load according to the number of processors and avoids computations over as many as land grid cells as possible. in addition, it maintains the use of logically rectangular local domains and achieves the exact same results as traditional domain decomposition algorithms (such as cartesian decomposition). however, the ilp decomposition algorithm may not converge to an exact solution for relatively large domains. to overcome this problem, we developed two ilp decomposition formulations. the first one (complete formulation) has no additional restriction, although it is impractical for large global domains. the second one (feasible) imposes local domains with the same dimensions and looks for the feasibility of such decomposition, which allows much larger global domains. parallel performance of both ilp formulations is compared to a base cartesian decomposition by simulating two cases with the newly created parallel version of the stevens institute of technology 's estuarine and coastal ocean model (secom). simulations with the ilp formulations run always faster than the ones with the base decomposition, and the complete formulation is better than the feasible one when it is applicable. in addition, parallel efficiency with the ilp decomposition may be greater than one.
computer_vision	zebrafish (danio rerio) is an important vertebrate model organism in biomedical research, especially suitable for morphological screening due to its transparent body during early development. deep learning has emerged as a dominant paradigm for data analysis and found a number of applications in computer vision and image analysis. here we demonstrate the potential of a deep learning approach for accurate high-throughput classification of whole-body zebrafish deformations in multifish microwell plates. deep learning uses the raw image data as an input, without the need of expert knowledge for feature design or optimization of the segmentation parameters. we trained the deep learning classifier on as few as 84 images (before data augmentation) and achieved a classification accuracy of 92.8% on an unseen test data set that is comparable to the previous state of the art (95%) based on user-specified segmentation and deformation metrics. ablation studies by digitally removing whole fish or parts of the fish from the images revealed that the classifier learned discriminative features from the image foreground, and we observed that the deformations of the head region, rather than the visually apparent bent tail, were more important for good classification performance.
data_structures	os-level virtualization is often used for server consolidation in data centers because of its high efficiency. however, the sharing of storage stack services among the colocated containers incurs contention on shared kernel data structures and locks within i/o stack, leading to severe performance degradation on manycore platforms incorporating fast storage technologies (e.g., ssds based on nonvolatile memories). this article presents multilanes, a virtualized storage system for os-level virtualization on manycores. multilanes builds an isolated i/o stack on top of a virtualized storage device for each container to eliminate contention on kernel data structures and locks between them, thus scaling them to manycores. meanwhile, we propose a set of techniques to tune the overhead induced by storage-device virtualization to be negligible, and to scale the virtualized devices to manycores on the host, which itself scales poorly. to reduce the contention within each single container, we further propose sfs, which runs multiple file-system instances through the proposed virtualized storage devices, distributes all files under each directory among the underlying file-system instances, then stacks a unified namespace on top of them. the evaluation of our prototype system built for linux container (lxc) on a 32-core machine with both a ram disk and a modern flash-based ssd demonstrates that multilanes scales much better than linux in micro-and macro-benchmarks, bringing significant performance improvements, and that multilanes with sfs can further reduce the contention within each single container.
relational_databases	data integration (di) is the problem of combining a set of heterogeneous, autonomous data sources and providing the user with a unified view of these data. integrating data raises several challenges, since the designer usually encounters incompatible data models characterized by differences in structure and semantics. one of the hardest challenges is to define correspondences between schema elements (e.g., attributes) to determine how they relate to each other. since most business data is currently stored in relational databases, here present a declarative and formal approach to specify 1-to-1, 1-m, and m-to-n correspondences between relational schema components. differently from usual approaches, our (cas) have semantics and can deal with outer-joins and data-metadata relationships. finally, we demonstrate how to use the cas to generate mapping expressions in the form of sql queries, and we present some preliminary tests to verify the performance of the generated queries.
machine_learning	we study geometric and topological properties of the image of a smooth submanifold of under a bi-lipschitz map to . in particular, we characterize how the dimension, diameter, volume, and reach of the embedded manifold relate to the original. our main result establishes a lower bound on the reach of the embedded manifold in the case where and the bi-lipschitz map is linear. we discuss implications of this work in signal processing and machine learning, where bi-lipschitz maps on low-dimensional manifolds have been constructed using randomized linear operators.
operating_systems	between 2008 and 2015, bus rapid transit system (brts) in india increased its implementation from two cities to eight cities with a significant increase in total ridership. this paper attempts to give a detailed review of brts implementation in cities of india. this is a systematic effort that could inform readers about the current system and network characteristics of indian brts. different system and corridor characteristics including off board and on board ticketing systems are adopted in india. gross cost revenue collection model is adopted by almost all special purpose vehicle (spv) companies developed to manage brt systems. a variety of carriageway concept designs for brts are implemented in these cities considering a right of way of 22, 24, 30, 32, 40, 45, 60 meters respectively. out of the eight cities, ahmedabad has almost 30% of the total fleet size. in terms of regulatory context, spv companies are formed in almost all eight cities after observing ahmedabad brt success. documentation of these operating systems shall provide a sound database to planners and decision makers actively involved with brt system implementation in developing countries.
distributed_computing	cloud computing is a new distributed computing paradigm that consists in provisioning of infrastructure, software and platform resources as services. this paradigm is being increasingly used for the deployment and execution of service-based applications. to efficiently manage them according the autonomic computing approach, service-based applications can be associated with autonomic managers that monitor them, analyze monitoring data, plan and execute configuration action on them. although, in these last years, autonomic management of cloud services has received an increasing attention, optimization of autonomic managers (ams) assigned to cloud services and their placement in the cloud remain not well explored. in fact, almost all the existing solutions on autonomic computing have been interested in modeling and implementing of autonomic environments without paying attention on optimization. to address this issue, we present in this paper a novel approach to optimize autonomic management of service-based applications that consists in minimizing both the cost of allocated ams while avoiding bottlenecks in management and the cost of their placement in the cloud (the inter-virtual machine communication cost). we propose two algorithms: (i) an algorithm that determines the optimal number of ams to be assigned to services of a managed service-based application, and (ii) an algorithm that approximates the optimal placement of ams in the cloud. experiments conducted show the efficiency of our finding.
parallel_computing	improper functioning of traffic signals at the intersections result in extreme congestion leading to increase in overall journey time and wastage of precious fuel. various algorithms have been proposed in literature for alleviating the problem of congestion. fixed-time, non-preemptive and preemptive approaches work towards reduction of queue length at the intersections to decrease the overall waiting time on roads. high traffic volume on the road results in large queue length which takes huge amount of time to process using a single processor. hence, there is a need for fast processing which can be obtained by parallelizing the algorithm. this paper proposes a parallel preemptive algorithm to reduce the average queue length resulting in decrease of overall waiting time. the implementation of parallel algorithm is done using compute unified device architecture (cuda) by harnessing the power of graphical processing units (gpus). the performance of the proposed parallel preemptive algorithm is compared with fixed-time, non-preemptive and preemptive approaches. obtained results show the reduction in queue length in case of the proposed algorithm which is also confirmed using t-test with 99% confidence. (c) 2016 elsevier inc. all rights reserved.
data_structures	while criteria for schenkerian analysis have been much discussed, such discussions have generally not been informed by data. kirlin [kirlin, phillipb., 2014 a probabilistic model of hierarchical music analysis. ph.d. thesis, university of massachusetts amherst] has begun to fill this vacuum with a corpus of textbook schenkerian analyses encoded using data structures suggested byyust [yust, jason, 2006 formal models of prolongation. ph.d. thesis, university of washington] and a machine learning algorithm based on this dataset that can produce analyses with a reasonable degree of accuracy. in this work, we examine what musical features (scale degree, harmony, metrical weight) are most significant in the performance of kirlin 's algorithm.
algorithm_design	raw signal simulation is a useful tool for synthetic aperture radar (sar) system design, mission planning, processing algorithm testing, and inversion algorithm design. time and frequency synchronization is the key technique of bistatic sar (bisar) system, and raw data simulation is an effective tool for verifying the time and frequency synchronization techniques. according to the two-dimensional (2-d) frequency spectrum of fixed-receiver bisar, a rapid raw data simulation approach with time and frequency synchronization errors is proposed in this paper. through 2-d inverse stolt transform in 2-d frequency domain and phase compensation in range-doppler frequency domain, this method can significantly improve the efficiency of scene raw data simulation. simulation results of point targets and extended scene are presented to validate the feasibility and efficiency of the proposed simulation approach.
computer_vision	accurate scale estimation and occlusion handling is a challenging problem in visual tracking. recently, correlation filter-based trackers have shown impressive results in terms of accuracy, robustness, and speed. however, the model is not robust to scale variation and occlusion. in this paper, we address the problems associated with scale variation and occlusion by employing a scale space filter and multi-block scheme based on a kernelized correlation filter (kcf) tracker. furthermore, we develop a more robust algorithm using an appearance update model that approximates the change of state of occlusion and deformation. in particular, an adaptive update scheme is presented to make each process robust. the experimental results demonstrate that the proposed method outperformed 29 state-of-the-art trackers on 100 challenging sequences. specifically, the results obtained with the proposed scheme were improved by 8% and 18% compared to those of the kcf tracker for 49 occlusion and 64 scale variation sequences, respectively. therefore, the proposed tracker can be a robust and useful tool for object tracking when occlusion and scale variation are involved.
machine_learning	while quantitative structure activity relationship (qsar) models have been employed successfully for the prediction of small model protein chromatographic behavior, there have been few reports to date on the use of this methodology for larger, more complex proteins. recently our group generated focused libraries of antibody fab fragment variants with different combinations of surface hydrophobicities and electrostatic potentials, and demonstrated that the unique selectivities of multimodal resins can be exploited to separate these fab variants. in this work, results from linear salt gradient experiments with these fabs were employed to develop qsar models for six chromatographic systems, including multimodal (capto mmc, nuvia cprime, and two novel ligand prototypes), hydrophobic interaction chromatography (hic; capto phenyl), and cation exchange (cex; cm sepharose ff) resins. the models utilized newly developed local descriptors to quantify changes around point mutations in the fab libraries as well as novel cluster descriptors recently introduced by our group. subsequent rounds of feature selection and linearized machine learning algorithms were used to generate robust, well-validated models with high training set correlations (r-2>0.70) that were well suited for predicting elution salt concentrations in the various systems. the developed models then were used to predict the retention of a deamidated fab and isotype variants, with varying success. the results represent the first successful utilization of qsar for the prediction of chromatographic behavior of complex proteins such as fab fragments in multimodal chromatographic systems. the framework presented here can be employed to facilitate process development for the purification of biological products from product-related impurities by in silico screening of resin alternatives. biotechnol. bioeng. 2017;114: 1231-1240. (c) 2016 wiley periodicals, inc.
parallel_computing	with the development of the design complexity in embedded systems, hardware/software (hw/sw) partitioning becomes a challenging optimization problem in hw/sw co-design. a novel hw/sw partitioning method based on position disturbed particle swarm optimization with invasive weed optimization (pdpso-iwo) is presented in this paper. it is found by biologists that the ground squirrels produce alarm calls which warn their peers to move away when there is potential predatory threat. here, we present pdpso algorithm, in each iteration of which the squirrel behavior of escaping from the global worst particle can be simulated to increase population diversity and avoid local optimum. we also present new initialization and reproduction strategies to improve iwo algorithm for searching a better position, with which the global best position can be updated. then the search accuracy and the solution quality can be enhanced. pdpso and improved iwo are synthesized into one single pdpso-iwo algorithm, which can keep both searching diversification and searching intensification. furthermore, a hybrid noderank (hnoderank) algorithm is proposed to initialize the population of pdpso-iwo, and the solution quality can be enhanced further. since the hw/sw communication cost computing is the most time-consuming process for hw/sw partitioning algorithm, we adopt the gpu parallel technique to accelerate the computing. in this way, the runtime of pdpso-iwo for large-scale hw/sw partitioning problem can be reduced efficiently. finally, multiple experiments on benchmarks from state-of-the-art publications and large-scale hw/sw partitioning demonstrate that the proposed algorithm can achieve higher performance than other algorithms.
distributed_computing	this paper presents the results obtained from the implementation of an infrastructure to improve technological services of email, virtual learning environment, digital repository and virtual library at the polytechnic agricultural higher school of manabi (polytechnic school of agriculture of manabi), espam, through the use of high availability and virtualization mechanisms to provide more reliable resources. virtualization is an empowering and cutting-edge technology that is transforming the operation of technological services, but it involves a paradigm shift in service-oriented information technologies and cloud computing. to execute each of the processes the v-cycle methodology was used as a strategy. virtualization services empowers companies and institutions by transforming how they operate to be at the forefront of innovation in their services as a technological solution. so the implementation of redundant technology in the espam, has allowed its technological services are always operative, for the benefit of the university community, because if there were failures in the main system or services, the backups will be enabled quickly allowing the systems come into operation immediately.
computer_vision	this paper aims to provide a road map for future works related to reverse engineering field of expertise. reverse engineering, in a mechanical context, relates to any process working in a bottom-up fashion, namely that it goes from a lower level concept or product (closer to the final product) to a higher level one (closer to the ideation step). nowadays, the manufacturing industry is facing unprecedented increase in data exchange and data warehousing. this comes with new issues that our work will not explore, such as ""how to store these data in an efficient manner?"", ""what should be stored?"" and so on. nonetheless, this trend also creates new opportunities if we manage to integrate these data into the expertise workflows. in this paper we will cover the possibilities offered by machine learning to succeed in this challenge. we will also present a first and major step in our road map in order to achieve our research goals. we plan to design a metric to quantify how well and how precise can we perform some specific reverse engineering tasks such as detection, segmentation and classification of mechanical parts in imagery data. we aspire to open this metric; and make it freely and widely available to researchers and industry in order to compare the effectiveness, robustness and preciseness of the existing and future approaches.
software_engineering	in our studies of global software engineering (gse) teams, we found that informal, non-work-related conversations are positively associated with trust. seeking to use novel analytical techniques to more carefully investigate this phenomenon, we described these non-work-related conversations by adapting the economics literature concept of ""cheap talk,"" and studied it using evolutionary game theory (egt). more specifically, we modified the classic stag-hunt game and analyzed the dynamics in a fixed population setting (an abstraction of a gse team). doing so, we were able to demonstrate how cheap talk over the internet (e-cheap talk) was powerful enough to facilitate the emergence of trust and improve the probability of cooperation where the punishment for uncooperative behavior is comparable to the cost of the cheap talk. to validate the results of our theoretical approach, we conducted two empirical case studies that analyzed the logged irc development discussions of apache lucene and chromium os using both quantitative and qualitative methods. the results provide general support to the theoretical propositions. we discuss our findings and the theoretical and practical implications to gse collaborations and research.
distributed_computing	this paper presents a distributed prony analysis algorithm using data fusion approach. this classic approach can be found in kalman filter 's measurement update. distributed optimization algorithms, e.g., alternating direction method of multipliers (admm), suitable for constrained optimization problems, have been proposed in the previous literature to develop distributed architecture. in this article, we show that prony analysis, a non-constrained least square estimation (lse) problem, can be solved using the classic data fusion approach. compared to the iterative distributed optimization algorithms (e.g., admm and subgradient methods), data fusion takes only one step. there is no need for iteration and there is no issue related to convergence. this approach leads to a distributed prony analysis architecture which requires a much less demanding communication system (the bandwidth can be less than 0.1 hz) compared to the conventional centralized prony analysis for multiple channels which requires a bandwidth of 30 hz. the application discussed in this paper is to identify oscillation modes from real world phasor measurement unit (pmu) data and further reconstruct signals. a key technical challenge to implement prony analysis for signals from multiple channels is the difficulty to identify the noise characteristics of each channel. in this paper, a method is proposed to identify the noise covariances, which leads to the construction of a weighted least square estimation (wise) problem. this problem is solved through a distributed architecture. the effectiveness of the proposed distributed prony analysis is demonstrated through case study results. the accuracy of the estimation is improved in one order compared with the centralized prony analysis. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	online photo sharing is an increasingly popular activity for internet users. more and more users are now constantly sharing their images in various social media, from social networking sites to online communities, blogs, and content sharing sites. in this article, we present an extensive study exploring privacy and sharing needs of users' uploaded images. we develop learning models to estimate adequate privacy settings for newly uploaded images, based on carefully selected image-specific features. our study investigates both visual and textual features of images for privacy classification. we consider both basic image-specific features, commonly used for image processing, as well as more sophisticated and abstract visual features. additionally, we include a visual representation of the sentiment evoked by images. to our knowledge, sentiment has never been used in the context of image classification for privacy purposes. we identify the smallest set of features, that by themselves or combined together with others, can perform well in properly predicting the degree of sensitivity of users' images. we consider both the case of binary privacy settings (i.e., public, private), as well as the case of more complex privacy options, characterized by multiple sharing options. our results show that with few carefully selected features, one may achieve high accuracy, especially when high-quality tags are available.
computer_programming	provide opportunities for educational and scientific development, but also poses a challenge. era of traditional teaching methods cannot meet the manpower needs of new curriculum reform requires renewing teaching ideas, changing teaching methods, reflect students 'subjectivity in the study and cultivation of students' innovative spirit, formed the awareness and capability for lifelong learning. inquiry teaching to cultivate students' innovative spirit and practical ability for objective, reflect and respond to the demand for education in the era, are effective means of implementation of the new curriculum.
algorithm_design	differential evolution (de) research for multi-objective optimization can be divided into proposals that either consider de as a stand-alone algorithm, or see de as an algorithmic component that can be coupled with other algorithm components from the general evolutionary multiobjective optimization (emo) literature. contributions of the latter type have shown that de components can greatly improve the performance of existing algorithms such as nsga-ii, spea2, and ibea. however, several experimental factors have been left aside from that type of algorithm design, compromising its generality. in this work, we revisit the research on the effectiveness of de for multi-objective optimization, improving it in several ways. in particular, we conduct an iterative analysis on the algorithmic design space, considering de and environmental selection components as factors. results show a great level of interaction between algorithm components, indicating that their effectiveness depends on how they are combined. some designs present state-of-the-art performance, confirming the effectiveness of de for multi-objective optimization.
symbolic_computation	the method of multiple scales is a global perturbation technique that has resulted to be very useful in perturbed ordinary differential equations characterized by disparate time scales. the general principle behind the method is that the solution to the differential equation is uniformly expanded in terms of two or more independent variables, referred to as time scales. in this article, we present a mathematical object based on a poisson series to apply the method of multiple scales via specific symbolic computation. copyright (c) 2016 john wiley & sons, ltd.
bioinformatics	introduction the alternative lengthening of telomeres (alt) mechanism was first observed in the model organism s. cerevisiae. interestingly, this mechanism is necessary for the viability of some tumor cells. unfortunately, its molecular underpinnings are not yet completely understood. objective here, we combine carefully designed non-targeted mass spectrometry-based metabolomics experiments with a bioinformatics approach to characterize the alt positive phenotype observed in yeast at the metabolomics level. methods we profiled the metabolome using mass spectrometry in yeast strains that have lost telomerase expression, as well as that in pre-senescence and the rescued states. to dissect unwanted technical variation from biologically relevant variation between these states, we used a two-step normalization strategy, i. e., first, an empirical bayesian framework; and next, we corrected for secondorder technical effects. results our results show that alt-positive yeast strains present two different types of metabolic responses to the genetically-induced telomerase dysfunction: (i) systemic and (ii) specific. the key-difference between these responses is that the systemic response lasts even after the yeast strains have been genetically rescued, while the specific response does not. interestingly, these metabolic changes can be associated with generic stress responses (e. g., dna damage) as well as specific responses like accelerated aging of early telomerase-inactivation. conclusions a mass spectrometry-based metabolomics approach reveals two distinct types of metabolomics response to telomerase dysfunction in yeast. by identifying these changes in protein (e. g., arg7, and arg1), and metabolite (e. g., datp, and ddtp) amounts, we complement the available information on alt at the genome-wide level.
image_processing	in this paper, 3d experiments are conducted aiming at the combined investigation of the hydroelastic and the structural (connectors' internal forces) response of a pontoon-type modular floating breakwater (fb), consisting of flexibly connected, moored with chains modules, under the action of perpendicular and oblique regular waves. regarding the fb 's hydroelastic response (i.e. 3d displacements of the modules under the wave action), video recording of the horizontal and the vertical displacements of specific points on the fb relatively to some fixed points is implemented, and this response is determined through an appropriate image processing procedure. for asseasing the fb 's structural response, strain rosettes are utilized, enabling the evaluation of the connectors' forces through the corresponding strains' measurements. the effect of the incident wave characteristics (period, height, obliquity) on the fb 's hydroelastic and structural response is analyzed. the correlation between the connectors' forces and the fb 's hydroelastic response is extensively discussed. the fb 's hydroelastic and structural response depends strongly upon the wave period, while the wave height and obliquity affect this response in the low frequency range. strong dependency of the connectors' forces upon the existence or not of a fb 's intense deformed shape is demonstrated.
distributed_computing	simple object access protocol (soap) among other techniques implements web services (ws). soap offers a lightweight and simple mechanism for exchange of structured and typed information among computing devices in a decentralized, distributed computing environment. however, soap transmits data in extensible markup language (xml) format. xml documents are huge in size and verbose thus becoming a major hindrance in performance for high-performance applications that process lots of data. in this paper, we develop, implement and evaluate soap performance optimization aggregated architecture in a disadvantaged network, i.e., 10mbps bandwidth. the aggregated architecture entailed: client side caching, document-literal web services description language (wsdl) description, simple database queries on the server side and gzip compression technique. the experimental results showed a relatively high turnaround time and low network throughput. nevertheless, improved performance of soap is evident in terms of bandwidth utilization and transfer time. this can be useful in disadvantaged networks.
bioinformatics	lysine crotonylation on histones is a recently identified post-translational modification that has been demonstrated to associate with active promoters and to directly stimulate transcription. given that crotonyl-coa is essential for the acyl transfer reaction and it is a metabolic intermediate widely localized within the cell, we postulate that lysine crotonylation on nonhistone proteins could also widely exist. using specific antibody enrichment followed by high resolution mass spectrometry analysis, we identified hundreds of crotonylated proteins and lysine residues. bioinformatics analysis reveals that crotonylated proteins are particularly enriched for nuclear proteins involved in rna processing, nucleic acid metabolism, chromosome organization, and gene expression. furthermore, we demonstrate that crotonylation regulates hdac1 activity, expels hp1 alpha from heterochromatin, and inhibits cell cycle progression through s-phase. our data thus indicate that lysine crotonylation could occur in a large number of proteins and could have important regulatory roles in multiple nuclei-related cellular processes.
cryptography	the security of traditional identity-based signature (ibs) is totally built upon the assumption that the private key is absolutely secure. however, with the increasing use of mobile and unprotected devices in today 's cryptosystems, the threat of key exposure represents a more serious and realistic concern. to mitigate the damage of key exposure in the setting of ibs, we propose to integrate key evolution and user revocation into ibs, and present forward-secure identity-based signature with user revocation (fs-ribs). specifically, we formalize the syntax and security definition of fs-ribs, and give a concrete construction. the proposed scheme is proven secure in the standard model under a q-type complexity assumption. to demonstrate the merits of our scheme, we theoretically analyse its performance by comparing it with other related works. moreover, we provide an implementation and the corresponding timing results of our scheme to show its practicability.
bioinformatics	pectin degrading enzymes are essential for quality of product from cocoa fermentation. previously, we studied purified pectate lyases (pel) produced by bacillus strains from fermenting cocoa and characterized the cloned pel genes. this study aims to search for biological signals that modulates pel production and regulators that influence pel gene expression. strains were grown to the end of exponential phase in media containing various carbon sources. pel enzymes production in bacillus was unaffected by simple sugar content variation up to 2%. additionally, it appeared that pel gene is not under the control of the most common carbon and pectin catabolism regulators ccpa and kdgr, which could explain the insensitivity of pel production to carbon source variation. however, a 6-fold decrease in pel production was observed when bacteria were grown in lb rich medium as opposed to basal mineral medium. subsequently, bioinformatics analysis of cloned pel gene promoter region revealed the presence of degu binding site. furthermore, the deletion of degu gene dramatically reduces the pel gene expression, as revealed by real time quantitative pcr, showing an activation effect of degu on pel synthesis in bacillus strains studied. we assumed that, during the latter stage of cocoa fermentation when simple sugars are depleted, production of pel in bacillus is stimulated by degu to supply microbial cells with carbon source from polymeric pectic compounds. (c) 2016 elsevier ltd. all rights reserved.
data_structures	with existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. the alternative is to prototype simulations in a high-level language like matlab, thereby sacrificing performance. the matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. however, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and to the machine. high-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. in this article, we present simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph and as a set of global vectors, matrices, and tensors depending on what is convenient at any given time. simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. we demonstrate that simit is easy to use: a simit program is typically shorter than a matlab program; that it is high performance: a simit program running sequentially on a cpu performs comparably to hand-optimized simulations; and that it is portable: simit programs can be compiled for gpus with no change to the program, delivering 4 to 20x speedups over our optimized cpu code.
distributed_computing	with advances in technology, frequent pattern mining has been used widely in our daily lives. by using this technology, one can obtain interesting or useful information that would help one make decisions and apply judgment. for example, marketplace managers mine transaction data to obtain information that can help improve services, understand customer buying habits, determine a suitable scheme for placement of goods to increase profits, or for medical and biotechnology applications. however, the rate at which data is generated is very rapid, leading to problems caused by big data. therefore, many researchers have studied distributed, parallel and cloud computing technology to select the best among them. however, data mining uses multiple computing nodes, which requires the transmission of a considerable amount of data in a network environment. the available network bandwidth is limited when many different tasks are being transmitted at the same time and many servers are working in the same network segment. this results in poor transmission, causing severe transfer delay, either internal or external to the network. thus, we propose the fast and distributed mining algorithm for discovering frequent patterns in congested networks (fdmcn) algorithm, which is based on carm. the main purpose is to reduce fp-tree transmission such that only a portion of the information is required for mining using computing nodes. the results of empirical evaluation under various simulation conditions show that the proposed method fdmcn delivers excellent performance in terms of execution efficiency and scalability when compared with the psws algorithm.
bioinformatics	introduction: casein201 is one of the human milk sourced peptides that differed significantly in preterm and full-term mothers. this study is designed to demonstrate the biological characteristics, antibacterial activity and mechanisms of casein201 against common pathogens in neonatal infection. methodology: the analysis of biological characteristics was done by bioinformatics. disk diffusion method and flow cytometry were used to detect the antimicrobial activity of casein201. killing kinetics of casein201 was measured using microplate reader. the antimicrobial mechanism of casein201 was studied by electron microscopy and electrophoresis. results: bioinformatics analysis indicates that casein201 derived from 0-casein and showed significant sequence overlap. antibacterial assays showed casein201 inhibited the growth of s taphylococcus aureus and y ersinia enterocolitica. ultrastructural analyses revealed that the antibacterial activity of casein201 is through cytoplasmic structures disintegration and bacterial cell envelope alterations but not combination with dna. conclusion: we conclude the antimicrobial activity and mechanism of casein201. our data demonstrate that casein201 has potential therapeutic value for the prevention and treatment of pathogens in neonatal infection. (c) 2017 elsevier inc. all rights reserved.
symbolic_computation	this paper investigates the integrability of a generalized seventh-order korteweg-de vries equation arising in fluids and plasmas. by means of singularity structure analysis, it is proven that this equation passes the painleve test for integrability in only three distinct cases. under three sets of painleve integrable conditions, the soliton solutions are obtained by using hirota 's bilinear method; the pseudopotentials and lax pairs are derived by virtue of the method developed by nucci. finally, the infinite conservation laws are found by using its lax pair, and all conserved densities and fluxes are presented with explicit recursion formulas.
cryptography	in this paper, a dynamical and adaptive ldpc coding scheme is proposed in order to improve the performance of the cryptographic key distillation protocol of an fso/cv-qkd system considering the atmospheric turbulence levels that may be present in the classic channel. in this scheme, the generator and parity-check matrices of the encoder are modified according to the rytov variance values estimated in the classical channel in order to improve the final secret key rate of the qkd system. the simulation results show that the final secret key was incremented 87.5 kbps (from 52.5 kbps to 140 kbps) using the adaptive code rate; meaning that the information encrypted and transmitted is increased. in addition, the use of the dynamical encoder avoids the drastically reduction of the final secret key rate when the conditions of the classical channel are considered. our proposal might be implemented based on the use of high-speed fpga 's and dsp 's commercially available.
parallel_computing	image denoising is one of the fundamental and essential tasks within image processing. in medical imaging, finding an effective algorithm that can remove random noise in mr images is important. this paper proposes an effective noise reduction method for brain magnetic resonance (mr) images. our approach is based on the collateral filter which is a more powerful method than the bilateral filter in many cases. however, the computation of the collateral filter algorithm is quite time-consuming. to solve this problem, we improved the collateral filter algorithm with parallel computing using gpu. we adopted cuda, an application programming interface for gpu by nvidia, to accelerate the computation. our experimental evaluation on an intel r xeon r cpu e5-2620 v3 2.40ghz with a nvidia tesla k40c gpu indicated that the proposed implementation runs dramatically faster than the traditional collateral filter. we believe that the proposed framework has established a general blueprint for achieving fast and robust filtering in a wide variety of medical image denoising applications.
distributed_computing	the equality problem is usually one 's first encounter with communication complexity and is one of the most fundamental problems in the field. although its deterministic and randomized communication complexity were settled decades ago, we find several new things to say about the problem by focusing on three subtle aspects. the first is to consider the expected communication cost (at a worst-case input) for a protocol that uses limited interaction-i.e., a bounded number of rounds of communication-and whose error probability is zero or close to it. the second is to treat the false negative error rate separately from the false positive error rate. the third is to consider the information cost of such protocols. we obtain asymptotically optimal rounds-versus-cost tradeoffs for equality: both expected communication complexity and information complexity scale as , where r is the number of rounds and , with k logs. these bounds hold even when the false negative rate approaches 1. for the case of zero-error communication cost, we obtain essentially matching bounds, up to a tiny additive constant. we also provide some applications. as an application of our information cost bounds, we obtain new bounded-round randomized lower bounds for the intersection problem, in which there are two players who hold subsets . in many realistic scenarios, the sizes of s and t are significantly smaller than n, so we impose the constraint that . we study the minimum number of bits the parties need to communicate in order to compute the entire intersection set , using r rounds. we show that any r-round protocol has information cost (and thus communication cost) bits. we also give an o(r)-round protocol achieving bits, which for gives a protocol with o(k) bits of communication. this is in contrast to other basic problems such as computing the union or symmetric difference, for which bits of communication is required for any number of rounds.
computer_graphics	prior to having available the high dynamic range (hdr) techniques, certain levels of luminance could only by captured by the human eye. currently, hdr technology overcomes the limitations of conventional imaging technology (also referred as low dynamic range or ldr) and allows the capture and delivery of contents that can match the dynamic range of the real world. however, the state of the art of hdr video focus mainly on conventional sized displays typical of tvs or pcs. as the usage of mobile devices for multimedia consumption is increasing considerably, there is a need for studying the impact of the viewing of hdr video on such devices. this will allow to take full advantage of hdr technology, creating a set of opportunities for the digital business as it allows, among others, the presentation of products in a much more efficient and captivating way. on this paper it is presented a study that evaluates the impact of the hdr video delivery on mobile devices and compares it with the impact of low dynamic range (ldr) content. results show that the delivery of hdr video on mobiles is possible without requiring much more resources when comparing the delivery of ldr video.
relational_databases	large, multi-institutional groups or collaborations of scientists are engaged in nuclear physics research projects, and the number of research facilities is dwindling. these collaborations have their own authorship rules, and they produce a large number of highly-cited papers. multiple authorship of nuclear physics publications creates a problem with the assessment of an individual author 's productivity relative to his/her colleagues and renders ineffective a performance metrics solely based on annual publication and citation counts. many institutions are increasingly relying on the total number of first-author papers; however, this approach becomes counterproductive for large research collaborations with an alphabetical order of authors. a concept of fractional authorship (the claiming of credit for authorship by more than one individual) helps to clarify this issue by providing a more complete picture of research activities. in the present work, nuclear physics fractional and total authorships have been investigated using nuclear data mining techniques. historic total and fractional authorship averages have been extracted from the nuclear science references database, and the current range of fractional contributions has been deduced. the results of this study and their implications are discussed and conclusions presented.
data_structures	a correspondence is a set of mappings that establishes a relation between the elements of two data structures (i.e. sets of points, strings, trees or graphs). if we consider several correspondences between the same two structures, one option to define a representative of them is through the generalised median correspondence. in general, the computation of the generalised median is an np-complete task. in this paper, we present two methods to calculate the generalised median correspondence of multiple correspondences. the first one obtains the optimal solution in cubic time, but it is restricted to the hamming distance. the second one obtains a sub-optimal solution through an iterative approach, but does not have any restrictions with respect to the used distance. we compare both proposals in terms of the distance to the true generalised median and runtime.
operating_systems	optimal supply of trace elements (te) is a prerequisite for microbial growth and activity in anaerobic digestion (ad) bioprocesses. however, the required concentrations and ratios of essential te for ad biotechnologies strongly depend on prevailing operating conditions as well as feedstock composition. furthermore, te in ad bioreactors undergo complex physicochemical reactions and may be present as free ions, complex bound or as precipitates depending on ph, or on the presence of sulfur compounds or organic macromolecules. to overcome te deficiency, various commercial mineral products are typically applied to ad processes. the addition of heavy metals poses the risk of overdosing operating systems, which may be toxic to microbial consortia and ultimately the environment. adequate supplementation, therefore, requires appropriate knowledge not only about the composition, but also on the speciation and bioavailability of te. however, very little is yet fully understood on this specific issue. evaluations of te typically only include the measurement of total te concentrations but do not consider the chemical forms in which te exist. thus detailed information on bioavailability and potential toxicity cannot be provided. this review provides an overview of the state of the art in approaches to determine bioavailable te in anaerobic bioprocesses, including sequential fractionation and speciation techniques. critical aspects and considerations, including with respect to sampling and analytical procedures, as well as mathematical modeling, are examined. the approaches discussed in this review are based on our experiences and on previously published studies in the context of the cost action 1302: european network on ecological roles of trace metals in anaerobic biotechnologies.
computer_graphics	since conventional evacuation drills do not adequately simulate disaster situations, participants do not feel a sense of tension during evacuation. we developed a game based evacuation drill (gbed) system that focuses on situational and audio-visual realities and scenario-based interactivity. to improve the visual reality in a gbed, we adopt simple augmented reality (ar) and a binocular opaque head-mounted display (hmd). the simple ar represents vague extensive disaster situations (i.e., rain, fog, smoke and fire) by superimposing the overall disaster situations (dynamic three-dimensional computer graphics) onto the real-time vision captured by a stereo camera (attached to the hmd).
computer_vision	many computer vision applications require finding corresponding points between images and using the corresponding points to estimate disparity. today 's correspondence finding algorithms primarily use image features or pixel intensities common between image pairs. some 3-d computer vision applications, however, do not produce the desired results using correspondences derived from image features or pixel intensities. two examples are the multimodal camera rig and the center region of a coaxial camera rig. we present an image correspondence finding technique that aligns pairs of image sequences using optical flow fields. the optical flow fields provide information about the structure and motion of the scene, which are not available in still images but can be used in image alignment. we apply the technique to a dual focal length stereo camera rig consisting of a visible light-infrared camera pair and to a coaxial camera rig. we test our method on real image sequences and compare our results with the state-of-the-art multimodal and structure from motion (sfm) algorithms. our method produces more accurate depth and scene velocity reconstruction estimates than the state-of-the-art multimodal and sfm algorithms. (c) 2017 spie and is&t
distributed_computing	distributed computing is a good alternative to expensive supercomputers. there are plenty of frameworks that enable programmers to harvest remote computing power. however, until today, much computation power in the edges of the internet remains unused. while idle devices could contribute to a distributed environment as generic computation resources, computation-intense applications could use this pool of resources to enhance their execution quality. in this paper, we identify heterogeneity as a major burden for distributed and edge computing. heterogeneity is present in multiple forms. we draw our vision of a comprehensive distributed computing system and show where existing frameworks fall short in dealing with the heterogeneity of distributed computing. afterwards, we present the tasklet system, our approach for a distributed computing framework. tasklets are fine-grained computation units that can be issued for remote and local execution. we tackle the different dimensions of heterogeneity and show how to make use of available computation power in edge resources. in our prototype, we use middleware and virtualization technologies as well as a host language concept.
parallel_computing	the rapid development of the latest distributed computing paradigm, i. e., cloud computing, generates a highly fragmented cloud market composed of numerous cloud providers and offers tremendous parallel computing ability to handle big data problems. one of the biggest challenges in multiclouds is efficient workflow scheduling. although the workflow scheduling problem has been studied extensively, there are still very few primal works tailored for multicloud environments. moreover, the existing research works either fail to satisfy the quality of service (qos) requirements, or do not consider some fundamental features of cloud computing such as heterogeneity and elasticity of computing resources. in this paper, a scheduling algorithm, which is called multiclouds partial critical paths with pretreatment (mcpcpp), for big data workflows in multiclouds is presented. this algorithm incorporates the concept of partial critical paths, and aims to minimize the execution cost of workflow while satisfying the defined deadline constraint. our approach takes into consideration the essential characteristics of multiclouds such as the charge per time interval, various instance types from different cloud providers, as well as homogeneous intrabandwidth vs. heterogeneous interbandwidth. various types of workflows are used for evaluation purpose and our experimental results show that the mcpcpp is promising.
machine_learning	explicit prediction of the suspended sediment loads in rivers or streams is very crucial for sustainable water resources and environmental systems. suspended sediments are a governing factor for the design and operation of hydraulic structures, like canals, diversions and dams. in recent decades, to model hydrological phenomena which are complex in nature the machine learning models are used commonly. in the present study, support vector machine (svm) with wavelet transform (wasvm) has been employed for prediction of daily suspended sediment load (sl) for two south indian watersheds (marol and muneru) using hydrometeorological data. a 40-year daily observed data (1972-2011) have been used for the analysis, where past sl, streamflow (q), and rainfall (r) data were used as the model inputs, and sl was the model output. using conventional correlation coefficient analysis between input and output variables, the best input of wasvm model was identified. the reliability of svm and wasvm models were evaluated on the basis of different performance criteria, i.e.,coefficient of determination (r2), root mean square error (rmse), normalized mean square error (nmse), and nash-sutcliffe coefficient (ns). initially, 1-day ahead sl prediction was performed using the best wasvm model. the results showed that, 1-day predictions were very precise, showing a close agreement with the observed sl data (r2=0.94, ns=0.94 for the marol watershed, and r2=0.77, ns=0.77 for the muneru watershed) in the testing period. the same wasvm model was then used for the prediction of sl for the higher lead periods. the nmse value for the marol watershed was found as low as 0.06 for 1-day ahead prediction, and increases subsequently as 0.29, 0.46, and 0.70 for 3-, 6-, and 9-day higher leads, respectively. likewise, for the muneru watershed, the nmse value was found as low as 0.21 for 1-day ahead prediction, and increases subsequently as 0.42, 0.53, and 0.68 for 3-, 6-, and 9-day higher leads, respectively. further, the model was evaluated on the basis of its capability of predicting peak sl and cumulative sl for 1- to 6-day leads. the statistical analysis shows that the developed wasvm model can predict the target value successfully up to a 6-day lead and is not suitable for higher lead specifically in the selected watersheds having similar hydroclimatic conditions like the ones selected in this study. predictions by the wasvm model were found significantly superior to the ones obtained by the conventional svm model. the results revealed that the wasvm model provides a very good accuracy in predicting sl and can be used as an effective forecasting tool for hydrological applications.
symbolic_computation	the purpose of this paper is to develop constructive versions of stafford 's theorems on the module structure of weyl algebras a (n) (k) (i.e., the rings of partial differential operators with polynomial coefficients) over a base field k of characteristic zero. more generally, based on results of stafford and coutinho-holland, we develop constructive versions of stafford 's theorems for very simple domains d. the algorithmization is based on the fact that certain inhomogeneous quadratic equations admit solutions in a very simple domain. we show how to explicitly compute a unimodular element of a finitely generated left d-module of rank at least two. this result is used to constructively decompose any finitely generated left d-module into a direct sum of a free left d-module and a left d-module of rank at most one. if the latter is torsion-free, then we explicitly show that it is isomorphic to a left ideal of d which can be generated by two elements. then, we give an algorithm which reduces the number of generators of a finitely presented left d-module with module of relations of rank at least two. in particular, any finitely generated torsion left d-module can be generated by two elements and is the homomorphic image of a projective ideal whose construction is explicitly given. moreover, a non-torsion but non-free left d-module of rank r can be generated by r+1 elements but no fewer. these results are implemented in the stafford package for d=a (n) (k) and their system-theoretical interpretations are given within a d-module approach. finally, we prove that the above results also hold for the ring of ordinary differential operators with either formal power series or locally convergent power series coefficients and, using a result of caro-levcovitz, also for the ring of partial differential operators with coefficients in the field of fractions of the ring of formal power series or of the ring of locally convergent power series.
parallel_computing	visual feature learning, which aims to construct an effective feature representation for visual data, has a wide range of applications in computer vision. it is often posed as a problem of nonnegative matrix factorization (nmf), which constructs a linear representation for the data. although nmf is typically parallelized for efficiency, traditional parallelization methods suffer from either an expensive computation or a high runtime memory usage. to alleviate this problem, we propose a parallel nmf method called alternating least square block decomposition (alsd), which efficiently solves a set of conditionally independent optimization subproblems based on a highly parallelized fine-grained grid-based blockwise matrix decomposition. by assigning each block optimization subproblem to an individual computing node, alsd can be effectively implemented in a mapreduce-based hadoop framework. in order to cope with dynamically varying visual data, we further present an incremental version of alsd, which is able to incrementally update the nmf solution with a low computational cost. experimental results demonstrate the efficiency and scalability of the proposed methods as well as their applications to image clustering and image retrieval.
computer_graphics	we present preliminary results of work on a low-cost multi-user immersive virtual reality system that enables collaborative experiences in large virtual environments. in the proposed setup at least three users can walk and interact freely and untethered in a 200 m(2) area. the required equipment is worn on the body and rendering is performed locally on each user to minimize latency. inside-out optical head tracking is coupled with a low-cost motion capture suit to track the full body and the head. movements of users, 3d interactions and the positions of selected real world objects are distributed over a wireless network in a server-client architecture. as a result, users see the effect of their interactions with objects and other users in real time. we describe the architecture of our implemented proof-of-concept system.
machine_learning	the quest to observe gravitational waves challenges our ability to discriminate signals from detector noise. this issue is especially relevant for transient gravitational waves searches with a robust eyes wide open approach, the so called all-sky burst searches. here we show how signal classification methods inspired by broad astrophysical characteristics can be implemented in all-sky burst searches preserving their generality. in our case study, we apply a multivariate analyses based on artificial neural networks to classify waves emitted in compact binary coalescences. we enhance by orders of magnitude the significance of signals belonging to this broad astrophysical class against the noise background. alternatively, at a given level of mis-classification of noise events, we can detect about 1/4 more of the total signal population. we also show that a more general strategy of signal classification can actually be performed, by testing the ability of artificial neural networks in discriminating different signal classes. the possible impact on future observations by the ligo-virgo network of detectors is discussed by analysing recoloured noise from previous ligo-virgo data with coherent waveburst, one of the flagship pipelines dedicated to all-sky searches for transient gravitational waves.
algorithm_design	in their celebrated paper (furst et al., math. syst. theory 17(1), 13-27 (12)), furst, saxe, and sipser used random restrictions to reveal the weakness of boolean circuits of bounded depth, establishing that constant-depth and polynomial-size circuits cannot compute the parity function. such local restrictions have played important roles and have found many applications in complexity analysis and algorithm design over the past three decades. in this article, we give a brief overview of two intriguing applications of local restrictions: the first one is for the isomorphism conjecture and the second one is for moderately exponential time algorithms for the boolean formula satisfiability problem.
computer_graphics	many real world information can be represented by a graph with a set of nodes interconnected with each other by multiple type of relations called edge layers (e.g., social network, biological data). edge bundling techniques have been proposed to solve cluttering issue for standard graphs while few efforts were done to deal with the similar issue for multilayer graphs. in multilayer graphs scenario, not only the clutter induced by large amount of edges is a problem but also the fact that different type of edges can overlap each other making useless the final visualization. in this paper we introduce a new multilayer graph edge bundling technique that firstly produces a preliminary edge bundling independently of the different edge layers and then deals with the specificity of multilayer graphs where more than one type of edges can be routed on the same bundle. the proposed visualization is tested on a real world case study and the outcomes point out the ability of our proposal to discover patterns present in the data.
software_engineering	three key drivers of change in the world of software are identified, and their impact on a range of software engineering research is assessed. the rise of agile approaches to software development adds further pressure for software process research to develop and evolve processes that are supportive of change while meeting the most stringent needs of society. two examples of successful process evolution, aimed at highly regulated industries, are outlined as pointers to the future direction of software process research. copyright (c) 2015 john wiley & sons, ltd.
parallel_computing	the deployment of smart grids and renewable energy dispatch centers motivates the development of forecasting techniques that take advantage of near real-time measurements collected from geographically distributed sensors. this paper describes a forecasting methodology that explores a set of different sparse structures for the vector autoregression (var) model using the least absolute shrinkage and selection operator (lasso) framework. the alternating direction method of multipliers is applied to fit the different lasso-var variants and create a scalable forecasting method supported by parallel computing and fast convergence, which can be used by system operators and renewable power plant operators. a test case with 66 wind power plants is used to show the improvement in forecasting skill from exploring distributed sparse structures. the proposed solution outperformed the conventional autoregressive and vector autoregressive models, as well as a sparse var model from the state of the art. copyright (c) 2016 john wiley & sons, ltd.
software_engineering	software defect prediction predicts fault-prone modules which will be tested thoroughly. thereby, limited quality control resources can be allocated effectively on them. without sufficient local data, defects can be predicted via cross-project defect prediction (cpdp) utilizing data from other projects to build a classifier. software defect datasets have the class imbalance problem, indicating the defect class has much fewer instances than the non-defect class does. unless defect instances are predicted correctly, software quality could be degraded. in this context, a classifier requires to provide high accuracy of the defect class without severely worsening the accuracy of the non-defect class. this class imbalance principle seamlessly connects to the purpose of the multi-objective (mo) optimization in that mo predictive models aim at balancing many of the competing objectives. in this paper, we target to identify effective multi-objective learning techniques under cross-project (cp) environments. three objectives are devised considering the class imbalance context. the first objective is to maximize the probability of detection (pd). the second objective is to minimize the probability of false alarm (pf). the third objective is to maximize the overall performance (e.g., balance). we propose novel mo naive bayes learning techniques modeled by a harmony search meta-heuristic algorithm. our approaches are compared with single-objective models, other existing mo models and within-project defect prediction models. the experimental results show that the proposed approaches are promising. as a result, they can be effectively applied to satisfy various prediction needs under cp settings. (c) 2016 elsevier b.v. all rights reserved.
network_security	with the rapid development of mobile internet, people pay increasing attention to the wireless network security problem. but due to the specificity of the wireless network, at present it is rare to see the research of wireless intrusion alerts clustering method for mobile internet. this paper proposes a wireless intrusion alert clustering method (wiacm) based on the information of the mobile terminal. the method includes alert formatting, alert reduction and alert classification. by introducing key information of the mobile terminal device, this method aggregates the original alerts into hyper alerts. the experimental results show that wiacm would be appropriate for real attack scenarios of mobile internet, and reduce the amount of alerts with more accuracy of alert analysis.
symbolic_computation	in this paper a generalized fractional modified korteweg-de vries (fmkdv) equation with time-dependent variable coefficients, which is a generalized model in nonlinear lattice, plasma physics and ocean dynamics, is investigated. with the aid of a simplified bilinear method, fractional transforms and symbolic computation, the corresponding n-soliton solutions are given and illustrated. the characteristic line method and graphical analysis are applied to discuss the solitonic propagation and collision, including the bidirectional solitons and elastic interactions. finally, the resonance phenomenon for the equation is examined.
distributed_computing	joint service involving several clouds is an emerging form of cloud computing. in hybrid clouds, the schedulers within 1 cloud must not only self-adapt to the job arrival processes and the workload but also mutually adapt to the scheduling polices of other schedulers. however, as a combinatorial optimization problem, scheduling is challenged by the adaptation to those dynamics and uncertain behaviors of the peers. this article studies the collaboration among benevolent clouds that are cooperative in nature and willing to accept jobs from other clouds. we take advantage of machine learning and propose a distributed scheduling mechanism to learn the knowledge of job model, resource performance, and others' policies. without explicit modeling and prediction, machine learning guides scheduling decisions based on experiences. to examine the performance of our approach, we conducted simulation using the sp2 job workload log of the san diego supercomputer center under a test bed based on agent-based systems-swarm. the results validate that our approach has much shorter mean response time than 5 typical dynamic scheduling algorithms-opportunistic load balancing, minimum execution time, minimum completion time, switching algorithm, and k-percent best. a better collaboration in hybrid cloud is achieved by full adaptation.
bioinformatics	satsuma myomphala is critically endangered through loss of natural habitats, predation by natural enemies, and indiscriminate collection. it is a protected species in korea but lacks genomic resources for an understanding of varied functional processes attributable to evolutionary success under natural habitats. for assessing the genetic information of s. myomphala, we performed for the first time, de novo transcriptome sequencing and functional annotation of expressed sequences using illumina next-generation sequencing (ngs) platform and bioinformatics analysis. we identified 103,774 unigenes of which 37,959, 12,890, and 17,699 were annotated in the panm (protostome db), unigene, and cog (clusters of orthologous groups) databases, respectively. in addition, 14,451 unigenes were predicted under gene ontology functional categories, with 4581 assigned to a single category. furthermore, 3369 sequences with 646 having enzyme commission (ec) numbers were mapped to 122 pathways in the kyoto encyclopedia of genes and genomes pathway database. the prominent protein domains included the zinc finger (c2h2-like), reverse transcriptase, thioredoxin-like fold, and rna recognition motif domain. many unigenes with homology to immunity, defense, and reproduction-related genes were screened in the transcriptome. we also detected 3120 putative simple sequence repeats (ssrs) encompassing dinucleotide to hexanucleotide repeat motifs from >1 kb unigene sequences. a list of pcr primers of ssr loci have been identified to study the genetic polymorphisms. the transcriptome data represents a valuable resource for further investigations on the species genome structure and biology. the unigenes information and microsatellites would provide an indispensable tool for conservation of the species in natural and adaptive environments. (c) 2016 the authors. published by elsevier inc.
parallel_computing	the investigation of solar-like oscillations for probing star interiors has enjoyed a tremendous growth in the last decade. once observations are over, the most notable difficulties in properly identifying the true oscillation frequencies of stars are due to the gaps in the observation time-series and the intrinsic stellar granulation noise. this paper presents an innovative neuro-wavelet reconstructor for the missing data of photometric signals. firstly, gathered data are transformed using wavelet operators and filters, and this operation removes granulation noise, then we predict missing data by a composite of two neural networks, which together allow a ""forward and backward"" reconstruction. this resulting error is greatly lower than the absolute a priori measurement error. the devised reconstruction approach gives a signal that is better suited to be fourier transformed when compared with other existing methods. (c) 2016 elsevier ltd. all rights reserved.
computer_vision	malaria in human is a serious and fatal tropical disease. this disease results from anopheles mosquitoes that are infected by plasmodium species. the clinical diagnosis of malaria based on the history, symptoms and clinical findings must always be confirmed by laboratory diagnosis. laboratory diagnosis of malaria involves identification of malaria parasite or its antigen / products in the blood of the patient. manual diagnosis of malaria parasite by the pathologists has proven to become cumbersome. therefore, there is a need of automatic, efficient and accurate identification of malaria parasite. in this paper, we proposed a computer vision based approach to identify the malaria parasite from light microscopy images. this research deals with the challenges involved in the automatic detection of malaria parasite tissues. our proposed method is based on the pixel-based approach. we used k-means clustering (unsupervised approach) for the segmentation to identify malaria parasite tissues.
software_engineering	context: software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. however, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. objective: in this paper, we propose a novel approach called libfinder to prevent missed reuse opportunities during software maintenance and evolution. the goal is to provide a decision support for developers to easily find ""useful"" third-party libraries to the implementation of their software systems. method: to this end, we used the non-dominated sorting genetic algorithm (nsga-ii), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. results: we evaluated our approach on 6083 different libraries from maven central super repository that were used by 32,760 client systems obtained from github super repository. our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial java systems with developers. results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. conclusion: this study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique. (c) 2016 elsevier by. all rights reserved.
operating_systems	heap-based priority queues are very common dynamical data structures used in several fields, ranging from operating systems to scientific applications. however, the rise of new multicore cpus introduced new challenges in the process of design of these data structures: in addition to traditional requirements like correctness and progress, the scalability is of paramount importance. it is a common opinion that these two demands are partially in conflict each other, so that in these computational environments it is necessary to relax the requirements of correctness and linearizability to achieve high performances. in this paper we introduce a loosely coordinated approach for the management of heap based priority queues on multicore cpus, with the aim to realize a tradeoff between efficiency and sequential correctness. the approach is based on a sharing of information among only a small number of cores, so that to improve performance without completely losing the features of the data structure. the results obtained on a scientific problem show significant benefits both in terms of parallel efficiency, as well as in term of numerical accuracy.
computer_vision	integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently. in this survey, we provide a comprehensive introduction of the integration of computer vision and natural language processing in multimedia and robotics applications with more than 200 key references. the tasks that we survey include visual attributes, image captioning, video captioning, visual question answering, visual retrieval, human-robot interaction, robotic actions, and robot navigation. we also emphasize strategies to integrate computer vision and natural language processing models as a unified theme of distributional semantics. we make an analog of distributional semantics in computer vision and natural language processing as image embedding and word embedding, respectively. we also present a unified view for the field and propose possible future directions.
parallel_computing	background: population structure inference using the software structure has become an integral part of population genetic studies covering a broad spectrum of taxa including humans. the ever- expanding size of genetic data sets poses computational challenges for this analysis. although at least one tool currently implements parallel computing to reduce computational overload of this analysis, it does not fully automate the use of replicate structure analysis runs required for downstream inference of optimal k. there is pressing need for a tool that can deploy population structure analysis on high performance computing clusters. results: we present an updated version of the popular python program strauto, to streamline population structure analysis using parallel computing. strauto implements a pipeline that combines structure analysis with the evanno delta k analysis and visualization of results using structure harvester. using benchmarking tests, we demonstrate that strauto significantly reduces the computational time needed to perform iterative structure analysis by distributing runs over two or more processors. conclusion: strauto is the first tool to integrate structure analysis with post- processing using a pipeline approach in addition to implementing parallel computation - a set up ideal for deployment on computing clusters. strauto is distributed under the gnu gpl (general public license) and available to download from http://strauto.popgen.org.
parallel_computing	given a graph g and a non-negative integer h, the h-restricted connectivity of g, denoted by kappa(h)(g), is defined as the minimum size of a set x of nodes in g (x subset of v (g)) such that g - x is disconnected, and the degree of each component in g - x is at least h. the h-restricted connectivity measure is a generalization of the traditional connectivity measure, and it improves the connectivity measurement accuracy. moreover, studies have revealed that if a network possesses a restricted connectivity property, it is more reliable and demonstrates a lower node failure rate compared with other networks. the n-dimensional locally twisted cube ltq(n), which is a well-known interconnection network for parallel computing, is a variant of the hypercube q(n). most studies have examined the h-restricted connectivity of networks under the conditions of h = 1 or h = 2. this paper examines a generalized h-restricted connectivity measure for n-dimensional locally twisted cube and reveals that kappa(h)(ltq(n)) = 2(h)(n - h) for 0 <= h <= n - 2. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	voltage scaling is a fundamental technique in the energy efficient computing field. recent studies tackling this topic show degraded system reliability as frequency scales. to address this conflict, the subject of reliability aware power management (rapm) has been extensively explored and is still under investigation. heterogeneous computing systems (hcs) provide high performance potential which attracts researchers to consider these systems. unfortunately, the existing scheduling algorithms for precedence constrained tasks with shared deadline in hcs do not adequately consider reliability conservation. in this study, we design joint optimization schemes of energy efficiency and system reliability for directed acyclic graph (dag) by adopting the shared recovery technique, which can achieve high system reliability and noticeable energy preservation. to the best of our knowledge, this is the first time to address the problem in hcs. the extensive comparative evaluation studies for both randomly generated and some real-world applications graphs show that our scheduling algorithms are compelling in terms of enhancement of both system reliability and energy saving. (c) 2015 elsevier ltd. all rights reserved.
cryptography	recently, wei et al. propose a 2-out-of-2 sharing digital image scheme (sdis) that shares a color secret image into two shadow images based on boolean exclusive-or operation. there are three types of shadow images for wei et al. 's sdis: noise-like, black-and-white meaningful, and color meaningful shadow images. however, there exist some wealmesses in wei et al. 's sdis: the incorrect assignment of color palette data for the color index 255, the erroneous recovery in secret image, and the partial region in shadow image revealing the cover image. in this paper, we solve the weaknesses and propose a new sdis. experimental results demonstrate that our scheme effectively avoids these weaknesses.
network_security	computer network vulnerability analysis is a method of analysis and evaluation of network security beforehand. the attacks method has occurred in the network, the previous network status change as input information, calculated by the model analysis. forecasting network node may be network attacks given the current security level value network, network security reinforcement measures taken before the danger. administrators can proactively identify network security issues, to take measures in advance to avoid information leakage, financial losses, ensure the safety of individuals and countries. therefore, vulnerability analysis computer network is very important. based on the properties of attack graph shows the method of attack graphs to bayesian network transformation, using the new algorithm to eliminate loops attribute attack graph optimization, building the bayesian attribute attack graph model used to evaluate the network itself security situation. in this model, based on bayes formula for calculating the probability of a new node probability calculation formula and attack paths occur for calculating network vulnerability assessment of the quantitative indicators. the model not only can visually process description of cyber attacks, but also into the bayesian network probabilistic thinking of possible network attack path prediction and assessment.
symbolic_computation	building fractional mathematical models for specific phenomena and developing numerical or analytical solutions for these fractional mathematical models are crucial issues in mathematics, physics, and engineering. in this work, a new analytical technique for constructing and predicting solitary pattern solutions of time-fractional dispersive partial differential equations is proposed based on the generalized taylor series formula and residual error function. the new approach provides solutions in the form of a rapidly convergent series with easily computable components using symbolic computation software. for method evaluation and validation, the proposed technique was applied to three different models and compared with some of the well-known methods. the resultant simulations clearly demonstrate the superiority and potentiality of the proposed technique in terms of the quality performance and accuracy of substructure preservation in the construct, as well as the prediction of solitary pattern solutions for time-fractional dispersive partial differential equations. (c) 2014 elsevier inc. all rights reserved.
symbolic_computation	in euclidean plane geometry, the evolute of both b-spline and nurbs curves are nurbs curves. moreover, this evolute can be computed symbolically. this article extends those results to the equiaffine plane geometry. analogously to euclidean geometry, the equi-affine evolute of both b-spline and nurbs curves are nurbs curves and an algorithm for the symbolic computation is given for b-spline curves. this results in a new method to analyze the global affine differential properties of b-spline curves and assess b-spline curve quality in an affine invariant context.
image_processing	in remote sensing image processing, the traditional fusion algorithm is based on the intensity-hue-saturation (ihs) transformation. this method does not take into account the texture or spectrum information, spatial resolution and statistical information of the photos adequately, which leads to spectrum distortion of the image. although traditional solutions in such application combine manifold methods, the fusion procedure is rather complicated and not suitable for practical operation. in this paper, an improved ihs transformation fusion algorithm based on the local variance weighting scheme is proposed for remote sensing images. in our proposal, firstly, the local variance of the spot (which comes from french ""systeme probatoire d'observation dela tarre"" and means ""earth observing system"") image is calculated by using different sliding windows. the optimal window size is then selected with the images being normalized with the optimal window local variance. secondly, the power exponent is chosen as the mapping function, and the local variance is used to obtain the weight of the i component and match spot images. then we obtain the i'component with the weight, the i component and the matched spot images. finally, the final fusion image is obtained by the inverse intensity-hue-saturation transformation of the i', h and s components. the proposed algorithm has been tested and compared with some other image fusion methods well known in the literature. simulation result indicates that the proposed algorithm could obtain a superior fused image based on quantitative fusion evaluation indices.
computer_programming	many of the students in our classrooms belong to the gamer generation. because of the success of digital games as entertainment products as well as formidable motivators, the possibility of using them in educational settings is being contemplated from a while. in this research we try to identify a set of digital games with the potential to be used to design learning activities, specifically to learn computer programming concepts like: algorithms, variables, and control structures. based on the contents and sequencing of learning, two games were chosen and incorporated into the activities of a workshop on introduction to programming in order to analyze their usefulness. the workshop was designed on the moodle platform using a gamification approach. here, we present a selection of serious games focused on computer language programming, the selection criteria of two of them and their use in a workshop for incoming students, as well as some of the results obtained from this experience.
symbolic_computation	a improvement of the expansion methods namely the improved tan(phi(xi)/2)-expansion method for solving the tzitzeica type nonlinear evolution equations is proposed. in this work, the dispersive optical solitons that are governed by the tzitzeica type nonlinear evolution equations. as a result, many new and more general exact travelling wave solutions are obtained including periodic function solutions, soliton-like solutions and trigonometric function solutions. the exact particular solutions containing four types hyperbolic function solution, trigonometric function solution, exponential solution and rational solution. we obtained the further solutions comparing with other methods. recently this method is developed for searching exact travelling wave solutions of nonlinear partial differential equations. abundant exact travelling wave solutions including solitons, kink, periodic and rational solutions have been found. these solutions might play important role in engineering fields. it is shown that this method, with the help of symbolic computation, provides a straightforward and powerful mathematical tool for solving the nonlinear problems.
algorithm_design	we are on the cusp of the emergence of a new wave of nonvolatile memory technologies that are projected to become the dominant type of main memory in the near future. a key property of these new memory technologies is their asymmetric read-write costs: writes can be an order of magnitude or more higher energy, higher latency, and lower (per module) bandwidth than reads. this high cost for writes motivates a rethinking of algorithm design towards ""write efficient"" algorithms and data structures that reduce their number of writes [1, 2, 3, 4, 5, 6]. many popular techniques for sequential, distributed, and parallel algorithms are tuned to the setting where reads and writes cost the same, and hence need to be revisited. prior work on reducing writes to contended cache lines in shared memory algorithms can be useful here, but with the new technologies, even writes to uncontended memory is costly. moreover, the new technologies are unlikely to replace the fastest cache memory, motivating the study of a multi-level memory hierarchy comprised of smaller symmetric level(s) and a larger asymmetric level. lower bounds, too, need to be revisited in light of asymmetric costs. this talk provides background on these emerging memory technologies, highlights the progress to date on these exciting research questions, and touches on a few of the many open problems.
image_processing	this work presents the application of terahertz imaging to three-dimensional formalin-fixed, paraffin-embedded human breast cancer tumors. the results demonstrate the capability of terahertz for in-depth scanning to produce cross section images without the need to slice the tumor. samples of tumors excised from women diagnosed with infiltrating ductal carcinoma and lobular carcinoma are investigated using a pulsed terahertz time domain imaging system. a time of flight estimation is used to obtain vertical and horizontal cross section images of tumor tissues embedded in paraffin block. strong agreement is shown comparing the terahertz images obtained by electronically scanning the tumor in-depth in comparison with histopathology images. the detection of cancer tissue inside the block is found to be accurate to depths over 1 mm. image processing techniques are applied to provide improved contrast and automation of the obtained terahertz images. in particular, unsharp masking and edge detection methods are found to be most effective for three-dimensional block imaging.
machine_learning	in this paper a novel tensor-based image segmentation algorithm (tbisa) is presented, which is dedicated for segmentation of colour images. a purpose of tbisa is to distinguish specific objects based on their characteristics, i.e. shape, colour, texture, or a mixture of these features. all of those information are available in colour channel data. nonetheless, performing image analysis on the pixel level using rgb values, does not allow to access information on texture which is hidden in relation between neighbouring pixels. therefore, to take full advantage of all available information, we propose to incorporate the structural tensors as a feature extraction method. it forms enriched feature set which, apart from colour and intensity, conveys also information of texture. this set is next processed by different classification algorithms for image segmentation. quality of tbisa is evaluated in a series of experiments carried on benchmark images. obtained results prove that the proposed method allows accurate and fast image segmentation.
cryptography	visual cryptography (vc) is a variant form of secret sharing. in general threshold setting, the k-out-of-n vc allows that, in a set of n participants, any k can recover and reconstruct the secret by stacking their shares. recently, the notion of multiple-secret vc has been introduced to embed multiple secrets. region incrementing visual cryptography (rivc) is referred to as a new type of multi-secret vc. rivc defines s layers and takes s secrets, and then embeds each secret into each layer. the layers are defined by the number of participants; for example, let two secrets and two layers be s-2, s-3 and l-2, l-3 in two-out-of-three rivc, where any two participants in l-2 can recover s-2 and three in l-3 can recover s-2, s-3. however, there is another multi-secret vc, called fully incrementing visual cryptography (fivc), which also has the layers, but only one secret s-i will reveal in one layer l-i. in this paper, our stating point is to propose a new notion of non-monotonic visual cryptography (nvc) for human vision system as a primitive to construct fivc. we first present an ideal construction of simple nvc, which relies on a slightly unreasonable assumption. based on the simple nvc, we show a few methods to extend the functionality for complicated cases of nvc. then, the generic construction is presented as a systematic manner to eliminate the above-mentioned assumption. finally, we formally introduce a transformation nvc-to-fivc algorithm, which takes nvc as input and then produce a construction of fivc. also, show a demonstration the nvc-to-rivc algorithm, and analyze some properties regarding nvc. we believe that the notion of nvc can potentially find other applications and is of independent interest.
symbolic_computation	recursive branch and bound algorithms are often used, either rigorously or non-rigorously, to refine and isolate solutions to global optimization problems or systems of equations and inequalities involving nonlinear functions. the presented software library, kodiak, integrates numeric and symbolic computation into a generic framework for the solution of such problems over hyper-rectangular variable and parameter domains. the correctness of both the generic branch and bound algorithm and the self-validating enclosure methods used, namely interval arithmetic and, for polynomials and rational functions, bernstein expansion, has been formally verified. the algorithm has three main instantiations, for systems of equations and inequalities, for constrained global optimization, and for the computation of equilibria and bifurcation sets for systems of ordinary differential equations. for the latter category, and to enable the computation of bisection heuristics to reduce the branching factor, advantage is taken of the partial derivatives of the constraint functions, which are symbolically manipulated. pavings (unions of box subsets) for a continuum of solutions to underdetermined systems may also be produced. the capabilities of the software tool are outlined, and computational examples are presented.
cryptography	in the russian cards problem, alice, bob and cath draw a, b and c cards, respectively, from a publicly known deck. alice and bob must then communicate their cards to each other without cath learning who holds a single card. solutions in the literature provide weak security, where alice and bob 's exchanges do not allow cath to know with certainty who holds each card that is not hers, or perfect security, where cath learns no probabilistic information about who holds any given card. we propose an intermediate notion, which we call -strong security, where the probabilities perceived by cath may only change by a factor of . we then show that strategies based on affine or projective geometries yield -strong safety for arbitrarily small and appropriately chosen values of a, b, c.
data_structures	medium and large construction projects typically involve multiple structural consultants who use a wide range of structural analysis applications. these applications and technologies have inadequate interoperability and there is still a dearth of investigations addressing interoperability issues in the structural engineering domain. this paper proposes a novel approach which combines an industry foundation classes (ifc)-based unified information model with a number of algorithms to enhance the interoperability: (a) between architectural and structural models, and (b) among multiple structural analysis models (bidirectional conversion or round tripping). the proposed approach aims to achieve the conversion by overcoming the inconsistencies in data structures, representation logics and syntax used in different software applications. the approach was implemented in both client server (c/s) and browser server (b/s) environments to enable central and remote collaboration among geographically dispersed users. the platforms were tested in four large real-life projects. the testing involved four key scenarios: (a) the bidirectional conversion among four structural analysis tools; (b) the comparison of the conversion via the proposed approach with the conversion via direct links among the involved tools; (c) the direct export from an ifc-based architectural tool through the application program interface (api), and (d) the conversion and visualization of structural analysis results. all these scenarios were successfully performed and tested in four significant case studies. in particular, the conversion among the four structural analysis applications (etabs, sap2000, ansys and midas) was successfully tested for all possible conversion routes among the four applications in two of the case studies (i.e., project a and project b). the first four steps of natural mode shapes and their natural vibration periods were calculated and compared with the converted models. they were all achieved within a standard deviation of 0.1 s and 0.2 sin project a and project b, respectively, indicating an accurate conversion. (c) 2016 elsevier b.v. all rights reserved.
image_processing	the use of white light based three fringe photoelasticity (tfp)/rgb photoelasticity has gained importance in the recent years. with recent advances in tfp, it is possible to resolve fringe orders upto twelve. the main advantage of this technique is that it requires only a single image for isochromatic demodulation, which makes it suitable especially for problems where recording multiple images is difficult. the accuracy of isochromatic data obtained using tfp/rgb photoelasticity is dependent on the scanning scheme used to refine the data, which is necessary to incorporate fringe order continuity. in this paper, the existing scanning schemes are critically evaluated for their ability to scan the entire model domain, influence of seed point selection and noise propagation. the scanning schemes are assessed using four problems of increasing level of geometric complexity - circular disc under compression (simply connected), bi-axially loaded cruciform specimen with an inclined crack, a thick ring subjected to internal pressure and a finite plate with a hole (multiply connected). (c) 2016 elsevier ltd. all rights reserved.
computer_graphics	this paper presents a method to solve - in real time the three dimensional workspace generation problem for arbitrary serial manipulators. our approach is based on monte carlo simulation, to process a high number of forward kinematics with randomly chosen joint values. this results in an asymptotic coverage of the reachable workspace. additionally, collision detection is integrated to consider obstacles within the manipulator 's environment. the method is implemented on the graphics processing unit (gpu), such that an extremely high number of workspace points can be processed in parallel. tests have shown that this approach is capable to generate acceptable workspace coverage within milliseconds. furthermore, the workspace is held as a three dimensional texture volume on the graphics memory, allowing for instant visualisation of the workspace during the generation process without the need for further time-intensive data exchange.
operating_systems	performance and determinism are two critical metrics in most embedded systems with real-time requirements. owing to the complexity of current embedded systems, along with increased application demands, real-time operating systems (rtoss) have become a de facto solution providing specific services to the system tasks. however, this extra layer, which abstracts the hardware from the software, makes it harder for a system to achieve good performance and determinism. to ease the impact of a rtos in the system, rtos run-time services are offloaded to the hardware layer. this paper presents a hybrid rtos implementation, where several critical rtos services were migrated from software to hardware, improving system latency and predictability. special focus was given to the rtos scheduler and to the mutexes handling subsystem. the developed hardware accelerators were synthesised on a field-programmable gate array (fpga), exploiting the point-to-point fast simplex link (fsl) bus to interconnect to the xilinx microbaze soft-core processor. our approach shows that hybrid rtos has a better performance and predictability when compared to its software-only version.
cryptography	proteins are one of the most versatile modular assembling systems in nature. experimentally, more than 110 000 protein structures have been identified and more are deposited every day in the protein data bank. such an enormous structural variety is to a first approximation controlled by the sequence of amino acids along the peptide chain of each protein. understanding how the structural and functional properties of the target can be encoded in this sequence is the main objective of protein design. unfortunately, rational protein design remains one of the major challenges across the disciplines of biology, physics and chemistry. the implications of solving this problem are enormous and branch into materials science, drug design, evolution and even cryptography. for instance, in the field of drug design an effective computational method to design protein-based ligands for biological targets such as viruses, bacteria or tumour cells, could give a significant boost to the development of new therapies with reduced side effects. in materials science, self-assembly is a highly desired property and soon artificial proteins could represent a new class of designable self-assembling materials. the scope of this review is to describe the state of the art in computational protein design methods and give the reader an outline of what developments could be expected in the near future.
computer_vision	both subspace learning methods and feature selection methods are often used for removing irrelative features from high-dimensional data. studies have shown that feature selection methods have interpretation ability and subspace learning methods output stable performance. this paper proposes a new unsupervised feature selection by integrating a subspace learning method (i.e., locality preserving projection (lpp)) into a new feature selection method (i.e., a sparse feature-level self-representation method), aim at simultaneously receiving stable performance and interpretation ability. different from traditional sample-level self-representation where each sample is represented by all samples and has been popularly used in machine learning and computer vision. in this paper, we propose to represent each feature by its relevant features to conduct feature selection via devising a feature-level self-representation loss function plus an l(2,1)-norm regularization term. then we add a graph regularization term (i.e., lpp) into the resulting feature selection model to simultaneously conduct feature selection and subspace learning. the rationale of the lpp regularization term is that lpp preserves the original distribution of data after removing irrelative features. finally, we conducted experiments on uci data sets and other real data sets and the experimental results showed that the proposed approach outperformed all comparison algorithms. (c) 2016 elsevier b.v. all rights reserved.
software_engineering	simulation-based studies (sbs) have become an interesting investigation approach for software engineering (se). however, the reports on experiments with dynamic simulation models found in the technical literature lack relevant information, hampering the full understanding of the procedures and results reported, as well as their replicability. apart from the limitations on the length in conferences and journal papers, some of the relevant information seems to be missing due to methodological issues not considered when conducting such studies. this is the case of missing research questions and goals, lack of evidence regarding the dynamic simulation model validity, poorly designed simulation experiments, amongst others. based on findings from a previous quasi-systematic literature review, we propose a set of reporting guidelines for sbs with dynamic models in the context of se aiming at providing guidance on which information the report should contain. furthermore, these guidelines were evolved to support sbs planning by identifying potential threats to simulation study validity and in making recommendations to avoid them, through qualitative analysis and external evaluation. finally, we conducted different evaluations regarding both the reporting and planning guidelines, apart from using them to support the planning of a sbs as regards software evolution. a set of 33 reporting and planning guidelines for different stages of the simulation lifecycle and focused on the experimentation with dynamic simulation models have been put together. the first assessments point to a comprehensive set of guidelines, supporting a comprehensive preparation and review of the plans and reports from the studies, apart from the planning of a sbs focused on software evolution, potentially reducing the threats to the experimentation with the validity of dynamic simulation models. the 33 guidelines cannot be understood as separate groups for reporting and planning as they overlap in many aspects. the main goal is to use the guidelines to support the planning of a simulation-based study with dynamic models so that experimenters may identify potential threats to validity and produce relevant information for a complete simulation experiment report in advance. despite their initial contribution to increase the validity of sbs, the reporting and planning of simulation-based experiments with dynamic models still has to be discussed and improved in se. therefore, additional assessments of this set of guidelines are needed to strengthen the confidence in their completeness and usefulness.
computer_programming	an important number of academic tasks should be solved collaboratively by groups of learners. the computer-supported collaborative learning (cscl) systems support this collaboration by means of shared workspaces and tools that enable communication and coordination between learners. successful collaboration and interaction can depend on the criteria followed when forming the groups of learners. this paper proposes a method that analyses the collaboration and interaction between learners using a set of indicators or variables about how they solve academic tasks. then, the concept of data depth is used as a measurement of the closeness of the analysis indicators' values for a learner with respect to the values that the same indicators take for the other learners. finally, the data depth is used to form new groups of learners whose analysis indicators take similar or different values. thus, the method enables teachers to form homogeneous and heterogeneous groups according to their preferences. this group formation process is carried out automatically by a software tool. this paper presents two case studies in which the method is applied to form groups of learners who solve academic tasks in different domains (computer programming and data mining). (c) 2014 elsevier ltd. all rights reserved.
network_security	due to the appearance of some new characteristics of the electric power industry such as widely interconnection, high intelligent, the open and interactive, energy internet architecture put forward more new requirements of the electric information network security prevention and the information security faced a severe challenge. at present, the method of information security is extensive, management model and prevention system exist some weak links. this paper put forward a new model to guide the overall development of information security work through the quantization of risk indicator and fuzzy intelligent analysis.
operating_systems	recent approaches to network functions virtualization (nfv) have shown that commodity network stacks and drivers struggle to keep up with increasing hardware speed. despite this, popular cloud networking services still rely on commodity operating systems (oss) and device drivers. taking into account the hardware underlying of commodity servers, we built an nfv profiler that tracks the movement of packets across the system 's memory hierarchy by collecting key hardware and os-level performance counters. leveraging the profiler 's data, our service chain coordinator 's (scc) run-time accelerates user-space nfv service chains, based on commodity drivers. to do so, scc combines multiplexing of system calls with scheduling strategies, taking time, priority, and processing load into account. by granting longer time quanta to chained network functions (nfs), combined with i/o multiplexing, scc reduces unnecessary scheduling and i/o overheads, resulting in three-fold latency reduction due to cache and main memory utilization improvements. more importantly, scc reduces the latency variance of nfv service chains by up to 40x compared to standard fastclick chains by making the average case for an nfv chain to perform as well as the best case. these improvements are possible because of our profiler 's accuracy. (c) 2017 the author(s). published by elsevier inc. this is an open access article under the cc by-nc-nd license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)
computer_vision	this paper presents an innovative solution based on time-of-flight (tof) video technology to motion patterns detection for real-time dynamic hand gesture recognition. the resulting system is able to detect motion-based hand gestures getting as input depth images. the recognizable motion patterns are modeled on the basis of the human arm anatomy and its degrees of freedom, generating a collection of synthetic motion patterns that is compared with the captured input patterns in order to finally classify the input gesture. for the evaluation of our system a significant collection of gestures has been compiled, getting results for 3d pattern classification as well as a comparison with the results using only 2d information.
network_security	a distributed denial of service (ddos) attack is an austere menace to extensively used internet-based services. the in-time detection of ddos attacks poses a tough challenge to network security. revealing a low-rate ddos (lr-ddos) attack is comparatively more difficult in modern high speed networks, since it can easily conceal itself due to its similarity with legitimate traffic, and so eluding current anomaly based detection methods. this paper investigates the aptness and impetus of the information theory-based generalized entropy (ge) and generalized information distance (gid) metrics in detecting different types of ddos attacks. the results of ge and gid metrics are compared with shannon entropy and other popular information divergence measures. in addition, the feasibility of using these metrics in discriminating a high-rate ddos (hr-ddos) attack from a similar looking legitimate flash event (fe) is also verified. we used real and synthetically generated datasets to elucidate the efficiency and effectiveness of the proposed detection scheme in detecting different types of ddos attacks and fes. the results clearly show that the ge and gid metrics perform well in comparison with other metrics and have reduced false positive rate (fpr). (c) 2017 elsevier b.v. all rights reserved.
machine_learning	most of the well-known supervised dimensionality reduction methods assume unimodal or gaussian like-lihoods, which may not be appropriate in the real life applications. in this manuscript, we introduce a novel supervised dimensionality reduction approach, moments discriminant analysis, which models linear relationships between the high-dimensional input space and a low-dimensional space by maximizing the discrimination between second order raw moments of different classes to improve the generalization capability of a classifier. unlike the state-of-the-art methods, moments discriminant analysis is intended to accommodate data distributions that may be multimodal and non-gaussian. initially, experiments using synthetic random data (generated from different probability distributions) are performed to prove the efficiency of the proposed method for multimodal and non-gaussian data with the help of five separability measures. also, extensive experimental results on uci machine learning repository and image retrieval on wang and mit (oliva and torralba) databases are carried out in order to exhibit the effectiveness of moments discriminant analysis over the state-of-the-art methods.
operating_systems	cloud-based mobile networks are foreseen to be a technological enabler for the next generation of mobile networks. their design requires substantial research as they pose unique challenges, especially from the point of view of additional delays in the fronthaul network. commonly used network protocols should therefore be adjusted to cope with the peculiarities of these networks. in this paper, we investigate an optimized design of the transmission control protocol (tcp), as it plays a central role in all-ip mobile networks to ensure optimal performances for application-layer services. tcp implementations of 3 popular operating systems are investigated in our network model. the results on the most influential parameters are used to design an optimized tcp for cloud-based mobile networks.
software_engineering	regarding the issue of role-playing games (rpg) and the experiential learning cycle (elc), the integration of rpg use as a pedagogical and simulation tool for practice and elc as a learning theoretical foundation is essential for promoting students' effective learning. however, few studies have applied rpg to simulate the practice with the elc stages, namely, concrete experience (ce), reflective observation (ro), abstract conceptualization, (ac) and active experimentation (ae), to examine the learning process and further enhance the effective learning outcomes for learners. this study integrates the rpg development and use for practice derived from the elc 's four stages based on practising the project assessment of software development in a software engineering course. the results show a significant improvement in students' learning outcomes after rpg use. more importantly, this study provides the major activities and findings of each elc stage via rpg use and the mapping of rpg activities with elc stages. the insightful implications and suggestions of this study are discussed. (c) 2014 elsevier ltd. all rights reserved.
bioinformatics	asian soybean rust (asr), caused by the obligate biotrophic fungus phakopsora pachyrhizi, can cause losses greater than 80%. despite its economic importance, there is no soybean cultivar with durable asr resistance. in addition, the p. pachyrhizi genome is not yet available. however, the availability of other rust genomes, as well as the development of sample enrichment strategies and bioinformatics tools, has improved our knowledge of the asr secretome and its potential effectors. in this context, we used a combination of laser capture microdissection (lcm), rnaseq and a bioinformatics pipeline to identify a total of 36 350 p. pachyrhizi contigs expressed in planta and a predicted secretome of 851 proteins. some of the predicted secreted proteins had characteristics of candidate effectors: small size, cysteine rich, do not contain pfam domains (except those associated with pathogenicity) and strongly expressed in planta. a comparative analysis of the predicted secreted proteins present in pucciniales species identified new members of soybean rust and new pucciniales- or p. pachyrhizi-specific families (tribes). members of some families were strongly up-regulated during early infection, starting with initial infection through haustorium formation. effector candidates selected from two of these families were able to suppress immunity in transient assays, and were localized in the plant cytoplasm and nuclei. these experiments support our bioinformatics predictions and show that these families contain members that have functions consistent with p. pachyrhizi effectors.
computer_graphics	e-learning is a new learning model based on computer, multimedia and network, and it is based on ""constructivism education theory"" and ""humanistic education theory"". after study the limits of classical methods of computer graphic experiment learning, this paper implemented an e-learning platform. this platform includes 4 parts: the basic knowledge module, the tools and material module, the learning and communication module, and the learning evaluation module. this platform can improve the interest and enthusiasm of student. it can achieve better learning effect.
computer_vision	estimating transformations from degraded point sets is necessary for many computer vision and pattern recognition applications. in this paper, we propose a robust non-rigid point set registration method based on spatially constrained context-aware gaussian fields. we first construct a context-aware representation (e.g., shape context) for assignment initialization. then, we use a graph laplacian regularized gaussian fields to estimate the underlying transformation from the likely correspondences. on the one hand, the intrinsic manifold is considered and used to preserve the geometrical structure, and a priori knowledge of the point set is extracted. on the other hand, by using the deterministic annealing, the presented method is extended to a projected high-dimensional feature space, i.e., reproducing kernel hilbert space through a kernel trick to solve the transformation, in which the local structure is propagated by the coarse-to-fine scaling strategy. in this way, the proposed method gradually recovers much more correct correspondences, and then estimates the transformation parameters accurately and robustly when facing degradations. experimental results on 2d and 3d synthetic and real data (point sets) demonstrate that the proposed method reaches better performance than the state-of-the-art algorithms.
image_processing	this study investigates the ignition characteristics of pulverised coal, biomass and co-firing by use of a visual drop tube furnace (vdtf) and a high speed imaging technique. three coals (anthracite, a bituminous coal and a lignite), four biomasses (pine, eucalyptus, olive residue and miscanthus) and various biomass-coal mixtures were tested. with each coal, biomass or their mixture, a distinct flame was established within the vdtf through the continuous feeding of the fuel under the environment of air and at a furnace teniperature of 800 degrees c. to observe the ignition point, a phantom v12.1 high-speed camera was used to capture the videos of fuel combustion at 500 frames per second (fps). a technique was developed using matiab 's image analysis tool to automate the ignition point detection. the results of the image processing were used to statistically analyse and determine the changes to the ignition behaviour with different fuels and co-firing ratios. the results obtained with the tested coals have shown that the distance to ignition increases as the coal volatile matter content decreases, whereas the opposite trend was found for the biomass fuels. further, the addition of biomass to the anthracite significantly reduces the distance to ignition but a much less pronounced effect on the ignition was found when biomass was co-fired with the bituminous coal or lignite. the synergistic effect on the ignition of biomass-anthracite mixture is mainly attributed to the high volatile content and the potential effects of catalysis from the alkali metals present in the biomass. the results of this study have shown that the vdtf testing coupled with the image analysis technique allows for an effective and simple method of characterising ignition behaviours of pulverised coal, biomass and their mixtures. (c) 2016 the authors. published by elsevier b.v.
computer_programming	machine learning is continuing to gain popularity due to its ability to solve problems that are difficult to model using conventional computer programming logic. much of the current and past work has focused on algorithm development, data processing, and optimization. lately, a subset of research has emerged which explores issues related to security. this research is gaining traction as systems employing these methods are being applied to both secure and adversarial environments. one of machine learning 's biggest benefits, its data-driven versus logic-driven approach, is also a weakness if the data on which the models rely are corrupted. adversaries could maliciously influence systems which address drift and data distribution changes using re-training and online learning. our work is focused on exploring the resilience of various machine learning algorithms to these data-driven attacks. in this paper, we present our initial findings using monte carlo simulations, and statistical analysis, to explore the maximal achievable shift to a classification model, as well as the required amount of control over the data.
distributed_computing	frequent items in high-speed streaming data are important to many applications like network monitoring and anomaly detecting. to deal with high arrival rate of streaming data, it is desirable that such systems be capable of supporting high processing throughput with tight guarantees on errors. in this paper, we address the problem of finding frequent and top-k items, and present a parallel version of the space saving algorithm in the context of the open source distributed computing system. based on the theoretical analysis, the errors are restrictively bounded in our algorithm, and our parallel design could achieve high throughput. taking advantage of the distributed computing resources, our evaluation reveals that such design delivers linear speedup with remarkable scalability.
computer_vision	the heart rate (hr) measurements based on the camera (visible light) can be used to detect hr in non-contact mode, which has great application prospects both in the clinical application and home health care. however, cmos sensors equipped with ""rolling shutters"", which distinguishes different lines per frame to become light sensitive at different moments in time, and stylized dithering of image acquisition (imaq) time caused by different computer programs running in the background will greatly influence the accuracy of the measured hr. in this paper, we analyze the phase error caused by cmos sensor and the system error introduced by system sampling clock jitters. according to derivation, we propose two methods, amplitude-frequency superposition and a cubic spline interpolation reconstruction method based on actual schedules, that can be widely utilized in computer vision to overcome the camera phase error and sampling time fluctuation error. amplitude of signal is analyzed and processed in amplitude-frequency domain in the method of amplitude-frequency superposition, which ignores the signal phase. thus it can eliminate the phase error effectively. the cubic spline interpolation reconstruction method based on actual schedules can reconstructed the non-uniform sampling of images as uniform ones, so it can eliminate the system error involved by the system clock jitters. what 's more, the properties of the methods are tested by applying them to both simulation experiments and real hr measurements. in the simulation, amplitude of measured signal is improved 4. 58% relative to the amplitude measured without the method of amplitude-frequency superposition; root mean square error of signal 's frequency, detected by the cubic spline interpolation reconstruction method based on actual schedules, is reduced more than 30%. in the real hr measurements, the amplitude of hr is raised to 33. 5% relatively based on amplitude-frequency superposition. and the accuracy of hr is raised to approximately 40% by the method of cubic spline interpolation reconstruction method based on actual schedules. therefore, the simulation experiments and real hr measurement proof that we can effectively eliminate the camera phase error based on the amplitude-frequency superposition extraction method, and the cubic spline interpolation based on the timetable method can effectively reduce the random error in imaq due to system clock jitters. these methods can both be widely used in dynamic signal detection based on machine vision.
computer_vision	cell tracking plays crucial role in biomedical and computer vision areas. as cells generally have frequent deformation activities and small sizes in microscope image, tracking the non-rigid and non-significant cells is quite difficult in practice. traditional visual tracking methods have good performances on tracking rigid and significant visual objects, however, they are not suitable for cell tracking problem. in this paper, a novel cell tracking method is proposed by using convolutional neural networks (cnns) as well as multi-task learning (mtl) techniques. the cnns learn robust cell features and mtl improves the generalization performance of the tracking. the proposed cell tracking method consists of a particle filter motion model, a multi-task learning observation model, and an optimized model update strategy. in the training procedure, the cell tracking is divided into an online tracking task and an accompanying classification task using the mtl technique. the observation model is trained by building a cnn to learn robust cell features. the tracking procedure is started by assigning the cell position in the first frame of a microscope image sequence. then, the particle filter model is applied to produce a set of candidate bounding boxes in the subsequent frames. the trained observation model provides the confidence probabilities corresponding to all of the candidates and selects the candidate with the highest probability as the final prediction. finally, an optimized model update strategy is proposed to enable the multi-task observation model for the variation of the tracked cell over the entire tracking procedure. the performance and robustness of the proposed method are analyzed by comparing with other commonly-used methods. experimental results demonstrate that the proposed method has good performance to the cell tracking problem. (c) 2016 elsevier b.v. all rights reserved.
network_security	acting as a focus of network security field, intrusion detection technology (idt) plays a very important role in different conditions. feature selection methods for intrusion detection directly affect the efficiency of intrusion detect system. in this paper, feature selection algorithm based on relief and relief sequential backward search (relief-sbs) is proposed under considering statistical correlation of relief and relief-sbs. the improved algorithm eliminates a feature after each round of iteration, and adopts the result of relief algorithm as the assessment criteria for feature. simulation results show that the proposed feature selection algorithm improves the efficiency of intrusion detection; moreover, it provides correlation technique support for idt.
computer_graphics	we present a general method for transferring skeletons and skinning weights between characters with distinct mesh topologies. our pipeline takes as inputs a source character rig (consisting of a mesh, a transformation hierarchy of joints, and skinning weights) and a target character mesh. from these inputs, we compute joint locations and orientations that embed the source skeleton in the target mesh, as well as skinning weights to bind the target geometry to the new skeleton. our method consists of two key steps. we first compute the geometric correspondence between source and target meshes using a semi-automatic method relying on a set of markers. the resulting geometric correspondence is then used to formulate attribute transfer as an energy minimization and filtering problem. we demonstrate our approach on a variety of source and target bipedal characters, varying in mesh topology and morphology. several examples demonstrate that the target characters behave well when animated with either forward or inverse kinematics. via these examples, we show that our method preserves subtle artistic variations; spatial relationships between geometry and joints, as well as skinning weight details, are accurately maintained. our proposed pipeline opens up many exciting possibilities to quickly animate novel characters by reusing existing production assets.
operating_systems	portable devices are today used in all areas of life thanks to their ease of use as well as their applications with unique features. the increase in the number of users, however, also leads to an increase in security threats. this study examines the threats to mobile operating systems. addressing the four mobile operating systems (android, apple os (ios), symbian and java me) with the highest number of users, the study provides statistical information about the features of the corresponding operating systems and their areas of use. in the study, the most important threats faced by the mobile operating systems (malware, vulnerabilities, attacks) and the risks posed by these threats were analyzed in chronological order and the future-oriented security perspective was suggested..
operating_systems	binary tree traversal algorithm can be applied in many aspects, such as information encryption, network, operating systems, cluster computing and so on. we have already proposed a useful method to verify the correctness of algorithmic programs based on isabelle proof assistant and dijkstra 's weakness precondition theory, and have manually derived and verified binary tree traversal non-recursive algorithms in our previous work. in order to ensure the security of the non-recursive algorithms, the focus of this paper is to construct a unified recurrence-relations expression about preorder, in-order, and post-order binary tree traversal non-recursive algorithms. the recurrence-relations expression make it easier to derive the loop invariants of three algorithms. meanwhile, we automatically verify the correctness of three kinds of non-recursive algorithms by using a generic proof assistant isabelle. this work realizes mechanically automatic-verification and overcomes the intricacies and weakness of manual verification, improves the verification efficiency, and ensures the trustworthiness and reliability of the algorithm program.
algorithm_design	increasing interest in simultaneously optimizing many objectives (typically more than three objectives) of problems leads to the emergence of various many-objective algorithms in the evolutionary multi-objective optimization field. however, in contrast to the development of algorithm design, how to assess many-objective algorithms has received scant concern. many performance indicators are designed in principle for any number of objectives, but in practice are invalid or infeasible to be used in many-objective optimization. in this paper, we explain the difficulties that popular performance indicators face and propose a performance comparison indicator (pci) to assess pareto front approximations obtained by many-objective algorithms. pci evaluates the quality of approximation sets with the aid of a reference set constructed by themselves. the points in the reference set are divided into many clusters, and the proposed indicator estimates the minimum moves of solutions in the approximation sets to weakly dominate these clusters. pci has been verified both by an analytic comparison with several wellknown indicators and by an empirical test on four groups of pareto front approximations with different numbers of objectives and problem characteristics.
parallel_computing	in this paper, we analyze the preconditioned gmres algorithm in detail and decompose it into components to implement on multiple-gpu architecture. the operations of vector updates, dot products and sparse matrix vector multiplication (spmv) are implemented in parallel. in addition, a specific communication mechanism for spmv is designed. the preconditioner is established on the host (cpu) and solved on the devices (gpus). validated by a series of numerical experiments, the gpu-based gmres solver is effective and favorable parallel performance is achieved. (c) 2016 elsevier ltd. all rights reserved.
computer_vision	a ground surface roughness measurement method is proposed to address current problems in the use of machine vision technology to measure roughness: the calculations are complex, and the measurement process is largely affected by the light source. based on the area of diffusion regions between the virtual images formed by a light source on ground surfaces with different roughness levels are different, a reference light source containing two base color is designed. red and green color space-based color distribution statistical matrices, as well as corresponding overlap indices, are proposed. a relationship model between overlap index and roughness is constructed. the effect of light source brightness and texture direction on the relationship model is discussed based on the experimental data. the results demonstrate that the surface roughness measurement method, which is based on the overlap degree of the color image, has relatively high accuracy and a relatively wide measurement range and is, to a certain degree, robust to the brightness of the light source and the texture direction. the surface roughness measurement method has huge potential for engineering applications. (c) 2017 elsevier ltd. all rights reserved.
computer_programming	failure to understand evolutionary dynamics has been hypothesized as limiting our ability to control biological systems. an increasing awareness of similarities between macroscopic ecosystems and cellular tissues has inspired optimism that game theory will provide insights into the progression and control of cancer. to realize this potential, the ability to compare game theoretic models and experimental measurements of population dynamics should be broadly disseminated. in this tutorial, we present an analysis method that can be used to train parameters in game theoretic dynamics equations, used to validate the resulting equations, and used to make predictions to challenge these equations and to design treatment strategies. the data analysis techniques in this tutorial are adapted from the analysis of reaction kinetics using the method of initial rates taught in undergraduate general chemistry courses. reliance on computer programming is avoided to encourage the adoption of these methods as routine bench activities.
parallel_computing	a robust multi-objective optimization method for truss optimum design is presented. in the robust design, materials and loads are assumed to be affected by epistemic uncertainties (imprecise or lack of knowledge). uncertainty quantification using evidence theory in optimum design subject to epistemic uncertainty is undertaken. in addition to a functional objective, an evidence-based plausibility measure of failure of constraint satisfaction is minimized to formulate the robust design into a multi-objective optimization problem. in order to alleviate the computational difficulties in the evidence theory-based uncertainty quantification analysis, a combined strategy of differential evolution-based interval optimization method and parallel computing technique is proposed. a population-based multi-objective differential evolution optimization algorithm is designed for searching robust pareto front. two truss structures with shape and sizing optimum design problems are presented to demonstrate the effectiveness and applicability of the proposed method.
symbolic_computation	bound-state vector soliton solutions for the coupled variable-coefficient higher-order nonlinear schrodinger equations, which describe the simultaneous propagation of nonlinear waves in the inhomogeneous optical fiber, are investigated. introducing auxiliary functions, we derive the bilinear forms and corresponding constraints on the variable coefficients. through symbolic computation, we construct the one- and two-soliton solutions. we see that the variable coefficients in the equations affect the soliton structures. with different choices of the variable coefficients, we obtain the cubic, periodic, and parabolic solitons. bound-state solitons and interactions are analyzed graphically.
network_security	embedded systems are routinely deployed in critical infrastructures nowadays, therefore their security is increasingly important. this, combined with the pressing requirement of deploying massive numbers of low-cost and low-energy embedded devices, stimulates the evolution of lightweight cryptography and other green-computing security mechanisms. new crypto-primitives are being proposed that offer moderate security and produce compact implementations. in this article, we present a lightweight authenticated encryption scheme based on the integrated hardware implementation of the lightweight block cipher present and the lightweight hash function spongent. the presented combination of a cipher and a hash function is appropriate for implementing authenticated encryption schemes that are commonly utilized in one-way and mutual authentication protocols. we exploit their inner structure to discover hardware elements usable by both primitives, thus reducing the circuit 's size. the integrated versions demonstrate a 27% reduction in hardware area compared to the simple combination of the two primitives. the resulting solution is ported on a field-programmable gate array (fpga) and a complete security application with input/output from a universal asynchronous receiver/transmitter (uart) gate is created. in comparison with similar implementations in hardware and software, the proposed scheme represents a better overall status.
cryptography	in a strong designated verifier signature with message recovery (sdvswmr) scheme, only the designated receiver has the capability to recover and validate the message-signature pair. in 2015, using the bilinear pairing, islam and biswas presented an sdvswmr scheme (we call it: isbi-sdvswmr) with non-delegatability, which has better performance than other schemes in terms of communication and computation cost. however, in this study, we address that isbi-sdvswm scheme does not satisfy the security property of non-delegatability as they claimed and we present two types of delegatability attack to their scheme. we also propose a new and pairing-free sdvswmr scheme that possesses the following security requirements: non-delegatability, unforgeability, non-transferability and privacy of signer 's identity (prsi). compared our scheme with other existing related schemes, our scheme obtains better performance; that is, the computational cost is only 58%(lower) of isbi-sdvswm scheme (other schemes), and the communication cost is 800bits that is only 68%(lower) of isbi-sdvswm scheme (other schemes). copyright (c) 2017 john wiley & sons, ltd.
relational_databases	the aim of this work was to check the possibilities of usage the beacons to find out the relations between the members of cattle herd. relations between animals are important because of animals welfare and can influence on animal health and productivity. in the early stage of the research, relations between animals were determined on the base of the distance between animals and described as like/not like"" (they like each other or not). it 's difficult to precisely determine when animals like each other. not only distance is the criterium but also time and direct behavior. the experiment was carried out in the cowshed with 70 dairy cows. ble beacons (bluetooth low energy beacons) were attached to the cows and they regularly transmitted data about the cows' localisation in the cowshed to the smartphones which were distributed in the cowshed. from smartphones with android system data were sent to the relational database. because of the large amount of data, in the database data were processed, i.e. aggregated. some of the data were transformed to the geometric form and then, thanks to the specially developed application, these geometric data were mapped to relations and written in the graph database. graph databases are very useful tool to analyse and show relations between animals. the neo4j technology was used to prepare the graph database. other ict technologies used in this work were: sql server 2014, visual studio 2015 and entity framework 6. relational databases are still very usefull to analyse data and the graph databases are especially useful to visualize the spatial relations between objects. new technologies allows to widen our perceptions and create new possibilities of getting knowledge. there are a lot of tools to connect these technoogies, but it requires multidisciplinary approach.
network_security	an increasing number of wireless internet users and deployed wireless access points over the past several years and have raised the importance of wireless security issues. the absolute majority of wireless users are not it professionals, but a population unaware of wireless security types, settings and importance. wireless security assessment and analytics can help in raising the security awareness of users and in increasing their skills, leading to improvement of the entire security situation. in this paper a short overview of wireless security assessment and history is presented. the methodology and tools for a more accurate wireless security assessment, including data acquisition, processing and analysis, are offered. the proposed methodology and tools are used for processing wireless scan results for the two capital cities, hungary (budapest) and serbia (belgrade). the possibility of access point configuration changes and security improvement has also been investigated. the research results and potential improvements of wireless security situation are discussed.
algorithm_design	as an overlay cognitive radio, transform domain communication system (tdcs) has been proposed to support reliable communications with low probability of interception. however, most of research on tdcs assume perfect synchronization to simplify algorithm design. in this paper, we present a low complexity symbol timing offset (sto) and carrier frequency offset (cfo) estimation method for practical tdcs applications. by utilizing preamble consisted of multiple identical training sequences, sto can be estimated by the demodulation procedure of cyclic code shift keying (ccsk), in which the circular correlation is performed by utilizing fast fourier transform (fft). thus the estimation procedure is significantly simplified with considerable complexity reduction, which is analyzed and compared to other two conventional methods. the simulation results demonstrates that the proposed sto and cfo estimation scheme is a low complexity solution for tdcs in multipath rayleigh fading channels with comparable estimation performance in terms of probability of incorrect timing offset and mean-square-error of cfo.
symbolic_computation	the transition phenomenon of few-cycle-pulse optical solitons from a pure modified korteweg-de vries (mkdv) to a pure sine-gordon regime can be described by the non-autonomous mkdv-sinh-gordon equation with time-dependent coefficients. based on the bell polynomials, hirota method and symbolic computation, bilinear forms and soliton solutions for this equation are obtained. backlund transformations (bts) in both the binary bell polynomial and bilinear forms are obtained. by virtue of the bts and ablowitz-kaup-newell-segur system, lax pair and infinitely many conservation laws for this equation are derived as well.
computer_programming	augmented reality allows to add virtual object in real scene. it has an increasing interest last years since mobile device becomes performant and cheap. the augmented reality is used in different domains, like maintenance, training, education, entertainment or medicine. the demonstrator we show is focused on maintenance operations. a step by step process is presented to the operator in order to maintain an element of a system. based on this demonstration, we will explain the modelling we propose allowing describing an entire maintenance process with augmented reality. indeed it is still difficult creating augmented reality application without computer programming skills. the proposed model will allow to create an authoring tool - or to plug to an existing one - in order to create augmented reality process without deep computer programming skills.
bioinformatics	background the electrocardiographically measured qt interval (qt) is heritable and its prolongation is an established risk factor for several cardiovascular diseases. yet, most qt genetic studies have been performed in european ancestral populations, possibly reducing their global relevance. objective to leverage diversity and improve biological insight, we fine mapped 16 of the 35 previously identified qt loci(46%) in populations of african american(n = 12,410) and hispanic/latino (n = 14,837) ancestry. methods racial/ethnic-specific multiple linear regression analyses adjusted for heart rate and clinical covariates were examined separately and in combination after inverse-variance weighted trans-ethnic meta-analysis. results the 16 fine-mapped qtlociincludedontheillumina metabochip represented 21 independent signals, of which 16(76%) were significantly (p-value100 kb). finally, bioinformatics-based functional characterization suggested a regulatory function in cardiac tissues for the majority of independent signals that generalized and the novel snps. conclusion our findings suggest that a majority of identified snps implicate gene regulatory dysfunction in qt prolongation, that the same loci influence variation in qt across global populations, and that additional, novel, population-specific qt signals exist.
network_security	today, the development of information and communications technologies have changed the utility landscape dramatically. in particular, electricity distribution networks rely heavily on a multitude of intelligent systems and devices that communicate among each other in much more advanced ways than in the past. as the smart grid is becoming nowadays a critical component in the electricity delivery system, it is important to make sure the grid is equipped with adequate security mechanisms that are able to guarantee its reliable operation and real-time information exchange within the power infrastructure. therefore, in this paper we analyze critical cybersecurity aspects associated with smart grid services, including previous cyber-attack cases on smart grids, potential vulnerabilities/threats, and advanced cybersecurity strategies for smart grids with technical and management measures. ultimately, while the service providers should continuously enhance the traditional security measures such as authentication, access control, authorization, data encryption, public key infrastructure (pki), firewalls, log analysis, intrusion detection systems, and network security protocols, we propose that the advanced technical measures should 1) make smart grids survivable even under cyber attacks and internal failures; 2) employ a defense-in-depth approach; 3) employ a defense-in-depth approach; and 4) provide more scalable security measures. furthermore, we also propose that the advanced management measures should 1) establish a cybersecurity governance strategy; 2) develop a strong incident response plan; 3) cultivate a culture of security; 4) employ a public-private partnership approach; and 5) comply with widely recognized security standards.
network_security	internet protocol (ip) spoofing is a serious problem on the internet. it is an attractive technique for adversaries who wish to amplify their network attacks and retain anonymity. many approaches have been proposed to prevent ip spoofing attacks; however, they do not address a significant deployment issue, i.e., filtering inefficiency caused by a lack of deployment incentives for adopters. to defeat attacks effectively, one mechanism must be widely deployed on the network; however, the majority of the antispoofing mechanisms are unsuitable to solve the deployment issue by themselves. each mechanism can work separately; however, their defensive power is considerably weak when insufficiently deployed. if we coordinate partially deployed mechanisms such that they work together, they demonstrate considerably superior performance by creating a synergy effect that overcomes their limited deployment. therefore, we propose a universal antispoofing (uas) mechanism that incorporates existing mechanisms to thwart ip spoofing attacks. in the proposed mechanism, intermediate routers utilize any existing anti-spoofing mechanism that can ascertain if a packet is spoofed and records this decision in the packet header. the edge routers of a victim network can estimate the forgery of a packet based on this information sent by the upstream routers. the results of experiments conducted with real internet topologies indicate that uas reduces false alarms up to 84.5% compared to the case where each mechanism operates individually.
machine_learning	forecasting stock returns and their risk represents one of the most important concerns of market decision makers. although many studies have examined single classifiers of stock returns and risk methods, fusion methods, which have only recently emerged, require further study in this area. the main aim of this paper is to propose a fusion model based on the use of multiple diverse base classifiers that operate on a common input and a meta classifier that learns from base classifiers' outputs to obtain more precise stock return and risk predictions. a set of diversity methods, including bagging, boosting and ada-boost, is applied to create diversity in classifier combinations. moreover, the number and procedure for selecting base classifiers for fusion schemes is determined using a methodology based on dataset clustering and candidate classifiers' accuracy. the results demonstrate that bagging exhibited superior performance within the fusion scheme and could achieve a maximum of 83.6% accuracy with decision tree, lad tree and rep tree for return prediction and 88.2% accuracy with bf tree, dtnb and lad tree in risk prediction. for feature selection part, a wrapper-ga algorithm is developed and compared with the fusion model. this paper seeks to help researcher select the best individual classifiers and fuse the proper scheme in stock market prediction. to illustrate the approach, we apply it to tehran stock exchange (tse) data for the period from 2002 to 2012. (c) 2016 elsevier b.v. all rights reserved.
algorithm_design	this paper studies robust resource-allocation algorithm design for a multiuser multiple-input-single-output (miso) cognitive radio (cr) downlink communication network. we focus on a secondary system that provides wireless unicast secure layered video information to multiple single-antenna secondary receivers. the resource-allocation algorithm design is formulated as a nonconvex optimization problem for the minimization of the total transmit power at the secondary transmitter. the proposed framework takes into account a quality-of-service (qos) requirement regarding video communication secrecy in the secondary system, the imperfection of the channel state information (csi) of potential eavesdroppers (primary receivers) at the secondary transmitter, and a limit for the maximum tolerable received interference power at the primary receivers. thereby, the proposed problem formulation exploits the self-protecting architecture of layered transmission and artificial noise generation to ensure communication secrecy. the considered nonconvex optimization problem is recast as a convex optimization problem via semidefinite programming (sdp) relaxation. it is shown that the global optimal solution of the original problem can be constructed by exploiting both the primal and the dual optimal solutions of the sdp-relaxed problem. in addition, two suboptimal resource-allocation schemes are proposed for the case when the solution of the dual problem is unavailable for constructing the optimal solution. simulation results demonstrate significant transmit power savings and robustness against csi imperfection for the proposed optimal and suboptimal resource-allocation algorithms employing layered transmission compared to baseline schemes employing traditional single-layer transmission.
computer_graphics	let x = {f(1), ..., f(n)} be a set of scalar functions of the form f(i) : (2) which satisfy some natural properties. we describe a subdivision algorithm for computing a clustered epsilon-isotopic approximation of the minimization diagram of x. by exploiting soft predicates and clustering of voronoi vertices, our algorithm is the first that can handle arbitrary degeneracies in x, and allow scalar functions which are piecewise smooth, and not necessarily semi-algebraic. we apply these ideas to the computation of anisotropic voronoi diagram of polygonal sets; this is a natural generalization of anisotropic voronoi diagrams of point sites, which extends multiplicatively weighted voronoi diagrams. we implement a prototype of our anisotropic algorithm and provide experimental results.
bioinformatics	recent research has proposed that git2 (g protein-coupled receptor kinase interacting protein 2) acts as an integrator of the aging process through regulation of 'neurometabolic' integrity. one of the commonly accepted hallmarks of the aging process is thymic involution. at a relatively young age, 12 months old, git2(-/-) mice present a prematurely distorted thymic structure and dysfunction compared to age-matched 12 month-old wild-type control (c57bl/6) mice. disruption of thymic structure in git2(-/-) (git2ko) mice was associated with a significant reduction in the expression of the cortical thymic marker, troma-i (cytokeratin 8). double positive (cd4(+) cd8(+)) and single positive cd4(+) t cells were also markedly reduced in 12 month-old git2ko mice compared to age-matched control wild-type mice. coincident with this premature thymic disruption in git2ko mice was the unique generation of a novel cervical 'organ', i.e. 'parathymic lobes'. these novel organs did not exhibit classical peripheral lymph node-like characteristics but expressed high levels of t cell progenitors that were reflexively reduced in git2ko thymi. using signaling pathway analysis of git2ko thymus and parathymic lobe transcriptomic data we found that the molecular signaling functions lost in the dysfunctional git2ko thymus were selectively reinstated in the novel parathymic lobe-suggestive of a compensatory effect for the premature thymic disruption. broader inspection of high-dimensionality transcriptomic data from git2ko lymph nodes, spleen, thymus and parathymic lobes revealed a systemic alteration of multiple proteins (dbp, tef, per1, per2, fbxl3, ddit4, sin3a) involved in the multidimensional control of cell cycle clock regulation, cell senescence, cellular metabolism and dna damage. altered cell clock regulation across both immune and nonimmune tissues therefore may be responsible for the premature 'aging' phenotype of git2ko mice.
image_processing	the k-svd algorithm has been successfully utilized for adaptively learning the sparse dictionary in 2-d seismic denoising. because of the high computational cost of many singular value decompositions (svds) in the k-svd algorithm, it is not applicable in practical situations, especially in 3-d or 5-d problems. in this paper, i extend the dictionary learning based denoising approach from 2-d to 3-d. to address the computational efficiency problem in k-svd, i propose a fast dictionary learning approach based on the sequential generalized k-means (sgk) algorithm for denoising multidimensional seismic data. the sgk algorithm updates each dictionary atom by taking an arithmetic average of several training signals instead of calculating an svd as used in k-svd algorithm. i summarize the sparse dictionary learning algorithm using k-svd, and introduce sgk algorithm together with its detailed mathematical implications. 3-d synthetic, 2-d and 3-d field data examples are used to demonstrate the performance of both k-svd and sgk algorithms. it has been shown that sgk algorithm can significantly increase the computational efficiency while only slightly degrading the denoising performance.
data_structures	background: a current focus of biofilm research is the chemical interaction between microorganisms within the biofilms. prerequisites for this research are bioassay systems which integrate reliable tools for the planning of experiments with robot-assisted measurements and with rapid data processing. here, data structures that are both human-and machine readable may be particularly useful. results: in this report, we present several simplification and robotisation options for an assay of bacteria-induced biofilm formation by the freshwater diatom achnanthidiumminutissimum. we also tested several proof-of-concept robotisation methods for pipetting, as well as for measuring the biofilm absorbance directly in the multi-well plates. furthermore, we exemplify the implementation of an improved data processing workflow for this assay using the konstanz information miner (knime), a free and open source data analysis environment. the workflow integrates experiment planning files and absorbance read-out data, towards their automated processing for analysis. conclusions: our workflow lead to a substantial reduction of the measurement and data processing workload, while still reproducing previously obtained results in the a. minutissimum biofilm assay. the methods, scripts and files we designed are described here, offering adaptable options for other medium-throughput biofilm screenings.
relational_databases	nosql and especially graph databases are constantly gaining popularity among developers as they promise to deliver superior performance when handling highly interconnected data compared to relational databases. apache shindig is the reference implementation for opensocial with a highly interconnected data model. however, it had a relational database as back-end. in this paper we describe our experiences with the graph database neo4j as back-end and compare cypher, gremlin and java as alternatives for querying data with mysql. we consider performance as well as usability from a developer 's perspective. our results show that cypher is a good query language in terms of code readability and has a moderate overhead for most queries (20-200%). however, it has to be supplemented with ""stored procedures"" to make up for some performance deficits in pattern matching queries (>1000%). the restful api is unusable slow, whereas our websocket connection performs significantly better (>650%). (c) 2015 elsevier inc. all rights reserved.
parallel_computing	mode division multiplexing (mdm) is a promising technology for increasing the aggregate bandwidth of multimode fiber (mmf) in conjunction with wavelength division multiplexing (wdm) in face of the impending capacity crunch in optical fiber networks. this paper investigates the effect of radial and azimuthal mode spacings in a 25-channel mdm-wdm system in mmf using a spatial light modulator-controlled vcsel array for excitation of laguerre-gaussian (lg) modes. a data rate of 25gbps is achieved at a central wavelength of 1550.12 nm. the effects of different azimuthal and radial mode spacings of lg modes are analyzed in terms of the channel impulse response, eye diagram and bit-error rate.
algorithm_design	as with other nature-inspired algorithms, the cuckoo optimization algorithm (coa) produces a population of candidate solutions to find the (near-) optimal solutions to a problem. in this paper, several modifications, including a dynamic mutation operator, are proposed for this algorithm. design of experiments is employed to determine factors controlling the value of parameters and the target levels of those values to achieve desirable output. the efficiency of the modified coa algorithm is substantiated with the help of several optimization test problems. the results are then compared to other well-known algorithms such as pso, de and harmony search using a non-parametric statistical procedure. in order to analyze its effectiveness, the proposed modified coa is applied to a feature selection problem and spacecraft attitude control problem.
symbolic_computation	quantum computing promises to improve the speed and scalability of computations over that of classical computing hardware. at this early stage of quantum computer hardware development, software frameworks which support rapid prototyping of quantum solutions on small-scale hardware or simulators are necessary to explore the application of quantum algorithms to hard computational problems. we present a software library, ""qxlib,"" which incorporates symbolic computation of optimization functions for quantum annealers as one means to enable rapid prototyping. we demonstrate its effectiveness on integer linear programming and integer factorization problems.
symbolic_computation	taking advantage of the hirota bilinear form, four classes of lump-type solutions to the (3+1)dimensional jimbo-miwa equation are presented through symbolic computation with maple. special choices of the involved parameters guaranteeing analyticity of the fourth solution are given, together with two particular lump-type solutions.
network_security	network security with encryption and decryption technology to complete the application layer, but this technology will bring computing resources and a waste of energy, particularly in these two resources are limited wireless communication system for this problem, we use a large-scale multi-antenna technology, using beamforming algorithm in power and spectrum limited conditions, to maximize the mutual information system security, the simulation results demonstrate the ability to secure transmission algorithms can effectively improve the system.
computer_vision	colour detection plays an important role for many computer vision-based applications. however, most existing colour detection methods tend to be environment dependent since slight changes of environmental factors such as illumination or shadowing effects could greatly reduce their performances. in this paper, a new colour model is introduced to allow enhanced colour detection from images, even with significantly different lighting conditions and image qualities. the proposed colour model is called the hpbr colour model. it is converted from the rgb colour model and it consists of three colour components, namely, hue (h), purity (p) and brightness (br). this colour model can be represented in three different geometric shapes: diamond, sphere and cylinder. to assess the effectiveness of the model, two different colour detection methods have been applied onto benchmark images. experimental results from both methods confirmed that the proposed colour model produced the best colour detection results among existing models.
network_security	internet technology today is not free from many problems or security holes. this security holes could be exploited by an unauthorized person to steal important data. the case of the attacks occurred because the party that was attacked also did not realize the importance of network security to be applied to the system. honeypot is a system that is designed to resemble the original production system and is made with the intention to be attacked or compromised. in this research, cubieboard implemented using low interaction honeypot as a decoy to attract attackers. the result of this research is a low interaction honeypot implemented on embedded system with the form of cubieboard that can emulates security vulnerabilities such as directory buster brute force, lfi, and rfi with 100% success rate, but still could not emulates sql injection vulnerability. one of the result of stress test with 773 samples, obtained average time of 5275 ms, deviation 2067 ms, sample throughput 367.012 per minute, and with median 5381 ms. the stress test is conducted with 50 threads and 10 ramp-ups per second.
software_engineering	with the explosion of 3d character animation across contemporary screen media, more people, disciplines and technologies are now engaging with its production. explicit representations of computer animation processes help facilitate engagement at a high level, however fail to convey the depth of specialised creative techniques, technical processes and discipline language that is prevalent during the act of animating. this paper introduces the mk i production model', a conceptual process which through its novel use of the software engineering methodology agent-oriented modelling', conveys such specialised attributes within an explicit process for producing 3d character animation. to gather insights into how this model is used and perceived by animators within a production environment, it was entrenched within a large undergraduate student animation project named gunter 's fables', where it was positioned as the principal device to inform animators of the production process and their expected activity. the project management team also used the model in weekly peer review sessions as a basis to evaluate animation, and to convey progress and achievement with a colour rating scale. upon completion of the production phase, the project 's 12 student animators successfully delivered 41 short, 10-15 second 3d character animation scenarios that were deemed to be of a consistent and fit for purpose quality. findings from regular sweatbox' review sessions and questionnaires suggest that further investigation and iterative development of the model may improve user engagement with the process. however, the model 's demonstrated ability to inform a depth of production process supports the notion that this novel production concept presents a way forward in the communication and production of 3d character animation, and allied animation activities.
machine_learning	global optimisation of unknown noisy functions is a daunting task that appears in domains ranging from games to control problems to meta-parameter optimisation for machine learning. we show how to incorporate heuristics to stochastic simultaneous optimistic optimization (stosoo), a global optimisation algorithm that has very weak requirements from the function. in our case, heuristics come in the form of covariance matrix adaptation evolution strategy (cma-es). the new algorithm, termed guided stosoo (stosoo-g), combines the ability of cma-es for fast local convergence (due to the algorithm following the ""natural"" gradient) and the global optimisation abilities of stosoo. we compare all three algorithms in the ""harder"" parts of the comparing continuous optimisers on black-box optimization benchmarking benchmark suite, which provides a default set of functions for testing. we show that our approach keeps the best of both worlds, i.e. the almost optimal exploration/exploitation of stosoo with the local optimisation strength of cma-es.
symbolic_computation	under investigation in this paper is a (2 + 1)-dimensional nonlinear schrodinger equation in the heisenberg ferromagnetic spin chain. via the symbolic computation and hirota method, the bilinear forms, dark one-, two- and three-soliton solutions are derived. propagation and interaction for the dark solitons are illustrated graphically: amplitude and shape of the dark one soliton keep invariant during the propagation, which imply the transport of the energy is stable in the (2 + 1)-dimensional heisenberg ferromagnetic spin chain. through the asymptotic analysis, elastic and inelastic interactions between the dark two solitons are discussed. for the elastic interaction, oblique, head-on and overtaking interactions between the dark two solitons are displayed, where the amplitudes and shapes remain unchanged after interaction except for certain phase shifts. however, in the area of the inelastic interaction, amplitudes of the dark two solitons vanish after interaction. for the elastic interaction among the dark three solitons, oblique, overtaking and interaction among the dark two parallel solitons and a single one are presented, whose characteristics are similar to those of the dark two solitons. inelastic-elastic interaction is investigated as well. linear stability analysis is used to analyze modulation instability and prove the dark solitons are stable. (c) 2016 elsevier ltd. all rights reserved.
cryptography	this work presents two implementation attacks against cryptographic algorithms. based on these two presented attacks, this thesis shows that the assessment of physical attack complexity is error-prone. hence, cryptography should not rely on it. cryptographic technologies have to be protected against all implementation attacks, have they already been realized or not. the development of countermeasures does not require the successful execution of an attack but can already be carried out as soon as the principle of a side channel or a fault attack is understood.
distributed_computing	distributed query processing is of paramount importance in next-generation distribution services, such as internet of things (iot) and cyber physical systems. even if several multi-attribute range queries supports have been proposed for peer-to-peer systems, these solutions must be rethought to fully meet the requirements of new computational paradigms for iot, like fog computing. this paper proposes dragon, an efficient support for distributed multi-dimensional range query processing targeting efficient query resolution on highly dynamic data. in dragon nodes at the edges of the network collect and publish multi-dimensional data. the nodes collectively manage an aggregation tree storing data digests which are then exploited, when resolving queries, to prune the sub-trees containing few or no relevant matches. multi-attribute queries are managed by linearizing the attribute space through space filling curves. we extensively analysed different aggregation and query resolution strategies in a wide spectrum of experimental set-ups. we show that dragon manages efficiently fast changing data values. further, we show that dragon resolves queries by contacting a lower number of nodes when compared to a similar approach in the state of the art. (c) 2015 elsevier b.v. all rights reserved.
computer_graphics	the traditional rubber hand illusion is a psychological experiment where participants are under the illusion that a rubber hand is part of their own body. this paper examines the use of real, virtual and augmented reality environments for identifying the elements that influence body ownership in healthy participants. compared to the classical experiment where a plastic rubber hand was used, a realistic 3d representation was chosen to create the same illusion this time in both immersive virtual reality and augmented reality. experiments were performed on 30 volunteers undergoing testing session composed of three stages. participants were asked to complete two different questionnaires, one measuring their cognitive workload and another one regarding their experience with the rubber hand illusion. in addition, eeg signals of the individuals were recorded, resulting in 90 electroencephalogram datasets. results indicate correlations between ownership statements with beta and gamma electroencephalogram bands in premotor cortex activity. link between higher gamma production in ventral premotor area during the illusion was established in previous studies.
image_processing	pointwise intensity-based algorithms are the most popular algorithms in dynamic laser speckle measurement of physical or biological activity. the output of this measurement is a two-dimensional map which qualitatively separates regions of higher or lower activity. in the paper, we have proposed filtering of activity maps to enhance visualization and to enable quantitative determination of activity time scales. as a first step, we have proved that the severe spatial fluctuations within the map resemble a signal-dependent noise. as a second step, we have illustrated implementation of the proposed idea by applying filters to non-normalized and normalized activity estimates derived from synthetic and experimental data. statistical behavior of the estimates has been analyzed to choose the filter parameters, and substantial narrowing of the probability density functions of the estimates has been achieved after the filtering. the filtered maps exhibit an improved contrast and allowed for quantitative description of activity.
computer_programming	computer aided x-ray microtomography is an increasingly popular method to investigate the structure of materials. continuing improvements in the technique are resulting in increasingly larger data sets. the analysis of these data sets generally involves executing a static workflow for multiple samples and is generally performed manually by researchers. executing these processes requires a significant time investment. a workflow which is able to automate the activities of the user would be useful. in this work, we have developed an automated workflow for the analysis of microtomography scanned bread dough data sets averaging 5gb in size. comparing the automated workflow with the manual workflow indicates a significant amount of the time spent (33% in the case of bread dough) on user interactions in manual method. both workflows return similar results for porosity and pore frequency distribution. finally, by implementing an automated workflow, users save the time which would be required to manually execute the workflow. this time can be spent on more productive tasks. lay description technological advances in x-ray scanning techniques have resulted in larger, more complex microtomographic datasets. processing these datasets can be both a time consuming and oftentimes repetitive task as datasets of similar materials tend to have similar characteristics. what if there was a better way to analyze these datasets? our research has investigated using computer programming languages instead of researchers to automatically perform tasks involved in the analysis of microtomographic datasets of ten scanned bread dough samples. our results highlighted the benefits of using computers to automate the analysis process, demonstrating that nearly 33% of the time required is due to researchers interacting with the analysis programs and not from the analysis itself. the quantitative data provided by the automated workflow are nearly identical to results found by researchers. this research highlights the benefits of using automated computer processing for analysis of microtomographic datasets, which would allow researchers to spend their efforts elsewhere.
symbolic_computation	liquids with gas bubbles are commonly seen in medical science, natural science, daily life and engineering. nonlinear-wave symbolic computation on the (3 + 1)-dimensional variable-coefficient kudryashov-sinelshchikov model for a bubbly liquid is hereby performed. an auto-backlund transformation and with some solitonic solutions are obtained. with respect to the density fluctuation of the bubble-liquid mixture, both the auto-backlund transformation and solitonic solutions depend on the bubble liquid-viscosity, transverse-perturbation, bubble-liquid-nonlinearity and bubble-liquid dispersion coefficient functions. we note that some shock waves given by our solutions have been observed by the gas-bubble/liquid-mixture experiments. effects on a bubbly liquid with respect to the bubble-liquid-viscosity, transverse-perturbation, bubble-liquidnonlinearity and bubble-liquid-dispersion coefficient functions might be detected by the future gas-bubble/liquid-mixture experiments.
software_engineering	the ability to exploit emerging exascale computational systems will require a careful review and redesign of core numerical algorithms and their implementations to fully exploit multiple levels of concurrency, hierarchical memory structures and heterogeneous processing units that will become available in these computational platforms. this paper presents the ""unite and conquer"" approach to solve linear systems of equations and eigenvalue problems for extreme scale computing. indeed, there are two ways to optimize the execution of a restarted method on a large-scale distributed system. the first one is to optimize the number of floating point operations per restart cycle through maximizing the concurrency inside a restart cycle while minimizing latencies. the second-way is to accelerate/improve the rate of convergence for a given computational scheme. the unite and conquer restarted approach focuses on decreasing the number of restart cycles by coupling either synchronously or asynchronously several restarted methods called also co-methods. in the end of a restart cycle, each co-method locally gathers available results of all collaborating co-methods and selects the best one in order to create its restarting information. consequently this permits the global reduction of the number of cycles to convergence. the unite and conquer restarted methods are heterogeneous, fault tolerant, support asynchronous communications and present a big potential of load balancing. due to these properties, they are well adapted to large-scale multi-level parallel architectures. we show the relevant programming paradigms that allow multi-level parallel expression of these methods and how the software engineering technology can contribute significantly in achieving high scalability. we present some experiments validating the approach for unite and conquer restarted krylov methods on several parallel and distributed platforms. (c) 2016 elsevier b.v. all rights reserved.
cryptography	the new styles and ways of life lead to greater use of wireless networks, the mobile device being a tool for data transmission, which are susceptible to threats in the transmission channels in the network. it security plays a very important role in guaranteeing the availability, privacy and integrity of information, one of the techniques that helps in this task is cryptography, whose foundation is to transform a message so that it is unintelligible except for those who have the key to decipher it. the research focuses on the use of the rsa algorithm between mobile devices, the encrypted data is sent through communication channels called threads that through formulas and processes executed on the server, will help to execute the encryption and decryption of the data. to carry it out, a prototype for the exchange of data between mobile devices wirelessly was designed and implemented, conducting performance tests with three nodes to improve the security. the results show the efficiency of the algorithm and additionally its functionality, the times of encryption and decryption are fast against the sending of information without any method or algorithm used.
computer_graphics	in this paper, we present a simple and efficient method to represent terrains as elevation functions built from linear combinations of landform features (atoms). these features can be extracted either from real world data-sets or procedural primitives, or from any combination of multiple terrain models. our approach consists in representing the elevation function as a sparse combination of primitives, a concept which we call sparse construction tree, which blends the different landform features stored in a dictionary. the sparse representation allows us to represent complex terrains using combinations of atoms from a small dictionary, yielding a powerful and compact terrain representation and synthesis tool. moreover, we present a method for automatically learning the dictionary and generating the sparse construction tree model. we demonstrate the efficiency of our method in several applications: inverse procedural modeling of terrains, terrain amplification and synthesis from a coarse sketch.
parallel_computing	we present a novel image-analysis based method for automatically distinguishing low, intermediate, and high grades of breast cancer in digitized histopathology. a multiple level feature set, including pixel-,object-,and semantic-level features derived from convolutional neural networks (cnn), is extracted from 106 hematoxylin and eosin stained breast biopsy tissue studies from 106 women patients. these multilevel features allow not only characterization of cancer morphology, but also extraction of structural and interpretable information within the histopathological images. in this study, an improved hybrid active contour model based segmentation method was used to segment nuclei from the images. the semantic level features were extracted by a cnn approach, which described the proportions of nuclei belonging to the different grades, in conjunction with pixel-level (texture) and object-level (architecture) features, to create an integrated set of image attributes that can potentially outperform either subtype of features individually. we utilized a cascaded approach to train multiple support vector machine (svm) classifiers using combinations of feature subtypes to enable the possibility of maximizing the performance by leveraging different feature sets extracted from multiple levels. final class (cancer grade) was detertnined by combining the scores produced by the individual svm classifiers. by employing a light (three-layer) cnn model and parallel computing, the presented approach is computationally efficient and applicable to large-scale datasets. the method achieved an accuracy of 0.92 for low versus high, 0.77 for low versus intermediate, and 0.76 for intermediate versus high, and an overall accuracy of 0.69 when discriminating low, intermediate, and high grades of histopathological breast cancer images. this suggested that our grading method could be useful in developing a computational diagnostic tool for differentiating breast cancer grades, which might enable objective and reproducible alternative for diagnosis. (c) 2016 elsevier b.v. all rights reserved.
distributed_computing	background: mapping disease rates over a region provides a visual illustration of underlying geographical variation of the disease and can be useful to generate new hypotheses on the disease aetiology. however, methods to fit the popular and widely used conditional autoregressive (car) models for disease mapping are not feasible in many applications due to memory constraints, particularly when the sample size is large. we propose a new algorithm to fit a car model that can accommodate both individual and group level covariates while adjusting for spatial correlation in the disease rates, termed indicar. our method scales well and works in very large datasets where other methods fail. results: we evaluate the performance of the indicar method through simulation studies. our simulation results indicate that the indicar provides reliable estimates of all the regression and random effect parameters. we also apply indicar to the analysis of data on neutropenia admissions in new south wales (nsw), australia. our analyses reveal that lower rates of neutropenia admissions are significantly associated with individual level predictors including higher age, male gender, residence in an outer regional area and a group level predictor of social disadvantage, the socio-economic index for areas. a large value for the spatial dependence parameter is estimated after adjusting for individual and area level covariates. this suggests the presence of important variation in the management of cancer patients across nsw. conclusions: incorporating individual covariate data in disease mapping studies improves the estimation of fixed and random effect parameters by utilizing information from multiple sources. health registries routinely collect individual and area level information and thus could benefit by using indicar for mapping disease rates. moreover, the natural applicability of indicar in a distributed computing framework enhances its application in the big data domain with a large number of individual/group level covariates. ci nsw study reference number: 2012/07/410. dated: july 2012.
network_security	most complex tasks on the internet-both malicious and benign-are collectively carried out by clusters of ip addresses. we demonstrate that it is often possible to discover such clusters by processing data sets and logs collected at various vantage points in the network. obviously, not all clusters discovered in this way are malicious. nevertheless, we show that malicious clusters can accurately be distinguished from benign ones by simply using an ip blacklist and without requiring any complex analysis to verify malicious behavior. in this paper, we first propose a novel clustering framework which can be applied on data sets of network interactions to identify ip clusters carrying out a specific task collectively. then, given such a list of identified clusters of ip addresses, we present a simple procedure to spot the malicious ones using an ip blacklist. we show that by choosing the parameter of the proposed clustering process optimally using a blacklist, hence making it blacklist-aware, we significantly improve our overall ability to detect malicious clusters. furthermore, we mathematically show that even a blacklist with poor accuracy can be used to detect malicious clusters with high precision and recall. finally, we demonstrate the efficacy of the proposed scheme using real-world login events captured at the login servers of a large webmail provider with hundreds of millions of active users.
software_engineering	the objective of this study is to investigate the use of an alternative working pair in a solar absorption cooling system. licl-h2o is the new examined pair and it is compared energetically and exegetically with the conventional pair libr-h2o, which is the most usual in air-conditioning applications. the simplest solar cooling system is analyzed in order to focus in the comparison between these working fluids. specifically, flat plate collectors, coupled with a storage tank, feed the single effect absorption chiller which produces 250 kw cooling at 10 degrees c. the two pairs are examined parametrically for various heat source temperature levels and for three ambient temperature levels (25 degrees c, 30 degrees c and 35 degrees c). the minimization of the collecting area, which means maximum exergetic efficiency, is the optimization goal in every case. the final results show that licl-h2o pair performs better in all cases by giving greater exergetic efficiency. more specifically, about 8% lower collecting area is required to cover the demanded cooling load with this working pair. another interesting result is that the optimum heat source temperature for the licl-h2o is roughly lower than the respective for the libr-h2o. the system is analyzed in steady state with the commercial software engineering equator solver (ees). (c) 2016 elsevier ltd. all rights reserved.
cryptography	in this study, the authors propose a novel method to encrypt a colour image by use of logistic mapping and double random-phase encoding. firstly, use logistic mapping to diffuse the colour image, then the red, green and blue components of the result are scrambled by replacement matrices generated by logistic mapping. secondly, by utilising double random-phase encoding to encrypt the three scrambled images into one encrypted image. experiment results reveal the fact that the proposed method not only can achieve good encryption result, but also that the key space is large enough to resist against common attack.
image_processing	this paper presents preliminary results of the application of two-kinect cameras system on a two wheeled indoor mobile robot for off-line optimal path planning and execution. in our approach, the robot makes use of depth information delivered by the vision system to accurately model its surrounding environment through image processing techniques. in addition, a genetic algorithm is implemented to generate a collision-free optimal path linking an initial configuration of the mobile robot (source) to a final configuration (target). after that, piecewise cubic hermite interpolating polynomial is used to smooth the generated optimal path. finally, an adaptive fuzzy-logic controller is designed to keep track of a mobile robot on the desired smoothed path (by transmitting the appropriate right and left velocities using wireless communication). in parallel, sensor fusion (odometry sensors and kinect sensors) is used to estimate the current position and orientation of the robot using kalman filter. the validation of the proposed solution is carried out using the differentially-driven mobile robot, robuter, to successfully achieve safe motion (without colliding with obstacles) in an indoor environment. (c) 2016 elsevier b.v. all rights reserved.
image_processing	a four-dimensional visualization approach, featuring three dimensions in space and one dimension in time, is proposed to study local electrode degradation effects during voltage cycling in fuel cells. noninvasive in situ micro x-ray computed tomography (xct) with a custom fuel cell fixture is utilized to track the same cathode catalyst layer domain throughout various degradation times from beginning-of-life (bol) to end-of-life (eol). with this unique approach, new information regarding damage features and trends are revealed, including crack propagation and catalyst layer thinning being quantified by means of image processing and analysis methods. degradation heterogeneities as a result of local environmental variations under land and channel are also explored, with a higher structural degradation rate under channels being observed. density and compositional changes resulting from carbon corrosion and catalyst layer collapse and thinning are observed by changes in relative x-ray attenuation from bol to eol, which also indicate possible vulnerable regions where crack initiation and propagation may occur. electrochemical diagnostics and morphological features observed by micro-xct are correlated by additionally collecting effective catalyst surface area, double layer capacitance, and polarization curves prior to imaging at various stages of degradation. (c) 2017 elsevier b.v. all rights reserved.
machine_learning	multiple instance (mi) learning aims at identifying the underlying concept from collectively labeled data. a training sample consists of a set, known as a bag, of unlabelled instances. the bag as a whole is labeled positive if at least one instance in the bag is positive, or negative otherwise. given such training samples, the goal is to learn a description of the common instance(s) among the positive bags, i.e., the underlying concept that is responsible for the positive label. in this work, we introduce a learning scheme based on the notion of partial entropy for mi concept learning. partial entropy accentuates the intra-class information by focusing on the information reflected from the positive class in proportion to the total entropy, maximization of which is to equalize the likelihoods of intra-class outcomes among the positive class, essentially reflecting the intended concept. when coupled with a distance-based probabilistic model for mi learning, it is equivalent to seeking out a concept estimate that equalizes the intra-class distances while the distance to negative bags is restrained. it produces patterns that are similar to at least one instance from each of the positive bags while dissimilar from all instances in negative bags. the generated patterns from the optimization process correspond to prototypical concepts. maximum partial entropy is conceptually simple and experimental results on different mi datasets demonstrate its effectiveness in learning an explicit representation of the concept and its competitive performance when applied to classification tasks.
symbolic_computation	in this paper, stability and local bifurcation behaviors for the nonlinear aeroelastic model of an airfoil with external store are investigated using both analytical and numerical methods. three kinds of degenerated equilibrium points of bifurcation response equations are considered. they are characterized as (1) one pair of purely imaginary eigenvalues and two pairs of conjugate complex roots with negative real parts; (2) two pairs of purely imaginary eigenvalues in nonresonant case and one pair of conjugate complex roots with negative real parts; (3) three pairs of purely imaginary eigenvalues in nonresonant case. with the aid of maple software and normal form theory, the stability regions of the initial equilibrium point and the explicit expressions of the critical bifurcation curves are obtained, which can lead to static bifurcation and hopf bifurcation. under certain conditions, 2-d tori motion may occur. the complex dynamical motions are considered in this paper. finally, the numerical solutions achieved by the fourth-order runge-kutta method agree with the analytic results.
network_security	wireless sensor networks (wsns) have become increasingly popular in many applications across a broad range of fields. securing wsns poses unique challenges mainly due to their resource constraints. traditional public key cryptography (pkc) for instance is considered to be too computationally expensive for direct implementation in wsns. elliptic curve cryptography (ecc) allows one to reach the same level of security as traditional pkc using smaller key sizes. in this paper, a key distribution protocol was designed to securely provide authenticated motes with secret system keys using ecc based cryptographic functions. the designed scheme met the minimum requirements for a key distribution scheme to be considered secure and efficient in wsns.
computer_graphics	many vocational schools and universities offer lectures on non-stereoscopic 3d computer graphics (3dcg) animation production, as well as practical 3dcg software operation, modeling, and animation production. however, relatively few of these educational institutions provide lectures on stereoscopic 3d (s3d). to address this gap, we developed two 15-week syllabuses with educational materials, which focused on both knowledge based and skills-based education about 3dcg animation and s3d computer graphics (s3dcg) animation production, considering the potential employment of students in animation studios. our investigation confirmed that the knowledge and skills of the subjects improved in this study, so we presented a report on the effectiveness of the educational materials for s3dcg animation production education at the education symposium of siggraph asia 2015. by developing these educational materials, we reaffirmed the importance of camerawork skills education, which may be regarded as the cornerstone of 3dcg animation and s3dcg animation production. however, camerawork skills education in 3dcg animation classes remains limited in vocational schools and universities at present. we hypothesized that one of the reasons for the paucity of camerawork skills education is the lack of suitable educational materials for practical classes in the use of 3dcg software. indeed, educational materials that allow teachers to begin camerawork skills education for 3dcg and s3dcg animation production without preparation are greatly lacking. furthermore, modeling and character animation are regarded as essential before camerawork practice. camerawork practice can be started immediately if sufficient educational materials are provided. the educational materials that we developed are suitable for s3dcg but also for 3dcg camerawork practice. in this study, we present the camerawork skills education and evaluation methods involved in the educational materials developed for the s3d lectures and for camerawork practical classes in s3dcg animation. we then discuss the experimental classes in which we used the educational materials and we report the results of s3d knowledge tests and camerawork practical tests, which were conducted in order to measure the learning/education outcomes for the participating subjects, as well as the results of a survey that focused on their subjective experiences. the experimental results clearly demonstrated improvements in both the knowledge and skills of subjects, thereby confirming the effectiveness of the camerawork educational materials. we suggest that if students with knowledge of s3d and 3dcg animation are well-practiced in the use of these educational materials, their skills in s3dcg camerawork would improve markedly. 2016 elsevier ltd. all rights reserved.
parallel_computing	in the framework of further development of a unified computational tool for the needs of biomedical optics, we introduce an electric field monte carlo (mc) model for simulation of backscattering of coherent linearly polarized light from a turbid tissue-like scattering medium with a rough surface. we consider the laser speckle patterns formation and the role of surface roughness in the depolarization of linearly polarized light backscattered from the medium. the mutual phase shifts due to the photons' pathlength difference within the medium and due to reflection/refraction on the rough surface of the medium are taken into account. the validation of the model includes the creation of the phantoms of various roughness and optical properties, measurements of co- and cross-polarized components of the backscattered/reflected light, its analysis and extensive computer modeling accelerated by parallel computing on the nvidia graphics processing units using compute unified device architecture (cuda). the analysis of the spatial intensity distribution is based on second-order statistics that shows a strong correlation with the surface roughness, both with the results of modeling and experiment. the results of modeling show a good agreement with the results of experimental measurements on phantoms mimicking human skin. the developed mc approach can be used for the direct simulation of light scattered by the turbid scattering medium with various roughness of the surface. (c) 2016 society of photo-optical instrumentation engineers (spie)
data_structures	cultural heritage (ch) documentation tasks usually involve professionals from different knowledge areas, which implies not only a huge amount of information and requirements, but also a very heterogeneous set of sources, data structures, content and formats. geographic information systems (gis) have been used extensively by cultural heritage specialists, but this is just working around the real problem: there is no specialized software for ch professionals to document their work in 3d. in this paper, we present software named agata that allows specialists to interact in real time with high resolution polygonal models, and to annotate different raster and vectorial information directly onto them that might be useful for current or future research. moreover, these annotations can be exported in a standard format that allows researchers from other disciplines that might be interested in the dataset to access such information easily. the system is able to manage and annotate not only on buildings or archaeological sites, but also sculptures or paintings directly into the 3d dataset of any ch physical element. (c) 2016 elsevier masson sas. all rights reserved.
data_structures	big data is growing remarkably with technological development. in the field of business, there is growing trend which try to find useful information to marketing activities from the enormous data. however, data analysis needs knowledge about data structures and programming skills. it is difficult for general employees to acquire these. in this study, we propose a system which visualizes the relation between the goods by using pos (point of sales) data with ids of a supermarket. we think that the analysts do not have the pre-requisite skills of programming, they can understand the relation between the goods by using our proposal system easily. then as the proposal system turns into a web application, they are able to share the result of analysis by using a web browser. it becomes clear that our proposal system is effective through a result of the interview evaluation.
computer_programming	since the shale gas production data predicted by traditional simulators is far below that of field test, reservoir volume parameters are often modified to meet the fitting demands. most pressure transient models of reservoir, merely based on darcy flow in the matrix and natural fractures, neglect the comprehensive influences of viscous flow, slip flow, transition flow and knudsen flow, etc. in this paper, two gas apparent permeability models considering multiple migration mechanisms are established through theoretical derivation and fitting. the models are applicable to various flow states and will help to simplify percolation models of matrix and fracture system. to achieve a better understanding of the migration mechanisms of shale matrix, the effects of compressed gas in the matrix pores, adsorbed gas on the pore wall and the diffused gas from kerogen on transient pressure should all be taken into account. based on the newly established gas apparent permeability models, a pseudo-triple-medium transient percolation mathematical model of multi-fractured horizontal well affected by multiple migration mechanisms is presented with the consideration of dissolved gas diffusion in kerogen and adsorbed gas desorption on matrix surface in the shale gas reservoir. source function idea combined with laplace transform, linear approximation and delta generalized function is used to get the point source solution of the mathematical model. by discretizing the artificial fractures, the pressure transient response of multi-fractured horizontal well in laplace space is obtained on the basis of point source solution. then pressure transient response type curves are plotted by computer programming, and pressure influence factors are also analyzed. besides, isothermal adsorption experiment data of shale cores indicates that the organic carbon content (toc) has obvious correlation with the adsorption coefficient defined in this paper, which gives a clue to link up the exploitation research with geological study, playing a guiding role in analyzing pressure response of the shale gas reservoir. (c) 2014 elsevier b.v. all rights reserved.
image_processing	the simultaneous three-dimensional (3d) visualization of intracranial tumors, brain structures, skull, and vessels is desired by neurosurgeons to create a clear mental picture of the anatomical orientation of the surgical field prior to the surgical intervention. different anatomical and pathological components are usually visualized separately on different magnetic resonance (mr) sequences; however, during surgery, they are tackled simultaneously. another problem is that most present day mr workstations enable review of two-dimensional (2d) slices only with limited postprocessing options. with recent software developments, a simultaneous 3d visualization simulating the real surgical field is possible using commercial or open source softwares. the authors have reviewed the important concepts and described a technique of interactive 3d visualization from routine 3d t1-weighted, mr angiography, and mr venography sequences using open source fsl ( pfunctional mri of the brain software library) and brainsuite softwares.
data_structures	we present a performance comparison of bounding volume hierarchies and kd-trees for ray tracing on many-core architectures (gpus). the comparison is focused on rendering times and traversal characteristics on the gpu using data structures that were optimized for very high performance of tracing rays. to achieve low rendering times, we extensively examine the constants used in termination criteria for the two data structures. we show that for a contemporary gpu architecture (nvidia kepler) bounding volume hierarchies have higher ray tracing performance than kd-trees for simple and moderately complex scenes. on the other hand, kd-trees have higher performance for complex scenes, in particular for those with high depth complexity. finally, we analyse the causes of the performance discrepancies using the profiling characteristics of the ray tracing kernels.
computer_vision	a new image analysis algorithm based on mathematical morphology and pixel classification for grapevine berry counting is presented in this paper. first, a set of berry candidates represented by connected components was extracted. then, six descriptors were calculated using key features of these components, and were employed for false positive (fp) discrimination using a supervised approach. more specifically, the set of descriptors modelled the grapes' distinctive shape, light reflection pattern and colour. two classifiers were tested, a three-layer neural network and an optimised support vector machine. a dataset of 152 images was acquired with a low-cost smart phone camera. images came from seven grapevine varieties, 18 per variety, at the two phenological stages in the baggiolini scale between berry set (named stage k; 94 images) and cluster-closure (named stage l; 32 images). 126 of these images were kept for external validation and the remaining 26 were used for training (12 at stage l and 14 at k). from these training images, 5438 true/false positive samples were generated and labelled in terms of the six descriptors. the neural network performed better than the support vector machine, yielding consistent recall and precision average values of 0.9572 and 0.8705, respectively. the presented algorithm, implemented as a smartphone application, can constitute a useful diagnosis tool for the in-the-field and non-destructive yield prediction and berry set assessing for the grape and wine industry. (c) 2017 iagre. published by elsevier ltd. all rights reserved.
operating_systems	memory corruption vulnerability is prevalent in software that are written using languages that lack memory safety features, e.g., c and c++. this has become a serious problem because the number of the attacks that exploit this vulnerability has increased. more specifically, this vulnerability allows control-flow-hijacking, a memory corruption attack that involves a well-known dangerous program stack. several countermeasures have been proposed both in academia and the information technology industry to thwart such attacks. some of these countermeasures have been implemented and used in practice. however, memory corruption attacks continue to be a serious problem because even these countermeasures are simply bypassed by new attacks. in this paper, we survey and classify protection and mitigation technologies that are especially pervasive in operating systems and compilers. this study aims to organize the pervasive countermeasures against these attacks. we present the existing countermeasures to address the current serious problem and propose modifications to these countermeasures that can be implemented in the future.
network_security	due to lack of the ""source-network-ioad"" global perception and multiple time-scale situation awareness, the current distribution network cannot meet the requirements of the active distribution network (adn) such as active control and active management. an adn situation awareness system based on distributed monitoring and multi-source information fusion is proposed in this paper. by constructing a multi-level distributed monitoring system based on internet of things (lot) technology, monitoring objects of distribution network are expanded to achieve global awareness in spatial scale. in the light of the randomness and volatility risk of distribution network operation, it is necessary to enhance observability of situation awareness in time scale. thus, the situation awareness technology of distribution network based on multi-source information fusion is provided, which is constituted of the multi-source information fusion, source-load forecasting, fast simulation analysis, risk assessment and warning and visualization modules. finally, the operation cockpit visualization technology enables operators to efficiently capture accurate distribution network security situation trend. the adn situation awareness system demonstration project is currently being carried out by state grid jiangsu power company, the actual operation indicates that it can effectively enhance the operation situation awareness and risk early-warning capability.
computer_programming	this practical report analyzes a programming class using a micro robot (mr), the smallest soccer robot in the robocup world competition. this class examined the effect of using the mr as the actual equipment employed in programming. questionnaire results on this class revealed that these teaching materials evoked a heightened programming interest among students. moreover, the problems related to programming instruction using an mr were better understood; therefore, a strategy for improving the related problems is discussed here. (c) 2013 wiley periodicals, inc. comput appl eng educ 23:109-116, 2015; view this article online at ; doi
operating_systems	hypervisors enable cloud computing model to provide scalable infrastructures and on-demand access to computing resources as they support multiple operating systems to run on one physical server concurrently. this mechanism enhances utilization of physical server thus reduces server count in the data center. hypervisors also drive the benefits of reduced it infrastructure setup and maintenance costs along with power savings. it is interesting to know different hypervisors' performance for the consolidated application workloads. three hypervisors esxi, xenserver, and kvm are carefully chosen to represent three categories full virtualized, para-virtualized, and hybrid virtualized respectively for the experiment. we have created a private cloud using cloudstack. hypervisors are deployed as hosts in the private cloud in the respective clusters. each hypervisor is deployed with three virtual machines. three applications web server, application server, and database servers are installed on three virtual machines. experiments are designed using design of experiments (doe) methodology. with concurrently running virtual machines, each hypervisor is stressed with the consolidated real-time workloads (web load, application load, and oltp) and important system information is gathered using sigar framework. the paper proposes a new scoring formula for hypervisors' performance in the private cloud for consolidated application workloads and the accuracy of the results are complemented with sound statistical analysis using doe. (c) 2016 elsevier b.v. all rights reserved.
cryptography	permutation polynomials over finite fields play important roles in finite fields theory. they also have wide applications in many areas of science and engineering such as coding theory, cryptography, combinatorial design, communication theory and so on. permutation binomials and permutation trinomials attract people 's interest due to their simple algebraic forms and additional extraordinary properties. in this paper, we find a new result about permutation binomials and construct several new classes of permutation trinomials. some of them are generalizations of known ones. (c) 2016 published by elsevier inc.
operating_systems	the mcs lock is one of the most prevalent queuing locks. it provides fair scheduling and high performance on massively parallel systems. however, the mcs lock mandates a bring-your-own-context policy: each lock user must provide an additional context (i.e., a queue node) to interact with the lock. this paper proposes mcsg, a variant of the mcs lock that relaxes this restriction. our key observation is that not all lock users are created equal. we analyzed how locks are used in massively-parallel modern systems, such as numa-aware operating systems and databases. we found that such systems often have a small number of ""regular"" code paths that enter the lock very frequently. such code paths are the primary beneficiary of the high scalability of mcs locks. however, there are also many ""guest"" code paths that infrequently enter the lock and do not need the same degree of fairness to access the lock (e.g., background tasks that only run periodically with lower priority). these guest users, which are typically spread out in various modules of the software, prefer context-free locks, such as ticket locks. mcsg provides these guests a context-free interface while regular users still enjoy the benefits provided by mcs. it can also be used as a drop-in replacement of mcs for more advanced locks, such as cohort locking. we also propose mcsg++, an extended version of mcsg, which avoids guest starvation and non-fifo behaviors that might happen with mcsg. our evaluation using microbenchmarks and the tpc-c database benchmark on a 16-socket, 240-core server shows that both mcsg and mcsg++ preserve the benefits of mcs for regular users while providing a context-free interface for guests.
bioinformatics	the serodiagnosis for tegumentary leishmaniasis (tl) presents problems related to the sensitivity and/or specificity of the tests. in the present study, an enzyme-linked immunosorbent assay (elisa) technique was used to evaluate the performance from a leishmania braziliensis hypothetical protein, lbhym, in an attempt to compare its serological reactivity with a soluble leishmania antigenic preparation (sla) for the serodiagnosis of cutaneous (cl) and mucosal (ml) leishmaniasis. lbhym was predicted to be a kinesin-like protein by bioinformatics tools. serum samples were collected from both cl and ml patients, as well as from those with chagas disease and from healthy subjects living in endemic or non-endemic areas of tl. also, sera were collected from patients before and after the treatments, seeking to evaluate their serological follow-up in relation to the anti-protein and anti-parasite antibody levels. when an elisa-rlbhym assay was performed, it proved to be significantly more sensitive than elisa-l. braziliensis sla in detecting both cl and ml patients. also, when using sera from chagas disease patients, the elisa-rlbhym proved to be more specific than elisa-sla. the anti-protein and anti-parasite antibody levels were also evaluated 6 months after the treatments, and treated patients showed significantly lower levels of specific-rlbhym antibodies, when compared to the anti-parasite antibody levels. in conclusion, the elisa-rlbhym assay can be considered a confirmatory serological technique for the serodiagnosis of l. braziliensis infection and can also be used in the serological follow-up of treated patients, aiming to correlate the low anti-protein antibody levels with the improvement of the healthy state of the patients.
computer_graphics	redirected walking techniques have been introduced to overcome physical space limitations for natural locomotion in virtual reality. these techniques decouple real and virtual user trajectories by subtly steering the user away from the boundaries of the physical space while maintaining the illusion that the user follows the intended virtual path. effectiveness of redirection algorithms can significantly improve when a reliable prediction of the users future virtual path is available. in current solutions, the future user trajectory is predicted based on non-standardized manual annotations of the environment structure, which is both tedious and inflexible. we propose a method for automatically generating environment annotation graphs and predicting the user trajectory using navigation meshes. we discuss the integration of this method with existing redirected walking algorithms such as force and mpcred. automated annotation of the virtual environments structure enables simplified deployment of these algorithms in any virtual environment.
symbolic_computation	based on generalized bilinear forms, lump solutions, rationally localized in all directions in the space, to dimensionally reduced p-gkp and p-gbkp equations in (2+1)-dimensions are computed through symbolic computation with maple. the sufficient and necessary conditions to guarantee analyticity and rational localization of the solutions are presented. the resulting lump solutions contain six parameters, two of which are totally free, due to the translation invariance, and the other four of which only need to satisfy the presented sufficient and necessary conditions. their three-dimensional plots with particular choices of the involved parameters are made to show energy distribution.
operating_systems	smartphones have become ubiquitous in our society. with a large number of users spending more time and sharing more personal data with these devices, it would be beneficial to gain some understanding of data security. this paper presents different security issues regarding applications of android systems which are one of the most popular mobile operating systems. the research also sheds a light on how the public feels about a number of privacy and security issues related to permissions and whether any additional factors play into an individual 's understanding of the application permission framework.
operating_systems	as automatic test systems continue to adopt architectures based on synthetic instrumentation and modular i/o platforms, software is eclipsing hardware as the primary input for determining technology insertion cadence and scope. while abstracting test programs sets from specific hardware is commonly referenced as a valuable tactic to reduce the risks of i/o obsolescence, it also requires significant up-front investment with a return that is later determined by the frequency of change. as systems become increasingly software-centric, a cost optimized development strategy requires bounding technology insertion options, evaluating the costs associated with developing driver and measurement layers across those options, and managing the costs of migrating across application software and operating systems as a function of time. this paper will discuss the evolving solution space for software dominated technology insertion strategies through an examination of the underlying compatibility of the cots components at play.
algorithm_design	the security game is a basic model for resource allocation in adversarial environments. here there are two players, a defender and an attacker. the defender wants to allocate her limited resources to defend critical targets and the attacker seeks his most favorable target to attack. in the past decade, there has been a surge of research interest in analyzing and solving security games that are motivated by applications from various domains. remarkably, these models and their game-theoretic solutions have led to real-world deployments in use by major security agencies like the lax airport, the us coast guard and federal air marshal service, as well as non-governmental organizations. among all these research and applications, equilibrium computation serves as a foundation. this paper examines security games from a theoretical perspective and provides a unified view of various security game models. in particular, each security game can be characterized by a set system e which consists of the defender 's pure strategies; the defender 's best response problem can be viewed as a combinatorial optimization problem over e. our framework captures most of the basic security game models in the literature, including all the deployed systems; the set system e arising from various domains encodes standard combinatorial problems like bipartite matching, maximum coverage, min-cost flow, packing problems, etc. our main result shows that equilibrium computation in security games is essentially a combinatorial problem. in particular, we prove that, for any set system e, the following problems can be reduced to each other in polynomial time: (0) combinatorial optimization over e; (1) computing the minimax equilibrium for zero-sum security games over e; (2) computing the strong stackelberg equilibrium for security games over e; (3) computing the best or worst (for the defender) nash equilibrium for security games over e. therefore, the hardness [polynomial solvability] of any of these problems implies the hardness [polynomial solvability] of all the others. here, by ""games over e"" we mean the class of security games with arbitrary payoff structures, but a fixed set e of defender pure strategies. this shows that the complexity of a security game is essentially determined by the set system e. we view drawing these connections as an important conceptual contribution of this paper.
computer_graphics	in this paper, we first introduce the generalized alternated system. the definition of the julia set in the generalized alternated system is given, which is called a generalized alternated julia set. then, we achieve the control of generalized alternated julia sets by applying the classic control methods, which are gradient control and optimal control. in addition, the synchronization between two different generalized alternated julia sets is implemented using gradient control and optimal control. the simulations illustrate the effectiveness and correctness of these two control methods, and the results are displayed in 2d computer graphics.
operating_systems	this article describes the approach of experts of the national research moscow state university of civil engineering (mgsu) to the construction of modern operating systems in buildings using bim-technology. this article was performed within the russian state task.
bioinformatics	dastarcus helophoroides, a predatory natural enemy of longhorned beetles, has a relatively longer lifespan compared to other insects. to determine the potential physiological roles of antioxidant enzymes superoxide dismutase (sod) in longevity and aging of d. helophoroides, analyses including molecular information, bioinformatics research, phylogenetic relationship and expression patterns were combined for investigation. four d.hsods were classified into three groups: one cytoplasmic cu/zn-sod, two extracellular cu/zn-sod and one mn-sod, were identified and characterized by multiple alignments. all d.hsods were highly homologous to sods of tribolium castaneum, and closely clustered together with sod genes from insects in phylogenetic analyses. comparison of the d.hsods expression in different tissues, stages and age groups showed that the sod transcripts could be detected in all examined specimens. the expression of d.hsods revealed tissue-specificity with relatively high levels in the male reproductive system and head and low levels in female reproductive systems and mid gut. expression analyses of d.hsods in different development stages demonstrated that d.hsod1 and d.hsod2 increased in 2nd and 5th instar larvae, whereas two extracellular cu/zn-sod genes (d.hsod3-a and d.hsod3-b) were much more expressed in newly emerged adults. the expression fluctuations of d.hsods during aging seemed to be less significant than during development, and exhibited relatively stable expression with an initial decline and then increased in older groups. the relatively stable and increased expression of d.hsods may indicate a strong ability of sods to eliminate oxidative damage products accumulated during aging and possibly retard aging. the research provides molecular biology and in vivo expression levels for future analysis of the sod family in d. helophoroides and other insects, and provides a basis for further study about the sod genes contribution on longevity of d. helophoroides. (c) 2017 elsevier b.v. all rights reserved.
computer_programming	background: flux analyses, including flux balance analysis (fba) and c-13-metabolic flux analysis (c-13-mfa), offer direct insights into cell metabolism, and have been widely used to characterize model and non-model microbial species. nonetheless, constructing the c-13-mfa model and performing flux calculation are demanding for new learners, because they require knowledge of metabolic networks, carbon transitions, and computer programming. to facilitate and standardize the c-13-mfa modeling work, we set out to publish a user-friendly and programming-free platform (wuflux) for flux calculations in matlab (r). results: we constructed an open-source platform for steady-state c-13-mfa. using guide (graphical user interface design environment) in matlab, we built a user interface that allows users to modify models based on their own experimental conditions. wuflux is capable of directly correcting mass spectrum data of tbdms (n-tertbutyldimethylsilyl-n-methyltrifluoroacetamide)-derivatized proteinogenic amino acids by removing background noise. to simplify c-13-mfa of different prokaryotic species, the software provides several metabolic network templates, including those for chemoheterotrophic bacteria and mixotrophic cyanobacteria. users can modify the network and constraints, and then analyze the microbial carbon and energy metabolisms of various carbon substrates (e.g., glucose, pyruvate/lactate, acetate, xylose, and glycerol). wuflux also offers several ways of visualizing the flux results with respect to the constructed network. to validate our model 's applicability, we have compared and discussed the flux results obtained from wuflux and other mfa software. we have also illustrated how model constraints of cofactor and atp balances influence fluxome results. conclusion: open-source software for c-13-mfa, wuflux, with a user-friendly interface and easy-to-modify templates, is now available at http://www.13cmfa.org/or (http://tang.eece.wustl.edu/tooldevelopment.htm). we will continue documenting curated models of non-model microbial species and improving wuflux performance.
bioinformatics	the wrky family, a large family of transcription factors (tfs) found in higher plants, plays central roles in many aspects of biological processes and adaption to environment. however, little information is available on this family in apple (malus domestica). in the present study, a total of 119 candidate wrky genes in apple genome were identified and classified into three main groups (group i-iii) based on the structure of the conserved domains. each group or subgroup showed similar exon-intron structures and motif compositions. the evolution analysis showed that 44 mdwrky genes clustered into 20 intensive regions (<100 kb) and 78 mdwrky formed 85 pairs of collinear relationships, suggesting that both tandem and segmental duplications played an important role in the evolution and diversification of the wrky gene family in apple. furthermore, the expression of the mdwrky genes in apple leaves in response to biotic stress (alternaria alternate) and hormone treatments [salicylic acid (sa), methyl jasmonate (meja) and ethephon] was examined by using rna-seq and qrt-pcr. the results showed that 63 mdwrky genes had differential expression in their transcript abundance in response to alternaria alternata apple pathotype infection. moreover, most pathogen responsive mdwrky genes were also changed significantly when apple leaves were treated by sa, meja or ethephon plant growth regulations, suggesting an interaction between sa, ja and ethylene (eth) hormone signaling under biotic stress. this work may provide the basis for future studies of the genetic modification of wrky genes for pathogen resistance in apple.
symbolic_computation	in this article, we investigate the lump solutions for the kadomtsev-petviashvili equation in (3 + 1) dimensions that describe the dynamics of plasmas or fluids. via the symbolic computation, lump solutions for the (3 + 1)-dimensional kadomtsev-petviashvili equation are derived based on the bilinear forms. the conditions to guarantee analyticity and rational localisation of the lump solutions are presented. the lump solutions contain eight parameters, two of which are totally free, and the other six of which need to satisfy the presented conditions. plots with particular choices of the involved parameters are made to show the lump solutions and their energy distributions.
computer_vision	introduction: use of computers is generally encouraged; this is to keep up with the fast-moving world of technology, research and science. extensive use of computers will result in computer vision syndrome (cvs), and the prevalence is increased dramatically. the main objective of the study was to assess the prevalence and associated factors of cvs among bank workers in gondar city, northwest ethiopia. methods: a cross-sectional institution-based study was conducted among computer-using bank workers in gondar city from april to june, 2015. data were collected through structured questionnaires and observations with checklists, entered with epi info (tm) 7 and analyzed by statistical package for the social sciences (spss) version 20. descriptive statistics and logistic regression were carried out to compute the different rates, proportion and relevant associations. results: among the total 304 computer-using bank workers, the prevalence of cvs was 73% (95% confidence interval [ci]= 68.04, 78.02). blurred vision (42.4%), headache (23.0%) and redness (23.0%) were the most experienced symptoms. inappropriate sitting position was 2.3 times (adjusted odds ratio [aor]= 2.33; 95% ci= 1.27, 4.28) more likely to be associated with cvs when compared with appropriate sitting position. those working on the computer for more than 20 minutes without break were nearly 2 times (aor= 1.93; 95% ci= 1.11, 3.35) more likely to have suffered from cvs when compared with those taking break within 20 minutes, and those wearing eye glasses were 3 times (aor= 3.19; 95% ci= 1.07, 9.51) more likely to suffer from cvs when compared with those not wearing glasses. conclusion: about three-fourths of computer-using bank workers suffered from cvs with the most experienced symptoms being blurred vision, headache and redness of eyes. in appropriate sitting position, working on the computer without a break for more than 20 minutes and wearing eye glasses were independently associated with cvs.
algorithm_design	generalized spatial modulation (gsm) is a spectral and energy efficient multiple-input-multiple-output transmission technique. the low-complexity detection algorithm design with near maximum likelihood (ml) performance at the receiver is very challenging, and is the focus of this letter. in specific, we exploit the fixed sparsity constraint in the transmitted gsm signals, and take advantage of bayesian compressive sensing (bcs) in sparse signal recovery. a new detection algorithm, referred to as enhanced bayesian compressive sensing (ebcs), is proposed. it features more than 75% complexity reduction at high signal-tonoise ratios compared with the ordered-blocked minimum-meansquared-error algorithm. furthermore, it is shown by simulation that its error performance is comparable to the ml algorithm, and the performance gap is negligible in many cases.
cryptography	this paper presents a new controlled quantum dialogue (cqd) protocol based on the cluster entangled states. the security analyses indicate that the proposed scheme is secure under not only various well-known attacks but also the collusive attack, where the participants may collude to communicate without the controller 's permission. compared to a previous cqd scheme, which is also robust against the conspiracy attack, the proposed protocol is more efficient in both the qubit efficiency and the hardware requirement.
bioinformatics	predicting protein submitochondrial locations has been studied for about ten years. a dozen of methods were developed in this regard. although a mitochondrion has four submitochondrial compartments, all existing studies considered only three of them. the mitochondrial intermembrane space proteins were always excluded in these studies. however, there are over 50 mitochondrial intermembrane space proteins in the recent release of uniprot database. we think it is time to incorporate these proteins in predicting protein submitochondrial locations. we proposed the functional domain enrichment score, which can be used as an enhancement to our positional-specific physicochemical properties method. we constructed a high-quality working dataset from the uniprot database. this dataset contains proteins from all four submitochondrial locations. proteins with multiple submitochondrial locations are also included. our method achieved over 70% prediction accuracy for proteins with single location on this dataset. on the m3-317 benchmarking dataset, our method achieved comparable prediction performance to other state-of-the-art methods. our results indicate that the intermembrane space proteins can be incorporated in predicting protein submitochondrial locations. by evaluating our method with the proteins that have multiple submitochondrial locations, we conclude that our method is capable of predicting multiple submitochondrial locations. this is the first report of ab initio methods that can identify intermembrane space proteins. this is also the first attempt to incorporate proteins with multiple submitochondrial locations. the benchmarking dataset can be obtained by emails to the corresponding author.
parallel_computing	the construction of large software systems is always achieved through assembly of independently written components - program modules. for these software components to work together, they must share a common set of data types and principles for representing structured data such as arrays of values and files. this common set of tools for creating and operating on data objects is provided by the infrastructure of the computer system: the hardware, operating system and runtime code. because the nature and properties of these tools are crucial for correct operation of software components and their inter-operation, it is essential to have a precise specification that may be used for verifying correctness of application software on one hand, and to verify correctness of system behavior on the other. we call such a specification a program execution model (pxm). it is evident that the properties of the pxm implemented by a computer system can have serious impact on the ability of application programmers to practice modular software construction. this paper discusses the concept of program execution models and presents a set of principles that a pxm must satisfy to provide a sound basis for modular software construction. because parallel program execution on computer systems with many processing units is an essential part of contemporary computing environments, the expression of parallelism and modular software construction using components involving parallel operations is included in this treatment. the conclusion is that it is possible to build computer systems that implement a pxm within which any parallel program may be used, unmodified, as a component for building more substantial parallel programs.
network_security	the network systems of the world are fragile, and can come under attack from any source. the attack can be a denial-of-service (dos) state or another type of threat. what keep the networks safe are the intrusion detection and prevention systems (idps). they constantly monitor network traffic and if a malicious threat is detected, the threat is blocked and reported for further analysis. however, every defensive system must always have some type of weakness. false negatives and false positives are some examples of how idps can fail to protect the network. in another instance, a skilled attacker may employ direct kernel object modification (dkom) to trick the idps into detecting no malicious activities. the idps is strong, yet not strong enough. this paper presents a hybrid solution that incorporates both signature and anomaly based systems to detect and prevent more malicious attacks by intensifying what is cataloged to include common anomalies to the baselines used by the signature based systems. we also propose an improvement in the framework to current host idps/network using signature and anomaly based methodologies by implementing a hybrid vmm-based honeypot into a theorized self-healing hybrid idps to further boost their advantages in efficiency and accuracy. (c) 2016 the authors. published by elsevier b.v.
machine_learning	noise addition is a data distortion technique widely used in data intensive applications. for example, in machine learning tasks it helps to reduce overfitting, whereas in data privacy protection it adds uncertainty to personally identifiable information. yet, due to its mathematical operating principle, noise addition is a method mainly intended for continuous numerical data. in fact, despite the large amount of nominal data that are being currently compiled and used in data analysis, only a few alternative techniques have been proposed to distort nominal data in a similar way as standard noise addition does for numerical data. furthermore, all these alternative methods rely on the distribution of the data rather than on the semantics of nominal values, which negatively affects the utility of the distorted outcomes. to tackle this issue, in this paper we present a semantically-grounded alternative to numerical noise suitable for nominal data, which we name semantic noise. by means of semantic noise, and by exploiting structured knowledge sources such as ontologies, we are able to distort nominal data while preserving better their semantics and thus, their analytical utility. to that end, we provide semantically and mathematically coherent versions of the statistical operators required in the noise addition process, which include the difference, the mean, the variance and the covariance. then, we propose semantic noise addition algorithms that cope with the finite, discrete and non-ordinal nature of nominal data. the proposed algorithms cover both uncorrelated noise addition, which is suited to independent attributes, and correlated noise addition, which can cope with multivariate datasets with dependent attributes. empirical results show that our proposals offer general and configurable mechanisms to distort nominal data while preserving data semantics better than baseline methods based only on the distribution of the data. (c) 2017 elsevier b.v. all rights reserved.
data_structures	to ease the programming burden and to make parallel programs more maintainable, computational scientists and engineers currently have the options to use software libraries, templates, and general purpose language extensions to compose their application programs. these existing options, unfortunately, have considerable limitations with compatibility, expressive power and delivered performance. to address these issues, we design a domain specific language, gridfor, for computational problems defined over regular geometric grids. this language allows the programmer to first implement an algorithm on simple data structures, as commonly illustrated in textbooks or papers. the programmer then specifies transformations to extend the algorithm for complex data structures required by the target applications. we build a compiler to automatically translate a gridfor program to a parallel fortran version with message passing interface calls. several optimization techniques are implemented in our compiler to enhance the program speed.
machine_learning	the excellent features of bearing vibration signal are helpful to obtain accurate diagnosis results for the failure of bearing. in this study, the feature extraction method of bearing vibration signal based on wavelet packet transform-phase space reconstruction-singular value decomposition (wps) is presented to improve the traditional feature extraction method of bearing vibration signal based on wavelet packet transform-singular value decomposition (ws). in the proposed feature extraction method, singular value decomposition is performed for phase space reconstruction signal of each wavelet packet coefficient 's reconstructed signal of bearing vibration signal. the dynamic characteristics of a certain frequency range can be reflected by phase space reconstruction for wavelet packet coefficients' reconstructed signals of bearing vibration signal. support vector machine (svm) is a machine learning method based on structural risk minimization principle, and svm classifier can solve the classification problems with small training samples, high dimensions, and nonlinearity. thus, the svm model of bearing is established by the features of bearing vibration signal based on wavelet packet transform-phase space reconstruction-singular value decomposition in this study. the experimental results show that the feature extraction method of bearing vibration signal based on wavelet packet transform-phase space reconstruction-singular value decomposition is better than the feature extraction method of bearing vibration signal based on wavelet packet transform-singular value decomposition, and svm established by the features of bearing vibration signal based on wavelet packet transform-phase space reconstruction-singular value decomposition (wps-svm) has a stronger fault diagnosis ability of bearing than svm established by the features of bearing vibration signal based on wavelet packet transform-singular value decomposition (ws-svm).
computer_graphics	mesh saliency was introduced and joined the community of computer graphics ten years ago, which can benefit various applications, for instance, mesh reduction, mesh segmentation, self-similarity matching, scan integration, volume rendering, 3d printing, etc. before, saliency detection had been successfully applied to image processing and pattern recognition to study how the world is perceptually intelligible for robots. in contrast to color of images and coherence of videos, geometric signals are defined with two-dimensional manifolds whose discrete representation is irregular, leading differences to the nature and difficulties to the solution of mesh saliency. to tackle the challenge, the last decade has witnessed significant advances in mesh saliency detection. however, a survey of recent advances in mesh saliency detection as well as its applications does not yet exist to date. this paper provides a first and comprehensive reference source of shape context based mesh saliency for researchers from a wide range of domains, including but not limited to computer graphics and vision. it reviews main contributions, advantages, drawbacks, and applications of known mesh saliency detection methods and discusses current trends and outlook for future study. (c) 2016 elsevier ltd. all rights reserved.
operating_systems	to enable a prosperous internet of things (iot), devices and services must be extensible and adapt to changes in the environment or user interaction patterns. these requirements manifest as a set of design principles for each of the layers in an iot ecosystem, from hardware to cloud services. this paper gives concrete guidelines learned from implementing and deploying a full-stack synergistic iot platform. we address hardware design concerns and present a reference platform, firestorm. upon this platform, we demonstrate firmware and personal-area networking concerns and solutions. moving out towards larger scales we address local service discovery and syndication, and show how these principles carry through to global operation where security concerns dominate.
cryptography	s-box plays an imperative role in designing a cryptographically strong block cipher. designing s-box based on chaos has attracted lots of attentions because of its distinct characteristics relevant to cryptography. in this paper, a 4d-4wing hyperchaotic system is investigated. its sophisticated nonlinear behaviors are used to generate two pseudorandom 8-bit integer sequences, which further drive iterative two-position swap on the identical map on gf(2(8)). according to the indicator of typical evaluation criteria including nonlinearity, differential uniformity, strict avalanche criterion, output bits independence criterion and bijective property, the preferred s-box is obtained from all those batch-generated ones. the comparison with the state-of-the-art chaos-based schemes shows that the obtained s-box achieves better cryptographical performance.
computer_programming	conventional taught learning practices often experience difficulties in keeping students motivated and engaged. video games, however, are very successful at sustaining high levels of motivation and engagement through a set of tasks for hours without apparent loss of focus. in addition, gamers solve complex problems within a gaming environment without feeling fatigue or frustration, as they would typically do with a comparable learning task. based on this notion, the academic community is keen on exploring methods that can deliver deep learner engagement and has shown increased interest in adopting gamification - the integration of gaming elements, mechanics, and frameworks into non-game situations and scenarios - as a means to increase student engagement and improve information retention. its effectiveness when applied to education has been debatable though, as attempts have generally been restricted to one-dimensional approaches such as transposing a trivial reward system onto existing teaching materials and/or assessments. nevertheless, a gamified, multi-dimensional, problem-based learning approach can yield improved results even when applied to a very complex and traditionally dry task like the teaching of computer programming, as shown in this paper. the presented quasi-experimental study used a combination of instructor feedback, real time sequence of scored quizzes, and live coding to deliver a fully interactive learning experience. more specifically, the ""kahoot!"" classroom response system (crs), the classroom version of the tv game show ""who wants to be a millionaire?"", and codecademy 's interactive platform formed the basis for a learning model which was applied to an entry-level python programming course. students were thus allowed to experience multiple interlocking methods similar to those commonly found in a top quality game experience. to assess gamification 's impact on learning, empirical data from the gamified group were compared to those from a control group who was taught through a traditional learning approach, similar to the one which had been used during previous cohorts. despite this being a relatively small-scale study, the results and findings for a number of key metrics, including attendance, downloading of course material, and final grades, were encouraging and proved that the gamified approach was motivating and enriching for both students and instructors.
operating_systems	programmable network like sdn allows administrators to program network infrastructure according to service demand and custom-defined policies. network policies are interpreted by the centralized controller to define actions and rules to process the network traffic on devices that belong to a single domain. however, actual networks are multi-domain where several domains are interconnected. then, because sdn controllers in a domain cannot define nor monitor policies in other domains, network administrators cannot ensure that their own policies, origin policies are being enforced by the domains not directly managed by them (i.e. foreign domains). we present audit, a multi-domain sdn policy verifier that identifies whether an origin policy is enforced by foreign domains. audit comprises (1) model for network topology, policies, and flows, (2) an audit protocol to gather information about the actions performed by network devices to carry the flows of interest, and (3) a validation engine that takes that information and detects security policy violations, and (4) an extension to the openflow protocol to enable external auditing. this paper presents our approach and illustrates its application using an example considering multiple sdn networks.
image_processing	in production processes that use surfacemount technology (smt) for the assembly of printed circuit boards, automated optical inspection is widely employed to diagnose component defects. however, commonly used inspection algorithms can hardly meet reliability and time efficiency requirements simultaneously, especially when applied to the components of high-density and large-scale integration, such as ball grid array (bga). in this paper, a novel approach is presented to inspect bga component defects. an adaptive thresholding combined with modified (e,d)-component segmentation is first performed to extract the grayscale image of solder balls. a line-based-clustering method is then proposed to recognize ball array. simultaneously, accurate position and orientation of bga are obtained based on the recognition results. finally, ball features are extracted to diagnose potential defects. the proposed algorithm is implemented on the host computer of samsung smt 482 machine. the results obtained show that the proposed approach is suitable for a vast majority of bgas with different ball arrays and also that it is robust to interferences caused by the image segmentation. furthermore, compared to samsung 's algorithm, it has significant advantages in time efficiency and high inspection accuracy under nonideal lighting conditions.
cryptography	because the nodes in a wireless sensor network (wsn) are mobile and the network is highly dynamic, monitoring every node at all times is impractical. as a result, an intruder can attack the network easily, thus impairing the system. hence, detecting anomalies in the network is very essential for handling efficient and safe communication. to overcome these issues, in this paper, we propose a rule-based anomaly detection technique using roaming honeypots. initially, the honeypots are deployed in such a way that all nodes in the network are covered by at least one honeypot. honeypots check every new connection by letting the centralized administrator collect the information regarding the new connection by slowing down the communication with the new node. certain pre-defined rules are applied on the new node to make a decision regarding the anomality of the node. when the timer value of each honeypot expires, other sensor nodes are appointed as honeypots. owing to this honeypot rotation, the intruder will not be able to track a honeypot to impair the network. simulation results show that this technique can efficiently handle the anomaly detection in a wsn.
computer_graphics	while display quality and rendering for head-mounted-displays (hmds) has increased in quality and performance, the interaction capabilities with these devices are still very limited or relying on expensive technology. current experiences offered for mobile hmds often stick to dome-like looking around, automatic or gaze-triggered movement, or flying techniques. we developed an easy to use walking-in-place technique that does not require additional hardware to enable basic navigation, such as walking, running, or jumping, in virtual environments. our approach is based on the analysis of data from the inertial unit embedded in mobile hmds. in a first prototype realized for the samsung galaxy gear vr we detect steps and jumps. a user study shows that users novice to virtual reality easily pick up the method. in comparison to a classic input device, using our walking-in-place technique study participants felt more present in the virtual environment and preferred our method for exploration of the virtual world.
software_engineering	defect management is a central task in software maintenance. when a defect is reported, appropriate resources must be allocated to analyze and resolve the defect. an important issue in resource allocation is the estimation of defect resolution time (drt). prior research has considered different approaches for drt prediction exploiting information retrieval techniques and similarity in textual defect descriptions. in this article, we investigate the potential of text clustering for drt prediction. we build on a study published by raja (2013) which demonstrated that clusters of similar defect reports had statistically significant differences in drt. raja 's study also suggested that this difference between clusters could be used for drt prediction. our aims are twofold: first, to conceptually replicate raja 's study and to assess the repeatability of its results in different settings; second, to investigate the potential of textual clustering of issue reports for drt prediction with focus on accuracy. using different data sets and a different text mining tool and clustering technique, we first conduct an independent replication of the original study. then we design a fully automated prediction method based on clustering with a simulated test scenario to check the accuracy of our method. the results of our independent replication are comparable to those of the original study and we confirm the initial findings regarding significant differences in drt between clusters of defect reports. however, the simulated test scenario used to assess our prediction method yields poor results in terms of drt prediction accuracy. although our replication confirms the main finding from the original study, our attempt to use text clustering as the basis for drt prediction did not achieve practically useful levels of accuracy.
relational_databases	this paper presents an analysis of the state of the art solutions for mapping a relational database and an ontology by adding reasoning capabilities and offering the possibility to query the inferred information. we analyzed four approaches: jena with d2rq, jena with r2rml, kaon2 and owl api. in order to highlight the differences between the four approaches, we used a nutrition diagnostics related ontology for the definition of the concepts and of the rules, and a relational database for the storage of the individuals. as performance evaluation, we focused on the time required to map the relational database to the ontology, and the time required to retrieve the information that is inferred about the diagnostics of a number of people. the obtained results show that the best performance in both cases is given by kaon2.
symbolic_computation	by using symbolic computation software(maple), a generalized dirac soliton hierarchy is derived from a new matrix spectral problem associated with the lie algebra sl(2,r). a bi-hamiltonian structure yielding liouville integrability is furnished by the trace identity. (c) 2016 elsevier ltd. all rights reserved.
distributed_computing	crowdsourcing is a new emerging distributed computing and business model on the backdrop of internet blossoming. with the development of crowdsourcing systems, the data size of crowdsourcers, contractors and tasks grows rapidly. the worker quality evaluation based on big data analysis technology has become a critical challenge. this paper first proposes a general worker quality evaluation algorithm that is applied to any critical tasks such as tagging, matching, filtering, categorization and many other emerging applications, without wasting resources. second, we realize the evaluation algorithm in the hadoop platform using the mapreduce parallel programming model. finally, to effectively verify the accuracy and the effectiveness of the algorithm in a wide variety of big data scenarios, we conduct a series of experiments. the experimental results demonstrate that the proposed algorithm is accurate and effective. it has high computing performance and horizontal scalability. and it is suitable for large-scale worker quality evaluations in a big data environment.
bioinformatics	hepatocellular carcinoma (hcca) is a primary malignancy of the liver. many different proteins are involved in hcca including insulin growth factor (igf) ii, signal transducers and activators of transcription (stat) 3, stat4, mothers against decapentaplegic homolog 4 (smad 4), fragile histidine triad (fhit) and selective internal radiation therapy (sirt) etc. the present study is based on the bioinformatics analysis of fhit protein in order to understand the proteomics aspect and improvement of the diagnosis of the disease based on the protein. different information related to protein were gathered from different databases, including national centre for biotechnology information (ncbi) gene, protein and online mendelian inheritance in man (omim) databases, uniprot database, string database and kyoto encyclopedia of genes and genomes (kegg) database. moreover, the structure of the protein and evaluation of the quality of the structure were included from easy modeler programme. hence, this analysis not only helped to gather information related to the protein at one place, but also analysed the structure and quality of the protein to conclude that the protein has a role in carcinoma.
cryptography	in this article, we present a compact implementation of the salsa20 stream cipher that is targeted towards lightweight cryptographic devices such as radio-frequency identification (rfid) tags. the salsa20 stream cipher, ann addition-rotation-xor (arx) cipher, is used for high-security cryptography in neon instruction sets embedded in arm cortex a8 cpu core-based tablets and smartphones. the existing literature shows that although classical cryptanalysis has been effective on reduced rounds of salsa20, the stream cipher is immune to software side-channel attacks such as branch timing and cache timing attacks. to the best of our knowledge, this work is the first to perform hardware power analysis attacks, where we evaluate the resistance of all eight keywords in the proposed compact implementation of salsa20. our technique targets the three subrounds of the first round of the implemented salsa20. the correlation power analysis (cpa) attack has an attack complexity of 2(19). based on extensive experiments on a compact implementation of salsa20, we demonstrate that all these keywords can be recovered within 20,000 queries on salsa20. the attacks show a varying resilience of the key words against cpa that has not yet been observed in any stream or block cipher in the present literature. this makes the architecture of this stream cipher interesting from the side-channel analysis perspective. also, we propose a lightweight countermeasure that mitigates the leakage in the power traces as shown in the results of welch 's t-test statistics. the hardware area overhead of the proposed countermeasure is only 14% and is designed with compact implementation in mind.
computer_programming	this paper addresses the continuing problem in the united states of a lack of female professionals in computer science. the research team conducted surveys of middle school students and working adults to examine their attitudes, motivations, and experience with computer science. based on the survey findings, the researchers are able to evaluate the effects of early, positive exposure to computer programming and give recommendations on how to improve the attitudes and confidence of young girls toward a possible career in computer science.
computer_graphics	objective visual quality assessment of 3d models is a fundamental issue in computer graphics. quality assessment metrics may allow a wide range of processes to be guided and evaluated, such as level of detail creation, compression, filtering, and so on. most computer graphics assets are composed of geometric surfaces on which several texture images can be mapped to make the rendering more realistic. while some quality assessment metrics exist for geometric surfaces, almost no research has been conducted on the evaluation of texture-mapped 3d models. in this context, we present a new subjective study to evaluate the perceptual quality of textured meshes, based on a paired comparison protocol. we introduce both texture and geometry distortions on a set of 5 reference models to produce a database of 136 distorted models, evaluated using two rendering protocols. based on analysis of the results, we propose two new metrics for visual quality assessment of textured mesh, as optimized linear combinations of accurate geometry and texture quality measurements. these proposed perceptual metrics outperform their counterparts in terms of correlation with human opinion. the database, along with the associated subjective scores, will be made publicly available online.
distributed_computing	most distributed computing applications require an effective scheduling algorithm to distribute and assign client 's tasks running on a set of processors. the existing algorithms assumed that the scheduled tasks can be simultaneously and ideally sent and received from the processors without any latent delay. however, this assumption is obviously impractical and unrealizable due to lack of consideration for constraints caused by the limited number of existing i/o ports at the client site. such an i/o port constraint confines the overall throughputs of the computing systems. this study investigates the theoretical scheduling patterns under the constraint of one i/o port which typically exists to achieve optimal makespan and latent delay. a heuristic algorithm to effectively schedule the given set of tasks is also proposed. two primary scheduling patterns leading to the optimal makespan and delay are discovered. performance of the proposed scheduling is better than other scheduling algorithms under the imposed constraints in terms of shorter makespan and less latent delay, in particular, the average time complexity is equal to o(n(2)). (c) 2016 elsevier b.v. all rights reserved.
relational_databases	many studies on reverse skyline query processing have been done for various services. the existing reverse skyline query processing methods are based on dynamic skylines. there are no reverse skyline query processing algorithms based on metric spaces for location-based services. the existing methods for processing a reverse skyline query have the limitation of service domains and require the high costs of computation to provide various location-based services. in this paper, we propose a new reverse skyline query processing method that efficiently processes a query over the moving objects. in addition, the proposed method processes a continuous reverse skyline query efficiently. in order to show the superiority of the proposed method, we compare it with the previous reverse skyline query processing method in various environments. as a result, the proposed method achieves better performance than the existing method. (c) 2015 elsevier b.v. all rights reserved.
bioinformatics	background: many plant pathogen secretory proteins are known to be elicitors or pathogenic factors, which play an important role in the host-pathogen interaction process. bioinformatics approaches make possible the large scale prediction and analysis of secretory proteins from the puccinia helianthi transcriptome. the internet-based software signalp v4.1, targetp v1.01, big-pi predictor, tmhmm v2.0 and protcomp v9.0 were utilized to predict the signal peptides and the signal peptide-dependent secreted proteins among the 35,286 orfs of the p. helianthi transcriptome. results: 908 orfs (accounting for 2.6% of the total proteins) were identified as putative secretory proteins containing signal peptides. the length of the majority of proteins ranged from 51 to 300 amino acids (aa), while the signal peptides were from 18 to 20 aa long. signal peptidase i (spi) cleavage sites were found in 463 of these putative secretory signal peptides. 55 proteins contained the lipoprotein signal peptide recognition site of signal peptidase ii (spii). out of 908 secretory proteins, 581 (63.8%) have functions related to signal recognition and transduction, metabolism, transport and catabolism. additionally, 143 putative secretory proteins were categorized into 27 functional groups based on gene ontology terms, including 14 groups in biological process, seven in cellular component, and six in molecular function. gene ontology analysis of the secretory proteins revealed an enrichment of hydrolase activity. pathway associations were established for 82 (9.0%) secretory proteins. a number of cell wall degrading enzymes and three homologous proteins specific to phytophthora sojae effectors were also identified, which may be involved in the pathogenicity of the sunflower rust pathogen. conclusions: this investigation proposes a new approach for identifying elicitors and pathogenic factors. the eventual identification and characterization of 908 extracellularly secreted proteins will advance our understanding of the molecular mechanisms of interactions between sunflower and rust pathogen and will enhance our ability to intervene in disease states.
distributed_computing	allocating tasks to processors is a well-known np-hard problem in distributed computing systems. due to the lack of practicable exact solutions, it has been attracted by the researchers working on heuristic based suboptimal search algorithms. with the recent inclusion of multiple objectives such as minimizing the cost, maximizing, the throughput and maximizing the reliability, the problem gets even more complex and an efficient approximate method becomes more valuable. in this work, i propose a new solution for the multi-objective task allocation problem. my solution consists in designing a problem-specific neighboring function for an existing metaheuristic algorithm that is proven to be successful in quadratic assignment problems. the neighboring function, namely greedy reassignment with maximum release (gr-mr), provides a dynamic mechanism to switch the preference of the search between the exploration and exploitation. the experiments validate both that the quality of the solutions are close to the optimal and the proposed method performs significantly better comparing to three other metaheuristic algorithms. neighboring functions being the common reusable components of metaheuristic algorithms, gr-mr can also be utilized by other metaheuristic-based solutions in the future. (c) 2016 elsevier ltd. all rights reserved.
computer_programming	this article reports on the development of two courses designed for students in higher education game development programs during the period of 2006 to 2015. the students are from three different arts and design-related strands of the program, and had in common that very few had taken advanced science classes as part of their upper-secondary education. this meant that they were rather poorly equipped to learn programming, mathematics and physics, which they needed to master in order to understand the basics of game development. consequently, the courses was designed in a way that allowed the students to practically engage in creating a computer games alongside being taught the actual science they needed in order to efficiently utilize those skills. a working hypothesis for the project was that if the responsible teachers were able to run the course in a way that cohered with the principles of problem-based learning, this would create an environment which would enhance the students' motivation to learn basic science as well as the operative and innovation skills needed for fulfilling the course requirements. in addition, ideas developed within the field of situated and experiential learning constituted theoretical points of departure for developing the course. the article describes the practical and theoretical points of departure for developing the courses and reflects on the experiences made from running it. summing up, the authors conclude that the why and how of teaching needs to be in line with students' worlds in order for educational experiences to be considered as meaningful.
data_structures	in this paper we show how to construct a data structure for a string s of size n compressed into a context-free grammar of size n that supports efficient karp-rabin fingerprint queries to any substring of s. that is, given indices i and j, the answer to a query is the fingerprint of the substring s[i, j]. we present the first o(n) space data structures that answer fingerprint queries without decompressing any characters. for straight line programs (slp) we get o(log n) query time, and for linear slps (an slp derivative that captures lz78 compression and its variations) we get o(log log n) query time. we extend the result to solve the longest common extension problem in query time o(log n log l) and o(log l log log l + log log n) for slps and linear slps, respectively. here, l denotes the length of the lce. (c) 2017 elsevier inc. all rights reserved.
symbolic_computation	with the help of the symbolic computation system, maple and riccati equation (xi ' = a(0) + a(1)xi + a(2)xi(2)), expansion method, and a linear variable separation approach, a new family of exact solutions with q = lx + my + nt + gamma (x, y, t) for the (2+1)-dimensional generalized calogero-bogoyavlenskii-schiff system (gcbs) are derived. based on the derived solitary wave solution, some novel localized excitations such as fusion, fission, and annihilation of complex waves are investigated.
image_processing	we discuss two semi-independent calibration techniques used to determine the inflight radiometric calibration for the new horizons' multi-spectral visible imaging camera (mvic). the first calibration technique compares the measured number of counts (dn) observed from a number of well calibrated stars to those predicted using the component-level calibration. the ratio of these values provides a multiplicative factor that allows a conversation between the preflight calibration to the more accurate inflight one, for each detector. the second calibration technique is a channel-wise relative radiometric calibration for mvic 's blue, near-infrared and methane color channels using hubble and new horizons observations of charon and scaling from the red channel stellar calibration. both calibration techniques produce very similar results (better than 7% agreement), providing strong validation for the techniques used. since the stellar calibration described here can be performed without a color target in the field of view and covers all of mvic 's detectors, this calibration was used to provide the radiometric keyword values delivered by the new horizons project to the planetary data system (pds). these keyword values allow each observation to be converted from counts to physical units; a description of how these keyword values were generated is included. finally, mitigation techniques adopted for the gain drift observed in the near-infrared detector and one of the panchromatic framing cameras are also discussed. (c) 2016 elsevier inc. all rights reserved.
network_security	the fast retrieval in archival traffic data is essential for network security and forensic analysis. a bitmap index is a data structure enabling fast search over large data collections in a limited time, but the space consumption is always a problem. wah, plwah and compax are proposed for compressing bitmap indexes for less storage. in this paper, a new bitmap index encoding scheme, named masc, is proposed to further improve the compression ratio without impairing the query performance. instead of being limited to a fixed length (31 bits) in plwah and compax, the stride size can be as long as possible to encode consecutive zero bits and nonzero bits in a more compact way. instead of piggyback used in plwah, a new structure in masc called carrier is introduced as piggyback in plwah only carries an individual nonzero bit. we also generalize the traditional literal word concept in plwah and compax. the validity of masc encoding scheme is demonstrated with the application in internet traffic archival system. based on experiments with real internet traffic data set from caida, masc has a better compression ratio than plwah and compax2 without the penalty in query performance.
operating_systems	we present bill2d, a modern and efficient c++ package for classical simulations of two-dimensional hamiltonian systems. bill2d can be used for various billiard and diffusion problems with one or more charged particles with interactions, different external potentials, an external magnetic field, periodic and open boundaries, etc. the software package can also calculate many key quantities in complex systems such as poincare sections, survival probabilities, and diffusion coefficients. while aiming at a large class of applicable systems, the code also strives for ease-of-use, efficiency, and modularity for the implementation of additional features. the package comes along with a user guide, a developer 's manual, and a documentation of the application program interface (api). program summary program title: bill2d catalogue identifier: aeylv1_0 program summary url: http://cpc.cs.qub.ac.uk/summaries/aeylv1_0.html program obtainable from: cpc program library, queen 's university, belfast, n. ireland licensing provisions: gnu general public license, version 3 no. of lines in distributed program, including test data, etc.: 37098 no. of bytes in distributed program, including test data, etc.: 1155037 distribution format: tar.gz programming language: c++(14). computer: tested on x86 and x86 64 architectures. operating systems: tested on linux, and os x versions 10.9-10.11. has the code been vectorized or parallelized?: shared memory parallelization when simulating ensembles of systems. vectorization of operations with r-2 vectors. ram: simulation dependent: kilobytes to gigabytes classification: 4.3, 7.8, 7.9, 7.10, 16.9. external routines: boost, cmake, gsl, hdf5; and optionally google-mock, googletest, and doxygen nature of problem: numerical propagation of classical two-dimensional single and many-body systems, possibly in a magnetic field, and calculation of relevant quantities such as poincare sections, survival probabilities, diffusion co-efficients, etc. solution method: symplectic numerical integration of hamilton 's equations of motion in cartesian coordinates, or solution of newton 's equations of motion if in a magnetic field. the program implements several well-established algorithms. restrictions: pointlike particles with equal masses and charges, although the latter restrictions are easy to lift. unusual features: program is efficient, extremely modular and easy to extend, and allows arbitrary particle-particle interactions. additional comments: the source code is also available at https://bitbucicet.orgisolanpaa/bill2d. see readme for locations of user guide, developer manual, and api docs. running time: from milliseconds to days, depends on type of simulation. (c) 2015 elsevier b.v. all rights reserved.
symbolic_computation	in this paper, with the aid of symbolic computation, we investigate the generalized nonlinear schrodinger maxwell-bloch equation, which describes the propagation of the optical soliton through an inhomogeneous two-level dielectric tapered fiber medium. by virtue of the darboux transformation method, two-soliton solutions are generated based on the constructed lax pair and figures are plotted to illustrate the properties of the obtained solutions. moreover, through manipulating the dispersion and nonlinearity profiles, various soliton control systems are investigated which is promising for potential applications in the design of soliton compressor, soliton amplification and highspeed optical devices in ultralarge capacity transmission systems. this means that we are able to control the soliton types with suitably selected values of the parameters. additionally more soliton control techniques are proposed and investigated. we expect that the above analysis could be observed in future experiments.
structured_storage	the aim of the chemotherapeutic regimens (chr) digitalization project is the proposal of a universal structure and creation of a publicly accessible database of contemporary chr as a universal utility for the communication and evaluation of contemporary and newly defined clinical schedules in anti-tumor chemotherapy. after analysis of contemporary anti tumor chr a standard xml structure was proposed, which enables the recording of simple chr from the field of chemotherapy in solid adult tumors, and also has the potential of recording the complex treatment protocols in the field of paediatric oncology. the resulting xml documents were saved on a web server. a publicly accessible chr database was constructed. there were a total of 130 xml documents with definitions of individual chr in the first phase. linked to this data store, three examples of web applications were added to demonstrate the potential uses of this newly created database.
computer_graphics	given a graph g that admits a perfect matching, we investigate the parameter eta(g) (originally motivated by computer graphics applications) which is defined as follows. among all nonnegative edge weight assignments, eta(g) is the minimum ratio between (i) the maximum weight of a perfect matching and (ii) the maximum weight of a general matching. in this paper, we determine the exact value of eta for all rectangular grids, all bipartite cylindrical grids, and all bipartite toroidal grids. we introduce several new techniques to this endeavor. (c) 2016 elsevier b.v. all rights reserved.
computer_programming	braille-character recognition is one of the foundational skills required for teachers of braille. prior research has evaluated computer programming for teaching braille-to-print letter relations (e.g., scheithauer & tiger, 2012). in the current study, we developed a program (the visual braille trainer) to teach not only letters but also numerals, punctuation, symbols, and contractions; we evaluated this program with 4 sighted undergraduate participants. exposure to this program resulted in mastery of all braille-to-print relations for each participant.
data_structures	we present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier. the framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns, and uses the sound and relatively complete reachability logic proof system to prove the specifications using the semantics. we instantiate the framework with the semantics of one academic language, kernelc, as well as with three recent semantics of real-world languages, c, java, and javascript, developed independently of our verification infrastructure. we evaluate our approach empirically and show that the generated program verifiers can check automatically the full functional correctness of challenging heap-manipulating programs implementing operations on list and tree data structures, like avl trees. this is the first approach that can turn the operational semantics of real-world languages into correct-by-construction automatic verifiers.
parallel_computing	the theory of three-way decisions is to consider a decision-making problem as a ternary classification one which is realized by the acceptance, rejection and non-commitment. recently, this theory has been integrated with formal concept analysis in two different ways: constructive and axiomatic methods. the constructive method is to define certain three-way concepts in a formal context to support three-way concept analysis, while the axiomatic one is to characterize general three-way concepts by axioms so as to perform three-way concept learning. nevertheless, there are similarities between the constructive and the axiomatic methods. in fact, both three-way concept analysis induced by the constructive method and three-way concept learning induced by the axiomatic one are realized by incorporating the idea of ternary classification into the design of extent or intent of a concept. however, their information fusion abilities need to be improved since neither of them is able to deal with large or multi-source data effectively. motivated by this problem, our paper is to reconsider three-way concept learning based on cognitive operators from the perspective of information fusion. that is, the parallel computing techniques of learning three-way concepts are developed for large and multi-source data. specifically, for large data, the relationship between the global granular concept and the local ones is first clarified, and then it is employed to design an information fusion algorithm. for multi-source data, the whole evaluation function used to induce three-way decisions is established by aggregating the results obtained in each single-source data, and three-way concept learning is made by constructing lower and upper approximation concepts. finally, we conduct some numerical experiments to evaluate the effectiveness of the proposed parallel computing algorithms. (c) 2017 elsevier inc. all rights reserved.
bioinformatics	objectives: the aim of this work was to construct inverpep, a database specialised in experimentally validated antimicrobial peptides (amps) from invertebrates. methods: amp data contained in inverpep were manually curated from other databases and the scientific literature. mysql was integrated with the development platform laravel; this framework allows to integrate programming in php with html and was used to design the inverpep web page 's interface. inverpep contains 18 separated fields, including inverpep code, phylum and species source, peptide name, sequence, peptide length, secondary structure, molar mass, charge, isoelectric point, hydrophobicity, boman index, aliphatic index and percentage of hydrophobic amino acids. calcampi, an algorithm to calculate the physicochemical properties of multiple peptides simultaneously, was programmed in perl language. results: to date, inverpep contains 702 experimentally validated amps from invertebrate species. all of the peptides contain information associated with their source, physicochemical properties, secondary structure, biological activity and links to external literature. most amps in inverpep have a length between 10 and 50 amino acids, a positive charge, a boman index between 0 and 2 kcal/mol, and 30-50% hydrophobic amino acids. inverpep includes 33 amps not reported in other databases. besides, calcampi and statistical analysis of inverpep data is presented. the inverpep database is available in english and spanish. conclusions: inverpep is a useful database to study invertebrate amps and its information could be used for the design of new peptides. the user-friendly interface of inverpep and its information can be freely accessed via a web-based browser at http://ciencias.medellin.unal.edu.co/gruposdeinvestigacion/prospeccionydisenobiomoleculas/inverpep/public/home_en. (c) 2016 international society for chemotherapy of infection and cancer. published by elsevier ltd. all rights reserved.
computer_graphics	terrain data can be processed from the double perspective of computer graphics and graph theory. we propose a hybrid method that uses geometrical and vertex attribute information to construct a weighted graph reflecting the variability of the vertex data. as a planar graph, a generic terrain data set is subjected to a geometry-sensitive vertex partitioning procedure. through the use of a combined, thin-plate energy and multi-dimensional quadric metric error, feature estimation heuristic, we construct even' and odd' node subsets. using an invertible lifting scheme, adapted from generic weighted graphs, detail vectors are extracted and used to recover or filter the node information. the design of the prediction and update filters improves the root mean squared error of the signal over general graph-based approaches. as a key property of this design, preserving the mean of the graph signal becomes essential for decreasing the error measure and conserving the salient shape features.
computer_vision	the accurate location of eyes in a facial image is important to many human facial recognition-related applications, and has attracted considerable research interest in computer vision. however, most prevalent methods are based on the frontal pose of the face, where applying them to non-frontal poses can yield erroneous results. in this paper, we propose an eye detection method that can locate the eyes in facial images captured at various head poses. our proposed method consists of two stages: eye candidate detection and eye candidate verification. in eye candidate detection, eye candidates are obtained by using multi-scale iris shape features and integral image. the size of the iris in face images varies as the head pose changes, and the proposed multi-scale iris shape feature method can detect the eyes in such cases. since it utilizes the integral image, its computational cost is relatively low. the extracted eye candidates are then verified in the eye candidate verification stage using a support vector machine (svm) based on the feature-level fusion of a histogram of oriented gradients (hog) and cell mean intensity features. we tested the performance of the proposed method using the chinese academy of sciences' pose, expression, accessories, and lighting (cas-peal) database and the pointing'04 database. the results confirmed the superiority of our method over the conventional haar-like detector and two hybrid eye detectors under relatively extreme head pose variations. (c) 2016 elsevier b.v. all rights reserved.
structured_storage	successful, quality software projects need to be able to rely on a sufficient level of security in order to manage the technical, legal and business risks that arise from distributed development. the definition of a 'sufficient' level of security however, is typically only captured in implicit requirements that are rarely gathered in a methodological way. such an unstructured approach makes the work of quality managers incredibly difficult and often forces developers to unwillingly operate in an unclear/undefined security state throughout the project. ideally, security requirements are elicited in methodological manner enabling a structured storage. retrieval, or checking of requirements. in this paper we report on the experiences of applying a structured requirements elicitation method and list a set of gathered reference security requirements. the reported experiences were gathered in an industrial setting using the open source platform opencit in cooperation with industry partners. the output of this work enables security and quality conscious stakeholders in a software project to draw from our experiences and evaluate against a reference base line.
machine_learning	muts alpha is a key component in the mismatch repair (mmr) pathway. this protein is responsible for initiating the signaling pathways for dna repair or cell death. herein we investigate this heterodimer 's post-recognition, post-binding response to three types of dna damage involving cytotoxic, anti-cancer agents-carboplatin, cisplatin, and fdu. through a combination of supervised and unsupervised machine learning techniques along with more traditional structural and kinetic analysis applied to all-atom molecular dynamics (md) calculations, we predict that muts alpha has a distinct response to each of the three damage types. via a binary classification tree (a supervised machine learning technique), we identify key hydrogen bond motifs unique to each type of damage and suggest residues for experimental mutation studies. through a combination of a recently developed clustering (unsupervised learning) algorithm, rmsf calculations, pca, and correlated motions we predict that each type of damage causes mutsa to explore a specific region of conformation space. detailed analysis suggests a short range effect for carboplatin-primarily altering the structures and kinetics of residues within 10 angstroms of the damaged dna-and distinct longer-range effects for cisplatin and fdu. in our simulations, we also observe that a key phenylalanine residue-known to stack with a mismatched or unmatched bases in mmr-stacks with the base complementary to the damaged base in 88.61% of md frames containing carboplatinated dna. similarly, this phe71 stacks with the base complementary to damage in 91.73% of frames with cisplatinated dna. this residue, however, stacks with the damaged base itself in 62.18% of trajectory frames with fdu-substituted dna and has no stacking interaction at all in 30.72% of these frames. each drug investigated here induces a unique perturbation in the mutsa complex, indicating the possibility of a distinct signaling event and specific repair or death pathway (or set of pathways) for a given type of damage.
computer_vision	the study aimed to determine if computer vision techniques rooted in deep learning can use a small set of radiographs to perform clinically relevant image classification with high fidelity. one thousand eight hundred eighty-five chest radiographs on 909 patients obtained between january 2013 and july 2015 at our institution were retrieved and anonymized. the source images were manually annotated as frontal or lateral and randomly divided into training, validation, and test sets. training and validation sets were augmented to over 150,000 images using standard image manipulations. we then pre-trained a series of deep convolutional networks based on the open-source googlenet with various transformations of the open-source imagenet (non-radiology) images. these trained networks were then fine-tuned using the original and augmented radiology images. the model with highest validation accuracy was applied to our institutional test set and a publicly available set. accuracy was assessed by using the youden index to set a binary cutoff for frontal or lateral classification. this retrospective study was irb approved prior to initiation. a network pre-trained on 1.2 million greyscale imagenet images and fine-tuned on augmented radiographs was chosen. the binary classification method correctly classified 100 % (95 % ci 99.73-100 %) of both our test set and the publicly available images. classification was rapid, at 38 images per second. a deep convolutional neural network created using non-radiological images, and an augmented set of radiographs is effective in highly accurate classification of chest radiograph view type and is a feasible, rapid method for high-throughput annotation.
relational_databases	data-related businesses is an emerging trend in the recent decade. however, the availability and amount of information make it difficult to ensure quality in terms of data fusion and version control. completely automated data aggregation systems fail to provide reliable and consistent data. in this paper we summarise existing knowledge on relation databases and augment it with description of business requirements for data version control. we propose an architecture that addresses the requirements, and discuss possible future work to improve and evaluate the approach.
algorithm_design	data access delay has become the prominent performance bottleneck of high-end computing systems. the key to reducing data access delay in system design is to diminish data stall time. memory locality and concurrency are the two essential factors influencing the performance of modern memory systems. however, existing studies in reducing data stall time rarely focus on utilizing data access concurrency because the impact of memory concurrency on overall memory system performance is not well understood. in this study, a pair of novel data stall time models, the l-c model for the combined effort of locality and concurrency and the p-m model for the effect of pure miss on data stall time, are presented. the models provide a new understanding of data access delay and provide new directions for performance optimization. based on these new models, a summary table of advanced cache optimizations is presented. it has 38 entries contributed by data concurrency while only has 21 entries contributed by data locality, which shows the value of data concurrency. the l-c and p-m models and their associated results and opportunities introduced in this study are important and necessary for future data-centric architecture and algorithm design of modern computing systems.
parallel_computing	in the supervised classification, large training data are very common, and decision trees are widely used. however, as some bottlenecks such as memory restrictions, time complexity, or data complexity, many supervised classifiers including classical c4.5 tree cannot directly handle big data. one solution for this problem is to design a highly parallelized learning algorithm. motivated by this, we propose a parallelized c4.5 decision tree algorithm based on mapreduce (mr-c4.5-tree) with 2 parallelized methods to build the tree nodes. first, an information entropy-based parallelized attribute selection method (mr-a-s) on several subsets for mr-c4.5-tree is proposed to confirm the best splitting attribute and the cut points. then, a data splitting method (mr-d-s) in parallel is presented to partition the training data into subsets. at last, we introduce the mr-c4.5-tree learning algorithm that grows in a top-down recursive way. besides, the depth of the constructed decision tree, the number of samples and the maximal class probability in each tree node are used as the termination conditions to avoid the over-partitioning problem. experimental studies show the feasibility and the good performance of the proposed parallelized mr-c4.5-tree algorithm.
data_structures	path polymorphism is the ability to define functions that can operate uniformly over arbitrary recursively specified data structures. its essence is captured by patterns of the form xy which decompose a compound data structure into its parts. typing these kinds of patterns is challenging since the type of a compound should determine the type of its components. we propose a static type system (i.e. no run-time analysis) for a pattern calculus that captures this feature. our solution combines type application, constants as types, union types and recursive types. we address the fundamental properties of subject reduction and progress that guarantee a well-behaved dynamics. both these results rely crucially on a notion of pattern compatibility and also on a coinductive characterisation of subtyping.
bioinformatics	this is the first report on a myophage that infects arthrobacter. a novel virus, vb_artm-arv1 (arv1), was isolated from soil using arthrobacter sp. strain 68b for phage propagation. transmission electron microscopy showed its resemblance to members of the family myoviridae: arv1 has an isometric head (similar to 74 nm in diameter) and a contractile, nonflexible tail (similar to 192 nm). phylogenetic and comparative sequence analyses, however, revealed that arv1 has more genes in common with phages from the family siphoviridae than it does with any myovirus characterized to date. the genome of arv1 is a linear, circularly permuted, double-stranded dna molecule (71,200 bp) with a gc content of 61.6%. the genome includes 101 open reading frames (orfs) yet contains no trna genes. more than 50% of arv1 genes encode unique proteins that either have no reliable identity to database entries or have homologues only in arthrobacter phages, both sipho- and myoviruses. using bioinformatics approaches, 13 arv1 structural genes were identified, including those coding for head, tail, tail fiber, and baseplate proteins. a further 6 arv1 orfs were annotated as encoding putative structural proteins based on the results of proteomic analysis. phylogenetic analysis based on the alignment of four conserved virion proteins revealed that arthrobacter myophages form a discrete clade that seems to occupy a position somewhat intermediate between myo-and siphoviruses. thus, the data presented here will help to advance our understanding of genetic diversity and evolution of phages that constitute the order caudovirales. importance bacteriophages, which likely originated in the early precambrian era, represent the most numerous population on the planet. approximately 95% of known phages are tailed viruses that comprise three families: podoviridae (with short tails), siphoviridae (with long noncontractile tails), and myoviridae (with contractile tails). based on the current hypothesis, myophages, which may have evolved from siphophages, are thought to have first emerged among gram-negative bacteria, whereas they emerged only later among gram-positive bacteria. the results of the molecular characterization of myophage vb_artm-arv1 presented here conform to the aforementioned hypothesis, since, at a glance, bacteriophage vb_artm-arv1 appears to be a siphovirus that possesses a seemingly functional contractile tail. our work demonstrates that such ""chimeric"" myophages are of cosmopolitan nature and are likely characteristic of the ecologically important soil bacterial genus arthrobacter.
computer_vision	we explored how computer vision techniques can be used to detect engagement while students (n = 22) completed a structured writing activity (draft-feedback-review) similar to activities encountered in educational settings. students provided engagement annotations both concurrently during the writing activity and retrospectively from videos of their faces after the activity. we used computer vision techniques to extract three sets of features from videos, heart rate, animation units (from microsoft kinect face tracker), and local binary patterns in three orthogonal planes (lbp-top). these features were used in supervised learning for detection of concurrent and retrospective self-reported engagement. area under the roc curve (auc) was used to evaluate classifier accuracy using leave-several-students-out cross validation. we achieved an auc = .758 for concurrent annotations and auc = .733 for retrospective annotations. the kinect face tracker features produced the best results among the individual channels, but the overall best results were found using a fusion of channels.
computer_graphics	this paper proposes a novel method of semi-regular remeshing for triangulated surfaces to achieve superior triangles lead to advanced visualization of 3d model. it is based on mesh segmentation and subdivision surface fitting which uses curvature-adapted polygon patches. our contribution lies in building a sophisticated system with three stages, i.e., curvature-aware mesh segmentation, submesh surface fitting to generate a high-quality semi-regular mesh and finally, stitching the segments using an efficient algorithm. our method uses centroidal voronoi tessellation and lloyd 's relaxation to generate curvature-adapted site centers. geodesic distances from site centers are used for labeling segments and indexing corner vertices for each segment boundary. using information of site centers and corner vertices, feature-adapted polygonal patches are generated for each segment. these patches are then subdivided and optimized using squared distance metric to adjust position of the subdivision sampling with segment details and prevent oversampling. at last, an efficient stitching algorithm is introduced to connect regular submeshes together and build the final semi-regular mesh. we have demonstrated the results of our semi-regular remeshing algorithm on meshes with different topology and complexity and compared them with known methods. superior triangle quality with higher aspect ratio together with acceptable distortion error is achieved according to the experimental results.
computer_programming	this paper describes the development, validation, and uses of the collaborative computing observation instrument (c-coi), a web-based analysis instrument that classifies individual and/or collaborative behaviors of students during computing problem-solving (e.g. coding, programming). the c-coi analyzes data gathered through video and audio screen recording software that captures students' computer screens as they program, and their conversations with their peers or adults. the instrument allows researchers to organize and quantify these data to track behavioral patterns that could be further analyzed for deeper understanding of persistence and/or collaborative interactions. the article provides a rationale for the c-coi including the development of a theoretical framework for measuring collaborative interactions in computer-mediated environments. this theoretical framework relied on the computer-supported collaborative learning literature related to adaptive help seeking, the joint problem-solving space in which collaborative computing occurs, and conversations related to outcomes and products of computational activities. instrument development and validation also included ongoing advisory board feedback from experts in computer science, collaborative learning, and k-12 computing as well as classroom observations to test out the constructs in the c-coi. these processes resulted in an instrument with rigorous validation procedures and a high inter-rater reliability.
image_processing	modern cars are equipped with both active and passive sensor systems that can detect potential collisions. in contrast, locusts avoid collisions solely by responding to certain visual cues that are associated with object looming. in neurophysiological experiments, i investigated the possibility that the 'collision-detector neurons' of locusts respond to impending collisions in films recorded with dashboard cameras of fast driving cars. in a complementary modelling approach, i developed a simple algorithm to reproduce the neuronal response that was recorded during object approach. instead of applying elaborate algorithms that factored in object recognition and optic flow discrimination, i tested the hypothesis that motion detection restricted to a 'danger zone', in which frontal collisions on the motorways are most likely, is sufficient to estimate the risk of a collision. furthermore, i investigated whether local motion vectors, obtained from the differential excitation of simulated direction-selective networks, could be used to predict evasive steering maneuvers and prevent undesired responses to motion artifacts. the results of the study demonstrate that the risk of impending collisions in real traffic scenes is mirrored in the excitation of the collision-detecting neuron (dcmd) of locusts. the modelling approach was able to reproduce this neuronal response even when the vehicle was driving at high speeds and image resolution was low (about 200 x 100 pixels). furthermore, evasive maneuvers that involved changing the steering direction and steering force could be planned by comparing the differences in the overall excitation levels of the simulated right and left direction-selective networks. additionally, it was possible to suppress undesired responses of the algorithm to translatory movements, camera shake and ground shadows by evaluating local motion vectors. these estimated collision risk values and evasive steering vectors could be used as input for a driving assistant, converting the first into braking force and the latter into steering responses to avoid collisions. since many processing steps were computed on the level of pixels and involved elements of directionselective networks, this algorithm can be implemented in hardware so that parallel computations enhance the processing speed significantly.
operating_systems	traditionally, digital forensics focused on artifacts located on the storage devices of computer systems, mobile phones, digital cameras, and other electronic devices. in the past decade, however, researchers have created a number of powerful memory forensics tools that expand the scope of digital forensics to include the examination of volatile memory as well. while memory forensic techniques have evolved from simple string searches to deep, structured analysis of application and kernel data structures for a number of platforms and operating systems, much research remains to be done. this paper surveys the state-of-the-art in memory forensics, provide critical analysis of current-generation techniques, describe important changes in operating systems design that impact memory forensics, and sketches important areas for further research. (c) 2017 elsevier ltd. all rights reserved.
computer_programming	computer programming as a process that embodies the creation of an executable computer program for a given computational problem by analyzing the task and developing an algorithm that computes the desired result. due to its complex and diverse nature, programming requires a certain level of expertise in analysis of algorithms, data structures, mathematics, formal logic as well as related tasks such as testing and debugging. due to increasing awareness of need for programming, there exists numerous competitive programming websites where students can practice and solve problems. the aim of our work is to assess the performance of students on such platforms. this work shall not only help the learners to self-assess themselves, but it will also aid the educators to evaluate the progress of their students. to meet this objective, the data was collected from two different competitive programming environments, namely, hackerearth- a globally accessible competitive programming website and our university 's in-house programming portal, a university-based programming environment. we used supervised learning to predict the performance of students for both the datasets. the accuracy obtained for the hackerearth dataset is 80%, while the accuracy for the university dataset was computed to be 91%. apart from predicting the performance, rigorous analyses were done unearth hidden trends responsible for a learners programming acumen.
network_security	the multi objective genetic algorithms (mo-gas) are one of the most widely used techniques that have the capability to find the solution to the problem having multiple conflicting objectives like intrusion de- tection. it is a population based technique capable of producing a set of non-inferior solutions that exhibit the classification trade-offs for the user. this capabil- ity of moga can be exploited for generating optimal base classifiers and ensembles thereof for intrusion de- tection. this paper explores the various mogas proposed in the literature along with their pros and cons. the motivation for the use of moga and its issues are high- lighted. finally, the chapter highlights the concluding remarks.
operating_systems	operating system is essential to operate computers. normally, computers come with preloaded operating systems. however, often the preloaded operating systems are not able to fulfill all requirements of users. the users sometimes need to change the operating system based on their needs. although some comparative studies and tools are available on operating systems, there is still a lack of tools that provide independent and objective review and recommendation to help the users understand and select from all major operating systems. this paper propose a tool called fsos, which analyses well-known operating systems used at domestic, commercial and industrial level and suggest suitable operating systems to the users as per their requirements.
machine_learning	we present a new approach to lightweight intelligent transportation systems. our approach does not rely on traditional expensive infrastructures, but rather on advanced machine learning algorithms. it takes images from traffic cameras at a limited number of locations and estimates the traffic over the entire road network. our approach features two main algorithms. the first is a probabilistic vehicle counting algorithm fromlow-quality images that falls into the category of unsupervised learning. the other is a network inference algorithm based on an inverse markov chain formulation that infers the traffic at arbitrary links from a limited number of observations. we evaluated our approach on two different traffic data sets, one acquired in nairobi, kenya, and the other in kyoto, japan.
computer_graphics	in this paper, we first apply cosine radial basis function neural networks to solve the fractional differential equations with initial value problems or boundary value problems. in the examples, we successfully obtained the numerical solutions for the fractional riccati equations and fractional langevin equations. the computer graphics and numerical solutions show that this method is very effective.
image_processing	inadequate skid resistance of pavement surface is a substantial reason for traffic accidents. there is a close relationship between sliding resistance and characteristics of texture morphology, demanding high precision and comprehensive acquisition of both macrotexture and microtexture morphology. the traditional three light sources photometric stereo method is improved in this study fourfold. first, six light sources are adopted to enhance the illumination and eliminate incomplete information retrieval of pavement surface image. second, a low-rank approximation is proposed in the image processing stage to significantly reduce the interference of noise, highlights, and shadow, resulting in a higher precision of reconstructed pavement surface compared with the existing photometric stereo method using three light sources. third, unlike the control point-based weighting algorithm, a control point-based surface interpolation algorithm is established, which can further optimizes the precision of the reconstructed surface by combining the effect of global integration with the elevation of positions of relative points. under testing in indoor conditions, a surface interpolation photometric stereo method with 2,500 control points and using low-rank approximation can effectively measure both macrotexture and microtexture morphology. last, low-rank approximation and global integration with six light sources is used to relax the requirement of the surface interpolation photometric stereo method on control points. statistical analysis indicates that low-rank approximation and global integration with six light sources can be an effective method for reconstructing three-dimensional (3d) macrotexture and microtexture morphology. (c) 2016 american society of civil engineers.
computer_vision	this work aims to discriminate among different species of the genus cistus, using seed parameters and following the scientific plant names included as accepted in the plant list. also, the intraspecific phenotypic differentiation of c.creticus, through comparison with three subspecies (c.creticus subsp. creticus, c.c.subsp. eriocephalus and c.c. subsp. corsicus), as well as the interpopulation variability among five c.creticus subsp. eriocephalus populations was evaluated. seed mean weight and 137 morphocolorimetric quantitative variables, describing shape, size, colour and textural seed traits, were measured using image analysis techniques. measured data were analysed applying step-wise linear discriminant analysis. an overall cross-validated classification performance of 80.6% was recorded at species level. with regard to c. creticus, as case study, percentages of correct discrimination of 96.7% and 99.6% were achieved at intraspecific and interpopulation levels, respectively. in this classification model, the relevance of the colorimetric and textural descriptive features was highlighted, as well as the seed mean weight, which was the most discriminant feature at specific and intraspecific level. these achievements proved the ability of the image analysis system as highly diagnostic for systematic purposes and confirm that seeds in the genus cistus have important diagnostic value.
software_engineering	generally, software re-engineering is economical and perfect way to provide much needed boost to a present software system. software re-engineering is like to obtain a fully completed software from existing software with additional features if needed. the overall process of software re-engineering is to analyze the needed requiements & its contents. it also changes the needed contents or transforms the existing software system for reconstructing a novel software system. the difficult part in re-engineering is to understand the traditional system. most of the software re-engineering mechanisms are aimed to achieve the common re-engineering objectives and the objectives are: improved software quality, reduced complexity, reduce maintenance cost and increased reliability. as a result, several traditional re-engineering mechanisms fail to verify the performance of individual functionality in existing software. this performance evaluation increases the complexity in re-engineering process. to minimizing the complexities in software re-engineering, this proposed system implements a novel approach named enhanced re-engineering mechanism. this enhanced mechanism introduces a new idea, before executing the re-build process the developer verifies the performance of particular function in existing system. after that, the function performance is compared with proposed algorithm. based on the comparison process only rebuild process should be carried out. finally this proposed mechanism reduces the complexities in software re-engineering.
relational_databases	during the last decades, we assisted to what is called ""information explosion"". with the advent of the new technologies and new contexts, the volume, velocity and variety of data has increased exponentially, becoming what is known today as big data. among them, we emphasize telecommunications operators, which gather, using network monitoring equipment, millions of network event records, the call detail records (cdrs) and the event detail records (edrs), commonly known as xdrs. these records are stored and later processed to compute network performance and quality of service metrics. with the ever increasing number of collected xdrs, its generated volume needing to be stored has increased exponentially, making the current solutions based on relational databases not suited anymore. to tackle this problem, the relational data store can be replaced by hadoop file system (hdfs). however, hdfs is simply a distributed file system, this way not supporting any aspect of the relational paradigm. to overcome this difficulty, this paper presents a framework that enables the current systems inserting data into relational databases, to keep doing it transparently when migrating to hadoop. as proof of concept, the developed platform was integrated with the altaia - a performance and qos management of telecommunications networks and services.
bioinformatics	purpose. applying cnga3 gene augmentation therapy to cure a novel causative mutation underlying achromatopsia (achm) in sheep. methods. impaired vision that spontaneously appeared in newborn lambs was characterized by behavioral, electroretinographic (erg), and histologic techniques. deep-sequencing reads of an affected lamb and an unaffected lamb were compared within conserved genomic regions orthologous to human genes involved in similar visual impairment. observed nonsynonymous amino acid substitutions were classified by their deleteriousness score. the putative causative mutation was assessed by producing compound cnga3 heterozygotes and applying gene augmentation therapy using the orthologous human cdna. results. behavioral assessment revealed day blindness, and subsequent erg examination showed attenuated photopic responses. histologic and immunohistochemical examination of affected sheep eyes did not reveal degeneration, and cone photoreceptors expressing cnga3 were present. bioinformatics and sequencing analyses suggested a c. 1618g>a, p. gly540ser substitution in the gmp-binding domain of cnga3 as the causative mutation. this was confirmed by genetic concordance test and by genetic complementation experiment: all five compound cnga3 heterozygotes, carrying both p. arg236* and p. gly540ser mutations in cnga3, were day-blind. furthermore, subretinal delivery of the intact human cnga3 gene using an adeno-associated viral vector (aav) restored photopic vision in two affected p. gly540ser homozygous rams. conclusions. the c. 1618g>a, p. gly540ser substitution in cnga3 was identified as the causative mutation for a novel form of achm in awassi sheep. gene augmentation therapy restored vision in the affected sheep. this novel mutation provides a large-animal model that is valid for most human cnga3 achm patients; the majority of them carry missense rather than premature-termination mutations.
parallel_computing	this article presents recent efforts in improving the efficiency and scalability of the mixed cell computation step in the context of the polyhedral homotopy method. solving systems of polynomial equations is an important problem in applied mathematics. the polyhedral homotopy method is an important numerical method for this task. in this method, a necessary preprocessing step, known as the ""mixed cell computation"" problem has been the main bottleneck in the parallel efficiency and scalability. this article presents recent remarkable improvements in the parallel scalability of the algorithm that are applicable to a wide range of hardware architectures including multi-core systems, numa systems, computer clusters, and gpus devices. (c) 2016 elsevier ltd. all rights reserved.
image_processing	previous studies have demonstrated that matrix factorization techniques, such as nonnegative matrix factorization (nmf) and concept factorization (cf), have yielded impressive results in image processing and data representation. however, conventional cf and its variants with single layer factorization fail to capture the intrinsic structure of data. in this paper, we propose a novel sequential factorization method, namely graph regularized multilayer concept factorization (gmcf) for clustering. gmcf is a multi-stage procedure, which decomposes the observation matrix iteratively in a number of layers. in addition, gmcf further incorporates graph laplacian regularization in each layer to efficiently preserve the manifold structure of data. an efficient iterative updating scheme is developed for optimizing gmcf. the convergence of this algorithm is strictly proved; the computational complexity is detailedly analyzed. extensive experiments demonstrate that gmcf owns the superiorities in terms of data representation and clustering performance. (c) 2017 elsevier b.v. all rights reserved.
cryptography	in a digital multisignature scheme, two or more signers are allowed to produce a single signature on a common message, which can be verified by anyone. in the literature, many schemes are available based on the public key infrastructure or identity-based cryptosystem with bilinear pairing and map-to-point (mtp) hash function. the bilinear pairing and the mtp function are time-consuming operations and they need a large super-singular elliptic curve group. moreover, the cryptosystems based on them are difficult to implement and less efficient for practical use. to the best of our knowledge, certificateless digital multisignature scheme without pairing and mtp hash function has not yet been devised and the same objective has been fulfilled in this paper. furthermore, we formally prove the security of our scheme in the random oracle model under the assumption that ecdlp is hard.
cryptography	mds matrices are of great importance in the design of block ciphers and hash functions. mds matrices are not sparse and have a large description and thus induce costly implementation in software/hardware. to overcome this problem, in particular for applications in light-weight cryptography, it was proposed by guo et al. to use recursive mds matrices. a recursive mds matrix is an mds matrix which can be expressed as a power of some companion matrix. following the work of guo et al., some ad-hoc search techniques are proposed to find recursive mds matrices which are suitable for hardware/software implementation. in another direction, coding theoretic techniques are used to directly construct recursive mds matrices: berger technique uses gabidulin codes and augot et al. technique uses shortened bch codes. in this paper, we first characterize the polynomials that yield recursive mds matrices in a more general setting. based on this we provide three methods for obtaining such polynomials. moreover, the recursive mds matrices obtained using shortened bch codes can also be obtained with our first method. in fact we get a larger set of polynomials than the method which uses shortened bch codes. our other methods appear similar to the method which uses gabidulin codes. we get a new infinite class of recursive mds matrices from one of the proposed methods. although we propose three methods for the direct construction of recursive mds matrices, our characterization results pave the way for new direct constructions.
symbolic_computation	in this paper, the solution of limit problems, which is an important subject of high school and university mathematics is presented by using javacc code generation tool and symbolic computation methods. although javacc is generally used for generating programming language interpreters, in a similar way it can also be used in the evaluation of mathematical expressions. in this work, first the general grammar rules of limit expressions is extracted. then parser code for the limit expressions is generated with javacc according to the grammar rules. using the list of the tokens into which a limit expression is parsed with this code, an abstract syntax tree (ast) is constructed. finally, the solution is obtained by interpreting the ast with a class of visitor design pattern. the study can be regarded as a promising contribution to computer assisted education.
data_structures	network intrusion detection systems (nids) are deployed to protect computer networks from malicious attacks. proper evaluation of nids requires more scrutiny than the evaluation for general network appliances. this evaluation is commonly performed by sending pregenerated traffic through the nids. unfortunately, this technique is often limited in diversity resulting in evaluations incapable of examining the complex data structures employed by nids. more sophisticated methods that generate workload directly from nids rules consume excessive resources and are incapable of running in real-time. this work proposes a novel approach to real-time workload generation for nids evaluation to improve evaluation diversity while maintaining much higher throughput. this work proposes a generative grammar which represents an optimized version of a context-free grammar derived from the set of strings matching to the given nids rule database. the grammar is memory-efficient and computationally light when generating workload. experiments demonstrate that grammar-generated workloads exert an order of magnitude more effort on the target nids. even better, this improved diversity comes at much smaller cost in memory and speeds four times faster than current approaches.
algorithm_design	opencl is a portable interface that can be used to program cluster nodes with heterogeneous compute devices. the opencl specification tightly binds its workflow abstraction, or ""command queue,"" to a specific device for the entire program. for best performance, the user has to find the ideal queue device mapping at command queue creation time, an effort that requires a thorough understanding of the match between the characteristics of all the underlying device architectures and the kernels in the program. in this paper, we propose to add scheduling attributes to the opencl context and command queue objects that can be leveraged by an intelligent runtime scheduler to automatically perform ideal queue device mapping. our proposed extensions enable the average opencl programmer to focus on the algorithm design rather than scheduling and automatically gain performance without sacrificing programmability. as an example, we design and implement an opencl runtime for task-parallel workloads, called multicl, which efficiently schedules command queues across devices. within multicl, we implement several key optimizations to reduce runtime overhead. our case studies include the snu-npb opencl benchmark suite and a real-world seismology simulation. we show that, on average, users have to apply our proposed scheduler extensions to only four source lines of code in existing opencl applications in order to automatically benefit from our runtime optimizations. we also show that multicl always maps command queues to the optimal device set with negligible runtime overhead.
structured_storage	the burgeoning volume of torrential data continues to grow exponentially in this very age of the internet of things. as this torrent of digital datasets continue to outgrow in datacenters, the focus needs to be shifted to stored data reduction methods and that too pertaining to nosql databases as traditional structured storage systems continuously tend to face challenges in providing the required storage, throughputs and computational power requirements necessary to capture, store, manage and analyze the deluge of data. deduplication systems, thus designed, retain a single copy of redundant data on disk to save disk space, but what if we want to keep certain copies intentionally and need wishful elimination. this paper leverages hadoop framework to design and develop a duplication detection system that detects multiple copies of the same data right at the file level itself and that too before transmission. thereafter, various datasets are tuned for better performance and analysed using mapreduce, hive and pig.
symbolic_computation	in this paper, we consider a variable coefficient calogero-degasperis equation, a variable coefficient potential kadomstev-petviashvili equation and the generalized (3+1)-dimensional variable coefficient kadomtsev-petviashvili equation with time-dependent coefficients. shock wave solutions for the considered models are obtained by using ansatz method in the form of tanhp function. the physical parameters in the soliton solutions are obtained as functions of the dependent coefficients. copyright (c) 2016 john wiley & sons, ltd.
distributed_computing	the local(a, b) randomized task scheduling algorithm is proposed for fully connected multiprocessors. it combines two given task scheduling algorithms (a, and b) using local neighborhood search to give a hybrid of the two given algorithms. objective is to show that such type of hybridization can give much better performance results in terms of parallel execution times. two task scheduling algorithms are selected: dsc (dominant sequence clustering as algorithms a), and cf'f's (cluster pair priority scheduling as algorithm b) and a hybrid is created (the local(dsc, cpps) or simply the local task scheduling algorithm). the local task scheduling algorithm has time complexity 0(broken vertical bar v broken vertical bar broken vertical bar 1e broken vertical bar(vertical bar v broken vertical bar + broken vertical bar e broken vertical bar)), where v is the set of vertices, and p is the set of edges in the task graph. the local task scheduling algorithm is compared with six other algorithms: cf'f's, dccl ((dynamic computation. communication load), dsc, ez (edge zeroing),. (linear clustering), and rdcc (randomized dynamic computation comm,unicalion). performance evaluation of the i oc al task scheduling algorithm shows that it gives up to 80.47 c improvement of nsl (normalized schedule length) over other algorithms.
symbolic_computation	systems of polynomial equations arise throughout mathematics, engineering, and the sciences. it is therefore a fundamental problem both in mathematics and in application areas to find the solution sets of polynomial systems. the focus of this paper is to compare two fundamentally different approaches to computing and representing the solutions of polynomial systems: numerical homotopy continuation and symbolic computation. several illustrative examples are considered, using the software packages bertini and singular. (c) 2014 elsevier inc. all rights reserved.
