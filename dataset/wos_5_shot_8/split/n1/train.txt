symbolic_computation	nowadays, marine scientists are making use of the kadomtsev-petviashvili (kp)-category equations in their investigations from the straits of georgia and gibraltar to the adriatic sea, north sea and south china sea. in fluid mechanics and other fields, the (3+1)-dimensional b-type kp equations have attracted a good size of recent research. for a generalized (3+1)-dimensional variable-coefficient b-type kp equation for the nonlinear waves in fluid mechanics, with symbolic computation, we obtain a variable-coefficient-dependent auto-backlund transformation, along with two variable-coefficient-dependent families of the shock-wave-type solutions. (c) 2015 elsevier ltd. all rights reserved.
symbolic_computation	under investigation in this paper is a (3 + 1)-dimensional variable-coefficient kadomtsev-petviashvili equation, which describes the propagation of surface and internal water waves. by virtue of the binary bell polynomials, symbolic computation and auxiliary independent variable, the bilinear forms, soliton solutions, backlund transformations and lax pair are obtained. variable coefficients of the equation can affect the solitonic structure, when they are specially chosen, while curved and linear solitons are illustrated. elastic collisions between/among two and three solitons are discussed, through which the solitons keep their original shapes invariant except for some phase shifts.
symbolic_computation	under investigation in this paper is a higher-order nonlinear schrodinger equation in an optical fiber. lax pair and infinitely-many conservation laws are derived via the symbolic computation. by virtue of the darboux transformation, one-, two- and three-soliton solutions are derived. propagation and interaction of the solitons are illustrated graphically: velocity of the soliton is linearly related to the higher-order dispersion coefficients, while amplitude of the soliton does not depend on them at all. width of the one soliton increases with the decrease of the intensity of the complex eigenvalue parameter. head-on interaction between the two bidirectional solitons as well as overtaking and oscillating interaction between the two unidirectional solitons are presented. for the interactions among the three solitons, we display two head-on and one overtaking interactions along with three overtaking interactions. graphical analysis shows that each interaction between the two solitons is elastic, and each interaction among the three solitons is pairwise elastic. (c) 2016 published by elsevier gmbh.
symbolic_computation	with the aid of symbolic computation maple, the discrete ablowitz ladik equation is studied via an algebra method, some new rational solutions with four arbitrary parameters are constructed. by analyzing related parameters, the discrete rogue wave solutions with alterable positions and amplitude for the focusing ablowitz ladik equations are derived. some properties are discussed by graphical analysis, which might be helpful for understanding physical phenomena in optics.
symbolic_computation	we explore the shape changing and clevaging nature of anti-kink solutions of a (3+1)-dimensional b-type kadomtsev-petviashvili equation. we achieved this by invoking the multiple exp-function method aided with symbolic computation which remains an indispensable tool to deal with computational algebraic systems.
computer_vision	speed and accuracy are important factors when dealing with time-constraint events for disaster, risk, and crisis-management support. object-based image analysis can be a time consuming task in extracting information from large images because most of the segmentation algorithms use the pixel-grid for the initial object representation. it would be more natural and efficient to work with perceptually meaningful entities that are derived from pixels using a low-level grouping process (superpixels). firstly, we tested a new workflow for image segmentation of remote sensing data, starting the multiresolution segmentation (mrs, using esp2 tool) from the superpixel level and aiming at reducing the amount of time needed to automatically partition relatively large datasets of very high resolution remote sensing data. secondly, we examined whether a random forest classification based on an oversegmentation produced by a simple linear iterative clustering (slic) superpixel algorithm performs similarly with reference to a traditional object-based classification regarding accuracy. tests were applied on quickbird and worldview-2 data with different extents, scene content complexities, and number of bands to assess how the computational time and classification accuracy are affected by these factors. the proposed segmentation approach is compared with the traditional one, starting the mrs from the pixel level, regarding geometric accuracy of the objects and the computational time. the computational time was reduced in all cases, the biggest improvement being from 5 h 35 min to 13 min, for a worldview-2 scene with eight bands and an extent of 12.2 million pixels, while the geometric accuracy is kept similar or slightly better. slic superpixel-based classification had similar or better overall accuracy values when compared to mrs-based classification, but the results were obtained in a fast manner and avoiding the parameterization of the mrs. these two approaches have the potential to enhance the automation of big remote sensing data analysis and processing, especially when time is an important constraint.
computer_vision	dual assignment clustering (dac) has been recently proposed in computer vision, shown to yield improved accuracy for action clustering tasks. the key idea of dac is to consider another view (different from the original features) for the same set of samples, and to exploit the statistical correlation between cluster assignments in two views. however, the existing optimization is heuristic, mainly due to the difficulty in combinatorial optimization for hard cluster assignment. in this paper, we introduce a novel dac optimization algorithm based on a probabilistic (soft) treatment, where the proposed objective function incorporates both the goodness of clustering in each view and the correlation between two views in a more principled and theoretically sound fashion. we also propose a lower-bound maximization technique that not only admits fast per-iteration solutions but also guarantees convergence to a local optimum. the superiority of the proposed approach to the existing methods is demonstrated for several activity video datasets. (c) 2016 elsevier inc. all rights reserved.
computer_vision	classifying 3d measurement data has become a core problem in photogrammetry and 3d computer vision, since the rise of modern multiview geometry techniques, combined with affordable range sensors. we introduce a markov random field-based approach for segmenting textured meshes generated via multi-view stereo into urban classes of interest. the input mesh is first partitioned into small clusters, referred to as superfacets, from which geometric and photometric features are computed. a random forest is then trained to predict the class of each superfacet as well as its similarity with the neighboring superfacets. similarity is used to assign the weights of the markov random field pairwise-potential and to account for contextual information between the classes. the experimental results illustrate the efficacy and accuracy of the proposed framework. (c) 2016 published by elsevier b.v. on behalf of international society for photogrammetry and remote sensing, inc. (isprs).
computer_vision	simple linear regression in the functional errors-in-variables (eiv) model is revisited from a different perspective, where the problem is addressed by using the small-sigma model instead of large sample theory. a general analysis is developed to study the slope 's estimator that minimizes a family of objective functions, of which the least-squares fit and the maximum likelihood estimator are minimizers of such special functions. general formulas for the higher-order terms of the bias, the variance, and the mean square error are derived. accordingly, two efficient estimators are proposed after implementing the pre- and the post-bias elimination techniques. numerical tests confirm the superiority of the proposed estimators over others.
computer_vision	due to its ability to eliminate the visual ambiguities in single-shot algorithms, video-based person re identification has received an increasing focus in computer vision. visual ambiguities caused by variations in view angle, lighting, and occlusions make the re-identification problem extremely challenging. to overcome the ambiguities, most previous approaches often extract robust feature representations or learn a sophisticated feature transformation. however, most of these approaches ignore the effect of the impostors arising from annotation or tracking process. in this case, impostors are regarded as genuine and applied in training process, leading to the model drift problem. in order to reduce the risk of model drifting, we propose to automatically discover impostors in a multiple instance metric learning framework. specifically, we propose a knn based confidence score to evaluate how much an impostor invades the interested target and utilize it as a prior in the framework. in the meanwhile, we integrate an impostor rejection mechanism in the multiple instance metric learning framework to automatically discover impostors, and learn the semantical similarity metrics with the refined training set. experiments show that the proposed system performs favorably against the state-of-the-art algorithms on two challenging datasets (ilids-vid and prid 2011). we have improved the rank 1 recognition rate on ilids-vid and prid 2011 dataset by 1.0% and 1.2%, respectively. (c) 2017 elsevier ltd. all rights reserved.
computer_graphics	the fashion industry is one of the most flourishing fields for visual applications of it. due to the importance of the concept of look in fashion, the most advanced applications of computer graphics and sensing may fruitfully be exploited. the existence of low cost solutions in similar fields, such as the ones that empower the domestic video games market, suggest that analogous low cost solutions are viable and can foster innovation even in small and medium enterprises. in this paper the current state of development of vmannequin, a dynamic, user mimicking, user enacted virtual mannequin software solution, is presented. in order to allow users designing dress concepts, the application simulate the creation and fitting of clothes on virtual models. the interaction is sensor based, in order to both simplify the user interface, and create a richer involvement inside the application.
computer_graphics	orientation and wayfinding in architectural immersive virtual environments (ives) are non-trivial, accompanying tasks which generally support the users' main task. world in miniatures (wims)-essentially 3d maps containing a scene replica-are an established approach to gain survey knowledge about the virtual world, as well as information about the user 's relation to it. however, for large-scale, information-rich scenes, scaling and occlusion issues result in diminishing returns. since there typically is a lack of standardized information regarding scene decompositions, presenting the inside of self-contained scene extracts is challenging. therefore, we present an automatic wim generation workflow for arbitrary, realistic in-and outdoor ives in order to support users with meaningfully selected and scaled extracts of the ive as well as corresponding context information. additionally, a 3d user interface is provided to manually manipulate the represented extract.
computer_graphics	the human visual system (hvs) attempts to select salient areas to reduce cognitive processing efforts. computational models of visual attention try to predict the most relevant and important areas of videos or images viewed by the human eye. such models, in turn, can be applied to areas such as computer graphics, video coding, and quality assessment. although several models have been proposed, only one of them is applicable to high dynamic range (hdr) image content, and no work has been done for hdr videos. moreover, the main shortcoming of the existing models is that they cannot simulate the characteristics of hvs under the wide luminous range found in hdr content. this paper addresses these issues by presenting a computational approach to model the bottom-up visual saliency for hdr input by combining spatial and temporal visual features. an analysis of eye movement data affirms the effectiveness of the proposed model. comparisons employing three well-known quantitative metrics show that the proposed model substantially improves predictions of visual attention for hdr content.
computer_graphics	in this paper we present different optimization techniques on look-up table based algorithms for double precision floating point arithmetic. based on our analysis of different look-up table based algorithms in the literature, we re-engineer basics blocks of the algorithms ( i.e. multiplier(s) and adder(s)) to facilitate area and timing benefits to achieve higher performance. we propose different look-up table optimization techniques for the algorithms. we also analyze trade-off in employing exact rounding ( 0.5ulp) ( unit in the last place) in the double precision floating point unit. based on performance and extensibility criteria we take algorithms proposed by wong and goto as a base case to validate our optimization techniques and compare the performance with other algorithms in the literature. we improve the performance ( latency x area) of wong and goto division algorithm by 26.94%.
computer_graphics	computer graphics and image processing technologies for editing photographs taken by a user, and for photographic entertainment, have increased in recent years. one of these techniques, non-photorealistic rendering (npr), is the focus of this study. in this study, we propose a method for applying david hockneys photographic collage filtering on mobile devices by using several well-known computer graphics and image processing techniques. this process involves taking a sequence of images and extracting feature points, and using these feature points to deform an image. finally, we obtain a photographic collage using the david hockney style by generating a patchwork.
operating_systems	the weak separation between user-and kernel-space in modern operating systems facilitates several forms of privilege escalation. this paper provides a survey of protection techniques, both cutting-edge and time-tested, used to prevent common privilege escalation attacks. the techniques are compared against each other in terms of their effectiveness, their performance impact, the complexity of their implementation, and their impact on diversification techniques such as aslr. overall the literature provides a litany of disjoint techniques, each of which trades some performance cost for effectiveness against a particular isolated threat. no single technique was found to effectively mitigate all known and potential attack vectors with reasonable performance cost overhead.
operating_systems	we describe the main features of the developed software tool, namely plate-motion 2.0 (pem2), which allows inferring the euler pole parameters by inverting the observed velocities at a set of sites located on a rigid block (inverse problem). pem2 allows also calculating the expected velocity value for any point located on the earth providing an euler pole (direct problem). pem2 is the updated version of a previous software tool initially developed for easy-to-use file exchange with the gamit/globk software package. the software tool is developed in matlab(a (r)) framework and, as the previous version, includes a set of matlab functions (m-files), guis (fig-files), map data files (mat-files) and user 's manual as well as some example input files. new changes in pem2 include (1) some bugs fixed, (2) improvements in the code, (3) improvements in statistical analysis, (4) new input/output file formats. in addition, pem2 can be now run under the majority of operating systems. the tool is open source and freely available for the scientific community.
operating_systems	a portable real-time facial recognition system that is able to play personalized music based on the identified person 's preferences was developed. the system is called portable facial recognition jukebox using fisherfaces (frj). raspberry pi was used as the hardware platform for its relatively low cost and ease of use. this system uses the opencv open source library to implement the computer vision fisherfaces facial recognition algorithms, and uses the simple directmedia layer (sdl) library for playing the sound files. frj is crossplatform and can run on both windows and linux operating systems. the source code was written in c++. the accuracy of the recognition program can reach up to 90% under controlled lighting and distance conditions. the user is able to train up to 6 different people (as many as will fit in the gui). when implemented on a raspberry pi, the system is able to go from image capture to facial recognition in an average time of 200ms.
operating_systems	the paper evaluates usability of information modelling tools on the most common operating systems (windows, linux, mac osx). stages of bim maturity and availability of building information modelling tools vary dramatically on these platforms. paper compares the current software tools, and how they can be used in the building process. the list of attributes that mature software platform should accommodate was created. evaluation criteria are determined by specific needs of the various participants of the building process. this leads to the successful project completion and subsequent management of the lifecycle of the building. (c) 2016 the authors. published by elsevier ltd.
operating_systems	background: previously, we described rover, a dna variant caller which identifies genetic variants from pcr-targeted massively parallel sequencing (mps) datasets generated by the hi-plex protocol. rover permits stringent filtering of sequencing chemistry-induced errors by requiring reported variants to appear in both reads of overlapping pairs above certain thresholds of occurrence. rover was developed in tandem with hi-plex and has been used successfully to screen for genetic mutations in the breast cancer predisposition gene palb2. rover is applied to mps data in bam format and, therefore, relies on sequence reads being mapped to a reference genome. in this paper, we describe an improvement to rover, called undr rover (unmapped primer-directed rover), which accepts mps data in fastq format, avoiding the need for a computationally expensive mapping stage. it does so by taking advantage of the location-specific nature of pcr-targeted mps data. results: the undr rover algorithm achieves the same stringent variant calling as its predecessor with a significant runtime performance improvement. in one indicative sequencing experiment, undr rover (in its fastest mode) required 8-fold less sequential computation time than the rover pipeline and 13-fold less sequential computation time than a variant calling pipeline based on the popular gatk tool. undr rover is implemented in python and runs on all popular posix-like operating systems (linux, os x). it requires as input a tab-delimited format file containing primer sequence information, a fasta format file containing the reference genome sequence, and paired fastq files containing sequence reads. primer sequences at the 5' end of reads associate read-pairs with their targeted amplicon and, thus, their expected corresponding coordinates in the reference genome. the primer-intervening sequence of each read is compared against the reference sequence from the same location and variants are identified using the same algorithm as rover. specifically, for a variant to be 'called' it must appear at the same location in both of the overlapping reads above user-defined thresholds of minimum number of reads and proportion of reads. conclusions: undr rover provides the same rapid and accurate genetic variant calling as its predecessor with greatly reduced computational costs.
machine_learning	we propose herein a new portfolio selection method that switches between two distinct asset allocation strategies. an important component is a carefully designed adaptive switching rule, which is based on a machine learning algorithm. it is shown that using this adaptive switching strategy, the combined wealth of the new approach is a weighted average of that of the successive constant rebalanced portfolio and that of the 1/n portfolio. in particular, it is asymptotically superior to the 1/n portfolio under mild conditions in the long run. applications to real data show that both the returns and the sharpe ratios of the proposed binary switch portfolio are the best among several popular competing methods over varying time horizons and stock pools.
machine_learning	cutting plane algorithm (cpa) is a generalization of iterative first-order gradient method, in which the objective function is approximated successively by supporting hyperplanes. cpa has been tailored to solve regularized loss minimization in machine learning by exploiting the regularization structure. in particular, for linear support vector machine (svm) embedding a line search procedure effectively remedies the fluctuations of function value and speeds up the convergence in practical issue. however, the existing line search strategy based on sorting algorithm takes o(mlogm) time. in this paper, we propose a more effective line search solver which spends only linear time. it can be extended to multiclass svm in which an optimized explicit piecewise linear function finding algorithm is prearranged. the total svm training time is proved to reduce theoretically and experiments consistently confirm the effectiveness of the proposed algorithms. (c) 2017 elsevier ltd. all rights reserved.
machine_learning	nonlinear similarity measures defined in kernel space, such as correntropy, can extract higher order statistics of data and offer potentially significant performance improvement over their linear counterparts especially in non gaussian signal processing and machine learning. in this paper, we propose a new similarity measure in kernel space, called the kernel risk-sensitive loss (krsl), and provide some important properties. we apply the krsl to adaptive filtering and investigate the robustness, and then develop the mkrsl algorithm and analyze the mean square convergence performance. compared with correntropy, the krsl can offer a more efficient performance surface, thereby enabling a gradient-based method to achieve faster convergence speed and higher accuracy while still maintaining the robustness to outliers. theoretical analysis results and superior performance of the new algorithm are confirmed by simulation.
machine_learning	gamma spectrometric field measurements may provide high resolution information on topsoil texture. yet, calibrations for the estimation of texture data usually have to be done site-specifically. the lack of site-independent calibrations thus limits the easy and universal use of proximal gamma-ray sensing in soil mapping and precision agriculture. our objective was to develop a study site-independent prediction model for topsoil texture from gamma-ray spectra. we surveyed ten study sites across germany with 417 reference samples (291 for calibration, 126 for test set-validation), providing soils from a broad range of parent materials and with widely varying soil texture. first, study site-specific models were calibrated by a linear regression approach. these models provided reliable estimations of sand, silt, and clay for most of the study sites. second, study site-independent models were calibrated via i) linear regression and ii) support vector machines (svm), the latter being mathematical methods of data pattern recognition. based on the non-linear relationship between gamma spectrum and soil texture, which varied widely between the different parent materials the linear models are not appropriate for satisfactory soil texture prediction (averaged r-2 of 0.73 for sand, 0.61 for silt, and 0.18 for clay and averaged absolute prediction errors of 9 to 5%, respectively). in contrast, the svm calibrated prediction models revealed reliable performance also for site-independent calibrations. with the non-linear svm approach we were able to include all sites in one single prediction model for each texture fraction although the different mineralogical composition of their parent materials led to complex and partly opposing relationships between gamma features and soil texture. site-independent predictions via svm were often even better than site-specific linear regression models. the site-independent svm calibrated predictions yielded an averaged r-2 of 0.96 (sand), 0.93 (silt), and 0.78 (clay), and corresponding averaged absolute prediction errors of 2 to 4%, respectively. to summarize, (i) non-linear prediction models are a feasible approach for capable site-independent texture estimations across a wide range of soils and (ii) gamma spectrometry based texture predictions are a valuable input for applications that require highly resolved texture information at low costs and efforts. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	lexicographic preferences on a set of attributes provide a cognitively plausible structure for modeling the behavior of human decision makers. therefore, the induction of corresponding models from revealed preferences or observed decisions constitutes an interesting problem from a machine learning point of view. in this paper, we introduce a learning algorithm for inducing generalized lexicographic preference models from a given set of training data, which consists of pairwise comparisons between objects. our approach generalizes simple lexicographic orders in the sense of allowing the model to consider several attributes simultaneously (instead of looking at them one by one), thereby significantly increasing the expressiveness of the model class. in order to evaluate our method, we present a case study of a highly complex real-world problem, namely the choice of the recognition method for actuarial gains and losses from occupational pension schemes. using a unique sample of european companies, this problem is well suited for demonstrating the effectiveness of our lexicographic ranker. furthermore, we conduct a series of experiments on benchmark data from the machine learning domain. (c) 2016 elsevier b.v. all rights reserved.
data_structures	the literature presents many application programming interfaces (apis) and frameworks that provide state of the art algorithms and techniques for solving optimisation problems. the same cannot be said about apis and frameworks focused on the problem data itself because with the peculiarities and details of each variant of a problem, it is virtually impossible to provide general tools that are broad enough to be useful on a large scale. however, there are benefits of employing problem-centred apis in a r&d environment: improving the understanding of the problem, providing fairness on the results comparison, providing efficient data structures for different solving techniques, etc. therefore, in this work we propose a novel design methodology for an api focused on an optimisation problem. our methodology relies on a data parser to handle the problem specification files and on a set of efficient data structures to handle the information on memory, in an intuitive fashion for researchers and efficient for the solving algorithms. also, we present the concepts of a solution dispenser that can manage solutions objects in memory better than built-in garbage collectors. finally, we describe the positive results of employing a tailored api to a project involving the development of optimisation solutions for workforce scheduling and routing problems.
data_structures	reaction mechanism generator (rmg) constructs kinetic models composed of elementary chemical reaction steps using a general understanding of how molecules react. species thermochemistry is estimated through benson group additivity and reaction rate coefficients are estimated using a database of known rate rules and reaction templates. at its core, rmg relies on two fundamental data structures: graphs and trees. graphs are used to represent chemical structures, and trees are used to represent thermodynamic and kinetic data. models are generated using a rate-based algorithm which excludes species from the model based on reaction fluxes. rmg can generate reaction mechanisms for species involving carbon, hydrogen, oxygen, sulfur, and nitrogen. it also has capabilities for estimating transport and solvation properties, and it automatically computes pressure-dependent rate coefficients and identifies chemically-activated reaction paths. rmg is an object-oriented program written in python, which provides a stable, robust programming architecture for developing an extensible and modular code base with a large suite of unit tests. computationally intensive functions are cythonized for speed improvements. program summary program title: rmg catalogue identifier: aezw_v1_0 program summary url: http://cpc.cs.qub.ac.uk/summaries/aezw_v1_0.html program obtainable from: cpc program library, queen 's university, belfast, n. ireland licensing provisions: mit/x11 license no. of lines in distributed program, including test data, etc.: 958681 no. of bytes in distributed program, including test data, etc.: 9495441 distribution format: tar.gz programming language: python. computer: windows, ubuntu, and mac os computers with relevant compilers. operating system: unix/linux/windows. ram: 1 gb minimum, 16 gb or more for larger simulations classification: 16.12. external routines: rdkit, open babel, dassl, daspk, dqed, numpy, scipy nature of problem: automatic generation of chemical kinetic mechanisms for molecules containing c, h, 0, s, and n. solution method: rate-based algorithm adds most important species and reactions to a model, with rate constants derived from rate rules and other parameters estimated via group additivity methods. additional comments: the rmg software package also includes cantherm, a tool for computing the thermodynamic properties of chemical species and both high-pressure-limit and pressure-dependent rate coefficients for chemical reactions using results from quantum chemical calculations. cantherm is compatible with a variety of ab initio quantum chemistry software programs, including but not limited to gaussian, mopac, qchem, and molpro. running time: from 30 s for the simplest molecules, to up to several weeks, depending on the size of the molecule and the conditions of the reaction system chosen. (c) 2016 the authors. published by elsevier b.v.
data_structures	this paper presents elastic transactions, an appealing alternative to traditional transactions, in particular to implement search structures in shared memory multicore architectures. upon conflict detection, an elastic transaction might drop what it did so far within a separate transaction that immediately commits, and resume its computation within a new transaction which might itself be elastic. we present the elastic transaction model and an implementation of it, then we illustrate its simplicity and performance on various concurrent data structures, namely double-ended queue, hash table, linked list, and skip list elastic transactions outperform classical ones on various workloads, with an improvement of 35% on average. they also exhibit competitive performance compared to lock-based techniques and are much simpler to program with than lock-free alternatives. (c) 2016 elsevier inc. all rights reserved.
data_structures	high-level synthesis (hls) promises a significant shortening of the fpga design cycle by raising the abstraction level of the design entry to high-level languages such as c/c++. however, applications using dynamic, pointer-based data structures and dynamic memory allocation remain difficult to implement well, yet such constructs are widely used in software. automated optimizations that leverage the memory bandwidth of fpgas by distributing the application data over separate banks of on-chip memory are often ineffective in the presence of dynamic data structures due to the lack of an automated analysis of pointerbased memory accesses. in this work, we take a step toward closing this gap. we present a static analysis for pointer-manipulating programs that automatically splits heap-allocated data structures into disjoint, independent regions. the analysis leverages recent advances in separation logic, a theoretical framework for reasoning about heap-allocated data that has been successfully applied in recent software verification tools. our algorithm focuses on dynamic data structures accessed in loops and is accompanied by automated source-to-source transformations that enable automatic loop parallelization and memory partitioning by off-the-shelf hls tools. we demonstrate the successful loop parallelization and memory partitioning by our tool flow using three real-life applications that build, traverse, update, and dispose of dynamically allocated data structures. our case studies, comparing the automatically parallelized to the direct hls implementations, show an average latency reduction by a factor of 2x across our benchmarks.
data_structures	we consider d-dimensional lattice path models restricted to the first orthant whose defining step sets exhibit reflective symmetry across every axis. given such a model, we provide explicit asymptotic enumerative formulas for the number of walks of a fixed length: the exponential growth is given by the number of distinct steps a model can take, while the sub-exponential growth depends only on the dimension of the underlying lattice and the number of steps moving forward in each coordinate. the generating function of each model is first expressed as the diagonal of a multivariate rational function, then asymptotic expressions are derived by analyzing the singular variety of this rational function. additionally, we show how to compute subdominant growth, reflect on the difference between rational diagonals and differential equations as data structures for d-finite functions, and show how to determine first order asymptotics for the subset of walks that start and end at the origin.
network_security	in this paper, a spectrally coded optical code division multiple access (ocdma) system using a hybrid modulation scheme has been investigated. the idea is to propose an effective approach for simultaneous improvement of the system capacity and security. data formats, nrz (non-return to zero), dqpsk (differential quadrature phase shift keying), and poisk (polarisation shift keying) are used to get the orthogonal modulated signal. it is observed that the proposed hybrid modulation provides efficient utilisation of bandwidth, increases the data capacity and enhances the data confidentiality over existing ocdma systems. further, the proposed system performance is compared with the current state-of-the-art ocdma schemes. (c) 2015 elsevier inc. all rights reserved.
network_security	as computer networks are emerging in everyday life, network security has become an important issue. simultaneously, attacks are becoming more sophisticated, making the defense of computer networks increasingly difficult. attack graph is a modelling tool used in the assessment of security of enterprise networks. since its introduction a considerable amount of research effort has been spent in the development of theory and practices around the idea of attack graph. this paper presents a consolidated view of major attack graph generation and analysis techniques.
network_security	network attacks and cyber-security breaches may be the cause of huge monetary damages in the modern information-based economy; thus, the need for network security is stronger than ever as it is the need for full traffic sanitization. nonetheless, the purge of malicious packets is still too often relegated to the destination of the attacks letting the unwanted traffic roam freely. at the same time, the next generation of routers promises to be able to modulate energy consumption on the basis of actual traffic, thus the presence of malicious traffic in the network is a cause of economic losses in itself, even when the attack is not successful. in past work, we modelled and analysed the energy savings enabled by aggressive intrusion detection; however, fluctuations in the traffic intensity has not been fully taken into account. in this article, we introduce a new enhanced adaptive model that takes into full account the actual load of routers including what is due to forecasting errors.
network_security	recently, in many countries, military has adopted cloud, big data, iot, etc. in order to win the war. therefore, a favorable environment for future battles based on the soldiers with a variety of iot technologies that will foster and build elite combat forces in the center can be expected. similar to the conventional internet environment, iot is not only the type of security threat for a variety of networks, data, and personal information for each of the features has also identified protocol management. therefore, to enhance the security of the environment in the future iot based on full-length, it is necessary to study the security priority. a recent survey of military it professionals shows communications / network security is the most important sector. in addition, the survey can distinguish between the security field will be considered fragile. to the study of future iot-based battlefield, see the results of this study are reflected in the professional military security structure, we should effectively against threats expected in a given amount of the budget.
network_security	with the explosive growth of mobile terminal access to the network and the shortage of ipv4, the network address translation (nat) technology has become more and more widely used. the technology not only provides users with convenient access to the internet, but also brings trouble to network operators and regulatory authorities. this system nat detection using netflow data, is often used for monitoring and forensics analysis in large networks. in the paper, in order to detect nat devices, an out-in activity degree method based on network behavior is proposed. our approach works completely passively and is based on netflow data only. our approach gets accuracy of 91.2 % in real large-scale network for a long time.
image_processing	using image processing to extract nodular or linear shadows is a key technique of computer-aided diagnosis schemes. this study proposes a new method for extracting nodular and linear patterns of various sizes in medical images. we have developed a morphology filter bank that creates multiresolution representations of an image. analysis bank of this filter bank produces nodular and linear patterns at each resolution level. synthesis bank can then be used to perfectly reconstruct the original image from these decomposed patterns. our proposed method shows better performance based on a quantitative evaluation using a synthesized image compared with a conventional method based on a hessian matrix, often used to enhance nodular and linear patterns. in addition, experiments show that our method can be applied to the followings: (1) microcalcifications of various sizes in mammograms can be extracted, (2) blood vessels of various sizes in retinal fundus images can be extracted, and (3) thoracic ct images can be reconstructed while removing normal vessels. our proposed method is useful for extracting nodular and linear shadows or removing normal structures in medical images.
image_processing	a modified gaussian current density is put forward, which is used to simulate the dynamic process of az31b magnesium alloy double-electrode gas metal arc welding (de-gmaw) droplet transfer. the process of droplet transfer in the az31b magnesium alloy de-gmaw was simulated using fluent software. the influence of different bypass welding currents was investigated with a constant total current. the simulated results revealed that when the bypass current was 0 a, the process was the globular transfer type, the droplet transfer with a bypass current of 80 a was a projected transfer type and the droplet transfer at a bypass current of 170 a was a spray transfer type. the critical current is decreased so that spray transfer would occur at a lower current level in the de-gmaw process. as the bypass current was increased, the shape of droplet changed from oval to round and the transition frequency of the droplet increased in a stepwise fashion. to confirm the accuracy of the simulated results, welding experiments were performed and the image processing method was used to obtain the droplet size. the simulated and experimental results were found to be in good agreement.
image_processing	landslides are considered as one of the natural hazards responsible for casualties, damage of assets, and infrastructures. in many situations, collection of field data from remote places is difficult due to inaccessibility of landslide area. this paper examines landslide susceptibility in the bukit antarabangsa, kuala lumpur, to ease geographical studies, using image processing and multivariate statistical tools by reviewing the digital images using remote-sensing technique without any physical survey. we considered different pixel resolutions and report the effectiveness of using factor analysis, principal component analysis, linear discriminant analysis, and their hybridization. eight types of databases for heavy, medium, and no landslide were created. the modeling works were carried out at 2 x 2, 4 x 4, 8 x 8, 16 x 16, 32 x 32, 64 x 64, 128 x 128, and 256 x 256 pixel resolutions. results indicate 2 x 2 was optimal in both heavy and medium while 8 x 8 found to be ideal for no landslide region. performance at different pixel resolutions was compared using receiver operating characteristic (roc) curves, and average success of 87.36% was found. this simple yet robust system holds great potential for saving lives.
image_processing	very little is known about how individual soil particles move over a soil surface as a result of rainfall. specifically there is virtually no information about the pathway a particle takes, the speed at which it travels and when it is in motion. here we present a novel technique that can give insight into the movement of individual soil particles. by combining novel fluorescent videography techniques with custom image processing and a fluorescent soil tracer we have been able to trace the motion of soil particles under simulated rainfall in a laboratory soil flume. the system is able track multiple sub-millimeter particles simultaneously, establishing their position 50 times a second with sub-millimeter precision. an analysis toolkit has been developed enabling graphical and numerical analysis of the data obtained. for example, we are able to visualise and quantify parameters such as distance and direction of travel. based on our observations we have created a conceptual model (stop, hop, roll) which attempts to present a unified model for the movement of soil particles across a soil surface. it is hoped that this technology will open up new opportunities to create, parameterise and evaluate soil models as the motion of individual soil particles can now be easily monitored. (c) 2016 the authors. published by elsevier b.v.
image_processing	in this paper, we analyse patterns in face shape variation due to weight gain. we propose the use of persistent homology descriptors to get geometric and topological information about the configuration of anthropometric 3d face landmarks. in this way, evaluating face changes boils down to comparing the descriptors computed on 3d face scans taken at different times. by applying dimensionality reduction techniques to the dissimilarity matrix of descriptors, we get a space in which each face is a point and face shape variations are encoded as trajectories in that space. our results show that persistent homology is able to identify features which are well related to overweight and may help assessing individual weight trends. the research was carried out in the context of the european project semeoticons, which developed a multisensory platform which detects and monitors over time facial signs of cardio-metabolic risk.
parallel_computing	traditional fluid-structure interaction techniques are based on arbitrary lagrangian-eulerian method, where strong coupling method is used to solve fundamental discretized equations. after discretization, resulting nonlinear systems are solved using an iterative method leading to set of linear subsystems. the resulting linear subsystem matrices are sparse non-symmetric indefinite with small or zero diagonal entries. the small pivots of these matrices may render the incomplete lu factorization unstable or inaccurate. to improve that, this paper proposes an approach that can be used together with a stabilization and offers an additional feature to be used to gain an advantage in its parallelization. (c) 2016 published by elsevier ltd.
parallel_computing	recently the hardware performance of mobile devices have been extremely increased and advanced mobile devices provide multi-cores and high clock speed. in addition, mobile devices have advantages in mobility and portability compared with pc and console, so many games and simulation programs have been developed under mobile environments. physically-based simulation is a one of the key issues for deformable object modeling which is widely used to represent the realistic expression of 3d soft objects with tetrahedrons for game and 3d simulation. however, it requires high computation power to plausibly and realistically represent the physical behaviors and interactions of deformable objects. in this paper, we implemented parallel cloth and mass-spring simulation using graphics processing unit (gpu) with opencl and multi-threaded central processing unit ( cpu) on a mobile device. we applied cpu and gpu parallel computing technique into spring force computation and integration methods such as euler, midpoint, 4th-order runge-kutta to optimize the computational burden of dynamic simulation. the integration methods compute the next step of positions and velocities in each node. in this paper, we tested the performance analysis for the spring force calculation and integration method process using cpu only, multi-threaded cpu, and gpu on mobile device respectively. our experimental results concluded that the calculation using proposed multi-threaded cpu and gpu multi-threaded cpu are much faster than using just the cpu only.
parallel_computing	modern gpgpu 's have enabled massively parallel computing with programmability that can exploit the highly parallel nature of ldpc decoding. previous works customized the design on a gpgpu towards specific execution attributes of a particular ldpc decoding matrix. supporting different ldpc decoding matrices requires either substantial rework on the current program, or a brand new parallel design. this paper proposes two unified designs that can achieve high performance for both regular and irregular ldpc decoding on a gpgpu. the first design introduces a node-based scheme with a versatile translation array mechanism that can efficiently handle the complex data access patterns of different ldpc decoding matrices. the second design proposes an edge-based parallel paradigm that uses more intuitive data layout. more edges than nodes in a tanner graph also give the edge-based design higher computation parallelism when there are limited concurrent codewords. with the proposed unified designs, designers can be ignorant of the types of ldpc matrices and achieve high performance ldpc decoding. the experiments on a gtx 470 gpgpu have demonstrated up to 134.56x runtime improvement, when compared with designs on a high-end cpu. the maximum throughput can reach 80.25 mbps. when compared with the previous customized designs, the proposed systematic designs can reach better performance while relieving the effort of customization.
parallel_computing	we propose a computational framework for the simulation of deformation and fracture in shells that is well suited to situations with widespread damage and fragmentation due to impulsive loading. the shell is modeled with a shear-flexible theory and discretized with a discontinuous galerkin finite element method, while fracture is represented with a cohesive zone model on element edges. a key feature of the method is that the underlying shear-flexible shell theory enables the description of transverse shear fracture modes, in addition to the in-plane and bending modes accessible to kirchhoff-love thin shell formulations. this is especially important for impulsive loading conditions, where shear-off failure near stiffeners and supports is common. the discontinuous galerkin formulation inherits the scalability properties demonstrated previously for large-scale simulation of fracture in solids, while avoiding artificial elastic compliance issues that are common in other cohesive model approaches. we demonstrate the ability of the framework to capture the transverse shear fracture mode through numerical examples, and the parallel computation capabilities of the method through the simulation of explosive decompression of the skin of a full-scale passenger aircraft fuselage. (c) 2016 published by elsevier b.v.
parallel_computing	3d finite-element (fe) mesh generation is a major hurdle for marine controlled-source electromagnetic (csem) modeling. in this paper, we present a fe discretization operator (fedo) that automatically converts a 3d finite difference (fd) model into reliable and efficient tetrahedral fe meshes for csem modeling. fedo sets up wireframes of a background seabed model that precisely honors the seafloor topography. the wireframes are then partitioned into multiple regions. outer regions of the wireframes are discretized with coarse tetrahedral elements whose maximum size is as large as a skin depth of the regions. we demonstrate that such coarse meshes can produce accurate fe solutions because numerical dispersion errors of tetrahedral meshes do not accumulate but oscillates. in contrast, central regions of the wireframes are discretized with fine tetrahedral elements to describe complex geology in detail. the conductivity distribution is mapped from fd to fe meshes in a volume-averaged sense. to avoid excessive mesh refinement around receivers, we introduce an effective receiver size. major advantages of fedo are summarized as follow. first, fedo automatically generates reliable and economic tetrahedral fe meshes without adaptive meshing or interactive cad workflows. second, fedo produces fe meshes that precisely honor the boundaries of the seafloor topography. third, fedo derives multiple sets of fe meshes from a given fd model. each fe mesh is optimized for a different set of sources and receivers and is fed to a subgroup of processors on a parallel computer. this divide and conquer approach improves the parallel scalability of the fe solution. both accuracy and effectiveness of fedo are demonstrated with various csem examples.
distributed_computing	using distributed task allocation methods for cooperating multivehicle systems is becoming increasingly attractive. however, most effort is placed on various specific experimental work and little has been done to systematically analyze the problem of interest and the existing methods. in this paper, a general scenario description and a system configuration are first presented according to search and rescue scenario. the objective of the problem is then analyzed together with its mathematical formulation extracted from the scenario. considering the requirement of distributed computing, this paper then proposes a novel heuristic distributed task allocation method for multivehicle multitask assignment problems. the proposed method is simple and effective. it directly aims at optimizing the mathematical objective defined for the problem. a new concept of significance is defined for every task and is measured by the contribution to the local cost generated by a vehicle, which underlies the key idea of the algorithm. the whole algorithm iterates between a task inclusion phase, and a consensus and task removal phase, running concurrently on all the vehicles where local communication exists between them. the former phase is used to include tasks into a vehicle 's task list for optimizing the overall objective, while the latter is to reach consensus on the significance value of tasks for each vehicle and to remove the tasks that have been assigned to other vehicles. numerical simulations demonstrate that the proposed method is able to provide a conflict-free solution and can achieve outstanding performance in comparison with the consensus-based bundle algorithm.
distributed_computing	cloud computing is one of the increasing technology that is connected with grid computing, utility computing, distributed computing. in today 's world, securing of data plays a vital role. in the computing environment, data privacy and data security is popular in fields like government, industry, and business for the future development. these are inter-related to both hardware and software. hence, this paper analyses the data security solutions in cloud computing.
distributed_computing	this paper describes the performance of the brain project, a distributed software tool for the formal modeling of numerical data using a hybrid neural-genetic programming technique. one of the most interesting characteristics of the brain project is its distributed implementation. unlike many other parallel and/or distributed solutions the only requirement of the brain project is that the collaborating personal computers must be 64-bit linux machines connected to internet via the transmission control protocol/internet protocol. the performance of the brain project is clearly enhanced with the very simple parallelization scheme illustrated in the paper. although the brain project presents many innovative solutions for the genetic programming research, this paper focuses mainly on its behavior in the distributed environment. (c) 2015 elsevier b.v. all rights reserved.
distributed_computing	this paper introduces a parallel and distributed algorithm for solving the following minimization problem with linear constraints: minimize f(1)(x(1))+ . . . + f(n) (x(n)) subject to a(1)x(1) + . . . + a(n)x(n) = c, x(1) is an element of chi(1), . . . , x(n) is an element of chi(n), where , are convex functions, are matrices, and are feasible sets for variable . our algorithm extends the alternating direction method of multipliers (admm) and decomposes the original problem into n smaller subproblems and solves them in parallel at each iteration. this paper shows that the classic admm can be extended to the n-block jacobi fashion and preserve convergence in the following two cases: (i) matrices are mutually near-orthogonal and have full column-rank, or (ii) proximal terms are added to the n subproblems (but without any assumption on matrices ). in the latter case, certain proximal terms can let the subproblem be solved in more flexible and efficient ways. we show that converges at a rate of o(1 / k) where m is a symmetric positive semi-definte matrix. since the parameters used in the convergence analysis are conservative, we introduce a strategy for automatically tuning the parameters to substantially accelerate our algorithm in practice. we implemented our algorithm (for the case ii above) on amazon ec2 and tested it on basis pursuit problems with >300 gb of distributed data. this is the first time that successfully solving a compressive sensing problem of such a large scale is reported.
distributed_computing	the primary objective of load balancing for distributed systems is to minimize the job execution time while maximizing the resource utilization. load balancing on decentralized systems need effective information exchange policy so that with minimum amount of communication the nodes have up to date information about other nodes in the system. periodic, event-based and on-demand information exchange are some important policies used for the same. all these approaches involve a lot of overhead and even sometime leading toward obsolete data with the nodes if there is a delay in the updation. this work presents an adaptive threshold-based hybrid load balancing scheme with sender and receiver initiated approach (hlbwsr) using random information exchange (rie). rie ensures that the information is exchanged in such a way that each node in the system has up-to-date state of the other nodes with much reduced communication overhead. further, the adaptive threshold ensures that almost an average numbers of jobs are executed by all the nodes in the system. the study of the effect of the use of rie on sender initiated, receiver initiated and hybrid of sender and receiver initiated load balancing approach establishes the superior performance of hlbwsr among its rie-based peers. a comparative analysis of hlbwsr, with periodic information exchange strategy, modified estimated load information scheduling algorithm and load balancing on arrival reveals its effectiveness under various test conditions. copyright (c) 2016 john wiley & sons, ltd.
algorithm_design	in this work, a novel surrogate-assisted memetic algorithm is proposed which is based on the preservation of genetic diversity within the population. the aim of the algorithm is to solve multi-objective optimization problems featuring computationally expensive fitness functions in an efficient manner. the main novelty is the use of an evolutionary algorithm as global searcher that treats the genetic diversity as an objective during the evolution and uses it, together with a non-dominated sorting approach, to assign the ranks. this algorithm, coupled with a gradient-based algorithm as local searcher and a back-propagation neural network as global surrogate model, demonstrates to provide a reliable and effective balance between exploration and exploitation. a detailed performance analysis has been conducted on five commonly used multi-objective problems, each one involving distinct features that can make the convergence difficult toward the pareto-optimal front. in most cases, the proposed algorithm outperformed the other state-of-the-art evolutionary algorithms considered in the comparison, assuring higher repeatability on the final non-dominated set, deeper convergence level and higher convergence rate. it also demonstrates a clear ability to widely cover the pareto-optimal front with larger percentage of non-dominated solutions if compared to the total number of function evaluations. (c) 2015 elsevier b.v. all rights reserved.
algorithm_design	hadoop is one of the most important big data processing and storage systems. in recent years, a lot of efforts have been put to enhance hadoop 's performance from networking perspectives. however, there are limited tools that can help researchers to verify their networking algorithm design in terms of hadoop 's performance. this paper proposes doopnet which is a framework and toolset for creating hadoop clusters in a virtualized environment and for monitoring/analysing of hadoop 's networking characteristics under different network configurations. doopnet enables users to automatically set up a hadoop cluster over docker containers running inside mininet. the hadoop traffic is collected inside the containers and virtual switches through network flow monitors. the users can easily modify network topologies or configurations through mininet, observe the networking behaviour through network flow monitors, and analyse the effects of different network settings on hadoop 's performance. examples are presented to demonstrate how to setup the doopnet testbed and analyse hadoop traffic.
algorithm_design	an efficient routing with quality of service (qos) guarantees for any safety traffic communication plays a vital role for the success of vehicle ad hoc networks (vanets). due to the connection less nature of such network, establishing stable route between connected nodes in vanets is extremely important and challenging. this paper, proposes a new algorithm for multi-constrained qos routing in vehicular network by adopting clustering approach. in the algorithm design, some qos metrics are used in addition to the stability metrics for finding and establishing a stable route to destination. this is achieved by calculating the corresponding qos provision values before selecting an optimal, reliable and stable route. nctuns 6.0 network simulator was used for the experiment of the proposed scheme. results show a significant improvement of the proposed approach in terms of broken links, routing overhead and end-to-end delay.
algorithm_design	we present in this paper a general framework to study issues of effective load balancing and scheduling in highly parallel and distributed environments such as currently built cloud computing systems. we propose a novel approach based on the concept of the sandpile cellular automaton: a decentralized multi-agent system working in a critical state at the edge of chaos. our goal is providing fairness between concurrent job submissions by minimizing slowdown of individual applications and dynamically rescheduling them to the best suited resources. the algorithm design is experimentally validated by a number of numerical experiments showing the effectiveness and scalability of the scheme in the presence of a large number of jobs and resources and its ability to react to dynamic changes in real time.
algorithm_design	traditional black-box optimization searches a set of potential solutions for those optimizing the value of a function whose analytical or algebraic form is unknown or inexistent, but whose value can be queried for any input. co-optimization is a generalization of this setting, in which fully evaluating a potential solution may require querying some function more than once, typically a very large number of times. when that 's the case, co-optimization poses unique difficulties to designing and assessing algorithms. a generally-applicable approach is to judge co-optimization algorithm performance via an aggregate over all possible functions in the problem domain. we establish formal definitions of such aggregate performance and then investigate the following questions concerning algorithm design; 1) are some algorithms strictly better than others? i.e. is there ""free lunch""? 2) do optimal algorithms exist? and 3) if so, are they practical? we formally define free lunch and aggregate optimality of co-optimization algorithms and derive generic conditions for their existence. we review and explain prior (no) free lunch results from the perspective of these conditions; we also show how this framework can be used to bridge several fields of research, by allowing formalization of their various problems and views on performance. we then apply and extend the generic results in a context involving a particular type of co-optimization called worst-case optimization. in this context we show that there exist algorithms that are aggregately-optimal for any budget (allowed number of function calls) and any starting point (set of previously uncovered function call outcomes), and also non-trivially strictly optimal for many budgets and starting points; moreover, we formalize the operation of such optimal algorithms and show that for certain domains, budgets and starting points this operation is equivalent to a simple procedure with tractable implementation a first-of-its-kind result for co-optimization. (c) 2014 the authors. published by elsevier b.v.
computer_programming	computer programming is a core subject of almost all degrees of engineering that is perceived as a very complex matter for the students, because it is very different from the other core subjects (physics, calculus, algebra, chemistry, graphics expression) that are more familiar to students. programming combines in a balanced way the two fundamental steps when developing an engineering product, design and implementation (coding), but with the important difference in the lower costs of the implementation phase, which permits students building and testing the proposed designs. besides, the new european higher education area (ehea) is based both in the acquisition of competencies and skills by the student rather than in the accumulation of knowledge, and in the use of the european credit transfer and accumulation system (ects), which supposes an important challenge for the teaching model. in this paper we show our eight years experience in developing a hybrid methodology for teaching computer programming in the degrees of mechanical, electrical, electronic and chemistry engineering at the university of almeria (spain). we integrate traditional teaching methods (participative master class for transmission of information) with modern methods (problem-based learning, collaborative-team working, autonomous working, or tutoring). the different methods of teaching are supported by a hardware-software infrastructure of virtual teaching (blackboard platform) and a very detailed planning where activities are highly focused and weekly organized. it includes periodical tests for the evaluation of the progress of the student. we think that the adoption and tuning of these learning methods will enhance the skills of the future engineers in the computer programming area.
computer_programming	natural language processing (nlp) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages. the idea of using a natural language for computer programming is to make it easier for people to talk to computers in their native tongue and spare them the pain of learning a computer friendly language like assembly, c, c++, java, lisp etc. among all the natural languages, sanskrit in its style is identified to be the best language which has minimum deviation. panini, the creator of sanskrit formulated 3,949 rules. this research paper explores varied distinctive features of al like nlp, semantic net, vibhakti, dual case, inflection based syntax etc. and how sanskrit effectively triumphs over these limitations and fulfills the prerequisites of a natural language processor.
computer_programming	many institutions are offering online courses, as part of their regular schedule, to provide more flexible course offerings to students with the hope of some cost savings in the end. in this paper, we will focus on examining some of the students' ratings and comments that have been reported in student rating of teaching effectiveness (srte) surveys, for an online introductory programming course, during the past few years. we will look at the issues that we have identified to play a role in the srte surveys of our online course and examine the teaching from several different perspectives that include effectiveness/appropriateness of online courses for students. the paper includes a set of specific and general recommendations to address the legitimate students' concerns that, we believe, would make the course delivery more effective and enjoyable for students.
computer_programming	one of the challenges to promoting computer science in schools is to create and retain interest in programming. we describe a pilot project for primary and secondary students in macau to introduce net gadgeteer, a rapid prototyping platform for building electronic devices. the aim of the yearlong project was to generate interest in building devices, while learning the basics of programming concepts. our experience with the weekend workshops indicates that the hands-on approach helped to engage the students. results from a survey of 155 students point to increased engagement through active learning and growing enthusiasm to learn programming. we report some issues that surfaced and give recommendations for teachers who wish to consider such devices for teaching programming to novices. we hope to further explore ways to incorporate learning programming through tangible devices in the curriculum in local schools.
computer_programming	computing education researchers have become increasingly interested in leveraging log data automatically collected within computer programming environments in order to understand students' learning processes and tailor instruction to student needs. while data on students' programming activities has been positively correlated with their learning outcomes, those data tell only part of the story. another part of the story lies in students' social activities, which, according to social learning theory, can also be predictive of students' learning outcomes. in order to gain further insight into how computing students' learning processes influence their learning outcomes, we present an empirical study that explores the interplay of students' social activities, programming activities, and course outcomes in an early computing course. by analyzing log data collected through a programming environment augmented with a social networking-style activity stream, we found that answers to questions posed through the activity stream were positively correlated with students' ability to make programming progress, and their eventual success in the course. based on our findings, we present recommendations for the design of pedagogical environments to support a more social programming process.
relational_databases	entity resolution (er) concerns identifying pairs of entities that refer to the same underlying entity. to avoid o(n(2)) pairwise comparison of n entities, blocking methods are used. sorted neighborhood is an established blocking method for relational databases. it has not been applied to schema-free resource description framework (rdf) data sources widely prevalent in the linked data ecosystem. this paper presents a sorted neighborhood workflow that may be applied to schema-free rdf data. the workflow is modular and makes minimal assumptions about its inputs. empirical evaluations of the proposed algorithm on five real-world benchmarks demonstrate its utility compared to two state-of-the-art blocking baselines.
relational_databases	intuitionistic fuzzy databases are used to handle imprecise and uncertain data as they represent the membership, nonmembership, and hesitancy associated with a certain element in a set. this paper presents the intuitionistic fuzzy fourth normal form to decompose the multivalued dependent data. a technique to determine intuitionistic fuzzy multivalued dependencies by working on the closure of dependencies has been proposed. we derive the closure by obtaining all the logically implied dependencies by a set of intuitionistic fuzzy multivalued dependencies, i.e., inference rules. a complete set of inference rules for the intuitionistic fuzzy multivalued dependencies has been given along with the derivation of each rule. these rules help us to compute the dependency closure and we further use the same for defining the intuitionistic fuzzy fourth normal form.
relational_databases	this article describes how practical lectures can be innovated by the experience of a practical case study. the design and implementation of the geographical information system botangis brought several innovations to the study branch geoinformatics at the faculty of science of palacky university in olomouc. this article deals mainly with introducing a new practical example into the course database systems. this new theme explains the design and inner structure of the relational database botangis. the valuable contribution lies in the way a real database example from practice is explained to the students in the process of education. the instruction started with a visit to the botanical garden and the collection greenhouses. subsequently, students worked at a computer laboratory, trying to find some information about plants by various queries in the botanical portal botangis. finally, a detailed model of the relational database was explained. the teacher 's experience enriched the students' knowledge in the field of the conceptual database design.
relational_databases	identifying similarities in large datasets is an essential operation in several applications such as bioinformatics, pattern recognition, and data integration. to make a relational database management system similarity-aware, the core relational operators have to be extended. while similarity-awareness has been introduced in database engines for relational operators such as joins and group-by, little has been achieved for relational set operators, namely intersection, difference, and union. in this paper, we propose to extend the semantics of relational set operators to take into account the similarity of values. we develop efficient query processing algorithms for evaluating them, and implement these operators inside an open-source database system, namely postgresql by extending several queries from the tpc-h benchmark to include predicates that involve similarity-based set operators, we perform extensive experiments that demonstrate up to three orders of magnitude speedup in performance over equivalent queries that only employ regular operators. (c) 2015 elsevier ltd. all rights reserved.
relational_databases	the knowledge discovery in distributed databases is the process of extracting useful information from a collection of data stored in distributed databases. a distributed database is a collection of data replicated over a number of different computers. the best-suited structures for working with distributed databases are the distributed committee-machines. distributed committee-machines are a combination of neural networks that work in a distributed manner as a group in order to obtain better performance than individual neural networks in solving data mining tasks inside the kdd process. in this paper, we aim to study the interaction between distributed committee-machines and distributed databases. the process of replication on multiple machines can become very slow once the number of the machines from the replication topology grows. such behavior is explicable because of the complex software that is used in real implementations of the replication process in order to make available the same data on multiple machines. in this paper, i propose a design that overcomes those disadvantages and a new type of approach in storing the neural networks. the developed system stores the entire neural network in real relational databases. the optimized dcm structure eliminates the problems inherited from replication by writing all the result locally in special tables that will not be replicated on all the distributed machines. here i used also a new approach, which consists of storing the entire neural network in the table as blob (binary large object) object. the method can be beneficial also in new types of elearning techniques such as the adaptive elearning method that uses neural networks. with the optimized design of dcm structures, the speedup in all the experiments is almost equal with the number of distributed machines that were used.
software_engineering	seeking product 's quality is essential nowadays. one of the many quality aspects in software development is the source code complexity. not taking care for the complexity during the development can result in unexpected cost, caused by the difficulty on the source code understanding. the goal of this paper is to introduce an initial approach to identify unnecessary complexity in source code. besides identifying, also show to its user how to properly rewrite the source code without the unnecessary complexity. the approach is based on the static analysis of the source code control flow graph. once the unnecessary complexity is identified, the graph is refactored in order to allow the user to understand the improvement on the source code. it was implemented in a software tool in order to prove its concept. a performance evaluation was performed, resulting in a high accuracy. two experimental studies were also performed to assess its feasibility when used by real users. the evidences provided by these studies suggests that the approach support the unnecessary complexity removal.
software_engineering	eliciting sufficient high-quality knowledge from individuals to build a robust and useful artificial intelligence or intelligent system solution is a very time-consuming and expensive activity, especially in domains where the knowledge is informal, partial, incomplete, implicit, tacit and unstructured. moreover, in order to develop a solution, a systematic way to incorporate the elicited knowledge into a specification is necessary. the goal of this thesis is to develop a strategy oriented to the transfer and transformation of knowledge with the aim of eliciting and structuring the most quantity of domain knowledge, either tacit or explicit; then incorporate it into a specification that covers the needs and expectations of domain specialists. the application of the strategy in real informally structured domain cases provides empirical insights about its usefulness and value.
software_engineering	problems of sustainable development today are identified and tried to solve in all fields of life. in this respect information technologies play a specific role. many studies have been devoted to analysis of information technology (ict) as tools for sustainable development. different contributions and threats posed by ict to sustainable development are discussed. however, methodological and legislative base of software development is not sufficiently researched from the perspective of sustainable development of information technologies. this paper presents literature review of conference and journal articles and materials of international projects on the topic of different ict roles in sustainable development. the review shows the lack of regulatory framework analysis of information technologies. this analysis is done in separate part of the present paper. we found and analysed weaknesses of evolution of software development methods which put marketing considerations in the first plan. on the basis of the done analysis we discuss possible future work to put sustainable foundations to information technology knowledge from the very beginning of ict specialists training.
software_engineering	context: according to the search reported in this paper, as of this writing (may 2015), a very large number of papers (more than 70,000) have been published in the area of software engineering (se) since its inception in 1968. citations are crucial in any research area to position the work and to build on the work of others. identification and characterization of highly-cited papers are common and are regularly reported in various disciplines. objective: the objective of this study is to identify the papers in the area of se that have influenced others the most as measured by citation count studying highly-cited se papers helps researchers to see the type of approaches and research methods presented and applied in such papers, so as to be able to learn from them to write higher quality papers which will likely receive high citations. method: to achieve the above objective, we conducted a study, comprised of five research questions, to identify and classify the top-100 highly-cited se papers in terms of two metrics: total number of citations and average annual number of citations. results: by total number of citations, the top paper is ""a metrics suite for object-oriented design"", cited 1817 times and published in 1994. by average annual number of citations, the top paper is ""qos-aware middleware for web services composition"", cited 154.2 times on average annually and published in 2004. conclusion: it is concluded that it is important to identify the highly-cited se papers and also to characterize the overall citation landscape in the se field. we hope that this paper will encourage further discussions in the se community towards further analysis and formal characterization of the highly-cited se papers. (c) 2015 elsevier b.v. all rights reserved.
software_engineering	software engineering is the study and an application of engineering to the design, development, and maintenance of software. web testing is the name given to software testing that focuses on web applications. regression testing means re-testing an application after its code has been modified to verify that it still functions correctly. in this test cases that have been develop using reusability constraints that must be act as priority based test cases for regression testing. to execute these cases on the basis of priority of area coverage adaptive tcp algorithm is used that provide as sequence of execution. this sequence has to be optimized by genetic algorithm which uses population size, crossover and mutation probability for generation of new childs.
bioinformatics	metalloproteins bind and utilize metal ions for a variety of biological purposes. due to the ubiquity of metalloprotein involvement throughout these processes across all domains of life, how proteins coordinate metal ions for different biochemical functions is of great relevance to understanding the implementation of these biological processes. toward these ends, we have improved our methodology for structurally and functionally characterizing metal binding sites in metalloproteins. our new ligand detection method is statistically much more robust, producing estimated false positive and false negative rates of approximate to 0.11% and approximate to 1.2%, respectively. additional improvements expand both the range of metal ions and their coordination number that can be effectively analyzed. also, the inclusion of additional quality control filters has significantly improved structure-function spearman correlations as demonstrated by rho values greater than 0.90 for several metal coordination analyses and even one rho value above 0.95. also, improvements in bond-length distributions have revealed bond-length modes specific to chemical functional groups involved in multidentation. using these improved methods, we analyzed all single metal ion binding sites with zn, mg, ca, fe, and na ions in the wwpdb, producing statistically rigorous results supporting the existence of both a significant number of unexpected compressed angles and subsequent aberrant metal ion coordination geometries (cgs) within structurally known metalloproteins. by recognizing these aberrant cgs in our clustering analyses, high correlations are achieved between structural and functional descriptions of metal ion coordination. moreover, distinct biochemical functions are associated with aberrant cgs versus nonaberrant cgs. proteins 2017; 85:885-907. (c) 2016 wiley periodicals, inc.
bioinformatics	type 2 diabetes mellitus (t2dm) is characterized by islet beta-cell dysfunction and insulin resistance, which leads to an inability to maintain blood glucose homeostasis. circulating micrornas (mirnas) have been suggested as novel biomarkers for t2dm prediction or disease progression. however, mirnas and their roles in the pathogenesis of t2dm remain to be fully elucidated. in the present study, the serum mirna expression profiles of t2dm patients in chinese cohorts were examined. total rna was extracted from serum samples of 10 patients with t2dm and five healthy controls, and these was used in reverse-transcription-quantitative polymerase chain reaction analysis with the exiqon pcr system of 384 serum/plasma mirnas. a total of seven mirnas were differentially expressed between the two groups (fold change >3 or < 0.33; p < 0.05). the serum expression levels of mir-455-5p, mir-454-3p, mir-144-3p and mir-96-5p were higher in patients with t2dm, compared with those of healthy subjects, however, the levels of mir-409-3p, mir-665 and mir-766-3p were lower. hierarchical cluster analysis indicated that it was possible to separate patients with t2dm and control individuals into their own similar categories by these differential mirnas. target prediction showed that 97 t2dm candidate genes were potentially modulated by these seven mirnas. kyoto encyclopedia of genes and genomes pathway analysis revealed that 24 pathways were enriched for these genes, and the majority of these pathways were enriched for the targets of induced and repressed mirnas, among which insulin, adipocytokine and t2dm pathways, and several cancer-associated pathways have been previously associated with t2dm. in conclusion, the present study demonstrated that serum mirnas may be novel biomarkers for t2dm and provided novel insights into the pathogenesis of t2dm.
bioinformatics	the twenty-first century vision for toxicology involves a transition away from high-dose animal studies to in vitro and computational models (nrc in toxicity testing in the 21st century: a vision and a strategy, the national academies press, washington, dc, 2007). this transition requires mapping pathways of toxicity by understanding how in vitro systems respond to chemical perturbation. uncovering transcription factors/signaling networks responsible for gene expression patterns is essential for defining pathways of toxicity, and ultimately, for determining the chemical modes of action through which a toxicant acts. traditionally, transcription factor identification is achieved via chromatin immunoprecipitation studies and summarized by calculating which transcription factors are statistically associated with up- and downregulated genes. these lists are commonly determined via statistical or fold-change cutoffs, a procedure that is sensitive to statistical power and may not be as useful for determining transcription factor associations. to move away from an arbitrary statistical or fold-change-based cutoff, we developed, in the context of the mapping the human toxome project, an enrichment paradigm called information-dependent enrichment analysis (idea) to guide identification of the transcription factor network. we used a test case of activation in mcf-7 cells by 17 beta estradiol (e2). using this new approach, we established a time course for transcriptional and functional responses to e2. er alpha and er beta were associated with short-term transcriptional changes in response to e2. sustained exposure led to recruitment of additional transcription factors and alteration of cell cycle machinery. tfap2c and sox2 were the transcription factors most highly correlated with dose. e2f7, e2f1, and foxm1, which are involved in cell proliferation, were enriched only at 24 h. idea should be useful for identifying candidate pathways of toxicity. idea outperforms gene set enrichment analysis (gsea) and provides similar results to weighted gene correlation network analysis, a platform that helps to identify genes not annotated to pathways.
bioinformatics	in this study, we report the gut microbial composition and predictive functional profiles of zebrafish, danio rerio, fed with a control formulated diet (cfd), and a gluten formulated diet (gfd) using a metagenomics approach and bioinformatics tools. the microbial communities of the gfd-fed d. rerio displayed heightened abundances of legionellales, rhizobiaceae, and rhodobacter, as compared to the cfd-fed counterparts. predicted metagenomics of microbial communities (picrust) in gfd-fed d. rerio showed kegg functional categories corresponding to bile secretion, secondary bile acid biosynthesis, and the metabolism of glycine, serine, and threonine. the cfd-fed d. rerio exhibited kegg functional categories of bacteria-mediated cobalamin biosynthesis, which was supported by the presence of cobalamin synthesizers such as bacteroides and lactobacillus. though these bacteria were absent in gfd-fed d. rerio, a comparable level of the cobalamin biosynthesis kegg functional category was observed, which could be contributed by the compensatory enrichment of cetobacterium. based on these results, we conclude d. rerio to be a suitable alternative animal model for the use of a targeted metagenomics approach along with bioinformatics tools to further investigate the relationship between the gluten diet and microbiome profile in the gut ecosystem leading to gastrointestinal diseases and other undesired adverse health effects. (c) 2017 published by-elsevier b.v.
bioinformatics	in our previous study, five different secretory proteins, including gsn, adamtsl4, calr, ppia and txn, have been identified to be associated with the nasopharyngeal carcinoma (npc) metastasis. in this work, the 5 proteins were further investigated. bioinformatics analysis suggested that they might play an important role in the process of npc development. western blotting analysis showed that all of these 5 targets could be secreted into extracellular by both high metastatic npc 5-8f cells and non-metastatic npc 6-10b cells. except for gsn, the expressions of adamtsl4, calr, ppia and txn proteins in extracts of the 5-8f and 6-10b cells were significantly different (p<0.05). thus, the expressions of these 4 differentially expressed proteins were further tested in a cohort of npc tissue specimens. the results indicated that the expression levels of adamtsl4 and txn were highly correlated with the lymph node and distant metastasis (p<0.05) in npc patients. moreover, enzyme-linked immunosorbent assay (elisa) was used to investigate the concentrations of the adamtsl4 and txn in serum specimens of npc patients. the results revealed that serum adamtsl4 expression level was closely correlated with lymph node metastasis and clinical stage (p<0.05) in npc patients, and it was able to discriminate metastasis npc from non-metastasis npc with a sensitivity of 75.6% and a specificity of 64.7%. the present data show for the first time that the adamtsl4 and txn may be novel and potential biomarkers for predicting the npc metastasis. furthermore, the serum adamtsl4 could be a potential serum tumor biomarker for prognosis of npc.
cryptography	visual cryptography scheme (vcs) is a cryptographic technique which can hide image based secrets. even though vcs has the major advantage that the decoding can be done with the help of human visual system (hvs), yet it does not provide sufficient reconstruction quality. hence, two in one image secret sharing scheme (tioisss) is used which provides two decoding phases. however, the existing tioisss method has several limitations. in this work, a modified tioisss is proposed in which an adaptive threshold is used for halftoning, which changes depending on the nature of the pixels present in image. by this, the quality of reconstructed secret image is improved in the first decoding stage compared to the existing scheme. in addition, the security is also enhanced by pixel and bit level permutation with a 64 bit key and embedding the key in gray vcs shadows. to verify the authenticity of the image, a secret message is also embedded in the shadows. security analysis shows that the modified tioisss is robust to brute force and man-in-middle attacks.
cryptography	we propose an ultra-lightweight, compact, and low power block cipher boron. boron is a substitution and permutation based network, which operates on a 64-bit plain text and supports a key length of 128/80 bits. boron has a compact structure which requires 1939 gate equivalents (ges) for a 128-bit key and 1626 ges for an 80-bit key. the boron cipher includes shift operators, round permutation layers, and xor operations. its unique design helps generate a large number of active s-boxes in fewer rounds, which thwarts the linear and differential attacks on the cipher. boron shows good performance on both hardware and software platforms. boron consumes less power as compared to the lightweight cipher led and it has a higher throughput as compared to other existing sp network ciphers. we also present the security analysis of boron and its performance as an ultra-lightweight compact cipher. boron is a well-suited cipher design for applications where both a small footprint area and low power dissipation play a crucial role.
cryptography	we present a new scheme on implementing the passive quantum key distribution with thermal distributed parametric down-conversion source. in this scheme, only one-intensity decoy state is employed, but we can achieve very precise estimation on the single-photon-pulse contribution by utilizing those built-in decoy states. moreover, we compare the new scheme with other practical methods, i.e., the standard three-intensity decoy-state bb84 protocol using either weak coherent states or parametric down-conversion source. through numerical simulations, we demonstrate that our new scheme can drastically improve both the secure transmission distance and the key generation rate.
cryptography	this study concentrates on finding all truncated impossible differentials in substitution-permutation networks (spns) ciphers. instead of using the miss-in-the-middle approach, the authors propose a mathematical description of the truncated impossible differentials. first, they prove that all truncated impossible differentials in an r+1 rounds spn cipher could be obtained by searching entry 0' in d(p)(r), where d(p) denotes the differential pattern matrix (dpm) of p-layer, thus the length of impossible differentials of an spn cipher is upper bounded by the minimum integer r such that there is no entry 0' in d(p)(r). second, they provide two efficient algorithms to compute the dpms for both bit-shuffles and matrices over gf(2(n)). using these tools they prove that the longest truncated impossible differentials in spn structure is 2-round, if the p-layer is designed as an maximum distance separable (mds) matrix. finally, all truncated impossible differentials of advanced encryption standard (aes), aria, aes-mds, present, maya and puffin are obtained.
cryptography	in this paper, a simple implementation for elliptic curve equation on field programmable gate array (fpga) will be proposed. as elliptic curve cryptography (ecc) offers a smaller key size without sacrificing security level. a brief survey on applying the main equation of elliptic curve (ec) with different values of the coefficients a and b. a comparison between results depended on the correlation coefficient of each value. value of a and b implemented on fpga according to correlation results between plaintext image and ciphertext image on matlab. this ec equation will be applied to an ultra-wide band (uwb) system to secure transmission of data in a wireless channel. here, a brief survey on uwb technology has been implemented with software simulation for a secured system based on ecc algorithm.
structured_storage	background: phenotypic data are routinely used to elucidate gene function in organisms amenable to genetic manipulation. however, previous to this work, there was no generalizable system in place for the structured storage and retrieval of phenotypic information for bacteria. results: the ontology of microbial phenotypes (omp) has been created to standardize the capture of such phenotypic information from microbes. omp has been built on the foundations of the basic formal ontology and the phenotype and trait ontology. terms have logical definitions that can facilitate computational searching of phenotypes and their associated genes. omp can be accessed via a wiki page as well as downloaded from sourceforge. initial annotations with omp are being made for escherichia coli using a wiki- based annotation capture system. new omp terms are being concurrently developed as annotation proceeds. conclusions: we anticipate that diverse groups studying microbial genetics and associated phenotypes will employ omp for standardizing microbial phenotype annotation, much as the gene ontology has standardized gene product annotation. the resulting omp resource and associated annotations will facilitate prediction of phenotypes for unknown genes and result in new experimental characterization of phenotypes and functions.
structured_storage	electronic storage of patient-related data will replace paper-based patient records in the near future. some steps in medical practice can even now not be achieved without electronic data processing. both systems, conventional paper-based and electronic-based records, have advantages and disadvantages which have to be taken into consideration. the advantages of electronic-based records are e.g. good availability of data, structured storage of data, scientific analysis of long-term data and possible data exchange with colleagues in the context of teleconsultation systems. problems have to be solved in the field of data security, initial high investment costs and time consumption in learning to use the system as well as in incompatibility of existing it systems.
structured_storage	dateview is a freeware desktop database system for the structured storage and retrieval of geochronological information. it provides a user-friendly interface for constructing queries based on information in the database so as to extract information on specific units, isotope systems, age interpretations, provinces, terranes, reference sources and many other characteristics which geochronologists and geologists might require. once a subset of the records in the database has been selected, users may choose from several forms of graph so as to better visualise the data. available graphs include probability histograms, age versus initial ratio or epsilon, and age versus closure temperature. simple locality (latitude vs longitude) graphs are also available. grouping of data by interpretation or age interval in the graphs is user customizable. the database may also be shared with colleagues on an intranet. (c) 2004 elsevier ltd. all rights reserved.
structured_storage	purpose: to evaluate image quality and anatomical detail depiction in dose-reduced digital plain chest radiograms using a new needle screen storage phosphor (nip) in comparison to full dose conventional powder screen storage phosphor (pip) images. materials and methods: 24 supine chest radiograms were obtained with pip at standard dose and compared to follow-up studies of the same patients obtained with nip with dose reduced to 50% of the pip dose (all imaging systems: agfa-gevaert, mortsel, belgium). in both systems identical versions of post-processing software supplied by the manufacturer were used with matched parameters. six independent readers blinded to both modality and dose evaluated the images for depiction and differentiation of defined anatomical regions (peripheral lung parenchyma, central lung parenchyma, hilum, heart, diaphragm, upper mediastinum, and bone). all nip images were compared to the corresponding pip images using a five-point scale (- 2, clearly inferior to + 2, clearly superior). overall image quality was rated for each pip and nip image separately (1, not usable to 5, excellent). results: pip and dose reduced nip images were rated equivalent. mean image noise impression was only slightly higher on nip images. mean image quality for nip showed no significant differences (p >0.05, mann-whitney u test). conclusion: with the use of the new needle structured storage phosphors in chest radiography, dose reduction of up to 50% is possible without detracting from image quality or detail depiction. especially in patients with multiple follow-up studies the overall dose can be decreased significantly.
structured_storage	windows azure storage (was) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. was customers have access to their data from anywhere at any time and only pay for what they use and store. in was, data is stored durably using both local and geographic replication to facilitate disaster recovery. currently, was storage comes in the form of blobs (files), tables (structured storage), and queues (message delivery). in this paper, we describe the was architecture, global namespace, and data model, as well as its resource provisioning, load balancing, and replication systems.
