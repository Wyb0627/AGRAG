symbolic_computation	nowadays, marine scientists are making use of the kadomtsev-petviashvili (kp)-category equations in their investigations from the straits of georgia and gibraltar to the adriatic sea, north sea and south china sea. in fluid mechanics and other fields, the (3+1)-dimensional b-type kp equations have attracted a good size of recent research. for a generalized (3+1)-dimensional variable-coefficient b-type kp equation for the nonlinear waves in fluid mechanics, with symbolic computation, we obtain a variable-coefficient-dependent auto-backlund transformation, along with two variable-coefficient-dependent families of the shock-wave-type solutions. (c) 2015 elsevier ltd. all rights reserved.
computer_vision	speed and accuracy are important factors when dealing with time-constraint events for disaster, risk, and crisis-management support. object-based image analysis can be a time consuming task in extracting information from large images because most of the segmentation algorithms use the pixel-grid for the initial object representation. it would be more natural and efficient to work with perceptually meaningful entities that are derived from pixels using a low-level grouping process (superpixels). firstly, we tested a new workflow for image segmentation of remote sensing data, starting the multiresolution segmentation (mrs, using esp2 tool) from the superpixel level and aiming at reducing the amount of time needed to automatically partition relatively large datasets of very high resolution remote sensing data. secondly, we examined whether a random forest classification based on an oversegmentation produced by a simple linear iterative clustering (slic) superpixel algorithm performs similarly with reference to a traditional object-based classification regarding accuracy. tests were applied on quickbird and worldview-2 data with different extents, scene content complexities, and number of bands to assess how the computational time and classification accuracy are affected by these factors. the proposed segmentation approach is compared with the traditional one, starting the mrs from the pixel level, regarding geometric accuracy of the objects and the computational time. the computational time was reduced in all cases, the biggest improvement being from 5 h 35 min to 13 min, for a worldview-2 scene with eight bands and an extent of 12.2 million pixels, while the geometric accuracy is kept similar or slightly better. slic superpixel-based classification had similar or better overall accuracy values when compared to mrs-based classification, but the results were obtained in a fast manner and avoiding the parameterization of the mrs. these two approaches have the potential to enhance the automation of big remote sensing data analysis and processing, especially when time is an important constraint.
computer_graphics	the fashion industry is one of the most flourishing fields for visual applications of it. due to the importance of the concept of look in fashion, the most advanced applications of computer graphics and sensing may fruitfully be exploited. the existence of low cost solutions in similar fields, such as the ones that empower the domestic video games market, suggest that analogous low cost solutions are viable and can foster innovation even in small and medium enterprises. in this paper the current state of development of vmannequin, a dynamic, user mimicking, user enacted virtual mannequin software solution, is presented. in order to allow users designing dress concepts, the application simulate the creation and fitting of clothes on virtual models. the interaction is sensor based, in order to both simplify the user interface, and create a richer involvement inside the application.
operating_systems	the weak separation between user-and kernel-space in modern operating systems facilitates several forms of privilege escalation. this paper provides a survey of protection techniques, both cutting-edge and time-tested, used to prevent common privilege escalation attacks. the techniques are compared against each other in terms of their effectiveness, their performance impact, the complexity of their implementation, and their impact on diversification techniques such as aslr. overall the literature provides a litany of disjoint techniques, each of which trades some performance cost for effectiveness against a particular isolated threat. no single technique was found to effectively mitigate all known and potential attack vectors with reasonable performance cost overhead.
machine_learning	we propose herein a new portfolio selection method that switches between two distinct asset allocation strategies. an important component is a carefully designed adaptive switching rule, which is based on a machine learning algorithm. it is shown that using this adaptive switching strategy, the combined wealth of the new approach is a weighted average of that of the successive constant rebalanced portfolio and that of the 1/n portfolio. in particular, it is asymptotically superior to the 1/n portfolio under mild conditions in the long run. applications to real data show that both the returns and the sharpe ratios of the proposed binary switch portfolio are the best among several popular competing methods over varying time horizons and stock pools.
data_structures	the literature presents many application programming interfaces (apis) and frameworks that provide state of the art algorithms and techniques for solving optimisation problems. the same cannot be said about apis and frameworks focused on the problem data itself because with the peculiarities and details of each variant of a problem, it is virtually impossible to provide general tools that are broad enough to be useful on a large scale. however, there are benefits of employing problem-centred apis in a r&d environment: improving the understanding of the problem, providing fairness on the results comparison, providing efficient data structures for different solving techniques, etc. therefore, in this work we propose a novel design methodology for an api focused on an optimisation problem. our methodology relies on a data parser to handle the problem specification files and on a set of efficient data structures to handle the information on memory, in an intuitive fashion for researchers and efficient for the solving algorithms. also, we present the concepts of a solution dispenser that can manage solutions objects in memory better than built-in garbage collectors. finally, we describe the positive results of employing a tailored api to a project involving the development of optimisation solutions for workforce scheduling and routing problems.
network_security	in this paper, a spectrally coded optical code division multiple access (ocdma) system using a hybrid modulation scheme has been investigated. the idea is to propose an effective approach for simultaneous improvement of the system capacity and security. data formats, nrz (non-return to zero), dqpsk (differential quadrature phase shift keying), and poisk (polarisation shift keying) are used to get the orthogonal modulated signal. it is observed that the proposed hybrid modulation provides efficient utilisation of bandwidth, increases the data capacity and enhances the data confidentiality over existing ocdma systems. further, the proposed system performance is compared with the current state-of-the-art ocdma schemes. (c) 2015 elsevier inc. all rights reserved.
image_processing	using image processing to extract nodular or linear shadows is a key technique of computer-aided diagnosis schemes. this study proposes a new method for extracting nodular and linear patterns of various sizes in medical images. we have developed a morphology filter bank that creates multiresolution representations of an image. analysis bank of this filter bank produces nodular and linear patterns at each resolution level. synthesis bank can then be used to perfectly reconstruct the original image from these decomposed patterns. our proposed method shows better performance based on a quantitative evaluation using a synthesized image compared with a conventional method based on a hessian matrix, often used to enhance nodular and linear patterns. in addition, experiments show that our method can be applied to the followings: (1) microcalcifications of various sizes in mammograms can be extracted, (2) blood vessels of various sizes in retinal fundus images can be extracted, and (3) thoracic ct images can be reconstructed while removing normal vessels. our proposed method is useful for extracting nodular and linear shadows or removing normal structures in medical images.
parallel_computing	traditional fluid-structure interaction techniques are based on arbitrary lagrangian-eulerian method, where strong coupling method is used to solve fundamental discretized equations. after discretization, resulting nonlinear systems are solved using an iterative method leading to set of linear subsystems. the resulting linear subsystem matrices are sparse non-symmetric indefinite with small or zero diagonal entries. the small pivots of these matrices may render the incomplete lu factorization unstable or inaccurate. to improve that, this paper proposes an approach that can be used together with a stabilization and offers an additional feature to be used to gain an advantage in its parallelization. (c) 2016 published by elsevier ltd.
distributed_computing	using distributed task allocation methods for cooperating multivehicle systems is becoming increasingly attractive. however, most effort is placed on various specific experimental work and little has been done to systematically analyze the problem of interest and the existing methods. in this paper, a general scenario description and a system configuration are first presented according to search and rescue scenario. the objective of the problem is then analyzed together with its mathematical formulation extracted from the scenario. considering the requirement of distributed computing, this paper then proposes a novel heuristic distributed task allocation method for multivehicle multitask assignment problems. the proposed method is simple and effective. it directly aims at optimizing the mathematical objective defined for the problem. a new concept of significance is defined for every task and is measured by the contribution to the local cost generated by a vehicle, which underlies the key idea of the algorithm. the whole algorithm iterates between a task inclusion phase, and a consensus and task removal phase, running concurrently on all the vehicles where local communication exists between them. the former phase is used to include tasks into a vehicle 's task list for optimizing the overall objective, while the latter is to reach consensus on the significance value of tasks for each vehicle and to remove the tasks that have been assigned to other vehicles. numerical simulations demonstrate that the proposed method is able to provide a conflict-free solution and can achieve outstanding performance in comparison with the consensus-based bundle algorithm.
algorithm_design	in this work, a novel surrogate-assisted memetic algorithm is proposed which is based on the preservation of genetic diversity within the population. the aim of the algorithm is to solve multi-objective optimization problems featuring computationally expensive fitness functions in an efficient manner. the main novelty is the use of an evolutionary algorithm as global searcher that treats the genetic diversity as an objective during the evolution and uses it, together with a non-dominated sorting approach, to assign the ranks. this algorithm, coupled with a gradient-based algorithm as local searcher and a back-propagation neural network as global surrogate model, demonstrates to provide a reliable and effective balance between exploration and exploitation. a detailed performance analysis has been conducted on five commonly used multi-objective problems, each one involving distinct features that can make the convergence difficult toward the pareto-optimal front. in most cases, the proposed algorithm outperformed the other state-of-the-art evolutionary algorithms considered in the comparison, assuring higher repeatability on the final non-dominated set, deeper convergence level and higher convergence rate. it also demonstrates a clear ability to widely cover the pareto-optimal front with larger percentage of non-dominated solutions if compared to the total number of function evaluations. (c) 2015 elsevier b.v. all rights reserved.
computer_programming	computer programming is a core subject of almost all degrees of engineering that is perceived as a very complex matter for the students, because it is very different from the other core subjects (physics, calculus, algebra, chemistry, graphics expression) that are more familiar to students. programming combines in a balanced way the two fundamental steps when developing an engineering product, design and implementation (coding), but with the important difference in the lower costs of the implementation phase, which permits students building and testing the proposed designs. besides, the new european higher education area (ehea) is based both in the acquisition of competencies and skills by the student rather than in the accumulation of knowledge, and in the use of the european credit transfer and accumulation system (ects), which supposes an important challenge for the teaching model. in this paper we show our eight years experience in developing a hybrid methodology for teaching computer programming in the degrees of mechanical, electrical, electronic and chemistry engineering at the university of almeria (spain). we integrate traditional teaching methods (participative master class for transmission of information) with modern methods (problem-based learning, collaborative-team working, autonomous working, or tutoring). the different methods of teaching are supported by a hardware-software infrastructure of virtual teaching (blackboard platform) and a very detailed planning where activities are highly focused and weekly organized. it includes periodical tests for the evaluation of the progress of the student. we think that the adoption and tuning of these learning methods will enhance the skills of the future engineers in the computer programming area.
relational_databases	entity resolution (er) concerns identifying pairs of entities that refer to the same underlying entity. to avoid o(n(2)) pairwise comparison of n entities, blocking methods are used. sorted neighborhood is an established blocking method for relational databases. it has not been applied to schema-free resource description framework (rdf) data sources widely prevalent in the linked data ecosystem. this paper presents a sorted neighborhood workflow that may be applied to schema-free rdf data. the workflow is modular and makes minimal assumptions about its inputs. empirical evaluations of the proposed algorithm on five real-world benchmarks demonstrate its utility compared to two state-of-the-art blocking baselines.
software_engineering	seeking product 's quality is essential nowadays. one of the many quality aspects in software development is the source code complexity. not taking care for the complexity during the development can result in unexpected cost, caused by the difficulty on the source code understanding. the goal of this paper is to introduce an initial approach to identify unnecessary complexity in source code. besides identifying, also show to its user how to properly rewrite the source code without the unnecessary complexity. the approach is based on the static analysis of the source code control flow graph. once the unnecessary complexity is identified, the graph is refactored in order to allow the user to understand the improvement on the source code. it was implemented in a software tool in order to prove its concept. a performance evaluation was performed, resulting in a high accuracy. two experimental studies were also performed to assess its feasibility when used by real users. the evidences provided by these studies suggests that the approach support the unnecessary complexity removal.
bioinformatics	metalloproteins bind and utilize metal ions for a variety of biological purposes. due to the ubiquity of metalloprotein involvement throughout these processes across all domains of life, how proteins coordinate metal ions for different biochemical functions is of great relevance to understanding the implementation of these biological processes. toward these ends, we have improved our methodology for structurally and functionally characterizing metal binding sites in metalloproteins. our new ligand detection method is statistically much more robust, producing estimated false positive and false negative rates of approximate to 0.11% and approximate to 1.2%, respectively. additional improvements expand both the range of metal ions and their coordination number that can be effectively analyzed. also, the inclusion of additional quality control filters has significantly improved structure-function spearman correlations as demonstrated by rho values greater than 0.90 for several metal coordination analyses and even one rho value above 0.95. also, improvements in bond-length distributions have revealed bond-length modes specific to chemical functional groups involved in multidentation. using these improved methods, we analyzed all single metal ion binding sites with zn, mg, ca, fe, and na ions in the wwpdb, producing statistically rigorous results supporting the existence of both a significant number of unexpected compressed angles and subsequent aberrant metal ion coordination geometries (cgs) within structurally known metalloproteins. by recognizing these aberrant cgs in our clustering analyses, high correlations are achieved between structural and functional descriptions of metal ion coordination. moreover, distinct biochemical functions are associated with aberrant cgs versus nonaberrant cgs. proteins 2017; 85:885-907. (c) 2016 wiley periodicals, inc.
cryptography	visual cryptography scheme (vcs) is a cryptographic technique which can hide image based secrets. even though vcs has the major advantage that the decoding can be done with the help of human visual system (hvs), yet it does not provide sufficient reconstruction quality. hence, two in one image secret sharing scheme (tioisss) is used which provides two decoding phases. however, the existing tioisss method has several limitations. in this work, a modified tioisss is proposed in which an adaptive threshold is used for halftoning, which changes depending on the nature of the pixels present in image. by this, the quality of reconstructed secret image is improved in the first decoding stage compared to the existing scheme. in addition, the security is also enhanced by pixel and bit level permutation with a 64 bit key and embedding the key in gray vcs shadows. to verify the authenticity of the image, a secret message is also embedded in the shadows. security analysis shows that the modified tioisss is robust to brute force and man-in-middle attacks.
structured_storage	background: phenotypic data are routinely used to elucidate gene function in organisms amenable to genetic manipulation. however, previous to this work, there was no generalizable system in place for the structured storage and retrieval of phenotypic information for bacteria. results: the ontology of microbial phenotypes (omp) has been created to standardize the capture of such phenotypic information from microbes. omp has been built on the foundations of the basic formal ontology and the phenotype and trait ontology. terms have logical definitions that can facilitate computational searching of phenotypes and their associated genes. omp can be accessed via a wiki page as well as downloaded from sourceforge. initial annotations with omp are being made for escherichia coli using a wiki- based annotation capture system. new omp terms are being concurrently developed as annotation proceeds. conclusions: we anticipate that diverse groups studying microbial genetics and associated phenotypes will employ omp for standardizing microbial phenotype annotation, much as the gene ontology has standardized gene product annotation. the resulting omp resource and associated annotations will facilitate prediction of phenotypes for unknown genes and result in new experimental characterization of phenotypes and functions.
