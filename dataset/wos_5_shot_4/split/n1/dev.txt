symbolic_computation	by employing the generalized ( g'/ g)- expansion method and symbolic computation, we obtain new exact solutions of the ( 3 + 1) dimensional generalized b-type kadomtsev- petviashvili equation, which include the traveling wave exact solutions and the non- traveling wave exact solutions showed by the hyperbolic function and the trigonometric function. meanwhile, some interesting physics structure are discussed.
symbolic_computation	the korteweg-de vries equation (kdv) and the (2+ 1)-dimensional nizhnik-novikov-veselov system (nnv) are presented. multi-soliton rational solutions of these equations are obtained via the generalized unified method. the analysis emphasizes the power of this method and its capability of handling completely (or partially) integrable equations. compared with hirota 's method and the inverse scattering method, the proposed method gives more general exact multi-wave solutions without much additional effort. the results show that, by virtue of symbolic computation, the generalized unified method may provide us with a straightforward and effective mathematical tool for seeking multi-soliton rational solutions for solving many nonlinear evolution equations arising in different branches of sciences.
symbolic_computation	in this paper, the (g'/g)-expansion method is suggested to establish new exact solutions for fractional differential-difference equations in the sense of modified riemann-liouville derivative. the fractional complex transform is proposed to convert a fractional partial differential difference equation into its differential difference equation of integer order. with the aid of symbolic computation, we choose nonlinear lattice equations to illustrate the validity and advantages of the algorithm. it is shown that the proposed algorithm is effective and can be used for many other nonlinear lattice equations in mathematical physics and applied mathematics.
symbolic_computation	we present an accurate method for the numerical integration of polynomials over arbitrary polyhedra. using the divergence theorem, the method transforms the domain integral into integrals evaluated over the facets of the polyhedra. the necessity of performing symbolic computation during such transformation is eliminated by using one dimensional gauss quadrature rule. the facet integrals are computed with the help of quadratures available for triangles and quadrilaterals. numerical examples, in which the proposed method is used to integrate the weak form of the navier-stokes equations in an embedded interface method (eim), are presented. the results show that our method is as accurate and generalized as the most widely used volume decomposition based methods. moreover, since the method involves neither volume decomposition nor symbolic computations, it is much easier for computer implementation. also, the present method is more efficient than other available integration methods based on the divergence theorem. efficiency of the method is also compared with the volume decomposition based methods and moment fitting methods. to our knowledge, this is the first article that compares both accuracy and computational efficiency of methods relying on volume decomposition and those based on the divergence theorem. (c) 2014 elsevier inc. all rights reserved.
symbolic_computation	the coupled equations for the incoherent optical spatial solitons in a nonlocal nonlinear medium is studied analytically. with the soliton solutions hereby obtained via the symbolic computation, the optical-soliton motion in the nonlocal nonlinearmedium is studied: |psi| is inversely related to kappa, omega, and n(0), while delta n is positively related to omega and n(0), but delta(n) is independent of kappa, with psi as the slowly varying amplitude of the beam, delta n as the refractive index change, kappa as the beam intensity distribution, omega as the frequency of the propagating beam, and n(0) as the unperturbed refractive index. head-on and overtaking interactions are observed, and head-on interaction is transformed into an overtaking one with. increasing. bound-state interaction is displayed, and with n0 increasing, the period of. decreases, while that of delta n increases. considering the external forces in the nonlocal nonlinear medium , we explore the chaotic motions in the nonlinear nonlocal medium, including effects of the external forces on such motions. it is seen that when vertical bar alpha(1)vertical bar >>vertical bar 2 omega(2)n(0)/c vertical bar and vertical bar alpha(2)vertical bar >>vertical bar kappa vertical bar, the two-dimensional attractors with stretching-and-folding structures are exhibited, and the developed chaos occurs, where alpha(1) and alpha(2) are the amplitudes of external forces, c is the speed of light in vacuum. such chaotic motions are weakened with omega, kappa,(omega) over bar (1) and (omega) over bar (2) increasing, or with n(0) decreasing, where (omega) over bar (1) and (omega) over bar (2) represent the frequencies of external forces.
computer_vision	the qualities of color images captured by digital imaging devices are vulnerable to the scene illumination settings of a given environment. the colors of captured objects may not be accurately reproduced when the illumination settings are uncontrollable or not known a priori. this undesirable property can inevitably degrade the qualities of captured images and lead to difficulties in subsequent image-processing stages. considering that the task of controlling scene illumination is nontrivial, color correction has emerged as a plausible post-processing procedure to efficiently restore the scene chromatics of a given image. in this study, a new color correction technique called the saturation avoidance color correction (sacc) algorithm is proposed to remove the undesirable effect of scene illuminants. unlike most well-established color correction algorithms, the proposed sacc comprises a nonlinear pixel adjustment mechanism to avoid the saturation effect during the color manipulation process. a collection of color images including indoor, outdoor, and underwater images are used to verify the capability of sacc. extensive experimental studies reveal that the proposed algorithm is preferable to some existing techniques because the former has a high capability to mitigate the color saturation issue and is able to produce corrected images with more pleasant visualization.
computer_vision	the use of unmanned aerial vehicles (uavs) is growing significantly for many and varied purposes. during the mission, an outdoor uav is guided by following the planned path using gps signals. however, the gps capability may become defective or the environment may be gps-denied, and an additional safety aid is therefore required for the automatic landing phase that is independent of gps data. most uavs are equipped with machine vision systems which, together with onboard analysis, can be used for safe, automatic landing. this contributes greatly to the overall success of autonomous flight. this paper proposes an automatic expert system, based on image segmentation procedures, that assists safe landing through recognition and relative orientation of the uav and platform. the proposed expert system exploits the human experience that has been incorporated into the machine vision system, which is mapped into the proposed image processing modules. the result is an improved reliability capability that could be incorporated into any uav, and is especially robust for rotary wing uavs. this is clearly a desirable fail-safe capability. (c) 2017 elsevier ltd. all rights reserved.
computer_vision	modern competitive shooting is a strenuous test of the human perceptual and motor systems. like driving a race car or piloting a high performance aircraft, speed shooting matches require precise, rapid movements coordinated using a careful mixture of planning and reaction. in some disciplines of competitive shooting, this mixture is further complicated by complex moving targets. this paper develops a set of state space equations for a moving, reactive steel target used in professional 3-gun competition. the equations for target motion are nonlinear, time-varying, and chaotic in certain regions of the state space. once derived, the equations for target motion are validated against motion extracted from video. then, the calibrated equations are implemented in real-time on a portable, laser-activated simulator. applications of the simulation environment to marksmanship training and human motor control studies are also discussed.
computer_vision	hypergraph is a powerful representation for several computer vision, machine learning, and pattern recognition problems. in the last decade, many researchers have been keen to develop different hypergraph models. in contrast, no much attention has been paid to the design of hyperedge weighting schemes. however, many studies on pairwise graphs showed that the choice of edge weight can significantly influence the performances of such graph algorithms. we argue that this also applies to hypergraphs. hi this paper, we empirically study the influence of hyperedge weights on hypergraph learning via proposing three novel hyperedge weighting schemes from the perspectives of geometry, multivariate statistical analysis, and linear regression. extensive experiments on orl, coil20, jaffe, sheffield, scenel 5 and caltech256 datasets verified our hypothesis for both classification and clustering problems. for each of these classes of problems, our empirical study concludes with suggesting a suitable hypergraph weighting scheme. moreover, the experiments also demonstrate that the combinations of such weighting schemes and conventional hyper graph models can achieve competitive classification and clustering performances in comparison with some recent state-of-the-art algorithms. (c) 2016 elsevier b.v. all rights reserved.
computer_vision	image segmentation is a fundamental problem in computer vision, and the color and texture information are usually both employed to obtain more satisfactory segmentation results. however, the traditional color-texture segmentation methods usually assume that the color-based and texture-based segmentation results are globally consistent, which is not always the case. sometimes, the color-texture based segmentation results may be worse than some single feature (color or texture) based ones if the consistency constraints are taken into account inappropriately. to address the problem, a graph cuts based color-texture cosegmentation method is proposed in this paper, where just the similarity constrains are considered rather than the global consistencies, and a penalty term is included to adaptively balance the possible local inconsistencies. additionally, in order to extract the texture features effectively, a comprehensive texture descriptor is designed by integrating the nonlinear compact multi-scale structure tensor (ncmsst) and total variation flow (tv-flow). a large number of segmentation comparison experiments using the synthesis color-texture images and real natural scene images verify the superiorities of our proposed texture descriptor and color-texture cosegmentation method. (c) 2016 elsevier b.v. all rights reserved.
computer_graphics	perfect matchings and maximum weight matchings are two fundamental combinatorial structures. we consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching. motivated by the computer graphics application in triangle meshes, where we seek to convert a triangulation into a quadrangulation by merging pairs of adjacent triangles, we focus mainly on bridgeless cubic graphs. first, we characterize graphs that attain the extreme ratios. second, we present a lower bound for all bridgeless cubic graphs. third, we present upper bounds for subclasses of bridgeless cubic graphs, most of which are shown to be tight. additionally, we present tight bounds for the class of regular bipartite graphs. (c) 2014 elsevier b.v. all rights reserved.
computer_graphics	traffic jam is one of the hardest problems of the crowded cities, and it needs to be solved. in this study, the effect of the minimum speed limit signs in addition to the maximum speed signs and their locations in traffic flow has been examined by using cellular automata (ca). urban traffic is modeled by two dimensional ca. the model includes traffic signs, traffic lights and some kinds of vehicles (such as automobiles, vans, buses, metro buses) that are often encountered in traffic.
computer_graphics	immersive multi-user virtual environments give good support for closely-coupled collaboration between co-located users. more complex collaborative tasks may require individual user navigation to achieve loosely-coupled collaboration. we designed a navigation framework based on the human joystick metaphor with some alterations for cohabitation management. this model allows each user to navigate independently using a human joystick based control while avoiding physical obstacles and staying within the usable part of the system. we conducted a series of user studies to investigate the influence of each alteration by testing their combinations on various navigation tasks. the results show that modified transfer functions and adaptive neutral orientations improve users' cohabitation performance, while the impact of adaptive neutral positions need to be further studied.
computer_graphics	accurately and reliably tracking the undulatory motion of deformable fish body is of great significance for not only scientific researches but also practical applications such as robot design and computer graphics. however, it remains a challenging task due to severe body deformation, erratic motion and frequent occlusions. this paper proposes a tracking method which is capable of tracking the midlines of multiple fish based on midline evolution and head motion pattern modeling with long short-term memory (lstm) networks. the midline and head motion state are predicted using two lstm networks respectively and the predicted state is associated with detections to estimate the state of each target at each moment. experiment results show that the system can accurately track midline dynamics of multiple zebrafish even when mutual occlusions occur frequently.
computer_graphics	the paper presents the formalism of the parametric integral equation system (pies) for two-dimensional elastoplastic problems and the algorithm for its numerical solution. the efficiency of the proposed approach lies in the global modeling of a plastic zone, without classic discretization into elements, using surface patches popular in computer graphics. lagrange polynomials with various number and arrangement of interpolation nodes are used to approximate plastic strains. three test examples are solved and the obtained results are compared with analytical and numerical solutions. (c) 2016 elsevier ltd. all rights reserved.
operating_systems	accurate measurement of cardiomyocyte contraction is a critical issue for scientists working on cardiac physiology and physiopathology of diseases implying contraction impairment. cardiomyocytes contraction can be quantified by measuring sarcomere length, but few tools are available for this, and none is freely distributed. we developed a plug-in (sarcoptim) for the imagej/fiji image analysis platform developed by the national institutes of health. sarcoptim computes sarcomere length via fast fourier transform analysis of video frames captured or displayed in imagej and thus is not tied to a dedicated video camera. it can work in real time or offline, the latter overcoming rotating motion or displacement-related artifacts. sarcoptim includes a simulator and video generator of cardiomyocyte contraction. acquisition parameters, such as pixel size and camera frame rate, were tested with both experimental recordings of rat ventricular cardiomyocytes and synthetic videos. it is freely distributed, and its source code is available. it works under windows, mac, or linux operating systems. the camera speed is the limiting factor, since the algorithm can compute online sarcomere shortening at frame rates >10 khz. in conclusion, sarcoptim is a free and validated user-friendly tool for studying cardiomyocyte contraction in all species, including human.
operating_systems	the advent of native mobile technologies particularly advances in the mobile operating systems (os), popularly among android, windows mobile os, ios and their acceptance as an alternative to existing browser based access; has enabled the remote access of equipment and instruments over the world wide web (www). while more and more remote control solutions have been implemented in the industry via local area networks (lans), wide area networks (wans), and the web, online remote accessible laboratory (oral) has surpassed the simulation as a tool for learning scientific methods with the results and observations being real time rather than ideal and facilitate the conduct of experiments on specific instrumentation from any location. the aim of this paper is to introduce an easy accessible remote laboratory measuring system (rlms) to provide the students to perform laboratory experiments on actual measurement instrumentation over any smart mobile device, tablets or desktops and pcs. the proposed architecture on real laboratory setup provides solutions for remote access and opens ways for future developments. the system is realized using labview graphical programming environment which confers its advantages.
operating_systems	this work suggests a method for systematically constructing a software-level environment model for safety checking automotive operating systems by introducing a constraint specification language, osek_csl. osek_csl is designed to specify the usage constraints of automotive operating systems using a pre-defined set of constraint types identified from the international standard osek/vdx. each constraint specified in osek_csl is interpreted as either a regular language or a context-free language that can be checked by a finite automaton or a pushdown automaton. the set of usage constraints is used to systematically classify the universal usage model of osek-/vdx-based operating systems and to generate test sequences with varying degrees of constraint satisfaction using ltl model checking. with pre-defined constraint patterns and the full support of automation, test engineers can choose the degree of constraint satisfaction and generate test cases using combinatorial intersections of selected constraints that cover all corner cases classified by constraints. a series of experiments on an open-source automotive operating system show that our approach finds safety issues more effectively than conventional specification-based testing, scenario-based testing, and conformance testing.
operating_systems	purpose - in recent times, progression of technology and growing demands of customers have substantially influenced the services sector to introduce fast real-time mechanisms for providing up-to-mark services. to meet these requirements, organizations are going to change their end-user operating systems but success rate of change is very low. the purpose of this paper is to address one of the practitioners' complaint ""no one tells us how to do it"" and uncovers the indirect effects of knowledge management (km) strategies: personalization and codification, toward organizational change via organizational learning and change readiness. the current study also highlights how organizational learning and change readiness are helpful to reduce the detrimental effects of organizational change cynicism toward success of a change process. design/methodology/approach - temporal research design is used to get the appropriate responses from the targeted population in two stages such as pre-change (time-1) and post-change (time-2). in cumulative, 206 responses have been obtained from the banking sector of pakistan. findings - the results of the current study are very promising as it has been stated that km strategies have an indirect effect on successful organizational change through organizational learning and change readiness. moreover, change cynicism has a weakening effect on a change process and can be managed through effective learning orientation of employees and developing readiness for change in organizations. research limitations/implications - change agents have to use an optimal mix of personalization and codification strategies to develop learning environment and readiness for change in organizations that are beneficial for implementing a change successfully. moreover, change readiness and organizational learning in the context of change are equally beneficial to reduce organizational change cynicism as well. originality/value - this study is introducing a unique model to initiate a change with the help of km strategies, organizational learning and readiness for change.
operating_systems	this article presents an introductory microcontroller programming course on digital signal processing for undergraduate university level. the course is intended to provide insight into information technology and to prepare students for more complex exercises later on in their studies. solutions to overcome pedagogical obstacles like the fear of new technologies and to minimise technological incompatibilities between different operating systems while setting up a programming tool chain are presented. this leads to an increased scalability of the course, allowing hundreds of students to attend each year. in the case presented here, the average number of participants was 300. the problem-oriented task assignments are defined leading to a final creative improvement task, for which the students' solutions are analysed. the course is evaluated and an outlook on further improvements is given.
machine_learning	one of the backbones of the indian economy is agriculture, which is conditioned by the poor soil fertility. in this study we use chemical soil measurements to classify many relevant soil parameters: village-wise fertility indices of organic carbon (oc), phosphorus pentoxide (p2o5), manganese (mn) and iron (fe); soil ph and type; soil nutrients nitrous oxide (n2o), p2o5 and potassium oxide (k2o), in order to recommend suitable amounts of fertilizers; and preferable crop. to classify these soil parameters allows to save time of specialized technicians developing expensive chemical analysis. these ten classification problems are solved using a collection of twenty very diverse classifiers, selected by their high performances, of families bagging, boosting, decision trees, nearest neighbors, neural networks, random forests (rf), rule based and support vector machines (svm). the rf achieves the best performance for six of ten problems, overcoming 90% of the maximum performance in all the cases, followed by adaboost, svm and gaussian extreme learning machine. although for some problems (ph,n2o,p2o5 and k2o) the performance is moderate, some classifiers (e.g. for fertility indices of p2o5, mn and fe) trained in one region revealed valid for other indian regions. (c) 2017 elsevier b.v. all rights reserved.
machine_learning	we demonstrate a model order reduction technique for a system of nonlinear equations arising from the finite element method (fem) discretization of the three-dimensional quasistatic equilibrium equation equipped with a perzyna viscoplasticity constitutive model. the procedure employs the proper orthogonal decomposition-galerkin (pod-g) in conjunction with the discrete empirical interpolation method (deim). for this purpose, we collect samples from a standard full order fem analysis in the offline phase and cluster them using a novel k-means clustering algorithm. the pod and the deim algorithms are then employed to construct a corresponding reduced order model. in the online phase, a sample from the current state of the system is passed, at each time step, to a nearest neighbor classifier in which the cluster that best describes it is identified. the force vector and its derivative with respect to the displacement vector are approximated using deim, and the system of nonlinear equations is projected onto a lower dimensional subspace using the pod-g. the constructed reduced order model is applied to two typical solid mechanics problems showing strain-localization (a tensile bar and a wall under compression) and a three-dimensional squarefooting problem. (c) 2017 the authors. published by elsevier b.v. this is an open access article under the cc by-nc-nd license.
machine_learning	demand management in residential buildings is a key component toward sustainability and efficiency in urban environments. the recent advancements in sensor based technologies hold the promise of novel energy consumption models that can better characterize the underlying patterns. in this paper, we propose a probabilistic data-driven predictive model for consumption forecasting in residential buildings. the model is based on bayesian network (bn) framework which is able to discover dependency relations between contributing variables. thus, we can relax the assumptions that are often made in traditional forecasting models. moreover, we are able to efficiently capture the uncertainties in input variables and quantify their effect on the system output. we test our proposed approach to the data provided by pacific northwest national lab (pnnl) which has been collected through a pilot smart grid project. we examine the performance of our model in a multiscale setting by considering various temporal (i.e., 15 min, hourly intervals) and spatial (i.e., all households in a region, each household) resolutions for analyzing data. demand forecasting at the individual households' levels is a first step toward designing personalized and targeted policies for each customer. while this is a widely studied topic in digital marketing, few researches have been done in the energy sector. the results indicate that bayesian networks can be efficiently used for probabilistic energy modeling in residential buildings by discovering the dependencies between variables. (c) 2017 elsevier ltd. all rights reserved.
machine_learning	this work presents a novel approach for decisionmaking for multi-objective binary classification problems. the purpose of the decision process is to select within a set of pareto-optimal solutions, one model that minimizes the structural risk (generalization error). this new approach utilizes a kind of prior knowledge that, if available, allows the selection of a model that better represents the problem in question. prior knowledge about the imprecisions of the collected data enables the identification of the region of equivalent solutions within the set of pareto-optimal solutions. results for binary classification problems with sets of synthetic and real data indicate equal or better performance in terms of decision efficiency compared to similar approaches.
machine_learning	oscillometric measurement is widely used to estimate systolic blood pressure (sbp) and diastolic blood pressure (dbp). in this paper, we propose a deep belief network (dbn)-deep neural network (dnn) to learn about the complex nonlinear relationship between the artificial feature vectors obtained from the oscillometric wave and the reference nurse blood pressures using the dbn-dnn-based-regression model. our dbn-dnn is a powerful generative network for feature extraction and can address to stick in local minima through a special pretraining phase. therefore, this model provides an alternative way for replacing a popular shallow model. based on this, we apply the dbn-dnn-based regression model to estimate the sbp and dbp. however, there are a small amount of data samples, which is not enough to train the dbn-dnn without the overfitting problem. for this reason, we use the parametric bootstrap-based artificial features, which are used as training samples to efficiently learn the complex nonlinear functions between the feature vectors obtained and the reference nurse blood pressures. as far as we know, this is one of the first studies using the dbn-dnn-based regression model for bp estimation when a small training sample is available. our dbn-dnn-based regression model provides a lower standard deviation of error, mean error, and mean absolute error for the sbp and dbp as compared with the conventional methods.
data_structures	data clustering is an important step in data mining and machine learning. it is especially crucial to analyze the data structures for further procedures. recently a new clustering algorithm known as 'neutrosophic c-means' (ncm) was proposed in order to alleviate the limitations of the popular fuzzy c-means (fcm) clustering algorithm by introducing a new objective function which contains two types of rejection. the ambiguity rejection which concerned patterns lying near the cluster boundaries, and the distance rejection was dealing with patterns that are far away from the clusters. in this paper, we extend the idea of ncm for nonlinear-shaped data clustering by incorporating the kernel function into ncm. the new clustering algorithm is called kernel neutrosophic c-means (kncm), and has been evaluated through extensive experiments. nonlinear-shaped toy datasets, real datasets and images were used in the experiments for demonstrating the efficiency of the proposed method. a comparison between kernel fcm (kfcm) and kncm was also accomplished in order to visualize the performance of both methods. according to the obtained results, the proposed kncm produced better results than kfcm. (c) 2016 elsevier b.v. all rights reserved.
data_structures	maze puzzle usually has only one user. this research has adopted network technology to develop a maze game, called multi-maze, which allows multiple players competing across the network concurrently. it contains three sub-systems. the maze editor is capable of building 3, 4, or 6 degrees multi-component maze and supporting automatic, semi-automatic, or manual construction of the maze. the contour of the maze can be configured freely and the process of building a maze is recorded for replaying. the maze runner supports single-user or multi-user mode. in multi-user mode, teammates communicate with multi-view display and real-time chats. foot prints, marks, flags are used for avoiding repeated movements. the maze can be shown as wall or channel map. the maze manager allows editing, upload, download, and grouping of the teams. before coding the multi-maze, two important utility tools have been developed: multi-view graph library and network communication module. formulas for calculating the coordination for different degrees has been presented. algorithms of spanning tree have been used for building a perfect maze. the adding and deleting of walls allow the maze to be separated into components and forming circuits. the state diagrams, data structures and class hierarchies are presented for related application projects.
data_structures	most published concurrent data structures which avoid locking do not provide any fairness guarantees. that is, they allow processes to access a data structure and complete their operations arbitrarily many times before some other trying process can complete a single operation. such a behavior can be prevented by enforcing fairness. however, fairness requires waiting or helping. helping techniques are often complex and memory consuming. furthermore, it is known that it is not possible to automatically transform every data structure, which has a non-blocking implementation, into the corresponding data structure which in addition satisfies a very weak fairness requirement. does it mean that for enforcing fairness it is best to use locks? the answer is negative. we show that it is possible to automatically transfer any non-blocking or wait-free data structure into a similar data structure which satisfies a strong fairness requirement, without using locks and with limited waiting. the fairness we require is that no process can initiate and complete two operations on a given resource while some other process is kept waiting on the same resource. our approach allows as many processes as possible to access a shared resource at the same time as long as fairness is preserved. to achieve this goal, we introduce and solve a new synchronization problem, called fair synchronization. solving the new problem enables us to add fairness to existing implementations of concurrent data structures, and to transform any solution to the mutual exclusion problem into a fair solution. (c) 2016 elsevier inc. all rights reserved.
data_structures	city governments and energy utilities are increasingly focusing on the development of energy efficiency strategies for buildings as a key component in emission reduction plans and energy supply strategies. to support these diverse needs, a new generation of urban building energy models (ubem) is currently being developed and validated to estimate citywide hourly energy demands at the building level. however, in order for cities to rely on ubems, effective model generation and maintenance workflows are needed based on existing urban data structures. within this context, the authors collaborated with the boston redevelopment authority to develop a citywide ubem based on official gis datasets and a custom building archetype library. energy models for 83,541 buildings were generated and assigned one of 52 use/age archetypes, within the cad modelling environment rhinoceros3d. the buildings were then simulated using the us doe energyplus simulation program, and results for buildings of the same archetype were crosschecked against data from the us national energy consumption surveys. a district-level intervention combining photovoltaics with demand side management is presented to demonstrate the ability of ubem to provide actionable information. lack of widely available archetype templates and metered energy data, were identified as key barriers within existing workflows that may impede cities from effectively applying ubem to guide energy policy. (c) 2016 elsevier ltd. all rights reserved.
data_structures	asynchronous tasks in programming are those tasks executed free of context of the main task. therefore asynchronous tasks are methods implemented in a non-blocking style, permitting the main method to continue running. functional programming is a programming style to express the hierarchy and components of computer code. in this style calculations are treated similarly to treating computations of functions in mathematics. hence memory-states and modifiable data structures are not needed. functional programming can be introduced as a declarative style of coding in the sense that expressions replace programs. on a functional object-oriented model, this paper presents an accurate type system for asynchronous operations. the job of the type system is to stop undefined functions from execution and hence from aborting programs. in other words, the type system ensures soundness of data types and hence avoiding static errors like field-not-defined and method-not-defined from occurring at execution time. the paper introduces as well a programming example for the proposed system.
network_security	with introducing demand response aggregator (dra) in smart paradigm, small customers can actively participate in price and incentive-based demand response programs. this new matter can significantly affect many factors of power system such as transmission network security. accordingly, it is notably useful to define an adjusted framework for transmission expansion planning in smart environment. a long-term market simulation is performed using a tri-level iterative framework to find the best expansion decisions for transmission company (transco) in a pool-based market. the transco 's investment decisions are made by a merchant approach consistent with transmission network security and smart environment. the effects of smart environment on future network configuration, strategic bidding of generators in the market operation, and contingency analysis are considered during planning process. the effectiveness of proposed method is examined on the ieee 24-bus system with one dra, one transco, and two generation companies under the independent system operator 's supervision. copyright (c) 2016 john wiley & sons, ltd.
network_security	session initiation protocol (sip) is an application layer protocol used for signaling purposes to manage voice over ip connections. sip being a text-based protocol is vulnerable to a range of denial of service (dos) attacks. these dos attacks can render the sip servers/sip proxy servers unusable by depleting memory and cpu time. in this paper, we consider two types of dos attacks, namely, flooding attacks and coordinated attacks for detection. flooding attacks affect both stateless and stateful sip servers while coordinated attacks affect stateful sip servers. we model the sip operation as discrete event system (des) and design a new state transition machine, which we name as probabilistic counting deterministic timed automata (pcdta) to describe the behavior of sip operations. we also identify different types of anomalies that can occur in a des model, which appear in the form of illegal transitions, violating timing constraints, and appear in number which is otherwise not seen. subsequently, we map various dos attacks in sip to a type of anomaly in des. pcdta can learn probabilities of various transitions and timings delay from a set of nonmalicious training sequences. a trained pcdta can detect anomalies, and hence various dos attacks in sip. we perform a thorough experiment with computer simulated sip traffic and report the detection performance of pcdta on various attacks generated through custom scripts.
network_security	coordinated intrusion, like ddos, worm outbreak and botnet, is a major threat to network security nowadays and will continue to be a threat in the future. to ensure the internet security, effective detection and mitigation for such attacks are indispensable. in this paper, we propose a novel collaborative intrusion prevention architecture, i.e. cipa, aiming at confronting such coordinated intrusion behavior. cipa is deployed as a virtual network of an artificial neural net over the substrate of networks. taking advantage of the parallel and simple mathematical manipulation of neurons in a neural net, cipa can disperse its lightweight computation power to the programmable switches of the substrate. each programmable switch virtualizes one to several neurons. the whole neural net functions like an integrated ids/ips. this allows cipa to detect distributed attacks on a global view. meanwhile, it does not require high communication and computation overhead. it is scalable and robust. to validate cipa, we have realized a prototype on software-defined networks. we also conducted simulations and experiments. the results demonstrate that cipa is effective. (c) 2016 elsevier ltd. all rights reserved.
network_security	detection of ddos (distributed denial of service) traffic is of great importance for the availability protection of services and other information and communication resources. the research presented in this paper shows the application of artificial neural networks in the development of detection and classification model for three types of ddos attacks and legitimate network traffic. simulation results of developed model showed accuracy of 95.6% in classification of pre-defined classes of traffic.
network_security	with rapidly growing internet traffic, energy efficient operation of ip over wdm networks with sleep enabled routers is of increasing interest. however, for network security and to provide guaranteed communications, it would still be desirable to ensure that a certain fraction of the bandwidth is assured through routers which cannot be put to sleep using software control. this paper presents an energy-minimized ip over wdm network using a mixture of sleep-enabled and non-sleep-enabled router cards where a certain percentage of the network bandwidth is guaranteed to the offered traffic. such a mixed configuration is also motivated by the fact that there will always be some traffic demand between each node pair at any time even though the traffic between node pairs may fluctuate to very low levels. this implies a need for some non-sleeping router cards at any time. another motivation for this mixed configuration is because in the course of migration from today 's networks with non-sleep-enabled cards to future networks with sleep-enabled cards, the non-sleep-enabled network devices will not be quickly abandoned but will be gradually replaced. this also causes a network situation with the mixed router card types. to design an ip over wdm network where both sleep-enabled and non-sleep-enabled router cards are used, we propose a mixed integer linear programming (milp) model which jointly minimizes the energy consumption of all the router cards while guaranteeing a secured fractional bandwidth for all the node pairs. modified milps with subsequent port-channel association are also proposed along with efficient heuristic algorithms which perform almost as well as the joint milp approaches. the performance of these approaches is studied through simulations on a wide variety of networks.
image_processing	in this article we propose a novel framework for the modelling of non-stationary multivariate lattice processes. our approach extends the locally stationary wavelet paradigm into the multivariate two-dimensional setting. as such the framework we develop permits the estimation of a spatially localised spectrum within a channel of interest and, more importantly, a localised cross-covariance which describes the localised coherence between channels. associated estimation theory is also established which demonstrates that this multivariate spatial framework is properly defined and has suitable convergence properties. we also demonstrate how this model-based approach can be successfully used to classify a range of colour textures provided by an industrial collaborator, yielding superior results when compared against current state-of-the-art statistical image processing methods.
image_processing	in animals, self-grooming is an important component of their overall hygiene because it reduces the risk of disease and parasites. the european honey bee (apis mellifera) exhibits hygienic behavior, which refers to the ability of the members of a colony to remove diseased or dead brood from the hive. individual grooming behavior, however, is when a bee grooms itself to remove parasites. while both behaviors are critical for the mitigation of disease, hygienic behavior is overwhelmingly more studied because, unlike grooming behavior, it has a simple bioassay to measure its phenotype. here, we develop a novel bioassay to expedite data collection of grooming behavior by testing different honey bee genotypes (stocks). individual worker bees from different commercial stocks were coated in baking flour, placed in an observation arena, and digitally recorded to automatically measure grooming rates. the videos were analyzed in matlab, and an exponential function was fit to the pixel data to calculate individual grooming rates. while bees from the different commercial stocks were not significantly different in their grooming rates, the automation of grooming measurements may facilitate future research and stock selection for this important mechanism of social immunity. (c) 2017 elsevier b.v. all rights reserved.
image_processing	in wire and arc additive manufacture (waam), the twist of wire during a robot 's movement can result in the sudden changes of the wire-feeding position and thus cause deposition defects and dimensional errors. in the worst case, it may cause wire jamming and damage of the wire-feeding system. therefore, online monitoring and correction of the wire deflection are very important for waam. in this paper, a vision-based measuring method is proposed for detecting the deviations of the wire-feeding position of a plasma welding-based waam process. it uses adaptive threshold and hough transform to extract the wire edges, judges and merges the coincident lines, and applies radon transform to measure the wire deflection. software to automatically detect the wire deviation was developed based on the proposed method. the method and the software were verified with experiments.
image_processing	in the last two decades, we have seen an amazing development of image processing techniques targeted for medical applications. we propose multi-gpu-based parallel real-time algorithms for segmentation and shape-based object detection, aiming at accelerating two medical image processing methods: automated blood detection in wireless capsule endoscopy (wce) images and automated bright lesion detection in retinal fundus images. in the former method we identified segmentation and object detection as being responsible for consuming most of the global processing time. while in the latter, as segmentation was not used, shape-based object detection was the compute-intensive task identified. experimental results show that the accelerated method running on multi-gpu systems for blood detection in wce images is on average 265 times faster than the original cpu version and is able to process 344 frames per second. by applying the multi-gpu framework for bright lesion detection in fundus images we are able to process 62 frames per second with a speedup average 667 times faster than the equivalent cpu version.
image_processing	halftoning and inverse halftoning algorithms are very important image processing tools, widely used in the development of digital printers, scanners, steganography and image authentication systems. because such applications require to obtain high quality gray scale images from its halftone versions, the development of efficient inverse halftoning algorithms, that be able to provide gray scale images with peak signal to noise ratio (psnr) higher than 25, have been research topic during the last several years. although a psnr of about 25db may be enough for several applications, exist several other that require higher image quality. to reduce this problem, this paper proposes inverse halftoning algorithms based on atomic function and multi-layer perceptron neural network which provides gray scale images with psnrs higher than 30db independently of the method used to generate the halftone image.
parallel_computing	new imaging techniques enable visualizing and analyzing a multitude of unknown phenomena in many areas of science at high spatio-temporal resolution. the rapidly growing amount of image data, however, can hardly be analyzed manually and, thus, future research has to focus on automated image analysis methods that allow one to reliably extract the desired information from large-scale multidimensional image data. starting with infrastructural challenges, we present new software tools, validation benchmarks and processing strategies that help coping with large-scale image data. the presented methods are illustrated on typical problems observed in developmental biology that can be answered, e.g., by using time-resolved 3d microscopy images.
parallel_computing	for many tiller crops, the plant architecture (pa), including the plant fresh weight, plant height, number of tillers, tiller angle and stem diameter, significantly affects the grain yield. in this study, we propose a method based on volumetric reconstruction for high-throughput three-dimensional (3d) wheat pa studies. the proposed methodology involves plant volumetric reconstruction from multiple images, plant model processing and phenotypic parameter estimation and analysis. this study was performed on 80 triticum aestivum plants, and the results were analyzed. comparing the automated measurements with manual measurements, the mean absolute percentage error (mape) in the plant height and the plant fresh weight was 2.71% (1.08 cm with an average plant height of 40.07 cm) and 10.06% (1.41 g with an average plant fresh weight of 14.06 g), respectively. the root mean square error (rmse) was 1.37 cm and 1.79 g for the plant height and plant fresh weight, respectively. the correlation coefficients were 0.95 and 0.96 for the plant height and plant fresh weight, respectively. additionally, the proposed methodology, including plant reconstruction, model processing and trait extraction, required only approximately 20 s on average per plant using parallel computing on a graphics processing unit (gpu), demonstrating that the methodology would be valuable for a high-throughput phenotyping platform.
parallel_computing	parallelization is applied to the neutron calculations performed by the heterogeneous method on a graphics processing unit. the parallel algorithm of the modified trec code is described. the efficiency of the parallel algorithm is evaluated.
parallel_computing	agent-based models (abms) are increasingly being used to study population dynamics in complex systems, such as the human immune system. previously, folcik et al. (the basic immune simulator: an agent-based model to study the interactions between innate and adaptive immunity. theor biol med model 2007; 4: 39) developed a basic immune simulator (bis) and implemented it using the recursive porous agent simulation toolkit (repast) abm simulation framework. however, frameworks such as repast are designed to execute serially on central processing units and therefore cannot efficiently handle large model sizes. in this paper, we report on our implementation of the bis using flame gpu, a parallel computing abm simulator designed to execute on graphics processing units. to benchmark our implementation, we simulate the response of the immune system to a viral infection of generic tissue cells. we compared our results with those obtained from the original repast implementation for statistical accuracy. we observe that our implementation has a 13x performance advantage over the original repast implementation.
parallel_computing	we propose a many-core gpu implementation of robotic motion planning formulated as a semi-infinite optimization program. our approach computes the constraints and their gradients in parallel, and feeds the result to a nonlinear optimization solver running on the cpu. to ensure the continuous satisfaction of our constraints, we use polynomial approximations over time intervals. because each constraint and its gradient can be evaluated independently for each time interval, we end up with a highly parallelizable problem that can take advantage of many-core architectures. classic robotic computations (geometry, kinematics, and dynamics) can also benefit from parallel processors, and we carefully study their implementation in our context. this results in having a full constraint evaluator running on the gpu. we present several optimization examples with a humanoid robot. they reveal substantial improvements in terms of computation performance compared to a parallel cpu version.
distributed_computing	as a commercial distributed computing mode, cloud computing needs to meet the quality of service (qos) requirement of users, which is its top priority. however, cloud computing service providers also need to consider how to reduce the overhead of data center, and keep load balancing is one of the key points to maximize the use of the resource in the data center. in this paper, we propose an improved multi-objective niched pareto genetic algorithm (npga) to take load balancing into consideration without affecting performance of time consumption and financial cost of handling the user 's cloud computing tasks by presenting the load balancing shift mutation operator. the simulation results and analysis show that the proposed algorithm performs better than npga in maintaining the diversity and the distribution of the pareto-optimal solutions in the cloud tasks scheduling under the same population size and evolution generation.
distributed_computing	the growing number of scientific computation-intensive applications calls for an efficient utilization of large-scale, potentially interoperable distributed infrastructures. parameter sweep applications represent a large body of workflows. while the principle of workflows is easy to conceive, their execution is very complex and no universally accepted solution exists. in this paper we focus on the resource allocation challenges of parameter study jobs in distributed computing infrastructures. to cope with this np-hard problem and the high uncertainty present in these systems, we propose a series of job allocation models that helps refining and simplifying the problem complexity. in this way we present some special cases that are polynomial and show how more complex scenarios can be reduced to these models. it is known from practice that a small number of job sizes improves the result of job allocation, therefore we state a hypothesis relying on this fact in one of our models. unfortunately, the reduction of the general problem (using k-means clustering) did not help, and thus the hypothesis has proved to be false. in the future, we shall look for clustering techniques which fit this goal better.
distributed_computing	compute-intensive applications have gradually changed focus from massively parallel supercomputers to capacity as a resource obtained on-demand. this is particularly true for the large-scale adoption of cloud computing and mapreduce in industry, while it has been difficult for traditional high-performance computing (hpc) usage in scientific and engineering computing to exploit this type of resources. however, with the strong trend of increasing parallelism rather than faster processors, a growing number of applications target parallelism already on the algorithm level with loosely coupled approaches based on sampling and ensembles. while these cannot trivially be formulated as mapreduce, they are highly amenable to throughput computing. there are many general and powerful frameworks, but in particular for sampling-based algorithms in scientific computing there are some clear advantages from having a platform and scheduler that are highly aware of the underlying physical problem. here, we present how these challenges are addressed with combinations of dataflow programming, peer-to-peer techniques and peer-to-peer networks in the copernicus platform. this allows automation of sampling-focused workflows, task generation, dependency tracking, and not least distributing these to a diverse set of compute resources ranging from supercomputers to clouds and distributed computing (across firewalls and fragile networks). workflows are defined from modules using existing programs, which makes them reusable without programming requirements. the system achieves resiliency by handling node failures transparently with minimal loss of computing time due to checkpointing, and a single server can manage hundreds of thousands of cores e.g. for computational chemistry applications. (c) 2016 the authors. published by elsevier b.v. this is an open access article under the cc by license
distributed_computing	with the increasing demand of machine to machine (m2m) communications and internet of things (iot) services it is necessary to develop a new network architecture and protocols to support cost effective, distributed computing systems. generally, m2m and iot applications serve a large number of intelligent devices, such as sensors and actuators, which are distributed over large geographical areas. to deploy m2m communication and iot sensor nodes in a cost-effective manner over a large geographical area, it is necessary to develop a new network architecture that is cost effective, as well as energy efficient. this paper presents an ieee 802.11 and ieee 802.15.4 standards-based heterogeneous network architecture to support m2m communication services over a wide geographical area. for the proposed heterogeneous network, we developed a new cooperative carrier sense multiple access with collision avoidance (csma/ca) medium access control (mac) protocol to transmit packets using a shared channel in the 2.4 ghz ism band. one of the key problems of the ieee 802.11/802.15.4 heterogeneous network in a dense networking environment is the coexistence problem in which the two protocols interfere with each other causing performance degradation. this paper introduces a cooperative mac protocol that utilizes a new signaling technique known as the blank burst (bb) to avoid the coexistence problem. the proposed mac protocol improves the network qos of m2m area networks. the developed network architecture offers significant energy efficiency, and operational expenditure (opex) and capital expenditure (capex) advantages over 3g/4g cellular standards-based wide area networks.
distributed_computing	linear max-plus systems describe the behavior of a large variety of complex systems. it is known that these systems show a periodic behavior after an initial transient phase. assessment of the length of this transient phase provides important information on complexity measures of such systems, and so is crucial in system design. we identify relevant parameters in a graph representation of these systems and propose a modular strategy to derive new upper bounds on the length of the transient phase. by that we are the first to give asymptotically tight and potentially subquadratic transience bounds. we use our bounds to derive new complexity results, in particular in distributed computing. (c) 2016 elsevier b.v. all rights reserved.
algorithm_design	counting the solution number of combinational optimization problems is an important topic in the study of computational complexity, which is concerned with vertex-cover in this paper. first, we investigate organizations of vertex-cover solution spaces by the underlying connectivity of unfrozen vertices and provide facts on the global and local environment. then, a vertex-cover solution number counting algorithm is proposed and its complexity analysis is provided, the results of which fit very well with the simulations and have a better performance than those by 1-rsb in the neighborhood of c = e for random graphs. based on the algorithm, variation and fluctuation on the solution number the statistics are studied to reveal the evolution mechanism of the solution numbers. furthermore, the marginal probability distributions on the solution space are investigated on both the random graph and scale-free graph to illustrate the different evolution characteristics of their solution spaces. thus, doing solution number counting based on the graph expression of the solution space should be an alternative and meaningful way to study the hardness of np-complete and #p-complete problems and the appropriate algorithm design can help to achieve better approximations of solving combinational optimization problems and the corresponding counting problems.
algorithm_design	mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property-incentive properties are only useful if the strategic agents also believe this fact. verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals. we introduce a relational refinement type system, called hoare(2), for verifying mechanism design and differential privacy. we show that hoare(2) is sound w.r.t. a denotational semantics, and correctly models (epsilon, delta)-differential privacy; moreover, we show that it subsumes dfuzz, an existing linear dependent type system for differential privacy. finally, we develop an smt-based implementation of hoare(2) and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy.
algorithm_design	this paper presents a novel approach for estimating the ego-motion of a vehicle in dynamic and unknown environments using tightly-coupled inertial and visual sensors. to improve the accuracy and robustness, we exploit the combination of point and line features to aid navigation. the mathematical framework is based on trifocal geometry among image triplets, which is simple and unified for point and line features. for the fusion algorithm design, we employ the extended kalman filter (ekf) for error state prediction and covariance propagation, and the sigma point kalman filter (spkf) for robust measurement updating in the presence of high nonlinearities. the outdoor and indoor experiments show that the combination of point and line features improves the estimation accuracy and robustness compared to the algorithm using point features alone.
algorithm_design	target classification algorithms have generally kept pace with developments in the academic and commercial sectors since the 1970s. however, most recently, investment into object classification by internet companies and various human brain projects have far outpaced that of the defense sector. implications are noteworthy. there are some unique characteristics of the military classification problem. target classification is not solely an algorithm design problem, but is part of a larger system design task. the design flows down from a concept of operations (conops) and key performance parameters (kpps). inputs are image and/or signal data and time-synchronized metadata. the operation is real-time. the implementation minimizes size, weight and power (swap). the output must be conveyed to a time-strapped operator who understands the rules of engagement. it is assumed that the adversary is actively trying to defeat recognition. the target list is often mission dependent, not necessarily a closed set, and may change on a daily basis. it is highly desirable to obtain sufficiently comprehensive training and testing data sets, but costs of doing so are very high and data on certain target types are scarce. the training data may not be representative of battlefield conditions suggesting the avoidance of highly tuned designs. a number of traditional and emerging target classification strategies are reviewed in the context of the military target problem.
algorithm_design	it was an important part of the aggregation simulation model building that optimization of the simulation model. in this paper, the concept of the model element correlation was introduced and the reduction algorithm for the model based on the genetic-rough set algorithm was designed. the algorithm design and implementation process were described in this paper, then the correctness of the algorithm was described. at last, applicability of this algorithm was proved by an example.
computer_programming	the usefulness of online and hybrid delivery methods in education has long been realized and with the advancement of computer and communication technologies and the web based authoring tools, their effectiveness have been further extended. we are at a point that online and hybrid course offerings for undergraduates are quickly becoming an integrated and regular part of engineering departments' course offerings. many institutions are offering online courses, as part of their regular schedule. in this paper, based on our experience that started with hybrid delivery and ultimately extended to a full online offering, we will present a set of recommended steps that can be used as a guide by those who are interested in designing an online introductory computer-programming course. we will then present a set of observations that we feel needs to be considered and discussed by the research community. the paper will include examples of students' comments, which were received in teaching evaluations, for hybrid and online offerings.
computer_programming	existing adaptive educational hypermedia systems have been using learning resources sequencing approaches in order to enrich the learning experience. in this context, educational resources, either expository or evaluative, play a central role. however, there is a lack of tools that support sequencing essentially due to the fact that existing specifications are complex. this paper presents seqins as a sequencing tool of digital educational resources. seqins includes a simple and flexible sequencing model that will foster heterogeneous students to learn at different rhythms. the tool communicates through the ims learning tools interoperability specification with a plethora of e-learning systems such as learning management systems, repositories, authoring and automatic evaluation systems. in order to validate seqins we integrate it in an e-learning ensemble framework instance for the computer programming learning domain.
computer_programming	computer programming and related courses account for a large proportion in current information courses. such courses typically include c/c++ language programming, vb language programming, java language programming, c # language programming, data structures and algorithms. there are many common aspects in terms of knowledge points among programming courses. however, currently, the courses are taught separately. furthermore, under the trend of shrinking time allocation and declining number of courses in undergraduate curriculum, it is critical to maximize the use of limited hours of programming courses to enhance teaching effectiveness. based on inquiry-based teaching model in accordance with cdio teaching philosophy, it is proposed in this paper to conduct programming curriculum group construction. therefore, knowledge of program design courses within the group will be shared and courses instruction within the group will be coordinated. knowledge sharing facilitates its application to experiments and projects, which can improve students' comprehensive capabilities, especially their ability to solve practical problems.
computer_programming	combined cycle power plant (ccpp) is one of the most efficient systems of energy conversion with different topping and bottoming cycles. one of the acceptable schemes, the combination of brayton and rankine cycle, is analyzed for various design parameters. in the present analysis thermodynamic modelling of a ccpp with single steam extraction from bottoming rankine cycle is carried out to study the effect of inlet air temperature (tat), cycle ratio (cr), turbine inlet temperature (tit), air compressor and gas turbine efficiency on the first and second law efficiency. for parametric analysis computer programming tool engineering equation solver (ees) is used and thermodynamic properties of many fluids and gases are inbuilt function of the software. from the results it is concluded that combustion chamber is the source of highest exergy destruction followed by heat recovery steam generator, gas turbine, air compressor and steam turbine.,with increase in tit, optimum cr is also found to be increased because both the gas turbine efficiency and the gas turbine exhaust temperature are increased for the optimum cycle ratio.
computer_programming	to implement bilingual teaching is an inevitable choice of higher vocational colleges to be geared to international standards and improve their professional competitive advantages. the practice of bilingual teaching of computer courses has been implemented by the school of information, guangzhou international economics college concerning foreign affairs or foreign nationals. in this paper, the actual problems in the bilingual teaching of computer programming course are emphatically analyzed, and then the related countermeasures are introduced from bilingual course selection, improving students' enthusiasm with mooc and flipped classroom, step-by-step bilingual teaching model, and practice teaching material selection, aiming to promote the development of bilingual teaching of computer courses in higher vocational colleges and the competitive power of students at workplace.
relational_databases	a potential problem for persisting large volume of streaming logs with conventional relational databases is that loading large volume of data logs produced at high rates is not fast enough due to the strong consistency model and high cost of indexing. as a possible alternative, state-of-the-art nosql data stores that sacrifice transactional consistency to achieve higher performance and scalability can be utilized. in this paper, we describe the challenges in large scale persisting and analysis of numerical streaming logs. we propose to develop a benchmark comparing relational databases with state-of-the-art nosql data stores to persist and analyze numerical logs. the benchmark will investigate to what degree a state-of-the-art nosql data store can achieve high performance persisting and large-scale analysis of data logs. the benchmark will serve as basis for investigating query processing and indexing of large-scale numerical logs.
relational_databases	the traditional olap (on-line analytical processing) systems store data in relational databases. unfortunately, it is difficult to manage big data volumes with such systems. as an alternative, nosql systems (not-only sql) provide scalability and flexibility for an olap system. we define a set of rules to map star schemas and its optimization structure, a precomputed aggregate lattice, into two logical nosql models: column-oriented and document-oriented. using these rules we analyse and implement two decision support systems, one for each model (using mongodb and hbase). we compare both systems during the phases of data (generated using the tpc-ds benchmark) loading, lattice generation and querying.
relational_databases	referential integrity is one of the three inherent integrity rules and can be enforced in databases using foreign keys. however, in many real world applications referential integrity is not enforced since foreign keys remain disabled to ease data acquisition. important applications such as anomaly detection, data integration, data modeling, indexing, reverse engineering, schema design, and query optimization all benefit from the discovery of foreign keys. therefore, the profiling of foreign keys from dirty data is an important yet challenging task. we raise the challenge further by diverting from previous research in which null markers have been ignored. we propose algorithms for profiling unary and multi-column foreign keys in the real world, that is, under the different semantics for null markers of the sql standard. while state of the art algorithms perform well in the absence of null markers, it is shown that they perform poorly in their presence. extensive experiments demonstrate that our algorithms perform as well in the real world as state of the art algorithms perform in the idealized special case where null markers are ignored.
relational_databases	in recent years, the application of laser point cloud data has increased dramatically. how to efficiently store and fast process the data becomes an important research direction at present. point cloud data contain a wealth of geographic information, belonging to the category of spatial data. traditional relational databases are relatively weak in massive spatial data storage and processing, while the application of non-relational databases provides a new perspective of study for this fact. sharding technology is a solution for database level extension. in this thesis, sharding cluster for mongodb is established under distributed environment, while distributed storage, spatial query and mapreduce operation test for numerous laser-point cloud data will be implemented through scope sharding and hash-based sharding, which completely reflects huge advantages of mongodb under distribution in storage and processing for spatial data.
relational_databases	the article deals with semantic data model and its application in data integration. it analyses the issue of mapping database schemes (particularly relational models) for common data models expressed in the form of ontology. substantial part is dedicated to methods of acquiring ontology from relational databases, where rules for creating classes, properties, hierarchies, cardinalities and instances are specified.
software_engineering	programming-specific q&a sites (e.g., stack overflow) are being used extensively by software developers for knowledge sharing and acquisition. due to the cross-reference of questions and answers (note that users also reference urls external to the q&a site. in this paper, url sharing refers to internal urls within the q&a site, unless otherwise stated), knowledge is diffused in the q&a site, forming a large knowledge network. in stack overflow, why do developers share urls? how is the community feedback to the knowledge being shared? what are the unique topological and semantic properties of the resulting knowledge network in stack overflow? has this knowledge network become stable? if so, how does it reach to stability? answering these questions can help the software engineering community better understand the knowledge diffusion process in programming-specific q&a sites like stack overflow, thereby enabling more effective knowledge sharing, knowledge use, and knowledge representation and search in the community. previous work has focused on analyzing user activities in q&a sites or mining the textual content of these sites. in this article, we present a methodology to analyze url sharing activities in stack overflow. we use open coding method to analyze why users share urls in stack overflow, and develop a set of quantitative analysis methods to study the structural and dynamic properties of the emergent knowledge network in stack overflow. we also identify system designs, community norms, and social behavior theories that help explain our empirical findings. through this study, we obtain an in-depth understanding of the knowledge diffusion process in stack overflow and expose the implications of url sharing behavior for q&a site design, developers who use crowdsourced knowledge in stack overflow, and future research on knowledge representation and search.
software_engineering	many automated finite state machine (fsm) based test generation algorithms require that a characterising set or a set of harmonised state identifiers is first produced. the only previously published algorithms for partial fsms were brute-force algorithms with exponential worst case time complexity. this paper presents polynomial time algorithms and also massively parallel implementations of both the polynomial time algorithms and the brute-force algorithms. in the experiments the parallel algorithms scaled better than the sequential algorithms and took much less time. interestingly, while the parallel version of the polynomial time algorithm was fastest for most sizes of fsms, the parallel version of the brute-force algorithm scaled better due to lower memory requirements.
software_engineering	modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. however, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). in this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: pluss and msvcm. the first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. after evaluating these approaches through the specifications of several systems, we find out that msvcm reduces feature scattering and improves scenario cohesion. these results suggest that evolving a product line specification using msvcm requires only localized changes. on the other hand, the results of six experiments reveal that msvcm requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.
software_engineering	online genetic improvement embeds the ability to evolve and adapt inside a target software system enabling it to improve at runtime without any external dependencies or human intervention. we recently developed a general purpose tool enabling online genetic improvement in software systems running on the java virtual machine. this tool, dubbed ecselr, is embedded inside extant software systems at runtime, enabling such systems to self-improve and adapt autonomously online. we present this tool, describing its architecture and focusing on its design choices and possible uses.
software_engineering	requirement prioritization are considered crucial towards the development of successful and high quality software. nowadays, most software development project and people in software industries are confronted with the challenge of implementing a large scale, complex and dynamic software applications. hence, the need to use a requirement prioritization technique which able to deliver a right set of requirement seems very crucial. the available techniques still suffer from several weaknesses. this motivate us to identify comprehensively what are the limitations and future directions of the existing requirement prioritization techniques. in this study, we conducted an in-depth software engineering based systematic literature review, as it helps to execute a thorough and fair literature review due to its predefined review protocol. we gathered a list of 55 requirement prioritization techniques and investigate the limitation and weaknesses each one of this technique. findings from this study later will be used to design a new effective and efficient requirement prioritization technique.
bioinformatics	background: micrornas (mirnas) are a class of small non-coding rnas that are strongly involved in various types of carcinogenesis, including hepatocellular carcinoma (hcc). this study aimed to clarify whether mir-4417 promotes hcc growth by targeting trim35 and regulating pkm2 phosphorylation. material/methods: online software, including targetscan and miranda, was used to predict the potential target of mir-4417. real-time pcr (qrt-pcr) and western blot assays were performed to detect the expression levels of mrna and protein, respectively. cell proliferation was measured by mtt assay and apoptosis in a549 cells was examined by flow cytometry. results: bioinformatics reveal that trim35 mrna contains 1 conserved target site of mir-4417. high level of mir-4417 and low levels of trim35 mrna and protein were observed in hcc cells compared with a normal liver cell line. biological function analysis showed that mir-4417 inhibitor inhibits cell proliferation and promotes apoptosis in hcc cells. furthermore, we verified that trim35 is a functional target of mir-4417 by use of luciferase reporter assay, and trim35 overexpressing showed an elevation of proliferation and a reduction of apoptosis in hcc cells. we subsequently investigated whether mir-4417 and trim35 regulate hcc cell proliferation and apoptosis through pkm2 y105 phosphorylation, and the results supported our speculation that mir-4417 targets trim35 and regulates the y105 phosphorylation of pkm2 to promote hepatocarcinogenesis. conclusions: our findings indicate that mir-4417 may function as an oncogene in hcc and is a potential alternative therapeutic target for this deadly disease.
bioinformatics	antimicrobial peptides (amps) from cuticular extracts of worker ants of trichomyrmex criniceps (mayr, hymenoptera: formicidae) were isolated and evaluated for their antimicrobial activity. eight peptides ranging in mass from 804.42 to 1541.04 da were characterized using a combination of analytical and bioinformatics approach. all the eight peptides were novel with no similarity to any of the amps archived in the antimicrobial peptide database. two of the eight novel peptides, the smallest and the largest by mass were named crinicepsin-1 and crinicepsin-2 and were chemically synthesized by solid phase peptide synthesis. the two synthetic peptides had antibacterial and weak hemolytic activity.
bioinformatics	background: bacteria present in cave often survive by modifying their metabolic pathway or other mechanism. understanding these adopted bacteria and their survival strategy inside the cave is an important aspect of microbial ecology. present study focuses on the bacterial community and geochemistry in five caves of mizoram, northeast india. the objective of this study was to explore the taxonomic composition and presumed functional diversity of cave sediment metagenomes using paired end illumina sequencing using v3 region of 16s rrna gene and bioinformatics pipeline. results: actinobacteria, proteobacteria, verrucomicrobia and acidobacteria were the major phyla in all the five cave sediment samples. among the five caves the highest diversity is found in lamsialpuk with a shannon index 12.5 and the lowest in bukpuk (shannon index 8.22). in addition, imputed metagenomic approach was used to predict the functional role of microbial community in biogeochemical cycling in the cave environments. functional module showed high representation of genes involved in amino acid metabolism in (20.9%) and carbohydrate metabolism (20.4%) in the kegg pathways. genes responsible for carbon degradation, carbon fixation, methane metabolism, nitrification, nitrate reduction and ammonia assimilation were also predicted in the present study. conclusion: the cave sediments of the biodiversity hotspot region possessing a oligotrophic environment harbours high phylogenetic diversity dominated by actinobacteria and proteobacteria. among the geochemical factors, ferric oxide was correlated with increased microbial diversity. in-silico analysis detected genes involved in carbon, nitrogen, methane metabolism and complex metabolic pathways responsible for the survival of the bacterial community in nutrient limited cave environments. present study with paired end illumina sequencing along with bioinformatics analysis revealed the essential ecological role of the cave bacterial communities. these results will be useful in documenting the biospeleology of this region and systematic understanding of bacterial communities in natural sediment environments as well.
bioinformatics	background: ascending thoracic aortic aneurysm (ataa) is a major cause of morbidity and mortality worldwide. the pathogenesis of medial degeneration of the aorta remains undefined. high-throughput secretome analysis by mass spectrometry may be useful to elucidate the molecular mechanisms involved in aneurysm formation as well as to identify biomarkers for early diagnosis or targets of therapy. the purpose of the present study was to analyze the secreted/released proteins from ataa specimens of both tricuspid aortic valve (tav) and bicuspid aortic valve (bav) patients. methods: aortic specimens were collected from patients undergoing elective surgery and requiring graft replacement of the ascending aorta. each sample of the ascending aortic aneurysm, 4 bav (3 males; aged 53.5 +/- 11.4 years) and 4 tav (1 male; 78 +/- 7.5 years), was incubated for 24 h in serum-free medium. released proteins were digested with trypsin. peptide mixtures were fractioned by nano-high performance liquid chromatography and analyzed by mass spectrometry. following identification of differentially expressed proteins, quantitative real time polymerase chain reaction (qrt-pcr) analysis was performed. results: the comparison between the proteins released from bav and tav aneurysmatic tissues showed significantly diverging expression fingerprints in the two groups of patients. bioinformatics analysis revealed 38 differentially released proteins; in particular 7 proteins were down-regulated while 31 were up-regulated in bav with respect to tav. most of the proteins that were up-released in bav were related to the activation of transforming growth factor (tgf)-beta signaling. latent tgf-beta binding protein 4 (ltbp4) exhibited one of the highest significant under-expressions (10-fold change) in bav secretomes with respect to tav. qrt-pcr analysis validated this significant difference at ltbp4 gene level (bav: 1.03 +/- 0.9 vs tav: 3.6 +/- 3.2; p < 0.05). conclusion: hypothesis-free secretome profiling clearly showed diverging expression fingerprints in the ataa of tav and bav patients, confirming the crucial role of tgf-beta signaling in modulating ataa development in bicuspid patients. (c) 2016 japanese college of cardiology. published by elsevier ltd. all rights reserved.
bioinformatics	the current study aimed to devise eco-friendly, safe, and cost-effective strategies for enhanced degradation of low- and high-density polyethylene (ldpe and hdpe) using newly formulated thermophilic microbial consortia from cow dung and to assess the biodegradation end products. the plastic-degrading bacteria from cow dung samples gathered from highly plastic-acclimated environments were enriched by standard protocols. the degradation ability was comprehended by zone of clearance method, and the percentage of degradation was monitored by weight reduction process. the best isolates were characterized by standard microbiological and molecular biology protocols. the best isolates were employed to form several combinations of microbial consortia, and the degradation end products were analyzed. the stability of 16s ribosomal dna (rdna) was predicted by bioinformatics approach. this study identified 75 +/- 2, 55 +/- 2, 60 +/- 3, and 43 +/- 3% degradation for ldpe strips, pellets, hdpe strips, and pellets, respectively, for a period of 120 days (p < 0.05) at 55 degrees c by the formulated consortia of is1-is4, and the degradation efficiency was found to be better in comparison with other formulations. the end product analysis by fourier transform infrared, scanning electron microscopy, energy-dispersive spectroscopy, and nuclear magnetic resonance showed major structural changes and formation of bacterial biofilm on plastic surfaces. these novel isolates were designated as bacillus vallismortis bt-dsce01, psuedomonas protegens bt-dsce02, stenotrophomonas sp. bt-dsce03, and paenibacillus sp.bt-dsce04 by 16s rdna sequencing and suggested good gene stability with minimum gibb 's free energy. therefore, this study imparts substantial information regarding the utilization of these thermophilic microbial consortia from cow dung for rapid polyethylene removal.
cryptography	substitution-boxes, or simply s-boxes, are used to increase confidentiality in the substitution stage of most cryptosystem approaches. in this paper, an efficient method for the construction of an s-box based on a sine map is proposed. the proposed s-box is able to generate random integer sequences with highly efficient nonlinearity in the generated values. the cryptographic analyses show that the proposed s-box method is of high performance and can be used with great potential for prominent prevalence in designing symmetric cryptosystems and copy right protection. (c) 2016 published by elsevier gmbh.
cryptography	for a large class of functions to the group of points of an elliptic curve (typically obtained from certain algebraic correspondences between e and ), farashahi et al. (math comput 82(281):491-512, 2013) established that the map is regular, in the sense that for a uniformly random choice of , the elliptic curve point is close to uniformly distributed in . this result has several applications in cryptography, mainly to the construction of elliptic curve-valued hash functions and to the ""elligator squared"" technique by tibouchi (in: christin and safavi-naini (eds) financial cryptography. lncs, vol 8437, pp 139-156. springer, heidelberg, 2014) for representating uniform points on elliptic curves as close to uniform bitstrings. in this paper, we improve upon farashahi et al. 's character sum estimates in two ways: we show that regularity can also be obtained for a function of the form where g has a much smaller domain than , and we prove that the functions f considered by farashahi et al. also satisfy requisite bounds when restricted to large intervals inside . these improved estimates can be used to obtain more efficient hash function constructions, as well as much shorter ""elligator squared"" bitstring representations.
cryptography	safer is a family of block ciphers, which is comprised of safer k, safer sk, safer+ and safer++. safer sk was proposed to strengthen the key schedule of safer k. safer+ was designed as an aes candidate and safer++ was among the cryptographic primitives selected for the second phase of the nessie project. this paper presented the first zero-correlation linear cryptanalytic attack against the safer block cipher family. we investigated the linear properties of pht employed as the linear layer of the safer block ciphers, and identified zero-correlation linear approximations for safer sk, safer+ and safer++. moreover, we displayed several characterizations of the undisturbed bits, and found that there exists an undisturbed bit in the exponential s-box, which can be applied to reduce the computational complexity in the key recovery attacks on 5 rounds of safer sk/128 and 4(5) rounds of safer+/128(256), 5(6) rounds of safer++/128(256). more rounds of the safer block ciphers can be attacked with the linear relations of correlation zero.
cryptography	session initiation protocol (sip) has proved to be the integral part and parcel of any multimedia based application or ip-based telephony service that requires signaling. sip supports http digest based authentication, and is responsible for creating, maintaining and terminating sessions. to guarantee secure sip based communication, a number of authentication schemes are proposed, typically most of these are based on smart card due to its temper resistance property. recently zhang et al. presented an authenticated key agreement scheme for sip based on elliptic curve cryptography. however tu et al. (peer to peer netw. appl 1-8, 2014) finds their scheme to be insecure against user impersonation attack, furthermore they presented an improved scheme and claimed it to be secure against all known attacks. very recently farash (peer to peer netw. appl 1-10, 2014) points out that tu et al. 's scheme is vulnerable to server impersonation attack, farash also proposed an improvement on tu et al. 's scheme. however, our analysis in this paper shows that tu et al. 's scheme is insecure against server impersonation attack. further both tu et al. 's scheme and farash 's improvement do not protect user 's privacy and are vulnerable to replay and denial of services attacks. in order to cope with these limitations, we have proposed a privacy preserving improved authentication scheme based on ecc. the proposed scheme provides mutual authentication as well as resists all known attacks as mentioned by tu et al. and farash.
cryptography	since substitution box (s-box) is the only nonlinear component related to confusion properties for many block encryption algorithms, it is a necessity for the strong block encryption algorithms. s-box is a vital component in cryptography due to having the effect on the security of entire system. therefore, alternative s-box construction techniques have been proposed in many researches. in this study, a new s-box construction method based on fractional-order (fo) chaotic chen system is presented. in order to achieve that goal, numerical results of the fo chaotic chen system for a = 35, b = 3, c = 28 and alpha = 0.9 are obtained by employing the predictor-corrector scheme. besides, a simpler algorithm is suggested for the construction of s-box via time response of the fo chaotic chen system. the performance of suggested s-box design is compared with other s-box designs developed by chaotic systems, and it is observed that this method provides a stronger s-box design.
structured_storage	geodynamics simulations are characterized by theological nonlinearity, localization, three-dimensional effects, and separate but interacting length scales. these features represent a challenge for computational science. we discuss how a leading software framework for advanced scientific computing (the portable extensible toolkit for scientific computation, petsc) can facilitate the development of geodynamics simulations. to illustrate our use of petsc, we describe simulations of (i) steady-state, non-newtonian passive flow and thermal structure beneath a mid-ocean ridge, (ii) magmatic solitary waves in the mantle, and (iii) the formation of localized bands of high porosity in a two-phase medium being deformed under simple shear. we highlight two supplementary features of petsc, structured storage of application parameters and self-documenting output, that are especially useful for geodynamics simulations. (c) 2007 elsevier b.v. all rights reserved.
structured_storage	the atlas experiment at the lhc collects billions of events each data-taking year, and processes them to make them available for physics analysis in several different formats. an even larger amount of events is in addition simulated according to physics and detector models and then reconstructed and analysed to be compared to real events. the eventindex is a catalogue of all events in each production stage; it includes for each event a few identification parameters, some basic non-mutable information coming from the online system, and the references to the files that contain the event in each format (plus the internal pointers to the event within each file for quick retrieval). each eventindex record is logically simple but the system has to hold many tens of billions of records, all equally important. the hadoop technology was selected at the start of the eventindex project development in 2012 and proved to be robust and flexible to accommodate this kind of information; both the insertion and query response times are acceptable for the continuous and automatic operation that started in spring 2015. this paper describes the eventindex data input and organisation in hadoop and explains the operational challenges that were overcome in order to achieve the expected performance.
structured_storage	engineering designs are typically constrained by requirements and specifications. the design analogy performance parameters system (d-apps) project seeks to provide engineers with a means to transform these product requirements and specifications into functional analogies. d-apps employs design by-analogy (dba) via critical functionality and the engineering performance parameters from the specifications to produce functional alternative options to design engineers. repositories, or databases, provide structured storage of information for retrieval by users in an systemic manner. d-apps utilizes a repository to store analogy entries, capable of being polled by the developed algorithm. analogous matches are formulated with the weighting scheme of the algorithm. this research investigates the structures of three alternative repositories, and the current d-apps database with its interface to the algorithm and design engineer. the architecture of the repository is discussed with example entries and rationale for inclusion.
structured_storage	hydrological models are always related to time and spatial domains, so the model results produced by these models are very large. microsoft component structured storage can be employed to save the model results, but it is lack of mechanism to reduce the data size. in order to tackle this situation, compressed structured storage method is introduced which based on combining component structured storage and zlib compression library. in this method, standard component rules are complied and containment as most common mechanism for object reuse in com is applied so as to simplify the usage.
structured_storage	due to the increasing complexity of the surgical working environment, increasingly technical solutions must be found to help relieve the surgeon. this objective is supported by a structured storage concept for all relevant device data. in this work, we present a concept and prototype development of a storage system to address intraoperative medical data. the requirements of such a system are described, and solutions for data transfer, processing, and storage are presented. in a subsequent study, a prototype based on the presented concept is tested for correct and complete data transmission and storage and for the ability to record a complete neurosurgical intervention with low processing latencies. in the final section, several applications for the presented data recorder are shown. the developed system based on the presented concept is able to store the generated data correctly, completely, and quickly enough even if much more data than expected are sent during a surgical intervention. the surgical data recorder supports automatic recognition of the interventional situation by providing a centralized data storage and access interface to the or communication bus. in the future, further data acquisition technologies should be integrated. therefore, additional interfaces must be developed. the data generated by these devices and technologies should also be stored in or referenced by the surgical data recorder to support the analysis of the or situation.
electric_motor	in this paper, an electric bicycle motor design study is presented according to the related standard. fundamental dimensions and characteristics are kept within some certain boundaries by means of motor rated power and physical constraints arising from standard bicycle sizes available on the market. slot/pole ratios and winding configurations are investigated comprehensively in order to obtain a proper design of electric motor due to the facts of pedal assisted electric bicycle application. after defining the motor parameters and the materials that are used the motor design an optimisation attempts are undertaken by using an analytical solver which is employing both numerical calculations and finite element method (fem) analyses. motor designs are assessed based on torque production, weigh, efficiency and ease-of-manufacture. a concise experimental study is conducted for prototype motor in order to evaluate validity of design study.
electric_motor	because of their improved magnetic properties, fe-si alloys are widely used for new electric motor generations. the use of punching process to obtain these components specially affects their mechanical behavior and fatigue strength. this work aims at studying the influence of punching operations on the fatigue behavior of a fe-si alloy. high cycle fatigue tests are performed on different smooth specimen configurations with either punched or polished edges. results show a significant decrease of the fatigue strength for punched specimens compared to polished ones. to understand the origin of the fatigue failure on punched specimens, sem observations of the fracture surfaces are carried out. they reveal that crack initiation always occurs on a punch defect. additional experimental techniques are combined to characterize how the edges are altered by punching. the impact of punching operations on residual stresses and hardening is then investigated. residual stresses are quantified on punched edges using x-ray diffraction techniques. important tensile residual stresses exist in the loading direction as a result of punching operations. also, according to xrd analyses and micro-hardness measurements, the hardened zone depth is about 200 mu m. to dissociate the respective influences of strain hardening, residual stresses and geometrical defects, a heat treatment is applied to both punched and polished specimens in order to quantify the contribution of each parameter to the high cycle fatigue resistance. results show that the geometry of defects is one of the most influent parameters. consequently, a finite element model is developed to simulate the influence of edge defects on the fatigue strength of punched components. a non-local high cycle fatigue criterion is finally used as post-processing of fea to consider the effect of defects and the associated stress-strain gradients in the hcf strength assessment. copyright (c) 2016 the authors. published by elsevier b.v.
electric_motor	this paper presents the ist rely project which aims at integrating satellite digital radio broadcasting (s-db), terrestrial cellular technology and egnos satellite navigation.
electric_motor	we aim at finding an optimal design for an interior permanent magnet electric motor by means of a sensitivity-based topology optimization method. the gradient-based on/off method has been successfully applied to optimization problems of this form. we show that this method can be improved by considering the mathematical concept of topological derivatives (tds). tds for optimization problems constrained by linear partial differential equations (pdes) are well understood, whereas little is known about tds in combination with nonlinear pde constraints. we derive the td for an optimization problem constrained by the equation of nonlinear 2-d magnetostatics, illustrate its advantages over the sensitivities used in the on/off method, and show numerical results for the optimization of an interior permanent magnet electric motor obtained by a level-set algorithm, which is based on the td.
electric_motor	in this paper the optimization procedure of a salient pole synchronous motor (spsm) for low rotational speed (lrs) marine electric propulsion systems (meps) is presented involving preliminary and critical design stages. the objective function introduced for motor critical design includes the construction, operating and maintenance costs of the motor. the proposed methodology includes sensitivity analysis and pareto front investigation techniques.
digital_control	high-performance phase-locked loops (plls) are critical for power control in grid-connected systems. this paper presents a new method of designing a pll for single-phase systems based on derivative elements (des). the quadrature signal generator (qsg) is constructed by two des with the same parameters. the pll itself is realized by using the de-based qsg. it avoids errors due to the overlap and accumulation that are present in plls based on integral elements, such as a pll based on a second-order generalized integrator. additionally, frequency feedback is not needed which allows the proposed pll to achieve high performance when the grid frequency changes rapidly. this paper presents the model of the pll and a theoretical performance analysis with respect to both the frequency-domain and time-domain behavior. the error arising from the discretization process is also compensated, ensuring this pll method is suitable for implementation in a digital control system. simulation and experimental results show that the proposed pll achieves good performance in both harmonic rejection and dynamic response.
digital_control	silicon rf power amplifiers (pas) are in various rf front end modules (fems) today for handset and wlan applications. even though iii-v semiconductor-based rf pas can still offer superior frequency and breakdown performance with higher pout and power-added-efficiency (pae) and faster time-to-market, silicon-based rf pas do have the advantages in offering higher monolithic integration with added functionalities (e.g., on-chip digital control and selection on power level, modulation, frequency band, matching, predistortion, etc.), which can translate to lower cost and smaller sizes attractive for broadband multi-mode multi-band handset transmitters. therefore, some key techniques for designing high-efficiency 4g/5g/wlan broadband wireless silicon pas will be discussed.
digital_control	loudspeakers line arrays are very common reproduction sound systems used in real environments (e.g., theaters, stadiums, cinemas, and conference halls) for synthesizing the directivity characteristics of a linear source. the sound field emitted by a vertical line array can be pointed in a particular direction by using digital signal processing techniques and avoiding any mechanical movements. the approach proposed in this paper to obtain the desired directivity behavior takes advantage of the reproduction of a virtual source with frequency-independent directivity characteristics using wave field synthesis theory and introducing a computational complexity reduction. a full investigation on the performance of the technique has been carried out making comparisons with other approaches proposed in the literature thus providing an overview of the results achievable with digital steering of line arrays. (c) 2015 elsevier ltd. all rights reserved.
digital_control	the extraction of molten iron and slag in the liquid phase from the lower part of a blast furnace (hearth) is usually accomplished according to operational experience and involves a high degree of uncertainty, mainly because the liquid level cannot be directly measured. this study presents a methodology for obtaining multistep models to forecast the hearth liquid level by measuring a voltage generated on the blast furnace shell, which is strongly correlated with the hearth liquid level. the results show that this electrical signal is a nonstationary and nonlinear time-series that, after appropriate treatment, can be represented by a time-delay neural network (tdnn) model. some comparisons are made with linear time-series models represented by an autoregressive moving average model and a seasonal autoregressive integrated moving average model, and the results indicate that the tdnn model provides better forecasting performance up to one hour ahead. note to practitioners-this work was motivated by the need for better knowledge regarding the liquid level in a blast furnace hearth because this information affects the strategy of the opening and closing of tapholes in the blast furnace and, consequently, the production control and operational quality. due to the difficulties of measuring the liquid level in the hearth directly, a system was installed that uses a voltage generated in the hearth shell as a liquid-level sensor in the hearth. in this study, the analysis and treatment of this signal is performed by achieving a stationary, nonlinear signal strongly correlated with the level of molten iron and slag inside the blast furnace hearth. a mathematical model that represents this signal was developed and implemented online in the blast furnace digital control system to enable forecasting of the liquid level up to one hour ahead. this computational tool aids operators and engineers in deciding in advance the instants to open or close the tapholes, thereby increasing safety and financial gains.
digital_control	wavelength stabilization for a pulsed laser presents more challenges than that of continuous wave laser. we have developed a simple and efficient long-term wavelength drifts compensation technique for tunable pulsed dye lasers (pdl) applied in sodium detection lidar system. wavelength calibration and locking are implemented by using optogalvanic (og) spectroscopy in a na hollow cathode lamp (hcl) in conjunction with a digital control software. optimization of og signals for better laser wavelength discrimination and feedback control is performed. test results indicate that locking the multimode broadband pdl to the na atomic transition corresponding to 589.158 nm is well achieved although the temperature in the laboratory is unstable. through active compensation, the maximum wavelength drift is reduced from over 5 pm to 0.42 pm in 10 h and the maximum wavelength drift rate of the pdl is improved from 3.3 pm/h to 0.3 pm/h it has been used to efficient sodium resonance fluorescence lidar detection. this technique is economical and easy to implement, and it provides flexible wavelength control and allows generalization for some other applications which require the wavelength of tunable pulsed lasers to be fixed at an atomic resonance transition references. (c) 2015 elsevier b.v. all rights reserved,
microcontroller	distributed maximum power point tracking (mppt) photovoltaic (pv) systems have drawn increased attention since the inhomogeneous irradiance on pv modules can significantly deteriorate the effectiveness of solar energy harvest. to address the issue, this paper focuses on developing a highly efficient submodule integrated converter (submic), which is connected across each submodule of a pv panel. gallium nitride (gan) field-effect transistors are utilized in the synchronous buck converter topology to achieve high conversion efficiency and reduced size. a converter prototype, which is digitally controlled using a microcontroller for mppt, is developed and tested, considering various irradiation conditions. the peak efficiency of the proposed gan-based submic is measured to be higher than 99% at 400-khz switching frequency, whereas the accuracy of mppt is above 99% considering low-power conditions.
microcontroller	this application note introduces an arduino-based led system useful for cultivation of various microalgae species as well as other plants. the system is based on rgb (apa102c) leds connected to an external arduino microcontroller allowing flexibility via programming, modification, and upgrades. we describe in detail the c functions that produce white light and mixed colors in led strips as well as the ability to generate intermittent/flashing/pulsing light. the capabilities of the system offer unique applications in industry and research. our aim is to provide a low-cost and open-source tool in order to improve and promote cultivation of photoautotroph species (bacteria, microalgae or plants) using leds. (c) 2016 elsevier b.v. all rights reserved.
microcontroller	in wireless data transmissions processes, data loss is an important factor that reduces the robustness of wireless sensor networks (wsns). in many practical engineering applications, data loss compensation algorithms are then required to maintain the robustness, and such algorithms are typically based on compressive sensing with a large memory of microcontrollers needed. this paper presents an improved algorithm, based on random demodulator, to overcome the difficulty of microcontroller-dependence in the traditional data loss algorithms. the newly developed algorithm demonstrates the following advantages: 1) low space complexity; 2) low floating-point calculations; and 3) low time complexity. therefore, it is more suitable to be embedded into ordinary nodes, comparing with the traditional algorithms. in this paper, a wsn based on wifi is also developed for verifying the effectiveness and feasibility of the proposed algorithm. field experiment on xinghai bay bridge is done. experimental results show that the wsn works properly and steady. moreover, the data loss can be compensated effectively and efficiently through the use of the present algorithm.
microcontroller	a compact 24-ghz doppler radar module is developed in this paper for non-contact human vital-sign detection. the 24-ghz radar transceiver chip, transmitting and receiving antennas, baseband circuits, microcontroller, and bluetooth transmission module have been integrated and implemented on a printed circuit board. for a measurement range of 1.5 m, the developed radar module can successfully detect the respiration and heartbeat of a human adult.
microcontroller	processes like landslides, debris flows, or bed load transport, at the intersection between the natural environment and human activity, constitute an increasing threat to people and property. the ability to detect these processes prematurely is an essential task for mitigating these hazards. past studies have shown that debris flows and debris floods emit detectable signals in the low-frequency infrasonic spectrum and induce characteristically seismic signals. a number of monitoring devices and detection methods to identify debris flows using these signals have been developed, but up to date, no warning system based on a combination of seismic and infrasound sensors has been considered. previous studies have already shown that seismic and infrasonic signals of alpine mass movements are correlated and complementary and that the combination of these two sensor types can serve as basis for an error-resistant detection and warning system. so this work aims to develop a detection system for detecting debris flows and debris floods by analyzing the seismic and infrasound waves. the system is build up on a minimum of one seismic and one infrasound sensor which are co-located and a microcontroller which runs a detection algorithm to detect debris flows and debris floods with high accuracy in real time directly on-site. the detection algorithm is based on an analysis of the evolution in time of the frequency content of the mass movement signal and has been tested with debris flows and debris flood signals monitored at different test sites in austria and switzerland. this paper describes the current version of the detection system and gives an example of event detection at the tyrolese test sites lattenbach, dristenau, and farstrinne.
electrical_network	the trend towards electrical drivers rather than mechanical drivers for offshore installation has led to implementation of large electrical systems and power generation. due to the offshore environment constraints such as weight and footprint limitations, most of the installation remains operated at voltage of 13,8 kv and lower which leads to specific constraints on the electrical network design. this paper provides an overview of the implementation of pyrotechnic current limiting devices from the design stage through precommissioning, commissioning and up to operation. a case study will also be developed based on fault event which occurred on a fpso (floating production storage and offloading) leading to severe damage and its consequences on operation conditions. the pyrotechnic current limiting devices operated correctly when the fault occurred. the damage and production losses were due to other causes which will be highlighted.
electrical_network	load flow studies are an essential task carried out in power system planning and operations. however, the widely used deterministic load flow analysis is limited in their handling of network uncertainties or inaccuracies in input data. therefore, probabilistic load flow (plf) based on either numerical methods (such as monte carlo simulation) or an analytical method (such as convolution techniques) was developed in the 1970s to handle power system uncertainties due to variations in electrical network variables. this paper presents a probabilistic load flow analysis method based on convolution techniques. the method is suitable for distribution systems and examines the effect of load, generation and network uncertainties either separately or in combinations thereof. the main features on the paper include: a critical appraisal of existing plf techniques, as published in the literature, is carried out to derive the optimised technique and methodology. the proposed method is applied to practical 47-bus radial distribution network modelled in powerfactory digsilent software package. the results obtained are then exported into matlab for detailed statistical analysis in terms of various probability distribution function (pdf) and cumulative density functions (cdf).
electrical_network	catastrophic circumstances and equipment failures in electrical networks are mostly caused by emerging unidentified phenomena. resonance and ferroresonance are those phenomena, which have been investigated since, many years ago. manitoba hydro 230 kv electrical network has experienced ferroresonant states several times. such conditions may occur in effect of short circuit, breaker phase failure, transformer energizing, load rejection, accidental or scheduled line disconnection, and plant outage. one of the significant consequences of resonant and ferroresonant states can be an apparent mis-operation or tripping some protective devices and inaccurate operation of instrument transformers. in this paper by means of pscad/emtdc simulation software, ferroresonant states are analyzed in manitoba hydro 230 kv network. ferroresonant states are classified in to adequate modes by ferro-resonance detection tools, furthermore; kinds of protective relays subjected to ferroresonance are simulated in the power network, and operation of relays is assessed.
electrical_network	the isotactic polypropylene/carbon black (ipp/cb) and the long-chain branched polypropylene/carbon black (lcbpp/cb) composite melts with the melt blending method and the solution process were chosen in this paper to know the relationship between rheological and electrical percolation process and learn the evolution and the destruction of rheological network. the more rheological percolation threshold than electrical percolation threshold in ipp/cb composites and the less rheological percolation threshold than electrical percolation threshold in lcbpp/cb composites are mainly attributed to the two kinds of mechanisms governing the electrical network and the rheological network. the agglomeration of cb particles which is accelerated by annealing at elevated temperatures promotes the self-perfection of rheological network. the strong interaction between the polar long-chain branched structure and filler also led to the reduced t(p). the network of lcbpp/cb composites is more difficult to be broken than the network of ipp/cb composites with the solution process.
electrical_network	the quality and performances of the compensation of harmonic currents depends strongly on the performances of the identification blocks of control side of the photovoltaic generators used as active filters. then, the use of harmonics identification methods is not valid because the network voltage must be sinusoidal and balanced, which is not the case in practice. hence, to make the application of the identification methods of harmonic currents versatile and for any voltage form, we use the detection system of the fundamental component of the direct voltage. in this paper, a comparison between the conventional method used for extracting the direct component of the network voltage which is based on the phase-locked loop (pll) and the new approach based on a multivariable extraction filter. finally, simulation results show that the proposed multivariable filter may better work even if the network voltage is (perturbed and unbalanced). furthermore, this filters permits to generalize the use of identification methods for filtering the different perturbations of active and reactive current. (c) 2015 the authors. published by elsevier ltd.
electricity	the transmission network switching is proposed in the literature as a way to improve social welfare in liberalized power markets. (7) moreover, exercise of market power by strategic generating companies (gencos) causes some extra cost in electricity market which can be alleviated by implementing appropriate switching policies. this paper contributes td the existing literature by developing a mathematical model that explores, from an economic perspective, the transmission network switching in the context of market power. the strategic gecnos are modelled based on the cournot game. the nash equilibrium of the game between gencos is formulated as an equilibrium problem with equilibrium constraint (epec). the epec problem is transformed to a mixed-integer linear feasibility problem. to handle the multiple nash-equilibria situations, the solution concept of the extremal-nash equilibrium (ene) is introduced. a mixed-integer linear program (milp) is derived for finding ene. the transmission switching decisions are modelled as binary variables controlled by the system operator (tso). the tso minimizes the system dispatch cost calculated at ene and network reconfiguration cost. the tso minimizes the cost using its transmission switching decisions. the problem faced by the tso is a mixed-integer bilevel linear program (miblp) with binary variables in both upper and lower levels. the upper level models the tso 's action and the lower level the oligopolistic gencos (competing in cournot game). a (parallel) branch and-bound technique is used to solve the developed miblp model. an illustrative 3-bus example system and the ieee-rts96 are modelled and carefully studied. (8)the numerical results demonstrate that: 1 - the (parallel) branch-and-bound technique can effectively solve the developed miblp, 2 - using the developed model, the system operator can change topology of the network by switching the lines in order to reduce the adverse effect of the strategic behaviour of gecnos. (c) 2017 elsevier b.v. all rights reserved.
electricity	the growing demand for electric vehicles entails an increased consumption of critical energetic and non-energetic abiotic resources, necessary for an optimal performance of the vehicle. the depletion of these resources and the future availability to meet their demand appears to be a potential limitation for the expansion of the electrified vehicle industry. the goal of this study is to perform a detailed life cycle analysis, including manufacturing, use and disposal, of key components of ev powertrains, identifying materials and processes responsible for abiotic depletion impact. this study also investigates the sensitivity of the results to the choice of life cycle assessment (lca) impact methods. for this, a lca is performed on an integrated electric drive, by considering seven impact methods. results show that energetic resources consumption generate the largest impact, followed by metals and lastly by mineral resources. the consumption of electricity in each life cycle is a crucial factor in the generation of total impact. there are agreements among methods on the materials and processes contributing the most to depletion, given the differences in approach used by each impact method. (c) 2016 elsevier b.v. all rights reserved.
electricity	the united states geological survey estimates that over four trillion barrels of crude oil are currently trapped within u.s. oil shale reserves. however, no cost-effective, environmentally sustainable method for oil production from oil shale currently exists. given the continuing demand for low-cost fossil-fuel production, alternative methods for shale-oil extraction are needed. geothermic fuel cells (tm) (gfc) harness the heat generated by high-temperature solid oxide fuel cells during electricity generation to process oil shale into ""sweet"" crude oil. in this paper, a thermo-electrochemical model is exercised to simulate the performance of a 4.5 kw(e) (gross) geothermic fuel cell module for in situ oil-shale processing. the gfc analyzed in this work is a prototype which contains three 1.5 kw(e) solid oxide fuel cell (sofc) stack-and-combustor assemblies packaged within a 0.3 m diameter, 1.8 m tall, stainless-steel housing. the high-temperature process heat produced by the sofcs during electricity generation is used to retort oil shale within underground geological formations into high-value shale oil and natural gas. a steady-state system model is developed in aspen plus (tm) using user-defined subroutines to predict the stack electrochemical performance and the heat-rejection from the module. the model is validated against empirical data from independent single-stack performance testing and full gfc-module experiments. following model validation, further simulations are performed for different values of current, fuel and air utilization to study their influence on system electrical and heating performance. the model is used to explore a wider range of operating conditions than can be experimentally tested, and provides insight into the competing physical processes at play during geothermic fuel cell operation. results show that the operating conditions can be tuned to generate desired heat-flux conditions as needed across applications. (c) 2017 elsevier ltd. all rights reserved.
electricity	several studies on sustainable technologies for the built environment (e.g., green roofs, green walls, etc.) have been carried out. many of these focus on the technical performance of such technologies. nonetheless, it also becomes necessary to examine how the public perceives such technologies. this understanding is clearly important to support their widespread use. more specifically, such perception can be incorporated into product design to increase people 's acceptance. the contribution of this paper is twofold. first, it presents results of a survey examining the perception of people from a small city in the southern region of brazil (feliz, rs) regarding three sustainable technologies: green roofs, green walls, and composting (or dry) toilets. second, it proposes a set of preliminary design guidelines to improve acceptance of more sustainable technologies based on people 's perception: (1) make the technology look good; (2) make visible the benefits provided by the technology; and (3) design the technology to avoid misunderstandings and concerns. (c) 2016 american society of civil engineers.
electricity	the pumped storage power station (psps) is a special power source that has flexible operation modes and multiple functions. with the rapid economic development in china, the energy demand and the peak-valley load difference of the power grid are continuing to increase. moreover, wind power, nuclear power, and other new energy sources also develop very fast. developing the psps is of great importance to the power source structure adjustment, and the secure and stable operation of the power grids in china in the 21st century. this paper provides a survey of the psps development in china. over the last two decades, china 's psps has developed quickly. the psps installed capacity had reached 21.83 gigawatts (gw) by the end of 2014, ranking among the top in the world. 27 pspss have been completed and put into production, and many with the installed capacity of more than 1200 megawatts (mw) are still under construction, including fengning psps. in addition, a lot of sites suitable for the psps construction have been planned. with regard to the challenges existing in the exploitation course, some suggestions are proposed. there is a bright future for the psps development in china.
operational_amplifier	a simple low-power and low-area metal-oxide-semiconductor field-effect transistor-only fully differential 1.5-bit pipelined analog-to-digital converter stage is proposed and designed in taiwan semiconductor manufacturing company 0.18m-technology using bsim3v3 parameters with supply voltage of 1.8v in inexpensive digital complementary metal-oxide semiconductor (cmos) technology. it is based on charge pump technique to achieve the desired voltage gain of 2, independent of capacitor mismatch and avoiding the need of power hungry operational amplifier-based architecture to reduce the power, si area and cost. various capacitances are implemented by metal-oxide semiconductor capacitors, offering compatibility with cheaper digital cmos process in order to reduce the much required manufacturing cost.
operational_amplifier	a novel frequency compensation technique, named regular miller plus reversed indirect compensation (rmric), is presented in this paper for fast-settling three-stage amplifiers. the rmric topology includes, on the one hand, a miller capacitor combined with one nulling resistor connected between the first stage and the third stage, and on the other hand, an indirect compensation capacitor in series with a resistor added between the first and the second stage, which improves remarkably the performance such as gain-bandwidth product (gbw) and sensitivity of the proposed amplifier. detailed design considerations are carried out to demonstrate the stability of the compensation technique. circuit simulation results show the amplifier driving a 2-pf load capacitance achieves a 9.25-ghz gbw dissipating only 16.5 mw with a 1.2 v supply voltage using a tsmc 65 nm cmos technology, which shows a significant improvement in figure of merits. the implemented amplifier reaches a settling time of 3.35 ns with 0.006 % accuracy.
operational_amplifier	this paper presents an efficient approach for the optimal designs of two analog circuits, namely complementary metal oxide semiconductor) two-stage comparator with p-channel metal oxide semiconductor input driver and n-channel input and folded-cascode operational amplifier using a recently proposed meta-heuristic-based optimization algorithm named as colliding bodies optimization (cbo). it is a multi-agent algorithm that does not depend upon any internal control parameter, making the algorithm extremely simple. the main objective of this paper is to optimize the metal oxide semiconductor (mos) transistors' sizes using cbo in order to reduce the areas occupied by the circuits and to get better performance parameters of the circuits. simulation program with integrated circuit emphasis simulation has been carried out by using the optimal values of mos transistors' sizes and other design parameters to validate that cbo-based design is satisfying the desired specifications. simulation results demonstrate that the design specifications are closely met and the required functionalities are achieved. the simulation results also confirm that the cbo-based approach is superior to the other algorithms in terms of mos area and performance parameters like gain, power dissipation, etc., for the examples considered. copyright (c) 2016 john wiley & sons, ltd.
operational_amplifier	this paper presents a novel adaptive bias technique based on the use of squaring circuits. here, the tail current of an operational amplifier (opamp) is controlled using an auxiliary circuit. by design, it generates a well-controlled tail current which - to a first order approximation-is independent of the opamp 's common-mode input voltage. as a result, parameters like common-mode rejection ratio (cmrr) and power supply rejection ratio (psrr) are enhanced. however, when a differential input signal is applied, it generates a tail current proportional to the square of the opamp 's differential input voltage. in this way, the currents of the opamp 's input pairs are boosted, thus improving slew rate. experimental results in 0.5 mu m cmos technology verify current and slew rate enhancement factors between 10 and 15 with less than 20% static current increase.
operational_amplifier	the swipe detector of the ballon borne mission lspe (see e.g. the contribution of p. de bernardis et al. in this conference) intends to measure the primordial 'b-mode' polarization of the cosmic microwave background (cmb). for this scope microwave telescopes need sensitive cryogenic bolometers with an overall equivalent noise temperature in the nk range. the detector is a spiderweb bolometer based on transition edge sensor and followed by a squid to perform the signal readout. this contribution will concentrate on the design, description and first tests on the front-end electronics which processes the squid output (and controls it). the squid output is first amplified by a very low noise preamplifier based on a discrete jfet input differential architecture followed by a low noise cmos operational amplifier. equivalent input noise density is 0.6 nv/hz and bandwidth extends up to at least 2 mhz. both devices (jfet and cmos amplifier) have been tested at liquid nitrogen. the second part of the contribution will discuss design and results of the control electronics, both the flux locked loop for the squid and the slow control chain to monitor and set up the system will be reviewed. the digital interface to the control computer will be presented.
analog_signal_processing	we present the vmm+wta structure as a general-purpose, low-power, compact, programmable classifier architecture and demonstrate its equivalence to a 2-layer perceptron. the classifier generates event outputs and is suitable for integration with event-driven systems. we present measured data from simple linear and non-linear classifier structures on a 0.35 mu m chip and demonstrate the implementation of an xor function using a 1-layer vmm+wta classifier.
analog_signal_processing	transferable skills are vital in modern engineering work. educational institutions are forced to reassess their education; along with theoretical content, transferable competences are emphasized. this paper demonstrates how teaching transferable skills, such as team work, oral and writing skills, project, time, and financial management, are integrated into a single course. we also show how to motivate students by interactive teaching methods and course evaluation.
analog_signal_processing	since refineries and petrochemical plants were connected to the mexican grid, the number and magnitude of failures of their electrical generators increased. the constant starts and stops and grounding schemes used in these plants, have been important factors for it. typical cases of failure reported in the last 15 years are described in this paper.
analog_signal_processing	the netx family of hilscher company are highly integrated soc with the system architecture based on arm9 cpu, optimized for communication in industrial automation. it offers the migration strategy from field bus to real-time-ethernet. besides the cpu, a couple of communication channels and peripherals are integrated on the chip. this contribution describes a study the goal of which is to test the usability of netx according to solving dsp tasks.
analog_signal_processing	in real-time catheter-based 3-d ultrasound imaging applications, gathering data from the transducer arrays is difficult, as there is a restriction on cable count due to the diameter of the catheter. although area and power hungry multiplexing circuits integrated at the catheter tip are used in some applications, these are unsuitable for use in small sized catheters for applications, such as intracardiac imaging. furthermore, the length requirement for catheters and limited power available to on-chip cable drivers leads to limited signal strength at the receiver end. in this paper, an alternative approach using analog time-division multiplexing (tdm) is presented, which addresses the cable restrictions of ultrasound catheters. a novel digital demultiplexing technique is also described, which allows for a reduction in the number of analog signal processing stages required. the tdm and digital demultiplexing schemes are demonstrated for an intracardiac imaging system that would operate in the 4- to 11-mhz range. a tdm integrated circuit (ic) with an 8: 1 multiplexer is interfaced with a fast analog-to-digital converter (adc) through a microcoaxial catheter cable bundle, and processed with a field-programmable gate array register-transfer level simulation. input signals to the tdm ic are recovered with -40-db crosstalk between the channels on the same microcoax, showing the feasibility of this system for ultrasound imaging applications.
state_space_representation	we consider exact sampling from the stationary distribution of a closed queueing network with finite capacities. in a recent work a compact representation of sets of states was proposed that enables exact sampling from the stationary distribution without considering all initial conditions in the coupling from the past (cftp) scheme. this representation reduces the complexity of the one-step transition in the cftp algorithm to o(km2), where k is the number of queues and m the total number of customers; while the cardinality of the state space is exponential in the number of queues. in this paper, we extend these previous results to the multiserver case. the main focus and the contribution of this paper is on the algorithmic and the implementation issues. we propose a new representation, that leads to one-step transition complexity of the cftp algorithm that is in o(km). we provide a detailed description of our matrix-based implementation. matlab toolbox clones (closed queueing networks exact sampling) can be downloaded at http://www.di.ens.fr/similar to rovetta/clones. (c) 2016 elsevier b.v. all rights reserved.
state_space_representation	this paper presents a comprehensive low-voltage residential load model of price-based demand response (dr). high-resolution load models are developed by combing monte carlo markov chain bottom-up demand models, hot water demand models, discrete state space representation of thermal appliances, and composite time-variant electrical load models. price-based dr is then modeled through control algorithms for thermostatically controlled loads, optimal scheduling of wet appliances, and price elasticity matrices for representing the inherent elastic response of the consumer. the developed model is used in a case study to examine the potential distribution network impacts of the introduction of dynamic pricing schemes. the effects of cold load pick-up, rebound peaks, decrease in electrical and demand diversity, and impacts on loading and voltage are presented.
state_space_representation	structural optimization has been shown to be an efficient and effective method to obtain the optimal design balancing competing objectives. however, literature on optimization of structures subject to random excitation is sparse. this study proposes a performance-based optimization approach for nonlinear structures subject to stochastic dynamic excitation. the optimization procedure is formulated as a multi objective problem considering various performance objectives. the excitation is modeled as a zero-mean filtered white noise and combined with the nonlinear equations of motion of the structure to create an augmented state space representation of the system. the optimization objectives are defined in terms of the variance of stationary structural responses, which are obtained via equivalent linearization. thus, the stochastic optimization problem is converted into its deterministic counterpart. numerical examples are provided to demonstrate the efficacy of the proposed approach. three levels of seismic magnitudes, i.e., low-level, frequent earthquake, medium-intensity earthquake and high-intensity earth-quake, are investigated. for each seismic magnitude, two performance objectives are considered. the first performance objective considers serviceability, seeking to minimize floor acceleration response; and the second performance objective considers structural safety and seeks to minimize interstory drift response. the pareto optimal fronts are calculated to illustrate the intrinsic tradeoffs between serviceability and safety of designs subject to all seismic magnitudes. (c) 2016 elsevier ltd. all rights reserved.
state_space_representation	this article considers the speed control of asymmetrical dual three-phase generator by using an inner loop of model based predictive control (mbpc) to predict the effects of future control actions on the state variables. in order to achieve this goal, the algorithm uses both a luenberger observer and a kalman filter with the purpose of estimating the rotor currents. thereafter, the speed control is used to reach the maximum power point tracking (mppt), which ensures that it is delivered to the load. finally, a comparison of the efficiency of the mbpc when considering the rotor current estimators and when they are used an estimator based on the state space representation.
state_space_representation	in this paper, stochastic optimal strategy for unknown linear discrete-time system quadratic zero-sum games in input-output form with communication imperfections such as network-induced delays and packet losses, otherwise referred to as networked control system (ncs) zero-sum games, relating to the h optimal control problem is solved in a forward-in-time manner. first, the linear discrete-time zero sum state space representation is transformed into a linear ncs in the state space form after incorporating random delays and packet losses and then into the input-output form. subsequently, the stochastic optimal approach, referred to as adaptive dynamic programming (adp), is introduced which estimates the cost or value function to solve the infinite horizon optimal regulation of unknown linear ncs quadratic zero-sum games in the presence of communication imperfections. the optimal control and worst case disturbance inputs are derived based on the estimated value function in the absence of state measurements. an update law for tuning the unknown parameters of the value function estimator is derived and lyapunov theory is used to show that all signals are asymptotically stable (as) and that the estimated control and disturbance signals converge to optimal control and worst case disturbances, respectively. simulation results are included to verify the theoretical claims.
signal-flow_graph	in this paper, the correspondence between the weighted line graph and the mason signal flow graph (msfg) has been established, which gives an interpretation of a convolutional network code (cnc) over a, cyclic network from a different perspective. furthermore, by virtue of mason theorem, we present two new equivalent conditions to evaluate whether the global encoding kernels (geks) can be uniquely determined by the given complete set of local encoding kernels (leks) in a cnc over a cyclic network. these two new equivalent conditions turn out to be more intuitive. moreover, we give an alternative simple proof of an existing result.
signal-flow_graph	this paper presents a new method to automatically generate hierarchical placement rules, which are crucial for a successful analog placement. the method is based on a novel symmetry computation method, introducing the structural signal flow graph. five types of proximity, matching and symmetry constraints are determined. according to the priority of the constraint types, a constraint requirement graph and a hierarchical partitioning of the circuit into matching, proximity and symmetry groups is then automatically computed. based on experimental results with a state-of-the-art placement tool, we show that the new approach generates more placement rules and can lead to better circuit performance and parametric yield according to post-layout simulation.
signal-flow_graph	in this work, two active only grounded-c equivalents of third-order voltage-mode (vm) elliptic low-pass (lp) lc ladder prototype are proposed. as active building blocks (abbs) the recently introduced current follower transconductance amplifier (cfta) were used. the first active only grounded-c lp filter employing eight cftas was proposed by interconnecting cfta-based active equivalent sub-blocks of passive components, where one of the low-impedance input terminals is not used. since such feature may cause some noise injection into the proposed circuit, the proposed filter was optimized using mason-coates' signal flow graph approach. in several steps the number of abbs was reduced by two and the unused input terminal was eliminated. the performance of the novel and optimized active only grounded-c third-order vm elliptic lp filter was tested experimentally using the readily available ucc-n1b integrated circuit.
signal-flow_graph	the paper deals with synthesis of active frequency filters and oscillators for applications in the video band. the synthesis is based on the modified signal flow graph design approach in the current mode. the commercially available high frequency current conveyors and second order structures are used. basic structure is employing loss-less and lossy current mode integrators in two distributed feedback loops. some of the used active elements allow to control current gain which is very useful in applications. high frequency multi-functional filter and electronically adjustable quadrature oscillator configurations are discussed. most of the used passive elements (mainly capacitors) are grounded which is suitable for ic implementation. simulation and experimental results demonstrate function and features very well.
signal-flow_graph	in this paper, we present branching oriented system equation based on-line error correction scheme for recursive digital signal processing. the target digital signal processing is linear and time-invariant, and the algorithm includes multiplications with constant coefficient, additions and delays. the difficulties of the algorithm-level fault tolerance for such algorithm without structural regularity include error distribution problem and right timing of error correction. to escape the error distribution problem, multiple fan-out nodes in an algorithm are specified as the nodes at which error corrections are performed. the branching oriented graph and branching oriented system equation are so introduced to formulate on-line correction schemes based on this strategy. the branching oriented graph is treated as the collection of computation sub-blocks. applying checksum code independently to each sub-block is our most trivial on-line error correction scheme, and it results in, with appropriate selection of error identification process, tmr in sub-block level. one of the advantages of our method is in the reduction of redundant operations performed by merging some computation sub-blocks. on the other hand, the schedulability of the system is an important issue for our method since our on-line error correction mechanism induces additional data dependencies. in this paper, the schedulability condition and some modifications on the scheme are also discussed.
electrical_circuits	while teaching network analysis to undergraduate students, the frequency response of electrical circuits consisting of passive elements such as resistors, capacitors and inductors are commonly analysed. students are familiar with the sharp amplitude peak or trough normally occurring at resonance. this observation is not only intuitively pleasing, but also consistent with the concept of resonance in many physical systems. in the well-known series and parallel rlc circuits, the maximum and minimum currents, respectively, occur at resonance. because of this, many students and even instructors (sometimes) may erroneously assume that the resonance in general rlc circuits implies maximum or minimum current. in this paper, it will be shown that this is generally untrue. the theoretical analyses and experimental validation of series, parallel and series-parallel rlc circuits are presented to address this common misconception.
electrical_circuits	this paper concerns the effect that a stochastic resonance can have on a vibration isolation system. rather than reducing the transmitted force, it is shown that it is possible to significantly mask the component of the force transmitted though the isolator, when the system is excited harmonically. this can be achieved by adding a very low intensity of random noise to the harmonic excitation force. the nonlinear mechanical vibration isolation system used in the study consists of a vertical linear spring in parallel with two horizontal springs, which are configured so that the potential energy of the system has a double-well. prior to the analytical and numerical study, an experiment to demonstrate stochastic resonance in a mechanical system is described. (c) 2016 elsevier ltd. all rights reserved.
electrical_circuits	we study the problem of inverse minimum dynamic cut (imdc), which changes a capacity vector u in such a way that a given dynamic cut and the applied changes are minimum, using the euclidean norm l(1) to measure these changes. our aim is to solve an inverse minimum dynamic cut, by finding a maximum dynamic flow in a time-expanded network flow and, having calculated the appropriate algorithm, analyzing the relationships between the time-expanded networks, the maximum flow, and the minimum cut. first, we show a specified dynamic cut, to be optimized; the capacity of arcs should be reduced. this study is useful in cases where the network should be divided into two parts by cutting such that this cut has the minimum weight. it is applicable in physics, chemistry, computer networks and electrical circuits, etc. the advantage of this method is the use of time-expanded network to be static corresponding to a dynamic network and algorithm is easily displayed on this network. finally, the algorithm is implemented on a numerical example of dynamic network flow.
electrical_circuits	graph clustering has been an essential part in many methods and thus its accuracy has a significant effect on many applications. in addition, exponential growth of real-world graphs such as social networks, biological networks and electrical circuits demands clustering algorithms with nearly-linear time and space complexity. in this paper we propose personalized pagerank clustering (ppc) that employs the inherent cluster exploratory property of random walks to reveal the clusters of a given graph. we combine random walks and modularity to precisely and efficiently reveal the clusters of a graph. ppc is a top-down algorithm so it can reveal inherent clusters of a graph more accurately than other nearly-linear approaches that are mainly bottom-up. it also gives a hierarchy of clusters that is useful in many applications. ppc has a linear time and space complexity and has been superior to most of the available clustering algorithms on many datasets. furthermore, its top-down approach makes it a flexible solution for clustering problems with different requirements. (c) 2013 elsevier b.v. all rights reserved.
electrical_circuits	the purpose of this paper is to determine physical electrical circuits, in both impedance and admittance forms, that match fractional-order integrators and differentiators, namely 1/s(q) and s(q). then, using these idealized infinite-dimensional circuits, the energy storage and loss expressions for them are determined, carefully relating the associated infinite dimensional state variables to physically meaningful quantities. the resulting realizations and energy expressions allow a variety of implementations for understanding the transient behavior of fractional-order systems.
lorentz_force_law	magnetically levitated planar motor is a new-generation motion device in modern precision industry, while its advanced motion controller design is still of main concern. in this paper, a learning adaptive robust control (larc) motion controller is proposed for a magnetically levitated planar motor developed in our laboratory to achieve good tracking performance. the planar motor consists of a halbach permanent magnetic array as the stator, and a levitated platen containing four groups of three-phase windings as the mover. based on the lorentz force law, the mover placed in the magnetic field is subjected to vertical force for levitation and horizonal force for planar motion through dynamics decoupling and current allocation. an larc control scheme containing adaptive robust control (arc) term and iterative learning control (ilc) term in a serial structure is then proposed for the magnetically levitated planar motor to achieve high-performance tracking even there exist parametric variations and uncertain disturbances. comparative experiments between traditional lead, arc, ilc, and the proposed larc are carried out on the planar motor to track sinusoidal, point-to-point, and planar circular motions, respectively. the experimental results consistently validate that the proposed larc control strategy outperforms other controllers much, and possesses not only good transient/steady-state tracking performance but also parametric adaptation ability and uncertain disturbance robustness. the proposed scheme actually provides a practically effective technique for motion control of magnetically levitated planar motors in industrial applications.
lorentz_force_law	the analytical calculation of magnetic forces is currently an interesting alternative to the time-consuming 3-d finite-element method due to their high accuracy and low computational cost. in this paper, a new semi-analytical solution for determining the levitation force of the repulsive magnetic guidance that has a significant role for controlling and developing high precision magnetic levitation positioning system is presented. an important result is that this new expression, compared with other known equations in the literature, considers the position dependence of the magnetic levitation force over the whole travel range. in order to derive this force, the magnetic field of the permanent magnet is first calculated by introducing and solving the magnetic scalar potential. taking lorentz force law into consideration, the proposed equation is finally derived. this semi-analytical equation is suitable for designing and optimizing the magnetic guidance. furthermore, it can also be used for developing other ironless actuators. the calculated levitation force from the derived equation and the verification by measurements are likewise presented in this paper.
lorentz_force_law	current sensing is widely used in power electronic applications such as dc-dc power converters and adjustable-speed motor drives. such power converters are the basic building blocks of drivetrains in electric, hybrid, and plug-in hybrid electric vehicles. the performance and control of such vehicles depend on the accuracy, bandwidth, and efficiency of its sensors. various current-sensing techniques based on different physical effects such as faraday 's induction law, ohm 's law, lorentz force law, the magnetoresistance effect, and the magnetic saturation effect are described in this paper. each technique is reviewed and examined. the current measurement methods are compared and analyzed based on their losslessness, simplicity, and ease of implementation.
lorentz_force_law	it is shown from newton 's second law and the conservation of linear momentum, for arbitrary types of force, that the full ""formula force"" used in the second law is effective in exerting observable ponderomotive action on a test element only if the force-exerting element is infinitely massive or is otherwise immobilized in an inertial system to preclude its recoil. in the more realistic case of finite mass m of a force-exerting element free to recoil under mutual action-reaction, the recoil motion ""steals"" energy from the observable force action, so that the physically effective force exerted on a test element of mass m can be represented as the product of formula force and an inertial modulation factor omega = m/(m + m) less than or equal to 1. application of this elementary result to classical electrodynamics shows that it can invalidate (as physics) many of the theorems universally taken for granted since the nineteenth century. for instance, the well-established mathematical theorem that a closed current loop external to a straight current-carrying test element necessarily exerts zero longitudinal ponderomotive force on the latter (for the lorentz force law, the original ampere force law, and all others differing from these only by an additive exact differential quantity) need not be valid physically, the reason is that the inertial factor omega has been overlooked. its presence as an extra factor (green 's function) multiplying the force differential can spoil the exact differential nature of the loop integrand. for instance, if the external loop is physically configured with a ""weak link,"" namely, an ""unanchored"" section of conductor of relatively low mass m much less than m and high mobility (freedom to recoil), then along this portion of the circuit omega approximate to 0, whereas for the remainder of the circuit, if anchored in the laboratory, omega approximate to 1. hence an integral around the whole circuit treats the low-mass portion of the circuit much as if it were an open gap despite the presence of current in it. this proposition is easily put to observational test. by using an electromagnetically driven tuning fork bearing straight segments of current-carrying conductor as sensor (test element), so arranged as to respond to longitudinal force, it has been confirmed through an observed alteration of fork resonance response under ac excitation that a suitably configured weak link in an external closed electrical circuit can, indeed, cause a readily detectable violation of the closed-loop-no-longitudinal-force theorem. in sum, we have exhibited ""cross talk"" between electromagnetic and inertial properties of conducting circuits that can limit the applicability of many of the classical electrodynamic theorems concerning ""closed loops."" such theorems are assuredly valid only in the special case of completely immobilized force-exerting circuits, or circuits all parts of which are constrained to prescribed states of motion (not free to recoil). the ability to violate classical theorems that for a century have made empirical choices among candidate force laws ""impossible"" implies that now experiments can be designed to allow such choices to be made unambiguously.
lorentz_force_law	it is known that for the magnetic force due to a closed circuit, the weber force law can be identical to the lorentz force law. in this investigation it is shown that for both the electric and the magnetic force of the quasi-static case, the riemann force law can be identical to the lorentz force law, while the former is based oil a potential energy depending on a relative speed and is in accord with newton 's law of action and reaction.
system_identification	inferring properties of the interaction matrix that characterizes how nodes in a networked system directly interact with each other is a well-known network reconstruction problem. despite a decade of extensive studies, network reconstruction remains an outstanding challenge. the fundamental limitations governing which properties of the interaction matrix (e.g. adjacency pattern, sign pattern or degree sequence) can be inferred from given temporal data of individual nodes remain unknown. here, we rigorously derive the necessary conditions to reconstruct any property of the interaction matrix. counterintuitively, we find that reconstructing any property of the interaction matrix is generically as difficult as reconstructing the interaction matrix itself, requiring equally informative temporal data. revealing these fundamental limitations sheds light on the design of better network reconstruction algorithms that offer practical improvements over existing methods.
system_identification	closed-loop systems of an electro-hydraulic servo system including position, acceleration, and force closed-loop systems and their closed-loop transfer functions based on parameter model are adaptive identified using a recursive extended least-squares algorithm. the position and force closed-loop tracking controllers are designed by a proportional-integral-derivative controller and are tuned by the position and force step signals. the acceleration closed-loop tracking controller is designed by a three-variable controller and the three states include position, velocity, and acceleration. experimental results of the estimated position, acceleration, and force closed-loop transfer functions are performed on an actual electro-hydraulic servo system using xpc rapid prototyping technology, which clearly demonstrate the benefit of the adaptive identification method.
system_identification	in this paper, an online system identification method is introduced for lifetime diagnostic of supercapacitors. the online strategy uses a lyapunov-based adaptation law to estimate online the supercapacitor 's parameters. therefore, the adaptive observer 's stability is guaranteed by lyapunov 's direct method. state-of-health (soh) estimation is crucial since aging introduces degradation in supercapacitors' performance, which might eventually lead to their failure. soh is usually measured by offline time or frequency domain characterization techniques such as spectroscopy. however, these methods require interruption of the system 's operation, and hence, they are not suitable for real-time applications such as electric vehicles. unlike other online estimation strategies, only voltage and current measurements are required. simulation and experimental results along with comparison against two different methods highlight the effectiveness of the proposed approach in estimating the soh.
system_identification	speech is a multisensory percept, comprising an auditory and visual component. while the content and processing pathways of audio speech have been well characterized, the visual component is less well understood. in this work, we expand current methodologies using system identification to introduce a framework that facilitates the study of visual speech in its natural, continuous form. specifically, we use models based on the unheard acoustic envelope (e), the motion signal (m) and categorical visual speech features (v) to predict eeg activity during silent lipreading. our results show that each of these models performs similarly at predicting eeg in visual regions and that respective combinations of the individual models (ev, mv, em and emv) provide an improved prediction of the neural activity over their constituent models. in comparing these different combinations, we find that the model incorporating all three types of features (emv) outperforms the individual models, as well as both the ev and mv models, while it performs similarly to the em model. importantly, em does not outperform ev and mv which, considering the higher dimensionality of the v model, suggests that more data is needed to clarify this finding. nevertheless, the performance of emv, and comparisons of the subject performances for the three individual models, provides further evidence to suggest that visual regions are involved in both low-level processing of stimulus dynamics and categorical speech perception. this framework may prove useful for investigating modality-specific processing of visual speech under naturalistic conditions.
system_identification	in many contemporary engineering problems, model uncertainty is inherent because accurate system identification is virtually impossible owing to system complexity or lack of data on account of availability, time, or cost. the situation can be treated by assuming that the true model belongs to an uncertainty class of models. in this context, an intrinsically bayesian robust (ibr) filter is one that is optimal relative to the cost function (in the classical sense) and the prior distribution over the uncertainty class (in the bayesian sense). ibr filters have previously been found for both wiener and granulometric morphological filtering. in this paper, we derive the ibr kalman filter that performs optimally relative to an uncertainty class of state-space models. introducing the notion of bayesian innovation process and the bayesian orthogonality principle, we show how the problem of designing an ibr kalman filter can be reduced to a recursive system similar to the classical kalman recursive equations, except with ""effective"" counterparts, such as the effective kalman gain matrix. after deriving the recursive ibr kalman equations for discrete time, we use the limiting method to obtain the ibr kalman-bucy equations for continuous time. finally, we demonstrate the utility of the proposed framework for two real world problems: sensor networks and gene regulatory network inference.
pid_controller	fuel cell (fc) systems are appropriate candidates as alternative energy sources for use in stand-alone applications. however, the fc generates an unregulated voltage, which is not suitable in stand-alone application usage solely. this paper presents the control of stand-alone application based on fuzzy pid (fpid) controller. the aim of this paper is designing and controlling a suitable power conditioning unit (pcu) that consists of two dc/dc converter stages and dc/ac inverter. in addition, an analysis of cascade structure based on fuzzy pid controller for a single phase inverter is done and also two feedback control loops are comprised. inductor current and capacitor voltage are measured and sent through feedback to the inner loop and the outer loop, respectively. besides, to evaluate the proposed controller robustness, the pcu is considered under step loads as disturbance. finally, it is shown that the proposed controller has a robust behavior and good transient response. (c) 2015 elsevier ltd. all rights reserved.
pid_controller	this paper addresses the design of a robust lyapunov-based controller for flexible-joint electrically driven robots considering to voltage as control input. the proposed approach is related to the key role of electrical subsystem of the motors, thus is free from mechanical subsystem of the actuator dynamics, considered here as unmodeled dynamics. the main contribution of this paper is to prove that the closed-loop system composed by full nonlinear actuated robot dynamics and the proposed controller is bibo stable, while actuator/link position errors are uniformly ultimately bounded stable in agreement with lyapunov 's direct method in any finite region of the state space. it also forms a constructive and conservative algorithm for suitable choice of gains in pid controller. the analytical studies as well as experimental results produced using matlab/simulink external mode control on a flexible-joint electrically driven robot demonstrate high performance of the proposed control schemes.
pid_controller	the controller design for bilateral teleoperation systems involves a delicate trade-off between performance and stability. to achieve high performance, high order robust controllers may not be feasible for real-time implementation because of hardware and computational limitations. the main purpose of this paper is to achieve stability and transparency in the presence of time delay in communication channel as well as model uncertainty. to address these problems, a novel robust fixed-structure controller is proposed for uncertain bilateral teleoperation systems. here, the traditional conventional proportional-integral-derivative (pid) controller is employed to achieve the requirements. the simplicity and straightforward design are the significant advantageous of the proposed method. robust stability analysis of the proposed technique is also provided. results demonstrate that the structure is effective in providing stability and transparency in teleoperation systems.
pid_controller	dc-dc power supplies are playing significant role in different domains of engineering applications. some converters such as boost, buck-boost, and fly-back have a right-half-plane zero (non-minimum phase system), hence it is difficult for the pid controller to exhibit good performance with load, line variations and parametric uncertainty. in this proposed work, design and implementation of type controllers have been performed by using k-factor approach and two different optimisation techniques (gravitational search algorithm and particle swarm optimisation) for obtaining better stability and performance for a closed loop dc-dc switched mode boost converter. the closed loop control system has been implemented in real time dspace platform. the comparative closed-loop performances of a boost converter with classical, optimised pid and optimised type ii/type iii controllers have been produced. simulations and experimental results are provided to demonstrate the effectiveness of optimised controllers for the proposed converter. design and implementation of optimised type controller for switch mode converters has not been reported earlier in any literature.
pid_controller	a design of fuzzy-based hierarchal approach is proposedto improve the frequency control in the power system. the simplified great britain (gb) frequency control model, was used as a single bus machine. a new simple tuning method is proposed with the aid of the conventional pid controller gains. the same gains are used to feed the fuzzy controller in the same loop without re-tuning the fuzzy gains. this method allows the fuzzy structure to supplement the conventional control rather than replacing it. particle swarm optimization (pso) is proposed to find the optimal value of the controller gains. five modeling scenarios for the fuzzy control and two cases of uncertain parameters were considered to validate the model. structures of fuzzy controller offered high stability and robustness in the simulation results. therefore, the proposed hierarchal structuresare used in the real-time applications of the load frequency control in the power systems. (c) 2016 elsevier b.v. all rights reserved.
voltage_law	this paper presents an investigation on high energy rate micro forming (hermf) of a thin aluminum foil using a high velocity forming (hvf) technique. a model is proposed for the work of deformation and efficiencies involved in every stage of energy transfer. the forming action has been achieved through incident shockwaves, which cause plastic deformation in the blank to take the shape of the die cavity. the shockwaves are generated by a rapid capacitive discharge of energy across a fuse wire inside the water. the paper presents an analysis to predict the amount of energy required for a desired depth of hemispherical deformation, based on the principles of plastic deformation and volume constancy. the energy transferred from the capacitor bank to the fuse wire is modeled using kirchhoff 's voltage law and the joule heating principle. an indigenous micro-electro-hydro-forming (ehf) setup developed in-house is used to validate the analytical model developed through a set of experiments. readings are optimized against the number of parameters present in the entire process. validation for the suggested analytical model is carried out by comparing theoretical and experimentally measured values of the depth of deformation of a 20-mu m thick aluminum foil. the results obtained are encouraging, and the highest error in the predictive ability of our model was found to be 30 %. to the best of our knowledge, this is the first attempt at analytically modeling the depth of deformation for a given value of supplied electrical energy in the micro-domain.
voltage_law	in this paper, a new formulation for coupled circuit-electromagnetic (em) simulation is presented. the formulation employs full-wave integral equations to model the em behavior of two- or three-dimensional structures while using modified nodal analysis to model circuit interactions. a coupling scheme based on charge and current continuity and potential matching, realized as a generalization of kirchoff 's voltage and current laws, ensures that the em and circuit interactions can be formulated as a seamless system. while rigorous port models for em structures can be obtained using the approach discussed herein, it is shown that the coupling paradigm can reveal additional details of the em-circuit interactions and can provide a path to analysis-based design iteration.
voltage_law	an optimal design strategy based on genetic algorithms (ga) is proposed for nonlinear hysteretic control devices that prevent pounding damage and achieve the best results in seismic response mitigation of two adjacent structures. an integrated fuzzy controller is used in order to provide the interactive relationships between damper forces and input voltages for mr dampers based on the modified bouc-wen model. furthermore, linear quadratic regulator (lqr) and h-2/lqg (linear quadratic gaussian) controllers based on clipped voltage law (cvl) are also used to compare the results obtained by fuzzy controller. this study employs the main objectives of the optimal design that are not only to reduce the seismic responses but also to minimize the total cost of the damper system. a set of pareto optimal solutions is also conducted with the corresponding results obtained from the optimal surface of pareto solutions in this study. as a result, decreasing the number of dampers does necessarily increase the efficiency of the system. in fact, reducing the number of dampers for the dynamic response of the system can contribute more than increasing the number of dampers. (c) 2014 elsevier ltd. all rights reserved.
voltage_law	renewable portfolio standards (rpss) are popular market-based mechanisms for promoting development of renewable power generation. however, they are usually implemented without considering the capabilities and cost of transmission infrastructure. we use single-and multi-stage planning approaches to find cost-effective transmission and generation investments to meet single and multi-year rps goals, respectively. using a six-node network and assuming a linearized dc power flow, we examine how the lumpy nature of network reinforcements and kirchhoff 's voltage law can affect the performance of rpss. first, we show how simplified planning approaches that ignore transmission constraints, transmission lumpiness, or kirchhoff 's voltage law yield distorted estimates of the type and location of infrastructure, as well as inaccurate compliance costs to meet the renewable goals. second, we illustrate how lumpy transmission investments and kirchhoff 's voltage law result in compliance costs that are nonconvex with respect to the rps targets, in the sense that the marginal costs of meeting the rps may decrease rather than increase as the target is raised. thus, the value of renewable energy certificates (recs) also depends on the network topology, as does the amount of noncompliance with the rps, if noncompliance is penalized but not prohibited. finally, we use a multi-stage planning model to determine the optimal generation and transmission infrastructure for rps designs that set multiyear goals. we find that the optimal infrastructure to meet rps policies that are enforced year-by-year differ from the optimal infrastructure if banking and borrowing is allowed in the rec market.
voltage_law	we consider the problem of allocating the cost of a transmission system among load and generator entities. it is a known instance of the classical cooperative game theoretic problem. to solve this problem is a formidable task, as we are dealing with a combinatorial game with transferable utilities. this is an np hard problem. therefore, it suffers from the curse of dimensionality. marginal pricing is a pragmatic alternative to solve this problem since both kirchhoff current law and voltage law are strictly adhered to. however, the generic complexity of cooperative game theoretic problems cannot be just wished away. it now manifests as the difficulty in choosing an economic slack bus, which may even be dispersed. we propose the application of min-max fairness policy to solve this problem and give an algorithm which will run in polynomial time. during network cost allocation, min-max fairness policy minimizes the maximum regret among participating entities at each step. maximum regret is measured in terms of price, and lexicographic application of this principle leads to a fair and unique equilibrium price vector. results on a large network demonstrate fairness as well as tractability of the proposed approach.
control_engineering	fractional calculus is more than a three hundred-year-old concept way back during the time of de l'hospital and leibniz focusing on derivative and integrals having non-integer orders. almost four decades ago, engineers and scientists began to venture into the field of fractional calculus by unfolding its applications where fractional differential equation models are valid. it has been found that fractional calculus indeed is becoming ubiquitous, seeing applications in many fields of sciences and engineering, from fractional diffusion equations and various biomedical applications, to signal processing and control engineering applications. a conclusion was then later proposed that fractional calculus is actually a generalization of integer-order calculus, being so powerful, it could overcome the advantages of its integer-order counterparts. this paper offers a comprehensive discussion on the applications of fractional calculus in the design and implementation of fractional-order systems in the form of electronic circuits which could be used for signal processing and control engineering applications. the article starts with the introduction to fractional calculus including some history and mathematical definitions. the second part of the article focuses on fractional-order differential equations and systems. example circuit designs and implementation are then discussed which includes an elaboration of some papers related to this area. the final part of the article presents possible research topics in this area.
control_engineering	this study presents an efficient algorithm for computing precisely the hankel singular values of linear singularly perturbed systems. the algorithm is obtained in terms of reduced-order problems and avoids numerical ill-conditioning associated with singularly perturbed systems when the singular perturbation parameter is very small. the study compares the presented algorithm with the algorithm that exists in the control engineering literature and demonstrates its superiority. in addition, an analysis of system order reduction via balancing and singular perturbations is performed and it is concluded that the system order reduction via balancing is more general than the system order reduction obtained via the method of singular perturbations.
control_engineering	we develop an evolutionary fuzzy proportional-integral-derivative (pid) controller for a permanent magnet synchronous motor (pmsm). we first consider a fuzzy pid control design problem based on the common control engineering knowledge that good transient performances can be obtained by increasing the p and i gains and decreasing the d gain when the transient error is big. then we give an evolutionary algorithm (ea) to autotune the control parameters of the fuzzy pid controller. we implement the proposed ea-based fuzzy pid control controller in real time on a texas instruments tms320f28335 floating-point dsp. we also give simulation and experimental results to show the effectiveness of the proposed intelligent digital control system under abrupt load torque variation using a prototype pmsm.
control_engineering	the purpose of this paper is to explore the private university engineering master 's service state special needs talent training project, how to improve the practical ability of engineering master to solve the practical problems of enterprises is a practical problem faced by the pilot colleges and universities. this paper discusses the application of the school practice base in the field of control engineering of xijing university, the school enterprise joint training base, relying on scientific research cooperation platform and so on; it is feasible to take the practice ability as the core of engineering master 's training mode in the field of control engineering of xijing university. the innovation of this paper is to discuss how to improve the practice ability of the students in private colleges and universities from the private colleges and universities.
control_engineering	this paper presents the structure, functionality and application of an improved remote laboratory for engineering students hosted at ghent university. the remote laboratory consists of two setups: ball and plate system and quadruple water tank system. these setups introduce basic control aspects such as pid control design and non-minimum phase systems. also more challenging aspects such as multiple-input-multiple-output control, decoupled and decentralized systems and advanced control strategies such as internal model control or model predictive control can be investigated on the setups. based on a feedback study dial targeted the bachelor degree students, the level of effectiveness of this concept has been shown but also possible functional enhancements that can be applied to the systems have been pointed out. the feedback survey data concluded that the remote laboratory has attracted the attention of students and had a positive impact in their training. (c) 2015, ifac (international federation of automatic control) hosting by elsevier ltd. all rights reserved,
