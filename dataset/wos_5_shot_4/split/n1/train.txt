symbolic_computation	nowadays, marine scientists are making use of the kadomtsev-petviashvili (kp)-category equations in their investigations from the straits of georgia and gibraltar to the adriatic sea, north sea and south china sea. in fluid mechanics and other fields, the (3+1)-dimensional b-type kp equations have attracted a good size of recent research. for a generalized (3+1)-dimensional variable-coefficient b-type kp equation for the nonlinear waves in fluid mechanics, with symbolic computation, we obtain a variable-coefficient-dependent auto-backlund transformation, along with two variable-coefficient-dependent families of the shock-wave-type solutions. (c) 2015 elsevier ltd. all rights reserved.
symbolic_computation	under investigation in this paper is a (3 + 1)-dimensional variable-coefficient kadomtsev-petviashvili equation, which describes the propagation of surface and internal water waves. by virtue of the binary bell polynomials, symbolic computation and auxiliary independent variable, the bilinear forms, soliton solutions, backlund transformations and lax pair are obtained. variable coefficients of the equation can affect the solitonic structure, when they are specially chosen, while curved and linear solitons are illustrated. elastic collisions between/among two and three solitons are discussed, through which the solitons keep their original shapes invariant except for some phase shifts.
symbolic_computation	under investigation in this paper is a higher-order nonlinear schrodinger equation in an optical fiber. lax pair and infinitely-many conservation laws are derived via the symbolic computation. by virtue of the darboux transformation, one-, two- and three-soliton solutions are derived. propagation and interaction of the solitons are illustrated graphically: velocity of the soliton is linearly related to the higher-order dispersion coefficients, while amplitude of the soliton does not depend on them at all. width of the one soliton increases with the decrease of the intensity of the complex eigenvalue parameter. head-on interaction between the two bidirectional solitons as well as overtaking and oscillating interaction between the two unidirectional solitons are presented. for the interactions among the three solitons, we display two head-on and one overtaking interactions along with three overtaking interactions. graphical analysis shows that each interaction between the two solitons is elastic, and each interaction among the three solitons is pairwise elastic. (c) 2016 published by elsevier gmbh.
symbolic_computation	with the aid of symbolic computation maple, the discrete ablowitz ladik equation is studied via an algebra method, some new rational solutions with four arbitrary parameters are constructed. by analyzing related parameters, the discrete rogue wave solutions with alterable positions and amplitude for the focusing ablowitz ladik equations are derived. some properties are discussed by graphical analysis, which might be helpful for understanding physical phenomena in optics.
symbolic_computation	we explore the shape changing and clevaging nature of anti-kink solutions of a (3+1)-dimensional b-type kadomtsev-petviashvili equation. we achieved this by invoking the multiple exp-function method aided with symbolic computation which remains an indispensable tool to deal with computational algebraic systems.
computer_vision	speed and accuracy are important factors when dealing with time-constraint events for disaster, risk, and crisis-management support. object-based image analysis can be a time consuming task in extracting information from large images because most of the segmentation algorithms use the pixel-grid for the initial object representation. it would be more natural and efficient to work with perceptually meaningful entities that are derived from pixels using a low-level grouping process (superpixels). firstly, we tested a new workflow for image segmentation of remote sensing data, starting the multiresolution segmentation (mrs, using esp2 tool) from the superpixel level and aiming at reducing the amount of time needed to automatically partition relatively large datasets of very high resolution remote sensing data. secondly, we examined whether a random forest classification based on an oversegmentation produced by a simple linear iterative clustering (slic) superpixel algorithm performs similarly with reference to a traditional object-based classification regarding accuracy. tests were applied on quickbird and worldview-2 data with different extents, scene content complexities, and number of bands to assess how the computational time and classification accuracy are affected by these factors. the proposed segmentation approach is compared with the traditional one, starting the mrs from the pixel level, regarding geometric accuracy of the objects and the computational time. the computational time was reduced in all cases, the biggest improvement being from 5 h 35 min to 13 min, for a worldview-2 scene with eight bands and an extent of 12.2 million pixels, while the geometric accuracy is kept similar or slightly better. slic superpixel-based classification had similar or better overall accuracy values when compared to mrs-based classification, but the results were obtained in a fast manner and avoiding the parameterization of the mrs. these two approaches have the potential to enhance the automation of big remote sensing data analysis and processing, especially when time is an important constraint.
computer_vision	dual assignment clustering (dac) has been recently proposed in computer vision, shown to yield improved accuracy for action clustering tasks. the key idea of dac is to consider another view (different from the original features) for the same set of samples, and to exploit the statistical correlation between cluster assignments in two views. however, the existing optimization is heuristic, mainly due to the difficulty in combinatorial optimization for hard cluster assignment. in this paper, we introduce a novel dac optimization algorithm based on a probabilistic (soft) treatment, where the proposed objective function incorporates both the goodness of clustering in each view and the correlation between two views in a more principled and theoretically sound fashion. we also propose a lower-bound maximization technique that not only admits fast per-iteration solutions but also guarantees convergence to a local optimum. the superiority of the proposed approach to the existing methods is demonstrated for several activity video datasets. (c) 2016 elsevier inc. all rights reserved.
computer_vision	classifying 3d measurement data has become a core problem in photogrammetry and 3d computer vision, since the rise of modern multiview geometry techniques, combined with affordable range sensors. we introduce a markov random field-based approach for segmenting textured meshes generated via multi-view stereo into urban classes of interest. the input mesh is first partitioned into small clusters, referred to as superfacets, from which geometric and photometric features are computed. a random forest is then trained to predict the class of each superfacet as well as its similarity with the neighboring superfacets. similarity is used to assign the weights of the markov random field pairwise-potential and to account for contextual information between the classes. the experimental results illustrate the efficacy and accuracy of the proposed framework. (c) 2016 published by elsevier b.v. on behalf of international society for photogrammetry and remote sensing, inc. (isprs).
computer_vision	simple linear regression in the functional errors-in-variables (eiv) model is revisited from a different perspective, where the problem is addressed by using the small-sigma model instead of large sample theory. a general analysis is developed to study the slope 's estimator that minimizes a family of objective functions, of which the least-squares fit and the maximum likelihood estimator are minimizers of such special functions. general formulas for the higher-order terms of the bias, the variance, and the mean square error are derived. accordingly, two efficient estimators are proposed after implementing the pre- and the post-bias elimination techniques. numerical tests confirm the superiority of the proposed estimators over others.
computer_vision	due to its ability to eliminate the visual ambiguities in single-shot algorithms, video-based person re identification has received an increasing focus in computer vision. visual ambiguities caused by variations in view angle, lighting, and occlusions make the re-identification problem extremely challenging. to overcome the ambiguities, most previous approaches often extract robust feature representations or learn a sophisticated feature transformation. however, most of these approaches ignore the effect of the impostors arising from annotation or tracking process. in this case, impostors are regarded as genuine and applied in training process, leading to the model drift problem. in order to reduce the risk of model drifting, we propose to automatically discover impostors in a multiple instance metric learning framework. specifically, we propose a knn based confidence score to evaluate how much an impostor invades the interested target and utilize it as a prior in the framework. in the meanwhile, we integrate an impostor rejection mechanism in the multiple instance metric learning framework to automatically discover impostors, and learn the semantical similarity metrics with the refined training set. experiments show that the proposed system performs favorably against the state-of-the-art algorithms on two challenging datasets (ilids-vid and prid 2011). we have improved the rank 1 recognition rate on ilids-vid and prid 2011 dataset by 1.0% and 1.2%, respectively. (c) 2017 elsevier ltd. all rights reserved.
computer_graphics	the fashion industry is one of the most flourishing fields for visual applications of it. due to the importance of the concept of look in fashion, the most advanced applications of computer graphics and sensing may fruitfully be exploited. the existence of low cost solutions in similar fields, such as the ones that empower the domestic video games market, suggest that analogous low cost solutions are viable and can foster innovation even in small and medium enterprises. in this paper the current state of development of vmannequin, a dynamic, user mimicking, user enacted virtual mannequin software solution, is presented. in order to allow users designing dress concepts, the application simulate the creation and fitting of clothes on virtual models. the interaction is sensor based, in order to both simplify the user interface, and create a richer involvement inside the application.
computer_graphics	orientation and wayfinding in architectural immersive virtual environments (ives) are non-trivial, accompanying tasks which generally support the users' main task. world in miniatures (wims)-essentially 3d maps containing a scene replica-are an established approach to gain survey knowledge about the virtual world, as well as information about the user 's relation to it. however, for large-scale, information-rich scenes, scaling and occlusion issues result in diminishing returns. since there typically is a lack of standardized information regarding scene decompositions, presenting the inside of self-contained scene extracts is challenging. therefore, we present an automatic wim generation workflow for arbitrary, realistic in-and outdoor ives in order to support users with meaningfully selected and scaled extracts of the ive as well as corresponding context information. additionally, a 3d user interface is provided to manually manipulate the represented extract.
computer_graphics	the human visual system (hvs) attempts to select salient areas to reduce cognitive processing efforts. computational models of visual attention try to predict the most relevant and important areas of videos or images viewed by the human eye. such models, in turn, can be applied to areas such as computer graphics, video coding, and quality assessment. although several models have been proposed, only one of them is applicable to high dynamic range (hdr) image content, and no work has been done for hdr videos. moreover, the main shortcoming of the existing models is that they cannot simulate the characteristics of hvs under the wide luminous range found in hdr content. this paper addresses these issues by presenting a computational approach to model the bottom-up visual saliency for hdr input by combining spatial and temporal visual features. an analysis of eye movement data affirms the effectiveness of the proposed model. comparisons employing three well-known quantitative metrics show that the proposed model substantially improves predictions of visual attention for hdr content.
computer_graphics	in this paper we present different optimization techniques on look-up table based algorithms for double precision floating point arithmetic. based on our analysis of different look-up table based algorithms in the literature, we re-engineer basics blocks of the algorithms ( i.e. multiplier(s) and adder(s)) to facilitate area and timing benefits to achieve higher performance. we propose different look-up table optimization techniques for the algorithms. we also analyze trade-off in employing exact rounding ( 0.5ulp) ( unit in the last place) in the double precision floating point unit. based on performance and extensibility criteria we take algorithms proposed by wong and goto as a base case to validate our optimization techniques and compare the performance with other algorithms in the literature. we improve the performance ( latency x area) of wong and goto division algorithm by 26.94%.
computer_graphics	computer graphics and image processing technologies for editing photographs taken by a user, and for photographic entertainment, have increased in recent years. one of these techniques, non-photorealistic rendering (npr), is the focus of this study. in this study, we propose a method for applying david hockneys photographic collage filtering on mobile devices by using several well-known computer graphics and image processing techniques. this process involves taking a sequence of images and extracting feature points, and using these feature points to deform an image. finally, we obtain a photographic collage using the david hockney style by generating a patchwork.
operating_systems	the weak separation between user-and kernel-space in modern operating systems facilitates several forms of privilege escalation. this paper provides a survey of protection techniques, both cutting-edge and time-tested, used to prevent common privilege escalation attacks. the techniques are compared against each other in terms of their effectiveness, their performance impact, the complexity of their implementation, and their impact on diversification techniques such as aslr. overall the literature provides a litany of disjoint techniques, each of which trades some performance cost for effectiveness against a particular isolated threat. no single technique was found to effectively mitigate all known and potential attack vectors with reasonable performance cost overhead.
operating_systems	we describe the main features of the developed software tool, namely plate-motion 2.0 (pem2), which allows inferring the euler pole parameters by inverting the observed velocities at a set of sites located on a rigid block (inverse problem). pem2 allows also calculating the expected velocity value for any point located on the earth providing an euler pole (direct problem). pem2 is the updated version of a previous software tool initially developed for easy-to-use file exchange with the gamit/globk software package. the software tool is developed in matlab(a (r)) framework and, as the previous version, includes a set of matlab functions (m-files), guis (fig-files), map data files (mat-files) and user 's manual as well as some example input files. new changes in pem2 include (1) some bugs fixed, (2) improvements in the code, (3) improvements in statistical analysis, (4) new input/output file formats. in addition, pem2 can be now run under the majority of operating systems. the tool is open source and freely available for the scientific community.
operating_systems	a portable real-time facial recognition system that is able to play personalized music based on the identified person 's preferences was developed. the system is called portable facial recognition jukebox using fisherfaces (frj). raspberry pi was used as the hardware platform for its relatively low cost and ease of use. this system uses the opencv open source library to implement the computer vision fisherfaces facial recognition algorithms, and uses the simple directmedia layer (sdl) library for playing the sound files. frj is crossplatform and can run on both windows and linux operating systems. the source code was written in c++. the accuracy of the recognition program can reach up to 90% under controlled lighting and distance conditions. the user is able to train up to 6 different people (as many as will fit in the gui). when implemented on a raspberry pi, the system is able to go from image capture to facial recognition in an average time of 200ms.
operating_systems	the paper evaluates usability of information modelling tools on the most common operating systems (windows, linux, mac osx). stages of bim maturity and availability of building information modelling tools vary dramatically on these platforms. paper compares the current software tools, and how they can be used in the building process. the list of attributes that mature software platform should accommodate was created. evaluation criteria are determined by specific needs of the various participants of the building process. this leads to the successful project completion and subsequent management of the lifecycle of the building. (c) 2016 the authors. published by elsevier ltd.
operating_systems	background: previously, we described rover, a dna variant caller which identifies genetic variants from pcr-targeted massively parallel sequencing (mps) datasets generated by the hi-plex protocol. rover permits stringent filtering of sequencing chemistry-induced errors by requiring reported variants to appear in both reads of overlapping pairs above certain thresholds of occurrence. rover was developed in tandem with hi-plex and has been used successfully to screen for genetic mutations in the breast cancer predisposition gene palb2. rover is applied to mps data in bam format and, therefore, relies on sequence reads being mapped to a reference genome. in this paper, we describe an improvement to rover, called undr rover (unmapped primer-directed rover), which accepts mps data in fastq format, avoiding the need for a computationally expensive mapping stage. it does so by taking advantage of the location-specific nature of pcr-targeted mps data. results: the undr rover algorithm achieves the same stringent variant calling as its predecessor with a significant runtime performance improvement. in one indicative sequencing experiment, undr rover (in its fastest mode) required 8-fold less sequential computation time than the rover pipeline and 13-fold less sequential computation time than a variant calling pipeline based on the popular gatk tool. undr rover is implemented in python and runs on all popular posix-like operating systems (linux, os x). it requires as input a tab-delimited format file containing primer sequence information, a fasta format file containing the reference genome sequence, and paired fastq files containing sequence reads. primer sequences at the 5' end of reads associate read-pairs with their targeted amplicon and, thus, their expected corresponding coordinates in the reference genome. the primer-intervening sequence of each read is compared against the reference sequence from the same location and variants are identified using the same algorithm as rover. specifically, for a variant to be 'called' it must appear at the same location in both of the overlapping reads above user-defined thresholds of minimum number of reads and proportion of reads. conclusions: undr rover provides the same rapid and accurate genetic variant calling as its predecessor with greatly reduced computational costs.
machine_learning	we propose herein a new portfolio selection method that switches between two distinct asset allocation strategies. an important component is a carefully designed adaptive switching rule, which is based on a machine learning algorithm. it is shown that using this adaptive switching strategy, the combined wealth of the new approach is a weighted average of that of the successive constant rebalanced portfolio and that of the 1/n portfolio. in particular, it is asymptotically superior to the 1/n portfolio under mild conditions in the long run. applications to real data show that both the returns and the sharpe ratios of the proposed binary switch portfolio are the best among several popular competing methods over varying time horizons and stock pools.
machine_learning	cutting plane algorithm (cpa) is a generalization of iterative first-order gradient method, in which the objective function is approximated successively by supporting hyperplanes. cpa has been tailored to solve regularized loss minimization in machine learning by exploiting the regularization structure. in particular, for linear support vector machine (svm) embedding a line search procedure effectively remedies the fluctuations of function value and speeds up the convergence in practical issue. however, the existing line search strategy based on sorting algorithm takes o(mlogm) time. in this paper, we propose a more effective line search solver which spends only linear time. it can be extended to multiclass svm in which an optimized explicit piecewise linear function finding algorithm is prearranged. the total svm training time is proved to reduce theoretically and experiments consistently confirm the effectiveness of the proposed algorithms. (c) 2017 elsevier ltd. all rights reserved.
machine_learning	nonlinear similarity measures defined in kernel space, such as correntropy, can extract higher order statistics of data and offer potentially significant performance improvement over their linear counterparts especially in non gaussian signal processing and machine learning. in this paper, we propose a new similarity measure in kernel space, called the kernel risk-sensitive loss (krsl), and provide some important properties. we apply the krsl to adaptive filtering and investigate the robustness, and then develop the mkrsl algorithm and analyze the mean square convergence performance. compared with correntropy, the krsl can offer a more efficient performance surface, thereby enabling a gradient-based method to achieve faster convergence speed and higher accuracy while still maintaining the robustness to outliers. theoretical analysis results and superior performance of the new algorithm are confirmed by simulation.
machine_learning	gamma spectrometric field measurements may provide high resolution information on topsoil texture. yet, calibrations for the estimation of texture data usually have to be done site-specifically. the lack of site-independent calibrations thus limits the easy and universal use of proximal gamma-ray sensing in soil mapping and precision agriculture. our objective was to develop a study site-independent prediction model for topsoil texture from gamma-ray spectra. we surveyed ten study sites across germany with 417 reference samples (291 for calibration, 126 for test set-validation), providing soils from a broad range of parent materials and with widely varying soil texture. first, study site-specific models were calibrated by a linear regression approach. these models provided reliable estimations of sand, silt, and clay for most of the study sites. second, study site-independent models were calibrated via i) linear regression and ii) support vector machines (svm), the latter being mathematical methods of data pattern recognition. based on the non-linear relationship between gamma spectrum and soil texture, which varied widely between the different parent materials the linear models are not appropriate for satisfactory soil texture prediction (averaged r-2 of 0.73 for sand, 0.61 for silt, and 0.18 for clay and averaged absolute prediction errors of 9 to 5%, respectively). in contrast, the svm calibrated prediction models revealed reliable performance also for site-independent calibrations. with the non-linear svm approach we were able to include all sites in one single prediction model for each texture fraction although the different mineralogical composition of their parent materials led to complex and partly opposing relationships between gamma features and soil texture. site-independent predictions via svm were often even better than site-specific linear regression models. the site-independent svm calibrated predictions yielded an averaged r-2 of 0.96 (sand), 0.93 (silt), and 0.78 (clay), and corresponding averaged absolute prediction errors of 2 to 4%, respectively. to summarize, (i) non-linear prediction models are a feasible approach for capable site-independent texture estimations across a wide range of soils and (ii) gamma spectrometry based texture predictions are a valuable input for applications that require highly resolved texture information at low costs and efforts. (c) 2016 elsevier b.v. all rights reserved.
machine_learning	lexicographic preferences on a set of attributes provide a cognitively plausible structure for modeling the behavior of human decision makers. therefore, the induction of corresponding models from revealed preferences or observed decisions constitutes an interesting problem from a machine learning point of view. in this paper, we introduce a learning algorithm for inducing generalized lexicographic preference models from a given set of training data, which consists of pairwise comparisons between objects. our approach generalizes simple lexicographic orders in the sense of allowing the model to consider several attributes simultaneously (instead of looking at them one by one), thereby significantly increasing the expressiveness of the model class. in order to evaluate our method, we present a case study of a highly complex real-world problem, namely the choice of the recognition method for actuarial gains and losses from occupational pension schemes. using a unique sample of european companies, this problem is well suited for demonstrating the effectiveness of our lexicographic ranker. furthermore, we conduct a series of experiments on benchmark data from the machine learning domain. (c) 2016 elsevier b.v. all rights reserved.
data_structures	the literature presents many application programming interfaces (apis) and frameworks that provide state of the art algorithms and techniques for solving optimisation problems. the same cannot be said about apis and frameworks focused on the problem data itself because with the peculiarities and details of each variant of a problem, it is virtually impossible to provide general tools that are broad enough to be useful on a large scale. however, there are benefits of employing problem-centred apis in a r&d environment: improving the understanding of the problem, providing fairness on the results comparison, providing efficient data structures for different solving techniques, etc. therefore, in this work we propose a novel design methodology for an api focused on an optimisation problem. our methodology relies on a data parser to handle the problem specification files and on a set of efficient data structures to handle the information on memory, in an intuitive fashion for researchers and efficient for the solving algorithms. also, we present the concepts of a solution dispenser that can manage solutions objects in memory better than built-in garbage collectors. finally, we describe the positive results of employing a tailored api to a project involving the development of optimisation solutions for workforce scheduling and routing problems.
data_structures	reaction mechanism generator (rmg) constructs kinetic models composed of elementary chemical reaction steps using a general understanding of how molecules react. species thermochemistry is estimated through benson group additivity and reaction rate coefficients are estimated using a database of known rate rules and reaction templates. at its core, rmg relies on two fundamental data structures: graphs and trees. graphs are used to represent chemical structures, and trees are used to represent thermodynamic and kinetic data. models are generated using a rate-based algorithm which excludes species from the model based on reaction fluxes. rmg can generate reaction mechanisms for species involving carbon, hydrogen, oxygen, sulfur, and nitrogen. it also has capabilities for estimating transport and solvation properties, and it automatically computes pressure-dependent rate coefficients and identifies chemically-activated reaction paths. rmg is an object-oriented program written in python, which provides a stable, robust programming architecture for developing an extensible and modular code base with a large suite of unit tests. computationally intensive functions are cythonized for speed improvements. program summary program title: rmg catalogue identifier: aezw_v1_0 program summary url: http://cpc.cs.qub.ac.uk/summaries/aezw_v1_0.html program obtainable from: cpc program library, queen 's university, belfast, n. ireland licensing provisions: mit/x11 license no. of lines in distributed program, including test data, etc.: 958681 no. of bytes in distributed program, including test data, etc.: 9495441 distribution format: tar.gz programming language: python. computer: windows, ubuntu, and mac os computers with relevant compilers. operating system: unix/linux/windows. ram: 1 gb minimum, 16 gb or more for larger simulations classification: 16.12. external routines: rdkit, open babel, dassl, daspk, dqed, numpy, scipy nature of problem: automatic generation of chemical kinetic mechanisms for molecules containing c, h, 0, s, and n. solution method: rate-based algorithm adds most important species and reactions to a model, with rate constants derived from rate rules and other parameters estimated via group additivity methods. additional comments: the rmg software package also includes cantherm, a tool for computing the thermodynamic properties of chemical species and both high-pressure-limit and pressure-dependent rate coefficients for chemical reactions using results from quantum chemical calculations. cantherm is compatible with a variety of ab initio quantum chemistry software programs, including but not limited to gaussian, mopac, qchem, and molpro. running time: from 30 s for the simplest molecules, to up to several weeks, depending on the size of the molecule and the conditions of the reaction system chosen. (c) 2016 the authors. published by elsevier b.v.
data_structures	this paper presents elastic transactions, an appealing alternative to traditional transactions, in particular to implement search structures in shared memory multicore architectures. upon conflict detection, an elastic transaction might drop what it did so far within a separate transaction that immediately commits, and resume its computation within a new transaction which might itself be elastic. we present the elastic transaction model and an implementation of it, then we illustrate its simplicity and performance on various concurrent data structures, namely double-ended queue, hash table, linked list, and skip list elastic transactions outperform classical ones on various workloads, with an improvement of 35% on average. they also exhibit competitive performance compared to lock-based techniques and are much simpler to program with than lock-free alternatives. (c) 2016 elsevier inc. all rights reserved.
data_structures	high-level synthesis (hls) promises a significant shortening of the fpga design cycle by raising the abstraction level of the design entry to high-level languages such as c/c++. however, applications using dynamic, pointer-based data structures and dynamic memory allocation remain difficult to implement well, yet such constructs are widely used in software. automated optimizations that leverage the memory bandwidth of fpgas by distributing the application data over separate banks of on-chip memory are often ineffective in the presence of dynamic data structures due to the lack of an automated analysis of pointerbased memory accesses. in this work, we take a step toward closing this gap. we present a static analysis for pointer-manipulating programs that automatically splits heap-allocated data structures into disjoint, independent regions. the analysis leverages recent advances in separation logic, a theoretical framework for reasoning about heap-allocated data that has been successfully applied in recent software verification tools. our algorithm focuses on dynamic data structures accessed in loops and is accompanied by automated source-to-source transformations that enable automatic loop parallelization and memory partitioning by off-the-shelf hls tools. we demonstrate the successful loop parallelization and memory partitioning by our tool flow using three real-life applications that build, traverse, update, and dispose of dynamically allocated data structures. our case studies, comparing the automatically parallelized to the direct hls implementations, show an average latency reduction by a factor of 2x across our benchmarks.
data_structures	we consider d-dimensional lattice path models restricted to the first orthant whose defining step sets exhibit reflective symmetry across every axis. given such a model, we provide explicit asymptotic enumerative formulas for the number of walks of a fixed length: the exponential growth is given by the number of distinct steps a model can take, while the sub-exponential growth depends only on the dimension of the underlying lattice and the number of steps moving forward in each coordinate. the generating function of each model is first expressed as the diagonal of a multivariate rational function, then asymptotic expressions are derived by analyzing the singular variety of this rational function. additionally, we show how to compute subdominant growth, reflect on the difference between rational diagonals and differential equations as data structures for d-finite functions, and show how to determine first order asymptotics for the subset of walks that start and end at the origin.
network_security	in this paper, a spectrally coded optical code division multiple access (ocdma) system using a hybrid modulation scheme has been investigated. the idea is to propose an effective approach for simultaneous improvement of the system capacity and security. data formats, nrz (non-return to zero), dqpsk (differential quadrature phase shift keying), and poisk (polarisation shift keying) are used to get the orthogonal modulated signal. it is observed that the proposed hybrid modulation provides efficient utilisation of bandwidth, increases the data capacity and enhances the data confidentiality over existing ocdma systems. further, the proposed system performance is compared with the current state-of-the-art ocdma schemes. (c) 2015 elsevier inc. all rights reserved.
network_security	as computer networks are emerging in everyday life, network security has become an important issue. simultaneously, attacks are becoming more sophisticated, making the defense of computer networks increasingly difficult. attack graph is a modelling tool used in the assessment of security of enterprise networks. since its introduction a considerable amount of research effort has been spent in the development of theory and practices around the idea of attack graph. this paper presents a consolidated view of major attack graph generation and analysis techniques.
network_security	network attacks and cyber-security breaches may be the cause of huge monetary damages in the modern information-based economy; thus, the need for network security is stronger than ever as it is the need for full traffic sanitization. nonetheless, the purge of malicious packets is still too often relegated to the destination of the attacks letting the unwanted traffic roam freely. at the same time, the next generation of routers promises to be able to modulate energy consumption on the basis of actual traffic, thus the presence of malicious traffic in the network is a cause of economic losses in itself, even when the attack is not successful. in past work, we modelled and analysed the energy savings enabled by aggressive intrusion detection; however, fluctuations in the traffic intensity has not been fully taken into account. in this article, we introduce a new enhanced adaptive model that takes into full account the actual load of routers including what is due to forecasting errors.
network_security	recently, in many countries, military has adopted cloud, big data, iot, etc. in order to win the war. therefore, a favorable environment for future battles based on the soldiers with a variety of iot technologies that will foster and build elite combat forces in the center can be expected. similar to the conventional internet environment, iot is not only the type of security threat for a variety of networks, data, and personal information for each of the features has also identified protocol management. therefore, to enhance the security of the environment in the future iot based on full-length, it is necessary to study the security priority. a recent survey of military it professionals shows communications / network security is the most important sector. in addition, the survey can distinguish between the security field will be considered fragile. to the study of future iot-based battlefield, see the results of this study are reflected in the professional military security structure, we should effectively against threats expected in a given amount of the budget.
network_security	with the explosive growth of mobile terminal access to the network and the shortage of ipv4, the network address translation (nat) technology has become more and more widely used. the technology not only provides users with convenient access to the internet, but also brings trouble to network operators and regulatory authorities. this system nat detection using netflow data, is often used for monitoring and forensics analysis in large networks. in the paper, in order to detect nat devices, an out-in activity degree method based on network behavior is proposed. our approach works completely passively and is based on netflow data only. our approach gets accuracy of 91.2 % in real large-scale network for a long time.
image_processing	using image processing to extract nodular or linear shadows is a key technique of computer-aided diagnosis schemes. this study proposes a new method for extracting nodular and linear patterns of various sizes in medical images. we have developed a morphology filter bank that creates multiresolution representations of an image. analysis bank of this filter bank produces nodular and linear patterns at each resolution level. synthesis bank can then be used to perfectly reconstruct the original image from these decomposed patterns. our proposed method shows better performance based on a quantitative evaluation using a synthesized image compared with a conventional method based on a hessian matrix, often used to enhance nodular and linear patterns. in addition, experiments show that our method can be applied to the followings: (1) microcalcifications of various sizes in mammograms can be extracted, (2) blood vessels of various sizes in retinal fundus images can be extracted, and (3) thoracic ct images can be reconstructed while removing normal vessels. our proposed method is useful for extracting nodular and linear shadows or removing normal structures in medical images.
image_processing	a modified gaussian current density is put forward, which is used to simulate the dynamic process of az31b magnesium alloy double-electrode gas metal arc welding (de-gmaw) droplet transfer. the process of droplet transfer in the az31b magnesium alloy de-gmaw was simulated using fluent software. the influence of different bypass welding currents was investigated with a constant total current. the simulated results revealed that when the bypass current was 0 a, the process was the globular transfer type, the droplet transfer with a bypass current of 80 a was a projected transfer type and the droplet transfer at a bypass current of 170 a was a spray transfer type. the critical current is decreased so that spray transfer would occur at a lower current level in the de-gmaw process. as the bypass current was increased, the shape of droplet changed from oval to round and the transition frequency of the droplet increased in a stepwise fashion. to confirm the accuracy of the simulated results, welding experiments were performed and the image processing method was used to obtain the droplet size. the simulated and experimental results were found to be in good agreement.
image_processing	landslides are considered as one of the natural hazards responsible for casualties, damage of assets, and infrastructures. in many situations, collection of field data from remote places is difficult due to inaccessibility of landslide area. this paper examines landslide susceptibility in the bukit antarabangsa, kuala lumpur, to ease geographical studies, using image processing and multivariate statistical tools by reviewing the digital images using remote-sensing technique without any physical survey. we considered different pixel resolutions and report the effectiveness of using factor analysis, principal component analysis, linear discriminant analysis, and their hybridization. eight types of databases for heavy, medium, and no landslide were created. the modeling works were carried out at 2 x 2, 4 x 4, 8 x 8, 16 x 16, 32 x 32, 64 x 64, 128 x 128, and 256 x 256 pixel resolutions. results indicate 2 x 2 was optimal in both heavy and medium while 8 x 8 found to be ideal for no landslide region. performance at different pixel resolutions was compared using receiver operating characteristic (roc) curves, and average success of 87.36% was found. this simple yet robust system holds great potential for saving lives.
image_processing	very little is known about how individual soil particles move over a soil surface as a result of rainfall. specifically there is virtually no information about the pathway a particle takes, the speed at which it travels and when it is in motion. here we present a novel technique that can give insight into the movement of individual soil particles. by combining novel fluorescent videography techniques with custom image processing and a fluorescent soil tracer we have been able to trace the motion of soil particles under simulated rainfall in a laboratory soil flume. the system is able track multiple sub-millimeter particles simultaneously, establishing their position 50 times a second with sub-millimeter precision. an analysis toolkit has been developed enabling graphical and numerical analysis of the data obtained. for example, we are able to visualise and quantify parameters such as distance and direction of travel. based on our observations we have created a conceptual model (stop, hop, roll) which attempts to present a unified model for the movement of soil particles across a soil surface. it is hoped that this technology will open up new opportunities to create, parameterise and evaluate soil models as the motion of individual soil particles can now be easily monitored. (c) 2016 the authors. published by elsevier b.v.
image_processing	in this paper, we analyse patterns in face shape variation due to weight gain. we propose the use of persistent homology descriptors to get geometric and topological information about the configuration of anthropometric 3d face landmarks. in this way, evaluating face changes boils down to comparing the descriptors computed on 3d face scans taken at different times. by applying dimensionality reduction techniques to the dissimilarity matrix of descriptors, we get a space in which each face is a point and face shape variations are encoded as trajectories in that space. our results show that persistent homology is able to identify features which are well related to overweight and may help assessing individual weight trends. the research was carried out in the context of the european project semeoticons, which developed a multisensory platform which detects and monitors over time facial signs of cardio-metabolic risk.
parallel_computing	traditional fluid-structure interaction techniques are based on arbitrary lagrangian-eulerian method, where strong coupling method is used to solve fundamental discretized equations. after discretization, resulting nonlinear systems are solved using an iterative method leading to set of linear subsystems. the resulting linear subsystem matrices are sparse non-symmetric indefinite with small or zero diagonal entries. the small pivots of these matrices may render the incomplete lu factorization unstable or inaccurate. to improve that, this paper proposes an approach that can be used together with a stabilization and offers an additional feature to be used to gain an advantage in its parallelization. (c) 2016 published by elsevier ltd.
parallel_computing	recently the hardware performance of mobile devices have been extremely increased and advanced mobile devices provide multi-cores and high clock speed. in addition, mobile devices have advantages in mobility and portability compared with pc and console, so many games and simulation programs have been developed under mobile environments. physically-based simulation is a one of the key issues for deformable object modeling which is widely used to represent the realistic expression of 3d soft objects with tetrahedrons for game and 3d simulation. however, it requires high computation power to plausibly and realistically represent the physical behaviors and interactions of deformable objects. in this paper, we implemented parallel cloth and mass-spring simulation using graphics processing unit (gpu) with opencl and multi-threaded central processing unit ( cpu) on a mobile device. we applied cpu and gpu parallel computing technique into spring force computation and integration methods such as euler, midpoint, 4th-order runge-kutta to optimize the computational burden of dynamic simulation. the integration methods compute the next step of positions and velocities in each node. in this paper, we tested the performance analysis for the spring force calculation and integration method process using cpu only, multi-threaded cpu, and gpu on mobile device respectively. our experimental results concluded that the calculation using proposed multi-threaded cpu and gpu multi-threaded cpu are much faster than using just the cpu only.
parallel_computing	modern gpgpu 's have enabled massively parallel computing with programmability that can exploit the highly parallel nature of ldpc decoding. previous works customized the design on a gpgpu towards specific execution attributes of a particular ldpc decoding matrix. supporting different ldpc decoding matrices requires either substantial rework on the current program, or a brand new parallel design. this paper proposes two unified designs that can achieve high performance for both regular and irregular ldpc decoding on a gpgpu. the first design introduces a node-based scheme with a versatile translation array mechanism that can efficiently handle the complex data access patterns of different ldpc decoding matrices. the second design proposes an edge-based parallel paradigm that uses more intuitive data layout. more edges than nodes in a tanner graph also give the edge-based design higher computation parallelism when there are limited concurrent codewords. with the proposed unified designs, designers can be ignorant of the types of ldpc matrices and achieve high performance ldpc decoding. the experiments on a gtx 470 gpgpu have demonstrated up to 134.56x runtime improvement, when compared with designs on a high-end cpu. the maximum throughput can reach 80.25 mbps. when compared with the previous customized designs, the proposed systematic designs can reach better performance while relieving the effort of customization.
parallel_computing	we propose a computational framework for the simulation of deformation and fracture in shells that is well suited to situations with widespread damage and fragmentation due to impulsive loading. the shell is modeled with a shear-flexible theory and discretized with a discontinuous galerkin finite element method, while fracture is represented with a cohesive zone model on element edges. a key feature of the method is that the underlying shear-flexible shell theory enables the description of transverse shear fracture modes, in addition to the in-plane and bending modes accessible to kirchhoff-love thin shell formulations. this is especially important for impulsive loading conditions, where shear-off failure near stiffeners and supports is common. the discontinuous galerkin formulation inherits the scalability properties demonstrated previously for large-scale simulation of fracture in solids, while avoiding artificial elastic compliance issues that are common in other cohesive model approaches. we demonstrate the ability of the framework to capture the transverse shear fracture mode through numerical examples, and the parallel computation capabilities of the method through the simulation of explosive decompression of the skin of a full-scale passenger aircraft fuselage. (c) 2016 published by elsevier b.v.
parallel_computing	3d finite-element (fe) mesh generation is a major hurdle for marine controlled-source electromagnetic (csem) modeling. in this paper, we present a fe discretization operator (fedo) that automatically converts a 3d finite difference (fd) model into reliable and efficient tetrahedral fe meshes for csem modeling. fedo sets up wireframes of a background seabed model that precisely honors the seafloor topography. the wireframes are then partitioned into multiple regions. outer regions of the wireframes are discretized with coarse tetrahedral elements whose maximum size is as large as a skin depth of the regions. we demonstrate that such coarse meshes can produce accurate fe solutions because numerical dispersion errors of tetrahedral meshes do not accumulate but oscillates. in contrast, central regions of the wireframes are discretized with fine tetrahedral elements to describe complex geology in detail. the conductivity distribution is mapped from fd to fe meshes in a volume-averaged sense. to avoid excessive mesh refinement around receivers, we introduce an effective receiver size. major advantages of fedo are summarized as follow. first, fedo automatically generates reliable and economic tetrahedral fe meshes without adaptive meshing or interactive cad workflows. second, fedo produces fe meshes that precisely honor the boundaries of the seafloor topography. third, fedo derives multiple sets of fe meshes from a given fd model. each fe mesh is optimized for a different set of sources and receivers and is fed to a subgroup of processors on a parallel computer. this divide and conquer approach improves the parallel scalability of the fe solution. both accuracy and effectiveness of fedo are demonstrated with various csem examples.
distributed_computing	using distributed task allocation methods for cooperating multivehicle systems is becoming increasingly attractive. however, most effort is placed on various specific experimental work and little has been done to systematically analyze the problem of interest and the existing methods. in this paper, a general scenario description and a system configuration are first presented according to search and rescue scenario. the objective of the problem is then analyzed together with its mathematical formulation extracted from the scenario. considering the requirement of distributed computing, this paper then proposes a novel heuristic distributed task allocation method for multivehicle multitask assignment problems. the proposed method is simple and effective. it directly aims at optimizing the mathematical objective defined for the problem. a new concept of significance is defined for every task and is measured by the contribution to the local cost generated by a vehicle, which underlies the key idea of the algorithm. the whole algorithm iterates between a task inclusion phase, and a consensus and task removal phase, running concurrently on all the vehicles where local communication exists between them. the former phase is used to include tasks into a vehicle 's task list for optimizing the overall objective, while the latter is to reach consensus on the significance value of tasks for each vehicle and to remove the tasks that have been assigned to other vehicles. numerical simulations demonstrate that the proposed method is able to provide a conflict-free solution and can achieve outstanding performance in comparison with the consensus-based bundle algorithm.
distributed_computing	cloud computing is one of the increasing technology that is connected with grid computing, utility computing, distributed computing. in today 's world, securing of data plays a vital role. in the computing environment, data privacy and data security is popular in fields like government, industry, and business for the future development. these are inter-related to both hardware and software. hence, this paper analyses the data security solutions in cloud computing.
distributed_computing	this paper describes the performance of the brain project, a distributed software tool for the formal modeling of numerical data using a hybrid neural-genetic programming technique. one of the most interesting characteristics of the brain project is its distributed implementation. unlike many other parallel and/or distributed solutions the only requirement of the brain project is that the collaborating personal computers must be 64-bit linux machines connected to internet via the transmission control protocol/internet protocol. the performance of the brain project is clearly enhanced with the very simple parallelization scheme illustrated in the paper. although the brain project presents many innovative solutions for the genetic programming research, this paper focuses mainly on its behavior in the distributed environment. (c) 2015 elsevier b.v. all rights reserved.
distributed_computing	this paper introduces a parallel and distributed algorithm for solving the following minimization problem with linear constraints: minimize f(1)(x(1))+ . . . + f(n) (x(n)) subject to a(1)x(1) + . . . + a(n)x(n) = c, x(1) is an element of chi(1), . . . , x(n) is an element of chi(n), where , are convex functions, are matrices, and are feasible sets for variable . our algorithm extends the alternating direction method of multipliers (admm) and decomposes the original problem into n smaller subproblems and solves them in parallel at each iteration. this paper shows that the classic admm can be extended to the n-block jacobi fashion and preserve convergence in the following two cases: (i) matrices are mutually near-orthogonal and have full column-rank, or (ii) proximal terms are added to the n subproblems (but without any assumption on matrices ). in the latter case, certain proximal terms can let the subproblem be solved in more flexible and efficient ways. we show that converges at a rate of o(1 / k) where m is a symmetric positive semi-definte matrix. since the parameters used in the convergence analysis are conservative, we introduce a strategy for automatically tuning the parameters to substantially accelerate our algorithm in practice. we implemented our algorithm (for the case ii above) on amazon ec2 and tested it on basis pursuit problems with >300 gb of distributed data. this is the first time that successfully solving a compressive sensing problem of such a large scale is reported.
distributed_computing	the primary objective of load balancing for distributed systems is to minimize the job execution time while maximizing the resource utilization. load balancing on decentralized systems need effective information exchange policy so that with minimum amount of communication the nodes have up to date information about other nodes in the system. periodic, event-based and on-demand information exchange are some important policies used for the same. all these approaches involve a lot of overhead and even sometime leading toward obsolete data with the nodes if there is a delay in the updation. this work presents an adaptive threshold-based hybrid load balancing scheme with sender and receiver initiated approach (hlbwsr) using random information exchange (rie). rie ensures that the information is exchanged in such a way that each node in the system has up-to-date state of the other nodes with much reduced communication overhead. further, the adaptive threshold ensures that almost an average numbers of jobs are executed by all the nodes in the system. the study of the effect of the use of rie on sender initiated, receiver initiated and hybrid of sender and receiver initiated load balancing approach establishes the superior performance of hlbwsr among its rie-based peers. a comparative analysis of hlbwsr, with periodic information exchange strategy, modified estimated load information scheduling algorithm and load balancing on arrival reveals its effectiveness under various test conditions. copyright (c) 2016 john wiley & sons, ltd.
algorithm_design	in this work, a novel surrogate-assisted memetic algorithm is proposed which is based on the preservation of genetic diversity within the population. the aim of the algorithm is to solve multi-objective optimization problems featuring computationally expensive fitness functions in an efficient manner. the main novelty is the use of an evolutionary algorithm as global searcher that treats the genetic diversity as an objective during the evolution and uses it, together with a non-dominated sorting approach, to assign the ranks. this algorithm, coupled with a gradient-based algorithm as local searcher and a back-propagation neural network as global surrogate model, demonstrates to provide a reliable and effective balance between exploration and exploitation. a detailed performance analysis has been conducted on five commonly used multi-objective problems, each one involving distinct features that can make the convergence difficult toward the pareto-optimal front. in most cases, the proposed algorithm outperformed the other state-of-the-art evolutionary algorithms considered in the comparison, assuring higher repeatability on the final non-dominated set, deeper convergence level and higher convergence rate. it also demonstrates a clear ability to widely cover the pareto-optimal front with larger percentage of non-dominated solutions if compared to the total number of function evaluations. (c) 2015 elsevier b.v. all rights reserved.
algorithm_design	hadoop is one of the most important big data processing and storage systems. in recent years, a lot of efforts have been put to enhance hadoop 's performance from networking perspectives. however, there are limited tools that can help researchers to verify their networking algorithm design in terms of hadoop 's performance. this paper proposes doopnet which is a framework and toolset for creating hadoop clusters in a virtualized environment and for monitoring/analysing of hadoop 's networking characteristics under different network configurations. doopnet enables users to automatically set up a hadoop cluster over docker containers running inside mininet. the hadoop traffic is collected inside the containers and virtual switches through network flow monitors. the users can easily modify network topologies or configurations through mininet, observe the networking behaviour through network flow monitors, and analyse the effects of different network settings on hadoop 's performance. examples are presented to demonstrate how to setup the doopnet testbed and analyse hadoop traffic.
algorithm_design	an efficient routing with quality of service (qos) guarantees for any safety traffic communication plays a vital role for the success of vehicle ad hoc networks (vanets). due to the connection less nature of such network, establishing stable route between connected nodes in vanets is extremely important and challenging. this paper, proposes a new algorithm for multi-constrained qos routing in vehicular network by adopting clustering approach. in the algorithm design, some qos metrics are used in addition to the stability metrics for finding and establishing a stable route to destination. this is achieved by calculating the corresponding qos provision values before selecting an optimal, reliable and stable route. nctuns 6.0 network simulator was used for the experiment of the proposed scheme. results show a significant improvement of the proposed approach in terms of broken links, routing overhead and end-to-end delay.
algorithm_design	we present in this paper a general framework to study issues of effective load balancing and scheduling in highly parallel and distributed environments such as currently built cloud computing systems. we propose a novel approach based on the concept of the sandpile cellular automaton: a decentralized multi-agent system working in a critical state at the edge of chaos. our goal is providing fairness between concurrent job submissions by minimizing slowdown of individual applications and dynamically rescheduling them to the best suited resources. the algorithm design is experimentally validated by a number of numerical experiments showing the effectiveness and scalability of the scheme in the presence of a large number of jobs and resources and its ability to react to dynamic changes in real time.
algorithm_design	traditional black-box optimization searches a set of potential solutions for those optimizing the value of a function whose analytical or algebraic form is unknown or inexistent, but whose value can be queried for any input. co-optimization is a generalization of this setting, in which fully evaluating a potential solution may require querying some function more than once, typically a very large number of times. when that 's the case, co-optimization poses unique difficulties to designing and assessing algorithms. a generally-applicable approach is to judge co-optimization algorithm performance via an aggregate over all possible functions in the problem domain. we establish formal definitions of such aggregate performance and then investigate the following questions concerning algorithm design; 1) are some algorithms strictly better than others? i.e. is there ""free lunch""? 2) do optimal algorithms exist? and 3) if so, are they practical? we formally define free lunch and aggregate optimality of co-optimization algorithms and derive generic conditions for their existence. we review and explain prior (no) free lunch results from the perspective of these conditions; we also show how this framework can be used to bridge several fields of research, by allowing formalization of their various problems and views on performance. we then apply and extend the generic results in a context involving a particular type of co-optimization called worst-case optimization. in this context we show that there exist algorithms that are aggregately-optimal for any budget (allowed number of function calls) and any starting point (set of previously uncovered function call outcomes), and also non-trivially strictly optimal for many budgets and starting points; moreover, we formalize the operation of such optimal algorithms and show that for certain domains, budgets and starting points this operation is equivalent to a simple procedure with tractable implementation a first-of-its-kind result for co-optimization. (c) 2014 the authors. published by elsevier b.v.
computer_programming	computer programming is a core subject of almost all degrees of engineering that is perceived as a very complex matter for the students, because it is very different from the other core subjects (physics, calculus, algebra, chemistry, graphics expression) that are more familiar to students. programming combines in a balanced way the two fundamental steps when developing an engineering product, design and implementation (coding), but with the important difference in the lower costs of the implementation phase, which permits students building and testing the proposed designs. besides, the new european higher education area (ehea) is based both in the acquisition of competencies and skills by the student rather than in the accumulation of knowledge, and in the use of the european credit transfer and accumulation system (ects), which supposes an important challenge for the teaching model. in this paper we show our eight years experience in developing a hybrid methodology for teaching computer programming in the degrees of mechanical, electrical, electronic and chemistry engineering at the university of almeria (spain). we integrate traditional teaching methods (participative master class for transmission of information) with modern methods (problem-based learning, collaborative-team working, autonomous working, or tutoring). the different methods of teaching are supported by a hardware-software infrastructure of virtual teaching (blackboard platform) and a very detailed planning where activities are highly focused and weekly organized. it includes periodical tests for the evaluation of the progress of the student. we think that the adoption and tuning of these learning methods will enhance the skills of the future engineers in the computer programming area.
computer_programming	natural language processing (nlp) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages. the idea of using a natural language for computer programming is to make it easier for people to talk to computers in their native tongue and spare them the pain of learning a computer friendly language like assembly, c, c++, java, lisp etc. among all the natural languages, sanskrit in its style is identified to be the best language which has minimum deviation. panini, the creator of sanskrit formulated 3,949 rules. this research paper explores varied distinctive features of al like nlp, semantic net, vibhakti, dual case, inflection based syntax etc. and how sanskrit effectively triumphs over these limitations and fulfills the prerequisites of a natural language processor.
computer_programming	many institutions are offering online courses, as part of their regular schedule, to provide more flexible course offerings to students with the hope of some cost savings in the end. in this paper, we will focus on examining some of the students' ratings and comments that have been reported in student rating of teaching effectiveness (srte) surveys, for an online introductory programming course, during the past few years. we will look at the issues that we have identified to play a role in the srte surveys of our online course and examine the teaching from several different perspectives that include effectiveness/appropriateness of online courses for students. the paper includes a set of specific and general recommendations to address the legitimate students' concerns that, we believe, would make the course delivery more effective and enjoyable for students.
computer_programming	one of the challenges to promoting computer science in schools is to create and retain interest in programming. we describe a pilot project for primary and secondary students in macau to introduce net gadgeteer, a rapid prototyping platform for building electronic devices. the aim of the yearlong project was to generate interest in building devices, while learning the basics of programming concepts. our experience with the weekend workshops indicates that the hands-on approach helped to engage the students. results from a survey of 155 students point to increased engagement through active learning and growing enthusiasm to learn programming. we report some issues that surfaced and give recommendations for teachers who wish to consider such devices for teaching programming to novices. we hope to further explore ways to incorporate learning programming through tangible devices in the curriculum in local schools.
computer_programming	computing education researchers have become increasingly interested in leveraging log data automatically collected within computer programming environments in order to understand students' learning processes and tailor instruction to student needs. while data on students' programming activities has been positively correlated with their learning outcomes, those data tell only part of the story. another part of the story lies in students' social activities, which, according to social learning theory, can also be predictive of students' learning outcomes. in order to gain further insight into how computing students' learning processes influence their learning outcomes, we present an empirical study that explores the interplay of students' social activities, programming activities, and course outcomes in an early computing course. by analyzing log data collected through a programming environment augmented with a social networking-style activity stream, we found that answers to questions posed through the activity stream were positively correlated with students' ability to make programming progress, and their eventual success in the course. based on our findings, we present recommendations for the design of pedagogical environments to support a more social programming process.
relational_databases	entity resolution (er) concerns identifying pairs of entities that refer to the same underlying entity. to avoid o(n(2)) pairwise comparison of n entities, blocking methods are used. sorted neighborhood is an established blocking method for relational databases. it has not been applied to schema-free resource description framework (rdf) data sources widely prevalent in the linked data ecosystem. this paper presents a sorted neighborhood workflow that may be applied to schema-free rdf data. the workflow is modular and makes minimal assumptions about its inputs. empirical evaluations of the proposed algorithm on five real-world benchmarks demonstrate its utility compared to two state-of-the-art blocking baselines.
relational_databases	intuitionistic fuzzy databases are used to handle imprecise and uncertain data as they represent the membership, nonmembership, and hesitancy associated with a certain element in a set. this paper presents the intuitionistic fuzzy fourth normal form to decompose the multivalued dependent data. a technique to determine intuitionistic fuzzy multivalued dependencies by working on the closure of dependencies has been proposed. we derive the closure by obtaining all the logically implied dependencies by a set of intuitionistic fuzzy multivalued dependencies, i.e., inference rules. a complete set of inference rules for the intuitionistic fuzzy multivalued dependencies has been given along with the derivation of each rule. these rules help us to compute the dependency closure and we further use the same for defining the intuitionistic fuzzy fourth normal form.
relational_databases	this article describes how practical lectures can be innovated by the experience of a practical case study. the design and implementation of the geographical information system botangis brought several innovations to the study branch geoinformatics at the faculty of science of palacky university in olomouc. this article deals mainly with introducing a new practical example into the course database systems. this new theme explains the design and inner structure of the relational database botangis. the valuable contribution lies in the way a real database example from practice is explained to the students in the process of education. the instruction started with a visit to the botanical garden and the collection greenhouses. subsequently, students worked at a computer laboratory, trying to find some information about plants by various queries in the botanical portal botangis. finally, a detailed model of the relational database was explained. the teacher 's experience enriched the students' knowledge in the field of the conceptual database design.
relational_databases	identifying similarities in large datasets is an essential operation in several applications such as bioinformatics, pattern recognition, and data integration. to make a relational database management system similarity-aware, the core relational operators have to be extended. while similarity-awareness has been introduced in database engines for relational operators such as joins and group-by, little has been achieved for relational set operators, namely intersection, difference, and union. in this paper, we propose to extend the semantics of relational set operators to take into account the similarity of values. we develop efficient query processing algorithms for evaluating them, and implement these operators inside an open-source database system, namely postgresql by extending several queries from the tpc-h benchmark to include predicates that involve similarity-based set operators, we perform extensive experiments that demonstrate up to three orders of magnitude speedup in performance over equivalent queries that only employ regular operators. (c) 2015 elsevier ltd. all rights reserved.
relational_databases	the knowledge discovery in distributed databases is the process of extracting useful information from a collection of data stored in distributed databases. a distributed database is a collection of data replicated over a number of different computers. the best-suited structures for working with distributed databases are the distributed committee-machines. distributed committee-machines are a combination of neural networks that work in a distributed manner as a group in order to obtain better performance than individual neural networks in solving data mining tasks inside the kdd process. in this paper, we aim to study the interaction between distributed committee-machines and distributed databases. the process of replication on multiple machines can become very slow once the number of the machines from the replication topology grows. such behavior is explicable because of the complex software that is used in real implementations of the replication process in order to make available the same data on multiple machines. in this paper, i propose a design that overcomes those disadvantages and a new type of approach in storing the neural networks. the developed system stores the entire neural network in real relational databases. the optimized dcm structure eliminates the problems inherited from replication by writing all the result locally in special tables that will not be replicated on all the distributed machines. here i used also a new approach, which consists of storing the entire neural network in the table as blob (binary large object) object. the method can be beneficial also in new types of elearning techniques such as the adaptive elearning method that uses neural networks. with the optimized design of dcm structures, the speedup in all the experiments is almost equal with the number of distributed machines that were used.
software_engineering	seeking product 's quality is essential nowadays. one of the many quality aspects in software development is the source code complexity. not taking care for the complexity during the development can result in unexpected cost, caused by the difficulty on the source code understanding. the goal of this paper is to introduce an initial approach to identify unnecessary complexity in source code. besides identifying, also show to its user how to properly rewrite the source code without the unnecessary complexity. the approach is based on the static analysis of the source code control flow graph. once the unnecessary complexity is identified, the graph is refactored in order to allow the user to understand the improvement on the source code. it was implemented in a software tool in order to prove its concept. a performance evaluation was performed, resulting in a high accuracy. two experimental studies were also performed to assess its feasibility when used by real users. the evidences provided by these studies suggests that the approach support the unnecessary complexity removal.
software_engineering	eliciting sufficient high-quality knowledge from individuals to build a robust and useful artificial intelligence or intelligent system solution is a very time-consuming and expensive activity, especially in domains where the knowledge is informal, partial, incomplete, implicit, tacit and unstructured. moreover, in order to develop a solution, a systematic way to incorporate the elicited knowledge into a specification is necessary. the goal of this thesis is to develop a strategy oriented to the transfer and transformation of knowledge with the aim of eliciting and structuring the most quantity of domain knowledge, either tacit or explicit; then incorporate it into a specification that covers the needs and expectations of domain specialists. the application of the strategy in real informally structured domain cases provides empirical insights about its usefulness and value.
software_engineering	problems of sustainable development today are identified and tried to solve in all fields of life. in this respect information technologies play a specific role. many studies have been devoted to analysis of information technology (ict) as tools for sustainable development. different contributions and threats posed by ict to sustainable development are discussed. however, methodological and legislative base of software development is not sufficiently researched from the perspective of sustainable development of information technologies. this paper presents literature review of conference and journal articles and materials of international projects on the topic of different ict roles in sustainable development. the review shows the lack of regulatory framework analysis of information technologies. this analysis is done in separate part of the present paper. we found and analysed weaknesses of evolution of software development methods which put marketing considerations in the first plan. on the basis of the done analysis we discuss possible future work to put sustainable foundations to information technology knowledge from the very beginning of ict specialists training.
software_engineering	context: according to the search reported in this paper, as of this writing (may 2015), a very large number of papers (more than 70,000) have been published in the area of software engineering (se) since its inception in 1968. citations are crucial in any research area to position the work and to build on the work of others. identification and characterization of highly-cited papers are common and are regularly reported in various disciplines. objective: the objective of this study is to identify the papers in the area of se that have influenced others the most as measured by citation count studying highly-cited se papers helps researchers to see the type of approaches and research methods presented and applied in such papers, so as to be able to learn from them to write higher quality papers which will likely receive high citations. method: to achieve the above objective, we conducted a study, comprised of five research questions, to identify and classify the top-100 highly-cited se papers in terms of two metrics: total number of citations and average annual number of citations. results: by total number of citations, the top paper is ""a metrics suite for object-oriented design"", cited 1817 times and published in 1994. by average annual number of citations, the top paper is ""qos-aware middleware for web services composition"", cited 154.2 times on average annually and published in 2004. conclusion: it is concluded that it is important to identify the highly-cited se papers and also to characterize the overall citation landscape in the se field. we hope that this paper will encourage further discussions in the se community towards further analysis and formal characterization of the highly-cited se papers. (c) 2015 elsevier b.v. all rights reserved.
software_engineering	software engineering is the study and an application of engineering to the design, development, and maintenance of software. web testing is the name given to software testing that focuses on web applications. regression testing means re-testing an application after its code has been modified to verify that it still functions correctly. in this test cases that have been develop using reusability constraints that must be act as priority based test cases for regression testing. to execute these cases on the basis of priority of area coverage adaptive tcp algorithm is used that provide as sequence of execution. this sequence has to be optimized by genetic algorithm which uses population size, crossover and mutation probability for generation of new childs.
bioinformatics	metalloproteins bind and utilize metal ions for a variety of biological purposes. due to the ubiquity of metalloprotein involvement throughout these processes across all domains of life, how proteins coordinate metal ions for different biochemical functions is of great relevance to understanding the implementation of these biological processes. toward these ends, we have improved our methodology for structurally and functionally characterizing metal binding sites in metalloproteins. our new ligand detection method is statistically much more robust, producing estimated false positive and false negative rates of approximate to 0.11% and approximate to 1.2%, respectively. additional improvements expand both the range of metal ions and their coordination number that can be effectively analyzed. also, the inclusion of additional quality control filters has significantly improved structure-function spearman correlations as demonstrated by rho values greater than 0.90 for several metal coordination analyses and even one rho value above 0.95. also, improvements in bond-length distributions have revealed bond-length modes specific to chemical functional groups involved in multidentation. using these improved methods, we analyzed all single metal ion binding sites with zn, mg, ca, fe, and na ions in the wwpdb, producing statistically rigorous results supporting the existence of both a significant number of unexpected compressed angles and subsequent aberrant metal ion coordination geometries (cgs) within structurally known metalloproteins. by recognizing these aberrant cgs in our clustering analyses, high correlations are achieved between structural and functional descriptions of metal ion coordination. moreover, distinct biochemical functions are associated with aberrant cgs versus nonaberrant cgs. proteins 2017; 85:885-907. (c) 2016 wiley periodicals, inc.
bioinformatics	type 2 diabetes mellitus (t2dm) is characterized by islet beta-cell dysfunction and insulin resistance, which leads to an inability to maintain blood glucose homeostasis. circulating micrornas (mirnas) have been suggested as novel biomarkers for t2dm prediction or disease progression. however, mirnas and their roles in the pathogenesis of t2dm remain to be fully elucidated. in the present study, the serum mirna expression profiles of t2dm patients in chinese cohorts were examined. total rna was extracted from serum samples of 10 patients with t2dm and five healthy controls, and these was used in reverse-transcription-quantitative polymerase chain reaction analysis with the exiqon pcr system of 384 serum/plasma mirnas. a total of seven mirnas were differentially expressed between the two groups (fold change >3 or < 0.33; p < 0.05). the serum expression levels of mir-455-5p, mir-454-3p, mir-144-3p and mir-96-5p were higher in patients with t2dm, compared with those of healthy subjects, however, the levels of mir-409-3p, mir-665 and mir-766-3p were lower. hierarchical cluster analysis indicated that it was possible to separate patients with t2dm and control individuals into their own similar categories by these differential mirnas. target prediction showed that 97 t2dm candidate genes were potentially modulated by these seven mirnas. kyoto encyclopedia of genes and genomes pathway analysis revealed that 24 pathways were enriched for these genes, and the majority of these pathways were enriched for the targets of induced and repressed mirnas, among which insulin, adipocytokine and t2dm pathways, and several cancer-associated pathways have been previously associated with t2dm. in conclusion, the present study demonstrated that serum mirnas may be novel biomarkers for t2dm and provided novel insights into the pathogenesis of t2dm.
bioinformatics	the twenty-first century vision for toxicology involves a transition away from high-dose animal studies to in vitro and computational models (nrc in toxicity testing in the 21st century: a vision and a strategy, the national academies press, washington, dc, 2007). this transition requires mapping pathways of toxicity by understanding how in vitro systems respond to chemical perturbation. uncovering transcription factors/signaling networks responsible for gene expression patterns is essential for defining pathways of toxicity, and ultimately, for determining the chemical modes of action through which a toxicant acts. traditionally, transcription factor identification is achieved via chromatin immunoprecipitation studies and summarized by calculating which transcription factors are statistically associated with up- and downregulated genes. these lists are commonly determined via statistical or fold-change cutoffs, a procedure that is sensitive to statistical power and may not be as useful for determining transcription factor associations. to move away from an arbitrary statistical or fold-change-based cutoff, we developed, in the context of the mapping the human toxome project, an enrichment paradigm called information-dependent enrichment analysis (idea) to guide identification of the transcription factor network. we used a test case of activation in mcf-7 cells by 17 beta estradiol (e2). using this new approach, we established a time course for transcriptional and functional responses to e2. er alpha and er beta were associated with short-term transcriptional changes in response to e2. sustained exposure led to recruitment of additional transcription factors and alteration of cell cycle machinery. tfap2c and sox2 were the transcription factors most highly correlated with dose. e2f7, e2f1, and foxm1, which are involved in cell proliferation, were enriched only at 24 h. idea should be useful for identifying candidate pathways of toxicity. idea outperforms gene set enrichment analysis (gsea) and provides similar results to weighted gene correlation network analysis, a platform that helps to identify genes not annotated to pathways.
bioinformatics	in this study, we report the gut microbial composition and predictive functional profiles of zebrafish, danio rerio, fed with a control formulated diet (cfd), and a gluten formulated diet (gfd) using a metagenomics approach and bioinformatics tools. the microbial communities of the gfd-fed d. rerio displayed heightened abundances of legionellales, rhizobiaceae, and rhodobacter, as compared to the cfd-fed counterparts. predicted metagenomics of microbial communities (picrust) in gfd-fed d. rerio showed kegg functional categories corresponding to bile secretion, secondary bile acid biosynthesis, and the metabolism of glycine, serine, and threonine. the cfd-fed d. rerio exhibited kegg functional categories of bacteria-mediated cobalamin biosynthesis, which was supported by the presence of cobalamin synthesizers such as bacteroides and lactobacillus. though these bacteria were absent in gfd-fed d. rerio, a comparable level of the cobalamin biosynthesis kegg functional category was observed, which could be contributed by the compensatory enrichment of cetobacterium. based on these results, we conclude d. rerio to be a suitable alternative animal model for the use of a targeted metagenomics approach along with bioinformatics tools to further investigate the relationship between the gluten diet and microbiome profile in the gut ecosystem leading to gastrointestinal diseases and other undesired adverse health effects. (c) 2017 published by-elsevier b.v.
bioinformatics	in our previous study, five different secretory proteins, including gsn, adamtsl4, calr, ppia and txn, have been identified to be associated with the nasopharyngeal carcinoma (npc) metastasis. in this work, the 5 proteins were further investigated. bioinformatics analysis suggested that they might play an important role in the process of npc development. western blotting analysis showed that all of these 5 targets could be secreted into extracellular by both high metastatic npc 5-8f cells and non-metastatic npc 6-10b cells. except for gsn, the expressions of adamtsl4, calr, ppia and txn proteins in extracts of the 5-8f and 6-10b cells were significantly different (p<0.05). thus, the expressions of these 4 differentially expressed proteins were further tested in a cohort of npc tissue specimens. the results indicated that the expression levels of adamtsl4 and txn were highly correlated with the lymph node and distant metastasis (p<0.05) in npc patients. moreover, enzyme-linked immunosorbent assay (elisa) was used to investigate the concentrations of the adamtsl4 and txn in serum specimens of npc patients. the results revealed that serum adamtsl4 expression level was closely correlated with lymph node metastasis and clinical stage (p<0.05) in npc patients, and it was able to discriminate metastasis npc from non-metastasis npc with a sensitivity of 75.6% and a specificity of 64.7%. the present data show for the first time that the adamtsl4 and txn may be novel and potential biomarkers for predicting the npc metastasis. furthermore, the serum adamtsl4 could be a potential serum tumor biomarker for prognosis of npc.
cryptography	visual cryptography scheme (vcs) is a cryptographic technique which can hide image based secrets. even though vcs has the major advantage that the decoding can be done with the help of human visual system (hvs), yet it does not provide sufficient reconstruction quality. hence, two in one image secret sharing scheme (tioisss) is used which provides two decoding phases. however, the existing tioisss method has several limitations. in this work, a modified tioisss is proposed in which an adaptive threshold is used for halftoning, which changes depending on the nature of the pixels present in image. by this, the quality of reconstructed secret image is improved in the first decoding stage compared to the existing scheme. in addition, the security is also enhanced by pixel and bit level permutation with a 64 bit key and embedding the key in gray vcs shadows. to verify the authenticity of the image, a secret message is also embedded in the shadows. security analysis shows that the modified tioisss is robust to brute force and man-in-middle attacks.
cryptography	we propose an ultra-lightweight, compact, and low power block cipher boron. boron is a substitution and permutation based network, which operates on a 64-bit plain text and supports a key length of 128/80 bits. boron has a compact structure which requires 1939 gate equivalents (ges) for a 128-bit key and 1626 ges for an 80-bit key. the boron cipher includes shift operators, round permutation layers, and xor operations. its unique design helps generate a large number of active s-boxes in fewer rounds, which thwarts the linear and differential attacks on the cipher. boron shows good performance on both hardware and software platforms. boron consumes less power as compared to the lightweight cipher led and it has a higher throughput as compared to other existing sp network ciphers. we also present the security analysis of boron and its performance as an ultra-lightweight compact cipher. boron is a well-suited cipher design for applications where both a small footprint area and low power dissipation play a crucial role.
cryptography	we present a new scheme on implementing the passive quantum key distribution with thermal distributed parametric down-conversion source. in this scheme, only one-intensity decoy state is employed, but we can achieve very precise estimation on the single-photon-pulse contribution by utilizing those built-in decoy states. moreover, we compare the new scheme with other practical methods, i.e., the standard three-intensity decoy-state bb84 protocol using either weak coherent states or parametric down-conversion source. through numerical simulations, we demonstrate that our new scheme can drastically improve both the secure transmission distance and the key generation rate.
cryptography	this study concentrates on finding all truncated impossible differentials in substitution-permutation networks (spns) ciphers. instead of using the miss-in-the-middle approach, the authors propose a mathematical description of the truncated impossible differentials. first, they prove that all truncated impossible differentials in an r+1 rounds spn cipher could be obtained by searching entry 0' in d(p)(r), where d(p) denotes the differential pattern matrix (dpm) of p-layer, thus the length of impossible differentials of an spn cipher is upper bounded by the minimum integer r such that there is no entry 0' in d(p)(r). second, they provide two efficient algorithms to compute the dpms for both bit-shuffles and matrices over gf(2(n)). using these tools they prove that the longest truncated impossible differentials in spn structure is 2-round, if the p-layer is designed as an maximum distance separable (mds) matrix. finally, all truncated impossible differentials of advanced encryption standard (aes), aria, aes-mds, present, maya and puffin are obtained.
cryptography	in this paper, a simple implementation for elliptic curve equation on field programmable gate array (fpga) will be proposed. as elliptic curve cryptography (ecc) offers a smaller key size without sacrificing security level. a brief survey on applying the main equation of elliptic curve (ec) with different values of the coefficients a and b. a comparison between results depended on the correlation coefficient of each value. value of a and b implemented on fpga according to correlation results between plaintext image and ciphertext image on matlab. this ec equation will be applied to an ultra-wide band (uwb) system to secure transmission of data in a wireless channel. here, a brief survey on uwb technology has been implemented with software simulation for a secured system based on ecc algorithm.
structured_storage	background: phenotypic data are routinely used to elucidate gene function in organisms amenable to genetic manipulation. however, previous to this work, there was no generalizable system in place for the structured storage and retrieval of phenotypic information for bacteria. results: the ontology of microbial phenotypes (omp) has been created to standardize the capture of such phenotypic information from microbes. omp has been built on the foundations of the basic formal ontology and the phenotype and trait ontology. terms have logical definitions that can facilitate computational searching of phenotypes and their associated genes. omp can be accessed via a wiki page as well as downloaded from sourceforge. initial annotations with omp are being made for escherichia coli using a wiki- based annotation capture system. new omp terms are being concurrently developed as annotation proceeds. conclusions: we anticipate that diverse groups studying microbial genetics and associated phenotypes will employ omp for standardizing microbial phenotype annotation, much as the gene ontology has standardized gene product annotation. the resulting omp resource and associated annotations will facilitate prediction of phenotypes for unknown genes and result in new experimental characterization of phenotypes and functions.
structured_storage	electronic storage of patient-related data will replace paper-based patient records in the near future. some steps in medical practice can even now not be achieved without electronic data processing. both systems, conventional paper-based and electronic-based records, have advantages and disadvantages which have to be taken into consideration. the advantages of electronic-based records are e.g. good availability of data, structured storage of data, scientific analysis of long-term data and possible data exchange with colleagues in the context of teleconsultation systems. problems have to be solved in the field of data security, initial high investment costs and time consumption in learning to use the system as well as in incompatibility of existing it systems.
structured_storage	dateview is a freeware desktop database system for the structured storage and retrieval of geochronological information. it provides a user-friendly interface for constructing queries based on information in the database so as to extract information on specific units, isotope systems, age interpretations, provinces, terranes, reference sources and many other characteristics which geochronologists and geologists might require. once a subset of the records in the database has been selected, users may choose from several forms of graph so as to better visualise the data. available graphs include probability histograms, age versus initial ratio or epsilon, and age versus closure temperature. simple locality (latitude vs longitude) graphs are also available. grouping of data by interpretation or age interval in the graphs is user customizable. the database may also be shared with colleagues on an intranet. (c) 2004 elsevier ltd. all rights reserved.
structured_storage	purpose: to evaluate image quality and anatomical detail depiction in dose-reduced digital plain chest radiograms using a new needle screen storage phosphor (nip) in comparison to full dose conventional powder screen storage phosphor (pip) images. materials and methods: 24 supine chest radiograms were obtained with pip at standard dose and compared to follow-up studies of the same patients obtained with nip with dose reduced to 50% of the pip dose (all imaging systems: agfa-gevaert, mortsel, belgium). in both systems identical versions of post-processing software supplied by the manufacturer were used with matched parameters. six independent readers blinded to both modality and dose evaluated the images for depiction and differentiation of defined anatomical regions (peripheral lung parenchyma, central lung parenchyma, hilum, heart, diaphragm, upper mediastinum, and bone). all nip images were compared to the corresponding pip images using a five-point scale (- 2, clearly inferior to + 2, clearly superior). overall image quality was rated for each pip and nip image separately (1, not usable to 5, excellent). results: pip and dose reduced nip images were rated equivalent. mean image noise impression was only slightly higher on nip images. mean image quality for nip showed no significant differences (p >0.05, mann-whitney u test). conclusion: with the use of the new needle structured storage phosphors in chest radiography, dose reduction of up to 50% is possible without detracting from image quality or detail depiction. especially in patients with multiple follow-up studies the overall dose can be decreased significantly.
structured_storage	windows azure storage (was) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. was customers have access to their data from anywhere at any time and only pay for what they use and store. in was, data is stored durably using both local and geographic replication to facilitate disaster recovery. currently, was storage comes in the form of blobs (files), tables (structured storage), and queues (message delivery). in this paper, we describe the was architecture, global namespace, and data model, as well as its resource provisioning, load balancing, and replication systems.
electric_motor	the power-on upshift control in automatic transmissions has a significant effect on the shift quality and is very complex. this paper aims to investigate the relationship between the limited motor torque control and the performance of the power-on upshift control in electric vehicles. first, a simplified model for an electric vehicle powertrain with a general two-speed automatic transmission was built. then, a power-on upshift control strategy was introduced into the powertrain model, with the aim of a constant output torque during power-on upshifts. the control strategy fully utilized the control flexibility of electric motors and specifically focused on the effects of the limited motor torque control during power-on upshifts. simulation results demonstrated that the proposed control strategy could significantly improve the shift quality of electric vehicles under mild driving conditions. however, the performance of the proposed control strategy under aggressive driving conditions was impaired by the limited motor torque control. an effective solution was proposed to improve the performance of the power-on upshift control under aggressive driving conditions. this work may be helpful to improve the shift quality of both electric vehicles and internal-combustion engine vehicles with automatic transmissions.
electric_motor	nowadays, increasingly stringent directives in the transport sector open interesting possibilities for the development of new traction technologies based on electrochemical storage and fuel cells, that are characterized by some limitations: the electrochemical storage systems have a limited range while fuel cells have not a proper response capability for very variable load demands. with the aim to realize a hybrid electric vehicle (hev) able to increase range of autonomy, cnr itae has designed a minibus (class b-category m3) having an electric motor of 40 kw powered by a polymeric fuel cell system of 20 kw and a lithium battery system of 70 kwh. the study is focused on evaluation of the hybrid propulsion system performance in different operating conditions. in particular, powertrain configuration and energy management strategies have been studied to optimize the efficiency of the hev, overcoming the limitations of these technologies, reducing energy losses and increasing range of the vehicle. tests on the road have been conducted and energy production and consumption of the vehicle have been logged. (c) 2016 hydrogen energy publications llc. published by elsevier ltd. all rights reserved.
electric_motor	this paper reports on a study on the spatial characteristics of rain rates recorded in the basque region in northern spain. to perform this study a network of eighty raingauges spread throughout the region has been used. statistical parameters such as the spatial cross-correlation coefficient and the space-time cross-correlation coefficient were calculated and their evolution with separation distance studied. it is hoped that information reported could be useful in better understanding characteristics of rain and in developing countermeasures for terrestrial and satellite radio networks operating at frequencies above 10 ghz.
electric_motor	this paper describes a method used at schaeffler to analyze integrated hybrid transmission architectures with one electric motor. both the properties of their modes and aspects of mode transitions become clear using special visualization techniques. the method was implemented in matlab, and several transmission examples will be discussed. this leads to design aspects and requirements concerning the overall transmission layout and the components. examples of beneficial components from schaeffler are discussed. small shifting elements allow more space for the electric motor. clutches, brakes and cvt subsystems that are the right size for hybrid transmissions provide the required comfort. low-friction bearings and on-demand actuators contribute to increased range of electric driving.
electric_motor	the electron density of the topside ionosphere and the plasmasphere contributes essentially to the overall total electron content (tec) budget affecting global navigation satellite systems (gnss) signals. the plasmasphere can cause half or even more of the gnss range error budget due to ionospheric propagation errors. this paper presents a comparative study of different plasmasphere and topside ionosphere data aiming at establishing an appropriate database for plasmasphere modelling. we analyze electron density profiles along the geomagnetic field lines derived from the imager for magnetopause-to-aurora global exploration (image) satellite/radio plasma imager (rpi) records of remote plasma sounding with radio waves. we compare these rpi profiles with 2d reconstructions of the topside ionosphere and plasmasphere electron density derived from gnss based tec measurements onboard the challenging minisatellite payload (champ) satellite. most of the coincidences between image profiles and champ reconstructions are detected in the region with l-shell between 2 and 5. in general the champ reconstructed electron densities are below the image profile densities, with median of the champ minus image residuals around -588 cm(-3). additionally, a comparison is made with electron densities derived from passive radio wave rpi measurements onboard the image satellite. over the available 2001-2005 period of image measurements, the considered combined data from the active and passive rpi operations cover the region within a latitude range of +/- 60 degrees n, all longitudes, and an l-shell ranging from 1.2 to 15. in the coincidence regions (mainly 2 <= l <= 4), we check the agreement between available active and passive rpi data. the comparison shows that the measurements are well correlated, with a median residual of similar to 52 cm(-3). the rms and std values of the relative residuals are around 22% and 21% respectively. in summary, the results encourage the application of image rpi data for plasmasphere and plasmapause modeling. (c) 2016 cospar. published by elsevier ltd. all rights reserved.
digital_control	this paper presents a three-phase transformerless uninterruptible power supply (ups) with sinusoidal pulse width modulation (spwm) based division-summation (d-sigma) digital control. a transformerless ups controls the power flow between dc link and utility grid, as well as tracks the ac reference voltage. the proposed control law derived with d-sigma digital approach takes into account the effects of dc-link voltage fluctuation, grid-voltage distortion and inductance variation due to different current levels. thus, distortion of input current and filter inductor core size can be reduced significantly. however, circulating current may flow through the common ground between the input power factor corrector (pfc) and the output three-phase four-wire inverter. the derived control law based on spwm can suppress this circulating current and regulate output voltages tightly. experimental results measured from a three-phase transformerless ups have confirmed the analysis and discussion of the proposed control approach.
digital_control	in boost converters and other indirect energy transfer topologies, the fastest transient response usually does not coincide with the minimum possible output voltage deviation. this paper introduces a practical mixed-signal current programmed mode (cpm) controller that, compared to time-optimal solutions, provides a smaller deviation, lower current stress, and simpler controller implementation. to recover from transients, the controller passes through two phases. in the first phase, the inductor current is set in the proximity of its steady-state value, so that the initial transient-caused capacitor charging/discharging process is reversed. in the second phase, the voltage is gradually recovered. the controller implements a simple algorithm for setting up the inductor current and the output voltage peak/valley values during transients, based on the output current estimate, which is obtained through a self-tuning procedure. the operation of the controller is verified both through simulations and experimentally, with a boost-based 12 to 48 v, 100-w prototype, operating at 100-khz switching frequency. a comparison with a time-optimal controller shows that the introduced programmable-deviation system results in up to 1.9 times smaller voltage deviation while limiting component stress.
digital_control	this paper introduces a highly flexible multichannel output stage for battery-powered portable electric stimulators (ess), based on novel power converter architecture. compared with other solutions, the presented output stage for transcutaneous (surface) stimulations increases the number of applicable therapies, improves the battery operating time through reduced power consumption, and potentially results in more comfortable and shorter healing therapies. the new hybrid switch-mode power converter is a combination of flyback and switched-capacitor (sc) topologies. the flyback steps up the battery voltage and provides galvanic isolation. the following power-efficient sc stage replaces lossy linear current sources (css) of conventional solutions and produces pulses with a much higher slew rate, reducing the pulse energy needed to cause the stimulus. the sc also inherently produces pulses with zero-net charge, eliminating bulky blocking capacitors and/or dedicated discharging circuits. the regulation of the amplitudes of the pulses is performed with a new digital voltage-programmed current mode controller, forcing the output of the sc stage to behave as a cs. the flexible digital controller allows for creation of various types of pulses and also features several levels of patient protection. an experimental prototype of the output stage has undergone proof of principle tests with able-bodied individuals. the results show that the new output stage produces pulses with a 1-ma/ns slew rate, about two orders of magnitude higher than the other known solutions. the trials show that the faster slew rate pulses generate the same muscle contraction with 34% lower amplitudes, reducing energy consumption by 55%, allowing longer battery life of portable es applications.
digital_control	a rapid prototyping methodology for digital controllers is presented in this paper. its main application is in the development, debugging, and test of microgrid inverter controllers. to fulfill the application requirements, these systems are characterized by complex multilayer architectures, extending from pulse width modulation (pwm) and current control loops up to global optimization and high level communication functions. the complexity and the wide variability of the different layer implementations make digital control mandatory. however, developing so complex digital controllers on conventional hardware platforms, like digital signal processors (dsps) or even fpgas, is not the most practical choice. this paper shows how multiplatform control devices, where software configurable dsp functions and programmable logic circuits are efficiently combined, represent the optimal solution for this field of application. furthermore, this paper proposes hardware-in-the-loop real-time simulation as an effective means of developing and debugging complex hardware and software codesigned controllers. a case study is presented and used to illustrate the different design and test phases, from initial concept and numerical simulation to final experimental verification.
digital_control	the aim of this paper is to present a superior transient response of a digital peak current mode dc-dc converter in order to address input voltage fluctuation. in the dc power feeding system, a quick response is important for not only load fluctuation but also input voltage fluctuation. since the proposed peak current mode control method can detect the peak current in real time, the transient response can improve compared with the conventional digital voltage mode pid control. as a result, the convergence time and undershoot of output voltage is suppressed by more than 50%.
microcontroller	this work presents the design, fabrication, and characterization of a passive printed radiofrequency identification tag in the ultra-high-frequency band with multiple optical sensing capabilities. this tag includes five photodiodes to cover a wide spectral range from near-infrared to visible and ultraviolet spectral regions. the tag antenna and circuit connections have been screen-printed on a flexible polymeric substrate. an ultra-low-power microcontroller-based switch has been included to measure the five magnitudes issuing from the optical sensors, providing a spectral fingerprint of the incident electromagnetic radiation from ultraviolet to infrared, without requiring energy from a battery. the normalization procedure has been designed applying illuminants, and the entire system was tested by measuring cards from a colour chart and sensing fruit ripening.
microcontroller	we present a high-voltage cmos neural-interface chip for a multichannel vestibular prosthesis (mvp) that measures head motion and modulates vestibular nerve activity to restore vision- and posture-stabilizing reflexes. this application specific integrated circuit neural interface (asic-ni) chip was designed to work with a commercially available microcontroller, which controls the asic-ni via a fast parallel interface to deliver biphasic stimulation pulses with 9-bit programmable current amplitude via 16 stimulation channels. the chip was fabricated in the onsemi c5 0.5 micron, high-voltage cmos process and can accommodate compliance voltages up to 12 v, stimulating vestibular nerve branches using biphasic current pulses up to 1.45 +/- 0.06 ma with durations as short as 10 mu s/phase. the asic-ni includes a dedicated digital-to-analog converter for each channel, enabling it to perform complex multipolar stimulation. the asic-ni replaces discrete components that cover nearly half of the 2nd generation mvp (mvp2) printed circuit board, reducing the mvp system size by 48% and power consumption by 17%. physiological tests of the asic-based mvp system (mvp2a) in a rhesus monkey produced reflexive eye movement responses to prosthetic stimulation similar to those observed when using the mvp2. sinusoidal modulation of stimulus pulse rate from 68-130 pulses per second at frequencies from 0.1 to 5 hz elicited appropriately-directed slow phase eye velocities ranging in amplitude from 1.9-16.7 degrees/s for the mvp2 and 2.0-14.27s for the mvp2a. the eye velocities evoked by mvp2 and mvp2a showed no significant difference (t-test, p = 0.34), suggesting that the mvp2a achieves performance at least as good as the larger mvp2.
microcontroller	numerous simple impedance analysers based on the microcontroller (mu c) and dedicated impedance converter integrated circuits (ic) were reported recently. in many applications sophisticated analogue circuitry has to be appended to enhance the measurement possibilities or to circumvent the limitations. in this paper the impedance analyser imp-stm32 based solely on the mu c and general purpose operational and instrumentation amplifiers is presented. it uses the internal dac and adcs in the mu c to generate the excitation and to measure the response of the measured object. it also uses the external analogue circuits to condition the excitation signal and measure voltage and current. the magnitudes and phase shifts of voltage and current are evaluated using the three parameter sine fitting algorithm allowing for fast low-frequency impedance measurements. the calibration procedure of completed device is presented as well as the tests of its accuracy. the device allowed for measurements at frequency range between 1 mhz and 100 khz in 1 omega to 1 g omega impedance range with 1% accuracy. imp-stm32 was also compared to the agilent 4294a precision impedance analyser. in the middle of the impedance ranges (1 omega to 300 k omega) the discrepancies between the two were less than 0.2%. (c) 2016 elsevier ltd. all rights reserved.
microcontroller	in this paper we proposed multi-stage algorithm of small wind turbine power plant control when it supports maximum extracted power independently on traditional tip speed ratio using the wind speed. we made measurements of wind turbine parameters on urban and prairie territories, showed the necessity of power controlling the turbine, and vindicated the efficiency of control of small wind turbines in urban regions. we also formulated the conditions and structure of microprocessor operation, described the algorithm of wind turbine power control showing the possible situations and states. each state is described with the corresponding microcontroller response and activity. we showed the advantages of pulse width modulation (pwm) is converting the electric values.
microcontroller	we present an electrostatically driven four-electrode micropump that operates with peristaltic motion. the working fluid is gas. in particular, we focus on the characteristics of the micropump as a function of the various actuating signals for four flexible electrodes. the micropump operated between 60 v and 120v with a customized pulse signal. whereas the maximum flow rate using a basic actuating signal was approximately 38 mu l/min at 90v and 15 hz, the maximum flow rate using an optimized actuating signal was approximately 136 mu l/min at 90 v and 15 hz. this is an increase of approximately 3.6 times the minimum flow rate. various actuating signals were generated from a microcontroller unit (mcu) equipped with a logic circuit and a high-voltage dc power supply and function generator. the minimum flow rate occurs with a four pumping sequence with three electrodes. however, the maximum flow rate occurs when these four electrodes participate sequentially. the proposed micropump consists of only a single chamber and a flexible membrane with four electrodes. the chamber is divided into smaller cells with the embedded electrodes controlled by a custom-made logic circuit that generates various phase-sequencing actuation signals. this micropump is applicable for gas chromatography. (c) 2016 elsevier b.v. all rights reserved.
electrical_network	when a magnetic field is applied to type ii superconductors, such as yba2cu3o7.delta (ybco), the flux quanta penetrate the material as a regular array of vortices. however when transport currents are applied, they act to move these vortices, thus lowers the critical current density (j(r)) as well as destroying superconductivity. the development of microstructures made of ybco materials has enabled engineers to increase the critical current density, within type ii materials by introducing flux pinning centres into the material. the microstnicture and flux pinning properties of yba2cu3o7.delta system with varying levels (0-5 wt. a) of a nano perovskite ceramic insulator; yba2hfo5.5 addition was studied in detail. orthorhombic yba2cu3o7-delta powder was prepared through conventional solid state route and a modified combustion method was used for synthesizing nanocrystalline yba2hfo5.5. the structure and microstnicture of the samples examined by x.-ray diffraction and scanning electron microscopy showed that yba2hfo5.5 5 and ybco remained unreacted even at higher processing temperature without deteriorating the superconducting properties. the scanning electron microscope image shows that yba(2)hfo(5.5) forms an electrical-network between grains. these observations suggest that the yba(2)hfo(5.5) addition to the y-123-compounds improve the electrical connection between superconducting grains and substantial improvements in the relative electrical transport properties of the composites. the variation of sintering temperature, density, critical transition temperature (t-c) and magnetic field dependence of critical current density (j(r)) of yba2cu3o7,6 having different proportions of yba2hfo5.5 in the matrix were also studied in detail. it is found that the addition of these elements considerably enhances the flux pinning strength of the system, and there is also an increase of critical temperature (tc) and critical current density (jr) up to an optimum value of 8.76 x 10(4) a/cm(2)for a concentration of 2 wt a addition of nano yba2hfo5.5; . the enhancement of the critical current density in the yba2cu3o7-delta,6 - yba2hfo5.5 samples is attributed to the sustained formation of the insulating and non-reacting yba2hf055 phase acting as pinning center. the addition of yba2hfo5.5 in yba2cu3o7-delta,6 bulk superconductor also enhances the pinning force density from 3 x 10(6) n/m(3) to 69.6 x l0(6)n/m(3) at 77 k.
electrical_network	this paper deals with the transfer of data by power-line communication (plc) on a pulse width modulated (pwm) energy network. target applications concern the adjustable speed drive for industrial infrastructures. in order to implement an efficient communication solution, it is necessary to have a model of the communication channel to determine certain parameters like bandwidth, carrier frequency and modulation formats. the originality of our work is the methodology used to determine the channel model. the first results of simulation and experimental validation are presented in this paper.
electrical_network	in this paper the reliability of ac contactor is studied. in the present scenario where the dependency on electrical energy has increased immensely, there is a huge pressure on power system network for its reliable operation. under this condition failure of any component in electrical network can cause huge loss of economy and continuity of supply. this paper is focused towards estimating the remaining useful life of the contactors through its characteristics parameter i.e. contact resistance and contact compression. sampies of rating 9a, 440v, 230v (coil voltage) underwent electrical endurance test and during the testing, behavior of these two characteristic parameters is studied in accordance to find the relation between contactors life, contact resistance and contact compression. the results obtained helped in predicting the health or remaining life of contactor. hence, based on experimental data reliability of the system is estimated using reliasoft software.
electrical_network	the research is aimed at developing algorithms for the construction of automated systems to control active components of the electrical network. the construction of automated systems intended for the control of electric power systems requires high-speed mathematical tools. the method applied in the research to describe the object of control is based on the universal approach to the mathematical modelling of nonlinear dynamic system of a black-box type represented by the volterra polynomials of the n-th degree. this makes it possible for the input and output characteristics of the object to obtain an adequate and fast mathematical description. results of the computational experiment demonstrate the applicability of the mathematical tool to the control of active components of the intelligent power system.
electrical_network	silicon is a remarkable candidate for lithium-ion battery anodes, due to the highest theoretical specific capacity, low working potential, and abundance on earth. however, the volume of si changes approximate to 300% during the lithiation-delithiation cycling, leading to pulverization and poor electrical contact between si and the current collector, resulting in fast fading capacity. to overcome these problems, we develop a novel free-standing anode by encapsulating si nanoparticles into a lightweight, flexible, and conductive graphene network by utilizing a facile freeze-drying strategy. the si nanoparticles are tightly anchored on the ultrathin and flexible graphene sheets, which can serve as mechanical support, electrical network, and buffer layer for si. this free-standing aerogel exhibits superior electrochemical performance, and the specific capacity is 617 mahg(-1) after 50 cycles at a current density of 0.8 ag-1, 4 times that of a traditional si electrode on cu foil fabricated by a typical slurry method.
electricity	china is quickly installing advanced metering infrastructure (ami), which could provide tremendous opportunities in developing and utilizing demand response resources. demand response may potentially create a profitable industry and contribute to efficiency improvement, cost reduction, and pollution mitigation of the entire electricity sector. however, china lags behind the developed world in utilizing demand response. institutional barriers, including the lack of competitive electricity market and the resistance by the state grid corporations, are preventing the commercialization of demand response. in order to fully realize the potential of smart grid, china needs to push forward the reforms toward establishing an open access electricity market so the pollution-free demand response resources may compete with power generators on leveled field. (c) 2015 elsevier b.v. all rights reserved.
electricity	this paper aims to propose a methodology for optimization of solar chimney power plants taking into account the techno-economic parameters. the indicator used for optimization is the comparison between the actual achieved simple payback period for the design and the minimum possible (optimum) simple payback period as a reference. an optimization model was executed for different twelve designs in the range 5-200 mw to cover reinforced concrete chimney, sloped collector, and floating chimney. the height of the chimney was optimized and the associated collector area was calculated accordingly. relationships between payback periods, electricity price, and the peak power capacity of each power plant were developed. the resulted payback periods for the floating chimney power plants were the shortest compared to the other studied designs. for a solar chimney power plant with 100 mw at electricity price 0.10 usd/kwh, the simple payback period for the reference case was 4.29 years for floating chimney design compared to 23.47 and 16.88 years for reinforced concrete chimney and sloped collector design, respectively. after design optimization for 100 mw power plant of each of reinforced concrete, sloped collector, and floating chimney, a save of 19.63, 2.22, and 2.24 million usd, respectively from the initial cost of the reference case is achieved. sensitivity analysis was conducted in this study to evaluate the impacts of varied running cost, solar radiation, and electricity price on the payback periods of solar chimney power plant. floating chimney design is still performing after applying the highest ratio of annual running cost to the annual revenue. the sensitivity analysis showed that at the same solar radiation and electricity price, the simple payback period for 200 mw with sloped collector design would almost have the double simple payback period for 5 mw with floating chimney design. (c) 2017 elsevier ltd. all rights reserved.
electricity	conventional time series forecast models can hardly develop the inherent rules of complex non-linear dynamic systems because the strict assumptions they need cannot always be met in reality, whereas fuzzy time series (fts) techniques can be used even the records of times series have uncertainty and instability since they do not need strict assumptions. in previous study of fts, the process of aggregating the past observations and assigning proper weights of fuzzy logical relationship groups are ignored, which may lead to poor forecasting accuracy since they are important aspects in time series prediction and analysis where determination of future trends depends only on past observations. in this paper, a novel high-order fts model is constructed to make time series forecasting. specifically, by applying the harmony search intelligence algorithm, the optimal lengths of intervals are tuned. moreover, regularly increasing monotonic quantifiers are employed on fuzzy sets to obtain the weights of ordered weighted aggregation. simultaneously, the weights of right-hand side of fuzzy logical relationship groups are explored to compensate the presence of bias in the prediction. in the part of empirical analysis, the developed model was applied to predict three well-known time series: numbers of enrollment of alabama university, taiex and electricity load demand of new south wales and the results obtained were compared with several counterparts, including some old and recently developed models. experimental results demonstrate that the developed model cannot only achieve higher accuracy of prediction, but also capture the fuzzy features and characters. (c) 2017 elsevier b.v. all rights reserved.
electricity	this work describes the co-fermentation to the simultaneous production of rhamnolipids and ethanol from exploded sugarcane bagasse (esb) using saccharomyces cerevisiae, pseudomonas aeruginosa and crude enzyme complexes (cecs). the cecs were produced by aspergillus niger in solid-state fermentation (ssf), using different levels of esb, rice bran (rb) and corn cob (cc) as substrates. the best results for rhamnolipids (9.1 g/l, emulsification index of 84%, surface tension of 35 mn/m) and ethanol (8.4 g/l) were obtained after 86 h of fermentation of esb (250 g/l, 30 degrees c), using cec from ssf of esb (20% wt/wt), rb (30% wt/wt) and water (50% wt/wt). it was also observed that the fermented bagasse had approximately the same calorific value of a bagasse in nature and could be used as a potential source of energy. these findings suggest that co-fermentation using p. aeruginosa, s. cerevisiae and esb can be used in the successful co-production of rhamnolipids and ethanol, with the recovery of energy present in the fermented bagasse. (c) 2017 elsevier ltd. all rights reserved.
electricity	energy management strategies (emss) are critical for the improvement of fuel economy of plug-in hybrid electric vehicles (phevs). however, conventional emss hardly consider the influence of uphill terrain on the fuel economy and battery life, leaving vehicles with insufficient battery power for continuous uphill terrains. hence, in this study, an optimal control strategy for a phev based on the road grade information is proposed. the target state of charge (soc) is estimated based on the road grade information as well as the predicted driving cycle on uphill road obtained from the gps/gis system. furthermore, the trajectory of the soc is preplanned to ensure sufficient electricity for the uphill terrain in the charge depleting (cd) and charge sustaining (cs) modes. the genetic algorithm is applied to optimize the parameters of the control strategy to maintain the soc of battery in the cd mode. the pre-charge mode is designed to charge the battery in the cs mode from a reasonable distance before the uphill terrain. finally, the simulation model of the powertrain system for the phev is established using matlab/simulink platform. the results show that the proposed control strategy based on road-grade information helps successfully achieve better fuel economy and longer battery life.
operational_amplifier	a low voltage and low power operational amplifier (opamp) using process, temperature insensitive current source has been presented. the design methodology of the addition based current source and the opamp implementation using this current source is presented. the spice simulations are carried out in gpdk 90 nm technology at 0.6 v supply. the results of addition based current source opamp are compared with the mosfet current source opamp. the proposed opamp shows better gain of 80.6db, gbw of 3mhz and low power consumption of 54 uw.
operational_amplifier	we propose an analog front-end integrated circuit (ic) design for a readout ic (roic), which applies a fixed-voltage-bias sensing method with a capacitance transimpedance amplifier (ctia) to an input stage in order to simplify the circuit structure of the roic and the ir sensor characteristic control. for a sample-and-hold stage, in order to display and control a signal detected by the ir sensor using a 2-d focal plane array, a differential delta sampling circuit is proposed, which effectively removes the fixed pattern noise. in addition, a two-stage variable-gain amplifier equipped with a rail-to-rail fully differential operational amplifier is applied to the roic to achieve high voltage sensitivity. the output characteristic of the proposed device is 30.84 mv/k and the linearity error rate is less than 0.07%. after checking the performance of the roic using an hspice simulation, the chip is manufactured and measured using the united microelectronics corporation japan 0.35-mu m standard cmos process to confirm that the simulation results from the actual design are in close agreement with the measurement results.
operational_amplifier	this paper presents a new memristor emulator circuit. the circuit is built around the current-feedback operational-amplifier and exploits the nonlinear transfer characteristic of the operational transconductance amplifier to provide continuous change in the memresistance. this is in contrast with many of the available memristor emulators which can provide only binary memresistance levels. moreover, the emulator enjoys a low impedance input and is suitable for current driving rather than voltage driving. the functionality of the proposed memristor emulator is confirmed by implementing a well-known reactance-less multivibrator circuit. experimentally obtained results are included. (c) 2014 elsevier gmbh. all rights reserved.
operational_amplifier	a systematic method is proposed to approach complete fault coverage for catastrophic faults in analog circuits. in the proposed method, a fault propagation graph is first created from the circuit netlist. standard graph theory techniques are then employed to identify a minimal set of observation points (mop) such that, by monitoring these points, complete fault coverage can be achieved, i.e., all potential catastrophic faults of the circuit can be detected theoretically. the developed method is valuable because of the increasingly critical quality requirements for modern ic applications and the lack of existing methods that can achieve sub-ppm test escapes in the state-of-the-art. a widely used benchmark circuit, a cmos operational amplifier, is utilized to demonstrate and validate the method. simulation results show that all catastrophic faults can be detected by monitoring the identified mop.
operational_amplifier	we have observed a quench of lasing at an exceptional point in an electronic circuit system by applying an asymmetric gain in coupled oscillators. because of the analogy between an oscillation in a laser and an oscillation in an operational amplifier, the oscillation stops when the system hits an exceptional point. this phenomenon is also theoretically investigated.
analog_signal_processing	with an increasing number of wind turbines being erected offshore, there is a need for cost-effective, predictive, and proactive maintenance. a large fraction of wind turbine downtime is due to bearing failures, particularly in the generator and gearbox. one way of assessing impending problems is to install vibration sensors in key positions on these subassemblies. such equipment can be costly and requires sophisticated software for analysis of the data. an alternative approach, which does not require extra sensors, is investigated in this paper. this involves monitoring the power output of a variable-speed wind turbine generator and processing the data using a wavelet in order to extract the strength of particular frequency components, characteristic of faults. this has been done for doubly fed induction generators (dfigs), commonly used in modern variable-speed wind turbines. the technique is first validated on a test rig under controlled fault conditions and then is applied to two operational wind turbine dfigs where generator shaft misalignment was detected. for one of these turbines, the technique detected a problem 3 months before a bearing failure was recorded.
analog_signal_processing	a procedure based on the multiple expansion of the brain electrical generator is used here to derive analytical expressions for the transfer matrix necessary to obtain potentials referenced to infinity. its features include: avoidance of computations that involve a large number of discrete dipole sources; faster evaluation compared to the use of the dipole layer; and a transparency showing the parameters that constitute the transfer matrix. the paper also proposes the construction of the standardization matrix without the use of the general inverse of a non-symmetrical matrix.
analog_signal_processing	piezoelectric materials have long been used as sensors and actuators; however their use as electrical generators is less established. such materials are capable of converting mechanical vibration energy into electrical energy, but developing piezoelectric generators is challenging because of their poor source characteristics (high voltage, low current, high impedance) and relatively low power output. this paper presents a theoretical analysis and design of piezoelectric power generation in low frequency applications (2 to 5 hz), where the design method is obtained by a piezoelectric equivalent circuit and an analysis of internal stress. some important considerations in designing such generators are explored, including of parameter estimation, load matching, efficiency, energy conversion, and energy storage. a prototype hand-shaking piezoelectric generator is proposed, which can be used to drive a wireless-switch device (the driving voltage being 5v). experimental results show that the average electrical power in the hand-shaking piezoelectric generator can be approximated to 48.5 mu w.
analog_signal_processing	the 4 pi beta(ls)-gamma coincidence counting system of kriss (korea research institute of standards and science) has been operated in the analog signal-processing mode. we develop and implement a digital sampling method using a commercially available analog to digital converter. in the implementation, we digitize the amplified pulse trains generated from three detector channels and determine the locations and amplitudes of peaks using a peak detection algorithm. we develop offline analysis routines to process the recorded locations and amplitudes for activity determinations by means of the efficiency-extrapolation method. in the activity determinations we can make corrections for the effects of out-of-channel events and correlation between data points. we validate the digital sampling method by measuring co-60 activity and comparing the measurement results with those obtained in the analog mode. the digital method is capable of facilitating the activity standardization of radionuclides with very short half-lives. (c) 2010 elsevier b.v. all rights reserved.
analog_signal_processing	in this paper the analysis of the dynamic behavior of a variable speed wind turbine (vswt) is presented. comparison between variable speed and constant speed (cs) mode of operation is also performed the examined wind turbine is equipped with a dual pwm voltage source converter cascade and connected to an electrical network represented by the thevenin equivalent. conventional scaler control is applied to the electrical generator, employing the constant v/f principle. furthermore, the grid side inverter is assumed capable of regulating its output reactive power in order to maintain the voltage within permissible variations. the objective is to point out some of the advantages of the variable speed (vs) operation, such as the improved output power quality, the decrease of the mechanical stresses and the capability of controlling the voltage of the local grid.
state_space_representation	one- and two-dimensional mems scanning mirrors for resonant or quasi-stationary beam deflection are primarily known as tiny micromirror devices with aperture sizes up to a few millimeters and usually address low power applications in high volume markets, e.g. laser beam scanning pico-projectors or gesture recognition systems. in contrast, recently reported vacuum packaged mems scanners feature mirror diameters up to 20 mm and integrated high-reflectivity dielectric coatings. these mirrors enable mems based scanning for applications that require large apertures due to optical constraints like 3d sensing or microscopy as well as for high power laser applications like laser phosphor displays, automotive lighting and displays, 3d printing and general laser material processing. this work presents modelling, control design and experimental characterization of gimbal-less mems mirrors with large aperture size. as an example a resonant biaxial quadpod scanner with 7 mm mirror diameter and four integrated pzt (lead zirconate titanate) actuators is analyzed. the finite element method (fem) model developed and computed in comsol multiphysics (r) is used for calculating the eigenmodes of the mirror as well as for extracting a high order (n >10000) state space representation of the mirror dynamics with actuation voltages as system inputs and scanner displacement as system output. by applying model order reduction techniques using matlab (r) a compact state space system approximation of order n = 6 is computed. based on this reduced order model feedforward control inputs for different, properly chosen scanner displacement trajectories are derived and tested using the original fem model as well as the micromirror.
state_space_representation	based on dynamical modeling, robust trajectory tracking control of a spherical mobile robot is proposed. the spherical robot is composed of a spherical shell and three independent rotors which act as the inner driver mechanism. owing to rolling without slipping assumption, the robot is subjected to two nonholonomic constraints. the state space representation of the system is developed using dynamical equations of the robot 's motion. as the main contribution, a dynamical model based smc (sliding mode controller) is designed for position control of the robot under parameters uncertainty and unmodeled dynamics. to decrease the chattering phenomena originated by the sign function, the well-known boundary layer technique is imposed on the smc. the control gains are determined through using lyapanov 's direct method in such a way that the robustness and to zero convergence of the controller 's tracking error are guaranteed. wide range computer simulations are performed to show the significant tracking performance of the proposed smc in particular against parameters uncertainty and white gaussian noises. the simulation results show the significant performance of the designed nonlinear control system in trajectory tracking control of the spherical robot even in the presence of parameters uncertainty and unmodeled dynamics.
state_space_representation	we propose to solve a black-oil reservoir optimal control problem with the direct multiple shooting method (ms). ms allows for parallelization of the simulation time and the handling of output constraints. however, it requires continuity constraints on state variables to couple simulation intervals. the black-oil fluid model, considering volatile oil or wet gas, requires a change of primary variables for simulation. this is a consequence of the absence of a fluid phase due to dissolution or vaporization. therefore, reservoir simulators parametrize the states with an augmented vector and select primary variables accordingly. however, the augmented state vector and the corresponding change of primary variables are not suitable for the application of ms because the optimization problem formulation must change according to the change of variables. thus, we propose a minimal state-space variable representation that prevents this shortcoming. we show that there is a bijective mapping between the proposed state-space representation and the augmented state-space. the minimal representation is used for optimization and the augmented representation for simulation, thereby keeping the simulator implementation unchanged. therefore, the proposed solution is not invasive. finally, the application of the method is exemplified with benchmark cases involving live oil or wet gas. both examples emphasize the requirement of output constraints which are efficiently dealt with the ms method. (c) 2016 elsevier b.v. all rights reserved.
state_space_representation	this paper describes the development of a simulation model in state space representation for a class of microwave heating model. under the assumptions on thermodynamics mechanism, one-dimensional heating model in rectangle waveguide is derived by combining lambert 's law and heat conduction equation. through introducing the spatial differential operator in inhomogeneous thermal conduction equation, one-dimensional heating model in form of partial differential equations (pde) can be divided into finite-dimensional slow one and infinite-dimensional complement, which provides a application for galerkin 's method to transform the pde into state space form. the simulation of state space model has shown that the trend and distribution of temperature are in good agreement with that of traditional numerical model.
state_space_representation	many global identification approaches described in the literature for estimating linear parameter-varying (lpv) discrete-tin time state-space (ss) models with affine dependence on the scheduling parameter suffer heavily from the curse of dimensionality, making identification of moderate sized systems computationally intensive or infeasible. in this paper, we present a novel two-step approach to estimate lpv-ss models based on a single data set with varying scheduling signal by combining, 1) lpv correlation analysis, and 2) a deterministic lpv realization scheme. step 1 includes the estimation of the sub-markov parameters of the system using correlation analysis of the involved signals. subsequently, for step 2, this paper presents a novel basis reduced exact ho-kalman like realization scheme, which uses only sub parts of the extended hankel matrix. therefore, the computational complexity is significantly reduced compared to the full scheme. to demonstrate that the basis reduction does not, lead to a loss in performance, a simulation study is provided. (c) 2015, ifac (international federation of automatic control) hosting by elsevier ltd. all rights reserved.
signal-flow_graph	there are various existing solution for solving the xor problem in artificial neural network. even the author in his previous work has proposed several solutions to the problem. in this paper we will see a new solution to the xor problem. the author has given a brief introduction to the artificial neural network concept. discussion on linear separablility and non-linear separability is made. fixation of the non-linearly separable problem is made. a solution to the non-linearly separable problem i.e. xor problem is proposed. architectural graph and signal flow graph proposing the final solution to the problem is given. mathematical explanation to the solution is given.
signal-flow_graph	this paper introduces a systematic methodology for applying mason 's signal flow graph to the analysis of feedback circuits. we demonstrate that the relationships among signals of feedback circuits can be represented in a intuitive and graphical manner utilizing signal flow graphs. once constructed, the desired solutions can be achieved by simple inspection of the signal flow graph. the thevenin and norton equivalent circuits are utilized to model the loading effects of the feedback network in the example circuits presented.
signal-flow_graph	a new adaptive filtering algorithm for time-series data based on the qrd inverse-updates method of pan and plemmons is derived from first principles. in common with other fast algorithms for time-series adaptive filtering, this algorithm only requires o(p) operations for the solution of a pth-order problem. unlike previous fast algorithms based on the qrd technique, the algorithm presented here explicitly produces the transversal filter weights. furthermore the derivation of the algorithm is achieved, quite simply, by means of signal-flow-graph manipulation. the relationship between this fast qrd inverse-updates algorithm and the ftf algorithm is briefly discussed. the results of some preliminary computer simulations of the algorithm, using finite-precision floating-point arithmetic, are presented.
signal-flow_graph	a systematic method for designing log-domain wave filters is presented. wave filters simulate topologically and functionally passive double terminated lc ladder prototype filters of low sensitivity. the design in the log-domain is based on a transposition of the signal flow graph (sfg) that corresponds to the wave equivalent of elementary two-port blocks in the linear domain, to the corresponding log-domain sfg. this is achieved by using an appropriate set of complementary operators, in order to preserve the linear operation of the whole circuit. simulation results of a fifth-order low-pass and a fourth-order bandpass log-domain wave filter are given, using hspice. the proposed circuits are suitable for low-voltage operation and in high-frequency applications.
signal-flow_graph	a study of critical information (maximum current and voltage estimation for the used components, mean values for semiconductor parts and an assessment of expected power losses) for a half-bridge bidirectional buck boost dc-dc converter equipped with an lc filter is carried out. a practical model from this converter is derived and a transfer function from the duty cycle and from the voltages (input to the output) is calculated via a linearization method. a signal flow graph of the converter can be obtained. to decrease the power losses, the concept of zero voltage switching is used and explained.
electrical_circuits	this paper investigates feasibility of developing a predictive rating method to optimize line rating of underground transmission lines based on weather forecast information. transmission lines, overhead or underground, are essential and indispensable parts of the electric grid which basically transfer energy from the production point to where it is needed. line rating defined as line 's maximum capacity to transfer electric current and power safely and reliably under certain constraints and criteria. to ensure safe operation, rating has been classically calculated according to the worst case scenario, where conductor 's temperature rise would remain within specified limit under most unfavorable conditions. obviously, such an approach leads to very conservative results, leaving line mostly underutilized throughout its life span. in order to optimize line utilization, ambient adjusted rating and more recently dynamic line rating methods are developed. for instance, in dynamic line rating, real-time data are used to determine instantaneous line rating. we have investigated necessity and possibility of developing a predictive model for underground transmission lines by employing weather forecast information, which would enable line operators or owners to anticipate optimized line ampacity and maximum rating over the next few days. the proposed basic model builds upon the already developed and well-documented analogy between thermal and electrical circuits, yet incorporates a time varying source to account for constantly changing ambient temperature. deterministic weather forecast information can be collected from environment canada.
electrical_circuits	knowledge representation plays a very important role for designing knowledge base systems as well as intelligent systems. nowadays, there are many effective methods for representing such as: semantic network, rule-base systems, computational network. computational objects knowledge base (cokb) can be used to represent the total knowledge and design the knowledge base of systems. in fact, a popular form of knowledge domain is knowledge about operations and computational relations, especially computational knowledge domain, such as: linear algebra, analytic geometry. however, cokb model and the other models have not solved yet some problems about operators: specification of operator, properties of operator, reducing an expression. in this paper, we will present a reducing model of cokb. this model, called ops-model, represents knowledge about operators between objects and solve some problems related to these operators. through that, the algorithms for designing inference engine of model have been built up. moreover, ops-model has been applied to specify a part of knowledge domain about direct current (dc) electrical circuits and construct a program for solving some problems on this knowledge domain.
electrical_circuits	this paper presents an alternative to existing techniques based on nodal analysis designed specifically to suit implementation in matrix-oriented programming languages, keeping the simplicity of original nodal analysis and bringing improvements to make virtually all circuit topologies solvable through it, dealing with potential singularities, hence the name singular nodal analysis (sna). two variants of this novel algorithm are described. their validity is proven and their applicability is shown through two practical examples.
electrical_circuits	we developed a simple method to prepare hybrid copper-silver conductive tracks under flash light sintering. the developed metal nanoparticle-based ink is convenient because its preparation process is, free of any tedious washing steps. the inks were composed of commercially available copper nanoparticles which were mixed with formic acid, silver nitrate, and diethylene glycol. the role of formic acid is to remove the native copper oxide layer on the surface of the copper nanoparticles. in this way, it facilitates the formation of a silver outer shell on the surface of the copper nanoparticles through a galvanic replacement. in the presence of formic acid, the copper nanoparticles formed copper formate, which was present in the unsintered tracks. however, under illumination by a xenon flash light, the copper formate was then converted to copper. moreover, the resistance of the copper-only films increased by 6 orders of magnitude when oxidized at high temperatures (similar to 220 degrees c). however, addition of silver nitrate to the inks suppressed the oxidation of the hybrid copper-silver films, and the resistance changes in these inks at high temperatures were greatly reduced. in addition, the hybrid inks proved to be advantageous for use in electrical circuits as they demonstrated a stable electrical conductivity after exposure to ambient air at 180 degrees c.
electrical_circuits	in this paper a novel set of indicators is presented for arc faults detection in electrical circuits. the indicators are defined starting from an experimental characterization of the arc fault phenomenon and the study of the arcing current in several test conditions, which were chosen in accordance with the ul 1699 standard requirements. the proposed parameters are measured by means of a high resolution low frequency spectral analysis of the arcing current, which allows to achieve a good spectral resolution even with short observation windows.
lorentz_force_law	in this paper we will consider the nature of electrodynamics in an absolute euclidean space-time. consequently, we will also consider the nature of dynamics in general. rn many respects the situation is similar to that in (electro)dynamics in the theory of relativity. in other respects there are some pronounced differences. they lead, for instance, to new kinds of modifications of the lorentz force law as well as coulomb 's law. we will show how easily the null result of trouton 's experiment can be understood by means of this new kind of electrodynamics. in addition, we will investigate the consequences of the reversal of proper time. it turns out that our theory leads to the prediction of the possible existence of particles with new electromagnetic characteristics. finally, we will make a few remarks about the concept of an internal space in an absolute euclidean space-time.
lorentz_force_law	the lagrangian and hamiltonian formulations of electromagnetism are reviewed and the maxwell equations are obtained from the hamiltonian for a system of many electric charges. it is shown that three of the equations which were obtained from the hamiltonian, namely the lorentz force law and two maxwell equations, can be obtained as well from a set of postulated poisson brackets. it is shown how the results derived from these brackets can be used to reconstruct the original lagrangian for the theory aided by some reasoning based on physical concepts.
lorentz_force_law	this letter discusses the failure of the lorentz force law to locate the recoil action in railguns. ampere 's force law makes predictions which agree with experiment.
lorentz_force_law	since the eddy current problem usually. depends on the geometry of the moving conductive sheet and pole shape, there is no general method to find out its analytical solution. the analyses of the eddy current in a rotating disk with an electromagnet and one 's extended array are performed in the case of time-invariant field to find their approximate analytical solution. as a method to solve the eddy current problem, firstly, the concept of the coulomb 's law and the mapping technique are introduced with the consideration of the boundary condition. secondly; the induced magnetic flux density is calculated in a lumped way by using ampere 's law. thirdly, the net magnetic flux density is defined by introducing the magnetic reynolds number r-m which is the ratio of the induced magnetic flux density to the applied magnetic flux density from the electromagnet. fourthly, the analyses are extended to the system with the extended array of two electromagnets. finally, the braking torque is calculated by applying lorentz force law and the computed result is compared with the experimental one.
lorentz_force_law	this paper presents an analytical model for the force and torque developed by a reaction sphere actuator for satellite attitude control. the reaction sphere is an innovative momentum exchange device consisting of a magnetic bearings spherical rotor that can be electronically accelerated in any direction making all the three axes of stabilized spacecrafts controllable by a unique device. the spherical actuator is composed of an 8-pole permanent magnet spherical rotor and of a 20-coil stator. force and torque analytical models are derived by solving the laplace equation and applying the lorentz force law. the novelty consists in exploiting powerful properties of spherical harmonic functions under rotation to derive closed-form linear expressions of forces and torques for all possible orientations of the rotor. specifically, the orientation of the rotor is parametrized using seven decomposition coefficients that can be determined noniteratively and in a linear fashion by measuring the radial component of the magnetic flux density from at least seven different locations. therefore, force and torque models for all possible orientations of the rotor are expressed in closed form as linear combination of mutually orthogonal force and torque characteristic matrices, which are computed offline. the proposed analytical models are experimentally validated using a developed laboratory prototype.
system_identification	model-based fault diagnosis has attracted considerable attention from researchers and developers of flight control systems, thanks to its hardware simplicity and cost-effectiveness. however, the airplane model, which is adopted commonly in fault diagnosis, only exists theoretically and is linearized in approximation. for this reason, uncertainties such as system non-linearity and subjectivity will degrade the fault diagnosis results. in this paper, we propose a novel actuator fault diagnosis scheme for flight control systems based on model identification techniques. with this scheme, system identification can be achieved with a linear model that uses a closed-loop subspace model identification algorithm, and a non-linear model that uses an extended state observer and neural networks. on this basis, the current actuator fault is estimated using an adaptive two-stage kalman filter. finally, the non-linear six-degree-of-freedom model of a b747 airplane is simulated in the matlab/simulink environment, where the effectiveness of the proposed scheme is verified from fault diagnosis tests.
system_identification	an optimal controller for high-precision spiral positioning of a piezoelectric tube scanner used in an atomic force microscope (afm) is proposed in this paper. in the proposed control scheme, a second-order vibration compensator is incorporated with the piezoelectric tube scanner (pts) to suppress the vibration of the pts at the resonant frequency. an internal model of a reference sinusoidal signal is included with the augmented plant model and an integrator is introduced with a linear quadratic gaussian controller which reduces the phase error between the input and output sinusoids. the proposed method allows a commercial afm to scan at high scanning speeds as an alternative to the raster scanning approach. the performance of this controller is assessed with closed-loop frequency response, tracking accuracy, and a set of spiral scanned images. the raster scanned images obtained using the standard afm pi controller is also presented for comparison with the spiral images. experimental results prove the effectiveness of the proposed method. note to practitioner-the main motivation for this work is to propose a control system for high-speed imaging using an afm. the scanning speed of a commercial afm is limited by the positioning accuracy of the scanner. the scanner is made of a piezoelectric tube actuator. the commercially available scanners have the following problems: 1) creep effect at slow speed scanning; 2) hysteresis effect during large range scanning (these two effects result in inaccurate reference motion tracking); and 3) vibration effect at high frequencies due to its mechanical flexibility. to overcome these limitations, an internal reference model-based linear quadratic gaussian control approach is designed and experimentally implemented using the dspace ds-1103 real-time prototype system. before the control design, the scanner is modelled (experimentally) by using a system identification method, and a vibration compensator is incorporated with the identified model. apart from the control design, a spiral motion of the scanner is considered instead of a well-known raster scanning which is adopted in all commercial afms. the designed controller is applied in the piezoelectric tube scanner experimentally, and the obtained results are presented in terms of tracking accuracy, damping of scanners vibration, and generating high-quality images as compared with the existing commercially available methods. the existing commercial system can generate a sensible image up to 30 hz scanning frequency but, using the proposed method, a high-quality image up to 120 hz is achieved. with a slight software modification, the proposed improvement can be used in commercial afms.
system_identification	output-only system identification is a very attractive technique for its implementation simplicity. however, it requires long records to validate the white-noise assumption of the excitation, mainly under transient forced vibration. alternatively, free-vibration record segments can be selected before the identification process. this improves the accuracy, even using less data, but it requires human intervention or input recording. in the present paper, an approach is proposed for accurate system identification from short output-only records of vibration induced by transient excitation, without human intervention. the approach is based on a novel heuristic search algorithm to find free-vibration record segments, which is fully automatic and it handles the possibility of free-vibration absence. tests with real-life data from structural health monitoring (shm) of a bridge showed that the free-vibration finding improves the accuracy of the modal parameter estimates up to ten times, as compared to using record segments starting at the response peak. the proposed approach drastically reduces the need to transmit large amounts of data, which impacts on hardware requirements of shm implementations. (c) 2016 elsevier ltd. all rights reserved.
system_identification	a novel normalized least mean square (nlms)-based algorithm is proposed to locate the active taps of the unknown impulse response in the application of network echo cancellation. starting with the full length of the unknown echo path, the start-tap and end-tap estimates of the active region adapt, respectively, in the form of fractional tap. the effective tap-length estimate is defined by the length between the start-tap and end-tap estimates. the filter tap-weight vector is then updated with this effective tap-length estimate following the nlms algorithm, while the tap weights located beyond the effective tap-length estimate are directly set to zero. in case that the active region of the unknown echo path shifts in a large scale, the start-tap and end-tap estimates are reset to the full length when the effective tap-length estimate is lower than a predetermined threshold value. simulation results demonstrate that the proposed fractional tap-based algorithm can efficiently locate and track the effective tap length of the unknown echo path when the active coefficients change abruptly in locations and magnitudes, outperforming the existing stochastic tap-based schemes.
system_identification	the tuning method for a controller suggested by yunwana and seborg (1982) could not be applied to a system with time delay due to phase error by pade' approximiation. in this paper theoretically analyzes the phase error to find that phase error by pade' approximation and time delay are the major causes and suggests a novel optimal tuning method to solve those problems through configuration of regulators by means of bilinear transformation based on a previous study suh (1984).
pid_controller	in this paper performance and stability analysis of lcl resonant converter is analyzed in presence of the source inductance. most of the researchers do not consider the source inductance value; they assumed it to be ideal. but in reality it has some negative impact over the systems stability and output of the system. resonant converter has wide applications in telecommunication, aerospace, radar power applications and power supplies. there are three topologies of rc 's like lcc, lcl and lcl-t rc. on comparing these three configurations of rc, the lcl rc has good stability region. stability analysis has been done by lyapunov stability analysis, root locus analysis and nyquist plot analysis. open loop response and closed loop response of pid controller is compared with and without source inductance and concluded that pid has better response than open loop system. the parameters like rise time, settling time and overshoot are taken in to account. the result of this paper proves that, on including source inductance, the rise time, settling time and overshoot becomes higher. the open loop system has 80% of increased oscillations and settling time. but on using pid controller the oscillations has been reduced to 50% and settling time also reduced. thus pid controller has better response than open loop system. these results are simulated using matlab/simulink software package.
pid_controller	this paper deals with the design of a controller possessing tracking capability of any realisable reference trajectory while rejecting measurement noise. we consider discrete-time-varying multi-input multi-output stable linear systems and a proportional-integral-derivative (pid) controller. a novel recursive algorithm estimating the time-varying pid gains is proposed. the development of the proposed algorithm is based on minimising a stochastic performance index. the implementation of the proposed algorithm is described and boundedness of trajectories and convergence characteristics are presented for a discretised continuous-time model. simulation results are included to illustrate the performance capabilities of the proposed algorithm.
pid_controller	multilevel inverters are nowadays widely used in high-power and high-voltage applications. a multilevel inverter synthesizes a large number of levels to get the desired output voltage levels and they have lot of merits such as improved output waveform, smaller passive filter size, lower electro magnetic interference and reduced harmonics. however, multilevel inverters also have some disadvantages such as increased number of components, voltage-balancing problem and higher switching losses. this paper presents a pi and pid controlled cascaded h-bridge eleven level inverter based sinusoidal pulse width modulation control technique suitable for improved power quality applications. the main objective of reducing the thd of output of the chosen eleven level cascaded inverter under set point tracking as well as steady-state with fast transient response are proposed from control point of view. simulation results have been discussed that the cascaded h-bridge eleven level inverter performs perfectly in connection with pi or pid. a comparative analysis of these two different controllers is revealed. harmonic spectrum and output voltage and current waveforms have been obtained to validate the role of controllers. experimental results are presented to confirm the simulation results.
pid_controller	nowadays, electronic throttle control system is widely adapted in the motorcycle for better drivability, fuel economy and reduces the emissions. in such systems, pedal follower or torque based approach are used for calculating the required throttle angle for the given torque demand by driver. this work presents a throttle control system for the precise estimation of throttle angle based on the integrated pedal follower and torque based approach for the given accelerator position and torque demand by the driver. a mathematical model for an electronic throttle body is developed to understand the effects of nonlinearities due to friction and limp home dual springs. a pid controller with compensators are developed to handle the nonlinearities due to the friction and limp home dual springs in the proposed electronic throttle control system. a simulation study has been carried out using software in loop and hardware in loop simulation approaches for step, sinusoidal, and ramp input signals. the responses of electronic throttle body for opening the throttle angle and error are analyzed for the given input signals. the simulation result shows that the proposed compensators has significant advantage in reducing the throttle angle error and gives the desired output.
pid_controller	in this paper, a two-axis compensation device was designed to eliminate the impact of the attitude deviations of the moving platform on the laser point cloud. in order to improve the dynamic performance of the compensation device, the fuzzy proportional-integral differential (pid) controller was employed for the compensation device. then, a semi-physical simulation experiment was carried out to quantitatively evaluate the dynamic performance of the compensation device using a three-axis turntable and a high-accuracy inertial measurement unit (imu), which was used to measure the attitude deviations of the three-axis turntable in real time. as a result, the roll and pitch deviations of turntable measured by imu can be compensated using the two orthogonal frameworks of the compensation device controlled by fuzzy pid controller. the experimental results showed that the fuzzy pid controller compensated the attitude deviations more accurately than the traditional pid controller and the laser pointing direction could be remained within 0.2 degrees using the compensation device.
voltage_law	a contact-free method to obtain the current-voltage characteristics (cvc) of hard superconductors by measuring the relaxation of the magnetization in a perpendicular magnetic field is developed. the relaxation curves obtained for melt-textured ybco samples are well fitted by curves calculated within the electrodynamic model using a power-law cvc. this procedure uses only two fitting parameters, namely, the critical sheet current j(c) and the exponent tit of the power-law cvc. (c) 2006 elsevier ltd. all rights reserved.
voltage_law	given a nonlinear infinite resistive network, an operating point can be determined by approximating the network by finite networks obtained by shorting together various infinite sets of nodes, and then taking a limit of the nodal potential functions of the finite networks. initially, by taking a completion of the node set of the infinite network under a metric given by the resistances, limit points are obtained that represent generalized ends, which we call ""terminals,"" of the infinite network. these terminals can be shorted together to obtain a generalized kind of node, a special case of a 1-node. an operating point will involve kirchhoff 's current law holding at 1-nodes, and so the ow of current into these terminals is studied. we give existence and bounds for an operating point that also has a nodal potential function, which is continuous at the 1-nodes. the existence is derived from the said approximations.
voltage_law	in this paper, based on the equivalent single diode circuit model of the solar cell, an equivalent circuit diagram for two serial solar cells is drawn. its equations of current and voltage are derived from kirchhoff 's current and voltage law. first, parameters are obtained from the i-v (current-voltage) curves for typical monocrystalline silicon solar cells (125 mm x 125 mm). then, by regarding photo-generated current, shunt resistance, serial resistance of the first solar cell, and resistance load as the variables. the properties of shunt currents (i-sh1 and i-sh2), diode currents (i-d1 and i-d2), and load current (i-l) for the whole two serial solar cells are numerically analyzed in these four cases for the first time, and the corresponding physical explanations are made. we find that these parameters have different influences on the internal currents of solar cells. our results will provide a reference for developing higher efficiency solar cell module and contribute to the better understanding of the reason of efficiency loss of solar cell module.
voltage_law	it is well known that optimizing network topology by switching on and off transmission lines improves the efficiency of power delivery in electrical networks. in fact, the usa energy policy act of 2005 (section 1223) states that the united states should ""encourage, as appropriate, the deployment of advanced transmission technologies"" including ""optimized transmission line configurations."" as such, many authors have studied the problem of determining an optimal set of transmission lines to switch off to minimize the cost of meeting a given power demand under the direct current (dc) model of power flow. this problem is known in the literature as the direct-current optimal transmission switching problem (dc-ots). most research on dc-ots has focused on heuristic algorithms for generating quality solutions or on the application of dc-ots to crucial operational and strategic problems such as contingency correction, real-time dispatch, and transmission expansion. the mathematical theory of the dc-ots problem is less well developed. in this work, we formally establish that dc-ots is np-hard, even if the power network is a series-parallel graph with at most one load/demand pair. inspired by kirchoff 's voltage law, we provide a cycle-based formulation for dc-ots, and we use the new formulation to build a cycle-induced relaxation. we characterize the convex hull of the cycle-induced relaxation; this characterization provides strong valid inequalities that can be used in a cutting-plane approach to solve the dc-ots. we give details of a practical implementation, and we show promising computational results on standard benchmark instances.
voltage_law	in this work a classical derivation of fractional order circuits models is presented. generalised constitutive equations in terms of fractional riemann-liouville derivatives are introduced in the maxwell 's equations for each circuit element. next the kirchhoff voltage law is applied in a rcl circuit configuration. it is shown that from basic properties of fractional calculus, a fractional differential equation model with caputo derivatives is obtained. thus standard initial conditions apply. finally, models for bioimpedance are revisited. (c) 2016 elsevier b.v. all rights reserved.
control_engineering	virtual and remote laboratories that are commonly used in engineering education are mainly designed using java programming language. the most popular and widely used learning management system is moodle. this paper describes the development of a virtual laboratory using easy java simulations software tool and its integration with e-learning tool moodle. the virtual laboratory demonstrates how easy java simulations and moodle are used in optimal control theory education by the example of the well-known brachistochrone problem solution.
control_engineering	we consider the problem of estimating an arbitrary dynamical parameter of an open quantum system in the input-output formalism. for irreducible markov processes, we show that in the limit of large times the system-output state can be approximated by a quantum gaussian state whose mean is proportional to the unknown parameter. this approximation holds locally in a neighbourhood of size t(-1/2) in the parameter space, and provides an explicit expression of the asymptotic quantum fisher information in terms of the markov generator. furthermore we show that additive statistics of the counting and homodyne measurements also satisfy local asymptotic normality and we compute the corresponding classical fisher informations. the general theory is illustrated with the examples of a two-level system and the atom maser. our results contribute towards a better understanding of the statistical and probabilistic properties of the output process, with relevance for quantum control engineering, and the theory of non-equilibrium quantum open systems.
control_engineering	dynamic loads of high capacity power grids under normal and severe conditions are the most concerns of system operators and dispatchers to maintain voltage stability. reactive power (q) are often drawn by loads from various generated sources disturbing the operational power factor. compensating methods are used, depending on load types and their reactive power patterns, to continuously monitor and correctly counteract fast changes by extremely rapid var compensation especially during transient conditions. using power electronics and combining both, power and control engineering, some facts devises equipped with selected power electronic switching are the appropriate methods to respond to system variations in almost no time. statcom regulates system voltages based on a voltage-source converters, by absorbing or generating reactive power. contrary to a thyristor-based static var compensator (svc), statcom output current (inductive or capacitive) can be controlled independent of the system voltage. this paper presents a design of voltage controller to analyze and validate the dynamic behavior of facts device (statcom)in riding through severe transient imbalance using a powerful computer simulation application pscad emtdc. the statcom was modeled in 3-phase network with various parameters in a steady state condition and during a short interval of system fault. the regulator provided the ride through during severe transient while maintaining sufficient bandwidth to perform necessary compensations. the controller behavior is simulated in distribution system model while it was subjected to severe three phases to ground fault with rapidly varying three phase load.
control_engineering	this paper introduced the improvement of the original control system of process control laboratory. the development of the liquid level, temperature and flow time-sharing control experiment platform adopting profibus-dp fieldbus technology of siemens. the newly designed control system using s7-300plc as the master control unit, s7-224 as the slave control unit, pc and touch screen as the upper computer is able to achieve real-time monitoring on-the-spot operation, adjust control parameters, centralized management and provide convenience for experimenters. the practical operating results show that the platform not only extends the function of equipment and improves the utilization rate of equipment, but also help students understand and master the advanced control technologies, such as the technology application of plc and touch screen, the process control engineering and the research of control algorithms.
control_engineering	continuum manipulator compliance enables operation in delicate environments while challenging the actuation and control approaches. in the case of a catheter ablation of atrial fibrillation, the compliance of the continuum backbone lends an inherent safety to the device. this inherent safety frustrates attempts at precise, accurate, and fast control, limiting the continuum devices to simple and static positioning tasks. this paper develops interleaved continuum-rigid manipulation, by which the hysteretic nonlinearities encountered in tendon-actuated continuum manipulators are compensated by the discrete rigid joints located between the continuum sections. the rigid joints introduce an actuation redundancy, which an interleaved controller may use to avoid the continuum nonlinearities and dynamic excitations, or to prefer particular configurations that may improve task accuracy, permit greater end-effector forces, or avoid environment obstacles. two experimental systems explore the potential of these joints to: 1) increase the manipulator 's dexterous workspace, and 2) correct for actuation nonlinearities and enhance manipulator performance. these experiments also expose important design and control observations that were not apparent in the general robotic and continuum literature.
